<html>
<head>
<meta charset="utf-8">
<title> SSD - Single Shot MultiBox Detector </title>
<style type="text/css">
    body {font-family: arial,verdana,geneva,sans-serif; font-size: 112%; color: #000; background-color: #eee;}
    .chapter_part {margin: 50px auto;
        background-color: #fefefe;
        padding:10px 10px 0;
        box-shadow: 0 0 0 20px #f8f8f8;
        border-radius:25px;
        -moz-border-radius:25px; /* Old Firefox */
    }
    .panel{ position: fixed;  top: 75%; }
@media screen and (min-width:1280px){
    .chapter_part {width: 64%; }
    .panel{margin-left: 6%;}
}
@media screen and (max-width:1280px){
    .chapter_part {width: 80%; }
    .panel{margin-left: 0;}
}
    .paragraph_part { margin: 0; text-indent:2em; }
    .panel-btn{
        position:relative; margin-bottom:.75rem;
        text-align: center; font-size:1.8em;
        color:#888; width:3rem; height:3rem;
        background-color:#fff;
        background-repeat:no-repeat;
        border:5px solid #ddd; border-radius:50%;
        box-shadow:0 2px 4px 0 rgba(0,0,0,.04);
        cursor:pointer;
    }
    .word_hot{ font-weight:bold; color:#ff4500; }
    .word_hot_rare{ font-weight:bold; color:#8fbc8f; }
    .eng {display:inline;}
    .eng.line {display:block;}
    .eng.para {display:inline;}
    .chs:before{content:"➤"; font-size:%25;}
    .chs.on:before{content:"➤"; font-size:%25;}
    .chs.off:before{content:"";}
</style>
</head>
<body>
<div class="chapter_part">
<div class="paragraph_part">
    <div class="eng"><span class="word_hot_rare">SSD</span>: Single Shot <span class="word_hot_rare">MultiBox</span> Detector<span class="chs" title="SSD：单次多盒检测器"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Abstract<span class="chs" title="摘要"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We present a method for detecting objects in images using a single deep neural network.<span class="chs" title="我们提出了一种使用单个深度神经网络来检测图像中的目标的方法。"><span></div>
    <div class="eng">Our approach, named <span class="word_hot_rare">SSD</span>, <span class="word_hot_rare">discretizes</span> the output space of <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes into a set of default boxes over different aspect ratios and scales per feature map location.<span class="chs" title="我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。"><span></div>
    <div class="eng">At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape.<span class="chs" title="在预测时，网络会在每个默认框中为每个目标类别的出现生成分数，并对框进行调整以更好地匹配目标形状。"><span></div>
    <div class="eng"><span class="word_hot" title="[ə'dɪʃənəlɪ]">Additionally</span>, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.<span class="chs" title="此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。"><span></div>
    <div class="eng"><span class="word_hot_rare">SSD</span> is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature <span class="word_hot" title="[re'sɑ:mplɪŋ]">resampling</span> stages and <span class="word_hot" title="[ɪnˈkæpsjuleɪt]">encapsulate</span>s all computation in a single network.<span class="chs" title="相对于需要目标提出的方法，SSD非常简单，因为它完全消除了提出生成和随后的像素或特征重新采样阶段，并将所有计算封装到单个网络中。"><span></div>
    <div class="eng">This makes <span class="word_hot_rare">SSD</span> easy to train and straightforward to integrate into systems that require a detection component.<span class="chs" title="这使得SSD易于训练和直接集成到需要检测组件的系统中。"><span></div>
    <div class="eng">Experimental results on the <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>, <span class="word_hot" title="['kəʊkəʊ]">COCO</span>, and <span class="word_hot_rare">ILSVRC</span> datasets confirm that <span class="word_hot_rare">SSD</span> has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 × 300 input, <span class="word_hot_rare">SSD</span> achieves 74.3% mAP on <span class="word_hot_rare">VOC</span>2007 test at 59 <span class="word_hot" title="['efp'i:'es]">FPS</span> on a <span class="word_hot" title="[ɪn'vɪdɪə]">Nvidia</span> <span class="word_hot" title="[ˈtaɪtn]">Titan</span> X and for 512 × 512 input, <span class="word_hot_rare">SSD</span> achieves 76.9% mAP, <span class="word_hot" title="[ˌaʊtpəˈfɔ:m]">outperform</span>ing a <span class="word_hot" title="[ˈkɒmpərəbl]">comparable</span> state-of-the-art Faster R-CNN model.<span class="chs" title="PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到74.3%的mAP，对于512×512的输入，SSD达到了76.9%的mAP，优于参照的最先进的Faster R-CNN模型。"><span></div>
    <div class="eng">Compared to other single stage methods, <span class="word_hot_rare">SSD</span> has much better accuracy even with a smaller input image size.<span class="chs" title="与其他单阶段方法相比，即使输入图像尺寸较小，SSD也具有更高的精度。"><span></div>
    <div class="eng">Code is available at: https://github.com/weiliu89/caffe/tree/ssd.<span class="chs" title="代码获取：https://github.com/weiliu89/caffe/tree/ssd。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">1. Introduction<span class="chs" title="1. 引言"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Current state-of-the-art object detection systems are <span class="word_hot" title="[ˈveəriənt]">variant</span>s of the following approach: <span class="word_hot" title="[haɪˈpɒθəsaɪz]">hypothesize</span> <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes, resample pixels or features for each box, and apply a high-quality classifier.<span class="chs" title="目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。"><span></div>
    <div class="eng">This pipeline has <span class="word_hot" title="[prɪˈveɪl]">prevail</span>ed on detection benchmarks since the <span class="word_hot" title="[sɪˈlektɪv]">Selective</span> Search work [1] through the current leading results on <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>, <span class="word_hot" title="['kəʊkəʊ]">COCO</span>, and <span class="word_hot_rare">ILSVRC</span> detection all based on Faster R-CNN[2] <span class="word_hot" title="[ˌɔ:lˈbi:ɪt]">albeit</span> with deeper features such as [3].<span class="chs" title="自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。"><span></div>
    <div class="eng">While accurate, these approaches have been too <span class="word_hot_rare">computationally</span> intensive for embedded systems and, even with high-end hardware, too slow for real-time applications.<span class="chs" title="尽管这些方法准确，但对于嵌入式系统而言，这些方法的计算量过大，即使是高端硬件，对于实时应用而言也太慢。"><span></div>
    <div class="eng">Often detection speed for these approaches is measured in seconds per frame (<span class="word_hot" title="[.es piː 'ef]">SPF</span>), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (<span class="word_hot" title="['efp'i:'es]">FPS</span>).<span class="chs" title="通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。"><span></div>
    <div class="eng">There have been many attempts to build faster detectors by attacking each stage of the detection pipeline (see related work in <span class="word_hot" title="[sek]">Sec</span>. 4), but so far, significantly increased speed comes only at the cost of significantly decreased detection accuracy.<span class="chs" title="已经有很多尝试通过处理检测流程中的每个阶段来构建更快的检测器（参见第4节中的相关工作），但是到目前为止，显著提高的速度仅以显著降低的检测精度为代价。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">This paper presents the first deep network based object detector that does not resample pixels or features for <span class="word_hot" title="[baundɪŋ]">bounding</span> box hypotheses and and is as accurate as approaches that do.<span class="chs" title="本文提出了第一个基于深度网络的目标检测器，它不对边界框假设的像素或特征进行重采样，并且与其它方法有一样精确度。"><span></div>
    <div class="eng">This results in a significant improvement in speed for high-accuracy detection (59 <span class="word_hot" title="['efp'i:'es]">FPS</span> with mAP 74.3% on <span class="word_hot_rare">VOC</span>2007 test, <span class="word_hot_rare">vs</span>. Faster R-CNN 7 <span class="word_hot" title="['efp'i:'es]">FPS</span> with mAP 73.2% or <span class="word_hot_rare">YOLO</span> 45 <span class="word_hot" title="['efp'i:'es]">FPS</span> with mAP 63.4%).<span class="chs" title="这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。"><span></div>
    <div class="eng">The fundamental improvement in speed comes from eliminating <span class="word_hot" title="[baundɪŋ]">bounding</span> box proposals and the subsequent pixel or feature <span class="word_hot" title="[re'sɑ:mplɪŋ]">resampling</span> stage.<span class="chs" title="速度的根本改进来自消除边界框提出和随后的像素或特征重采样阶段。"><span></div>
    <div class="eng">We are not the first to do this (<span class="word_hot_rare">cf</span> [4,5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts.<span class="chs" title="我们并不是第一个这样做的人（查阅[4,5]），但是通过增加一系列改进，我们设法比以前的尝试显著提高了准确性。"><span></div>
    <div class="eng">Our improvements include using a small convolutional filter to predict object categories and offsets in <span class="word_hot" title="[baundɪŋ]">bounding</span> box locations, using separate <span class="word_hot" title="[prɪˈdɪktə(r)]">predictor</span>s (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.<span class="chs" title="我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。"><span></div>
    <div class="eng">With these modifications——especially using multiple layers for prediction at different scales——we can achieve high-accuracy using relatively low resolution input, further increasing detection speed.<span class="chs" title="通过这些修改——特别是使用多层进行不同尺度的预测——我们可以使用相对较低的分辨率输入实现高精度，进一步提高检测速度。"><span></div>
    <div class="eng">While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span> from 63.4% mAP for <span class="word_hot_rare">YOLO</span> to 74.3% mAP for our <span class="word_hot_rare">SSD</span>.<span class="chs" title="虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。"><span></div>
    <div class="eng">This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on <span class="word_hot" title="[rɪˈzɪdjuəl]">residual</span> networks [3].<span class="chs" title="相比于最近备受瞩目的残差网络方面的工作[3]，在检测精度上这是相对更大的提高。"><span></div>
    <div class="eng">Furthermore, significantly improving the speed of high-quality detection can broaden the range of settings where computer vision is useful.<span class="chs" title="而且，显著提高的高质量检测速度可以扩大计算机视觉使用的设置范围。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We summarize our contributions as follows:<span class="chs" title="我们总结我们的贡献如下："><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We introduce <span class="word_hot_rare">SSD</span>, a <span class="word_hot" title="[ˈsiŋɡl ʃɔt]">single-shot</span> detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (<span class="word_hot_rare">YOLO</span>), and significantly more accurate, in fact as accurate as slower techniques that perform explicit region proposals and pooling (including Faster R-CNN).<span class="chs" title="我们引入了SSD，这是一种针对多个类别的单次检测器，比先前的先进的单次检测器（YOLO）更快，并且准确得多，事实上，与执行显式区域提出和池化的更慢的技术具有相同的精度（包括Faster R-CNN）。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The core of <span class="word_hot_rare">SSD</span> is predicting category scores and box offsets for a fixed set of default <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes using small convolutional filters applied to feature maps.<span class="chs" title="SSD的核心是预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">To achieve high detection accuracy we produce predictions of different scales from feature maps of different scales, and explicitly separate predictions by aspect ratio.<span class="chs" title="为了实现高检测精度，我们根据不同尺度的特征映射生成不同尺度的预测，并通过纵横比明确分开预测。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed <span class="word_hot_rare">vs</span> accuracy <span class="word_hot" title="[ˈtreɪdˌɔ:f, -ˌɔf]">trade-off</span>.<span class="chs" title="这些设计功能使得即使在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Experiments include timing and accuracy analysis on models with varying input size evaluated on <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>, <span class="word_hot" title="['kəʊkəʊ]">COCO</span>, and <span class="word_hot_rare">ILSVRC</span> and are compared to a range of recent state-of-the-art approaches.<span class="chs" title="实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">2. The Single Shot Detector (<span class="word_hot_rare">SSD</span>)<span class="chs" title="2. 单次检测器(SSD)"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">This <span class="word_hot" title="[sek]">sec</span>tion describes our proposed <span class="word_hot_rare">SSD</span> framework for detection (<span class="word_hot" title="[sek]">Sec</span>. 2.1) and the associated training methodology (<span class="word_hot" title="[sek]">Sec</span>. 2.2).<span class="chs" title="本节描述我们提出的SSD检测框架（2.1节）和相关的训练方法（2.2节）。"><span></div>
    <div class="eng">Afterwards, <span class="word_hot" title="[sek]">Sec</span>. 2.3 presents <span class="word_hot_rare">dataset-specific</span> model details and experimental results.<span class="chs" title="之后，2.3节介绍了数据集特有的模型细节和实验结果。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">2.1 Model<span class="chs" title="2.1 模型"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The <span class="word_hot_rare">SSD</span> approach is based on a <span class="word_hot" title="['fi:df'ɔ:wəd]">feed-forward</span> convolutional network that produces a <span class="word_hot_rare">fixed-size</span> collection of <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum <span class="word_hot" title="[səˈpreʃn]">suppression</span> step to produce the final detections.<span class="chs" title="SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。"><span></div>
    <div class="eng">The early network layers are based on a standard architecture used for high quality image classification (<span class="word_hot" title="['trʌŋkeɪtɪd]">truncated</span> before any classification layers), which we will call the base network.<span class="chs" title="早期的网络层基于用于高质量图像分类的标准架构（在任何分类层之前被截断），我们将其称为基础网络。"><span></div>
    <div class="eng">We then add <span class="word_hot" title="[ɔ:gˈzɪliəri]">auxiliary</span> structure to the network to produce detections with the following key features:<span class="chs" title="然后，我们将辅助结构添加到网络中以产生具有以下关键特征的检测："><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Multi-scale feature maps for detection We add convolutional feature layers to the end of the <span class="word_hot" title="['trʌŋkeɪtɪd]">truncated</span> base network.<span class="chs" title="用于检测的多尺度特征映射。我们将卷积特征层添加到截取的基础网络的末端。"><span></div>
    <div class="eng">These layers decrease in size <span class="word_hot" title="[prəˈgresɪvli]">progressively</span> and allow predictions of detections at multiple scales.<span class="chs" title="这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。"><span></div>
    <div class="eng">The convolutional model for predicting detections is different for each feature layer (<span class="word_hot_rare">cf</span> <span class="word_hot_rare">Overfeat</span>[4] and <span class="word_hot_rare">YOLO</span>[5] that operate on a single scale feature map).<span class="chs" title="用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Convolutional <span class="word_hot" title="[prɪˈdɪktə(r)]">predictor</span>s for detection<span class="chs" title="用于检测的卷积预测器。"><span></div>
    <div class="eng">Each added feature layer (or <span class="word_hot" title="['ɒpʃənəlɪ]">optionally</span> an existing feature layer from the base network) can produce a fixed set of detection predictions using a set of convolutional filters.<span class="chs" title="每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。"><span></div>
    <div class="eng">These are indicated on top of the <span class="word_hot_rare">SSD</span> network architecture in Fig.2.<span class="chs" title="这些在图2中的SSD网络架构的上部指出。"><span></div>
    <div class="eng">For a feature layer of size m×n with p channels, the basic element for predicting parameters of a potential detection is a 3×3×p small kernel that produces either a score for a category, or a shape offset relative to the default box coordinates.<span class="chs" title="对于具有p通道的大小为m×n的特征层，潜在检测的预测参数的基本元素是3×3×p的小核得到某个类别的分数，或者相对于默认框坐标的形状偏移。"><span></div>
    <div class="eng">At each of the m×n locations where the kernel is applied, it produces an output value.<span class="chs" title="在应用卷积核的m×n的每个位置，它会产生一个输出值。"><span></div>
    <div class="eng">The <span class="word_hot" title="[baundɪŋ]">bounding</span> box offset output values are measured relative to a default box position relative to each feature map location (<span class="word_hot_rare">cf</span> the architecture of <span class="word_hot_rare">YOLO</span>[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span class="chs" title="边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 2: A comparison between two single shot detection models: <span class="word_hot_rare">SSD</span> and <span class="word_hot_rare">YOLO</span> [5].<span class="chs" title="图2：两个单次检测模型的比较：SSD和YOLO[5]。"><span></div>
    <div class="eng">Our <span class="word_hot_rare">SSD</span> model adds several feature layers to the end of a base network, which predict the offsets to default boxes of different scales and aspect ratios and their associated confidences.<span class="chs" title="我们的SSD模型在基础网络的末端添加了几个特征层，它预测了不同尺度和长宽比的默认边界框的偏移量及其相关的置信度。"><span></div>
    <div class="eng"><span class="word_hot_rare">SSD</span> with a 300 × 300 input size significantly <span class="word_hot" title="[ˌaʊtpəˈfɔ:m]">outperform</span>s its 448 × 448 <span class="word_hot_rare">YOLO</span> counterpart in accuracy on <span class="word_hot_rare">VOC</span>2007 test while also improving the speed.<span class="chs" title="300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Default boxes and aspect ratios We associate a set of default <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes with each feature map cell, for multiple feature maps at the top of the network.<span class="chs" title="默认边界框和长宽比。"><span></div>
    <div class="eng">The default boxes <span class="word_hot" title="[taɪl]">tile</span> the feature map in a convolutional manner, so that the position of each box relative to its corresponding cell is fixed.<span class="chs" title="对于网络顶部的多个特征映射，我们将一组默认边界框与每个特征映射单元相关联。"><span></div>
    <div class="eng">At each feature map cell, we predict the offsets relative to the default box shapes in the cell, as well as the <span class="word_hot_rare">per-class</span> scores that indicate the presence of a class instance in each of those boxes.<span class="chs" title="默认边界框以卷积的方式平铺特征映射，以便每个边界框相对于其对应单元的位置是固定的。"><span></div>
    <div class="eng"><span class="word_hot" title="[spəˈsɪfɪkli]">Specifically</span>, for each box out of k at a given location, we compute c class scores and the 4 offsets relative to the original default box shape.<span class="chs" title="在每个特征映射单元中，我们预测单元中相对于默认边界框形状的偏移量，以及指出每个边界框中存在的每个类别实例的类别分数。"><span></div>
    <div class="eng">This results in a total of (c+4)k filters that are applied around each location in the feature map, yielding (c+4)<span class="word_hot_rare">kmn</span> outputs for a m×n feature map.<span class="chs" title="具体而言，对于给定位置处的k个边界框中的每一个，我们计算c个类别分数和相对于原始默认边界框形状的4个偏移量。"><span></div>
    <div class="eng">For an illustration of default boxes, please refer to Fig.<span class="chs" title="这导致在特征映射中的每个位置周围应用总共(c+4)k个滤波器，对于m×n的特征映射取得(c+4)kmn个输出。"><span></div>
    <div class="eng">1.<span class="chs" title="有关默认边界框的说明，请参见图1。"><span></div>
    <div class="eng">Our default boxes are similar to the anchor boxes used in Faster R-CNN[2], however we apply them to several feature maps of different resolutions.<span class="chs" title="我们的默认边界框与Faster R-CNN[2]中使用的锚边界框相似，但是我们将它们应用到不同分辨率的几个特征映射上。"><span></div>
    <div class="eng">Allowing different default box shapes in several feature maps let us efficiently <span class="word_hot" title="['diskri:taiz]">discretize</span> the space of possible output box shapes.<span class="chs" title="在几个特征映射中允许不同的默认边界框形状让我们有效地离散可能的输出框形状的空间。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 1: <span class="word_hot_rare">SSD</span> framework.<span class="chs" title="图1：SSD框架。"><span></div>
    <div class="eng">(a) <span class="word_hot_rare">SSD</span> only needs an input image and ground truth boxes for each object during training.<span class="chs" title="（a）在训练期间，SSD仅需要每个目标的输入图像和真实边界框。"><span></div>
    <div class="eng">In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 × 8 and 4 × 4 in (b) and (c)).<span class="chs" title="以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。"><span></div>
    <div class="eng">For each default box, we predict both the shape offsets and the confidences for all object categories ((c1,c2,…,cp)).<span class="chs" title="对于每个默认边界框，我们预测所有目标类别（(c1,c2,…,cp)）的形状偏移量和置信度。"><span></div>
    <div class="eng">At training time, we first match these default boxes to the ground truth boxes.<span class="chs" title="在训练时，我们首先将这些默认边界框与实际的边界框进行匹配。"><span></div>
    <div class="eng">For example, we have matched two default boxes with the cat and one with the dog, which are treated as positives and the rest as negatives.<span class="chs" title="例如，我们已经与猫匹配两个默认边界框，与狗匹配了一个，这被视为积极的，其余的是消极的。"><span></div>
    <div class="eng">The model loss is a weighted sum between <span class="word_hot" title="[ˌləʊkəlaɪ'zeɪʃn]">localization</span> loss (e.g. Smooth L1 [6]) and confidence loss (e.g. Softmax).<span class="chs" title="模型损失是定位损失（例如，Smooth L1[6]）和置信度损失（例如Softmax）之间的加权和。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">2.2 Training<span class="chs" title="2.2 训练"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The key difference between training <span class="word_hot_rare">SSD</span> and training a typical detector that uses region proposals, is that ground truth information needs to be assigned to specific outputs in the fixed set of detector outputs.<span class="chs" title="训练SSD和训练使用区域提出的典型检测器之间的关键区别在于，需要将真实信息分配给固定的检测器输出集合中的特定输出。"><span></div>
    <div class="eng">Some version of this is also required for training in <span class="word_hot_rare">YOLO</span>[5] and for the region proposal stage of Faster R-CNN[2] and <span class="word_hot_rare">MultiBox</span>[7].<span class="chs" title="在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。"><span></div>
    <div class="eng">Once this assignment is determined, the loss function and back <span class="word_hot" title="[ˌprɒpə'ɡeɪʃn]">propagation</span> are applied end-to-end.<span class="chs" title="一旦确定了这个分配，损失函数和反向传播就可以应用端到端了。"><span></div>
    <div class="eng">Training also involves choosing the set of default boxes and scales for detection as well as the hard negative <span class="word_hot" title="[ˈmaɪnɪŋ]">mining</span> and data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> strategies.<span class="chs" title="训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Matching strategy<span class="chs" title="匹配策略。"><span></div>
    <div class="eng">During training we need to determine which default boxes correspond to a ground truth detection and train the network accordingly.<span class="chs" title="在训练过程中，我们需要确定哪些默认边界框对应实际边界框的检测，并相应地训练网络。"><span></div>
    <div class="eng">For each ground truth box we are selecting from default boxes that vary over location, aspect ratio, and scale.<span class="chs" title="对于每个实际边界框，我们从默认边界框中选择，这些框会在位置，长宽比和尺度上变化。"><span></div>
    <div class="eng">We begin by matching each ground truth box to the default box with the best <span class="word_hot_rare">jaccard</span> overlap (as in <span class="word_hot_rare">MultiBox</span> [7]).<span class="chs" title="我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。"><span></div>
    <div class="eng">Unlike <span class="word_hot_rare">MultiBox</span>, we then match default boxes to any ground truth with <span class="word_hot_rare">jaccard</span> overlap higher than a threshold (0.5).<span class="chs" title="与MultiBox不同的是，我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框。"><span></div>
    <div class="eng">This simplifies the learning problem, allowing the network to predict high scores for multiple overlapping default boxes rather than requiring it to pick only the one with maximum overlap.<span class="chs" title="这简化了学习问题，允许网络为多个重叠的默认边界框预测高分，而不是要求它只挑选具有最大重叠的一个边界框。(注：Jaccard重叠即IoU。)"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Training objective<span class="chs" title="训练目标函数。"><span></div>
    <div class="eng">The <span class="word_hot_rare">SSD</span> training objective is derived from the <span class="word_hot_rare">MultiBox</span> objective[7,8] but is extended to handle multiple object categories.<span class="chs" title="SSD训练目标函数来自于MultiBox目标[7,8]，但扩展到处理多个目标类别。"><span></div>
    <div class="eng">Let xpij={1,0} be an <span class="word_hot" title="[ˈɪndɪkeɪtə(r)]">indicator</span> for matching the i-th default box to the j-th ground truth box of category p.<span class="chs" title="设xpij={1,0}是第i个默认边界框匹配到类别p的第j个实际边界框的指示器。"><span></div>
    <div class="eng">In the matching strategy above, we can have ∑ixpij≥1.<span class="chs" title="在上面的匹配策略中，我们有∑ixpij≥1。"><span></div>
    <div class="eng">The overall objective loss function is a weighted sum of the <span class="word_hot" title="[ˌləʊkəlaɪ'zeɪʃn]"><span class="word_hot_rare">loc</span>alization</span> loss (<span class="word_hot_rare">loc</span>) and the <span class="word_hot_rare">conf</span>idence loss (<span class="word_hot_rare">conf</span>):<span class="chs" title="总体目标损失函数是定位损失（loc）和置信度损失（conf）的加权和："><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">L(x,c,l,g)=1N(Lconf(x,c)+αLloc(x,l,g))(1)<span class="chs" title="L(x,c,l,g)=1N(Lconf(x,c)+αLloc(x,l,g))(1)"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">where N is the number of matched default boxes.<span class="chs" title="其中N是匹配的默认边界框的数量。"><span></div>
    <div class="eng">If N=0, wet set the loss to 0.<span class="chs" title="如果N=0，则将损失设为0。"><span></div>
    <div class="eng">The <span class="word_hot" title="[ˌləʊkəlaɪ'zeɪʃn]">localization</span> loss is a Smooth L1 loss[6] between the predicted box (l) and the ground truth box (g) parameters.<span class="chs" title="定位损失是预测框(l)与真实框(g)参数之间的Smooth L1损失[6]。"><span></div>
    <div class="eng">Similar to Faster R-CNN[2], we <span class="word_hot" title="[rɪˈgres]">regress</span> to offsets for the center (cx,cy) of the default <span class="word_hot" title="[baundɪŋ]">bounding</span> box (d) and for its width (w) and height (h).<span class="chs" title="类似于Faster R-CNN[2]，我们回归默认边界框(d)的中心偏移量(cx,cy)和其宽度(w)、高度(h)的偏移量。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Lloc(x,l,g)=∑i∈PosN∑m∈{cx,cy,w,h}xkijsmoothL1(lmi−g^mj)g^cxj=(gcxj−dcxi)/dwig^cyj=(gcyj−dcyi)/dhig^wj=log(gwjdwi)g^hj=log(ghjdhi)(2)<span class="chs" title="Lloc(x,l,g)=∑i∈PosN∑m∈{cx,cy,w,h}xkijsmoothL1(lmi−g^mj)g^cxj=(gcxj−dcxi)/dwig^cyj=(gcyj−dcyi)/dhig^wj=log(gwjdwi)g^hj=log(ghjdhi)(2)"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The confidence loss is the softmax loss over multiple classes confidences (c).<span class="chs" title="置信度损失是在多类别置信度(c)上的softmax损失。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Lconf(x,c)=−∑i∈PosNxpijlog(c^pi)−∑i∈Neglog(c^0i)wherec^pi=exp(cpi)∑pexp(cpi)(3)<span class="chs" title="Lconf(x,c)=−∑i∈PosNxpijlog(c^pi)−∑i∈Neglog(c^0i)wherec^pi=exp(cpi)∑pexp(cpi)(3)"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">and the weight term α is set to 1 by cross validation.<span class="chs" title="通过交叉验证权重项α设为1。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Choosing scales and aspect ratios for default boxes To handle different object scales, some methods [4,9] suggest processing the image at different sizes and combining the results afterwards.<span class="chs" title="为默认边界框选择尺度和长宽比。为了处理不同的目标尺度，一些方法[4,9]建议处理不同尺寸的图像，然后将结果合并。"><span></div>
    <div class="eng">However, by utilizing feature maps from several different layers in a single network for prediction we can <span class="word_hot" title="[ˈmɪmɪk]">mimic</span> the same effect, while also sharing parameters across all object scales.<span class="chs" title="然而，通过利用单个网络中几个不同层的特征映射进行预测，我们可以模拟相同的效果，同时还可以跨所有目标尺度共享参数。"><span></div>
    <div class="eng">Previous works [10,11] have shown that using feature maps from the lower layers can improve <span class="word_hot" title="[sɪˈmæntɪk]">semantic</span> <span class="word_hot" title="[ˌsegmenˈteɪʃn]">segmentation</span> quality because the lower layers capture more fine details of the input objects.<span class="chs" title="以前的工作[10,11]已经表明，使用低层的特征映射可以提高语义分割的质量，因为低层会捕获输入目标的更多细节。"><span></div>
    <div class="eng">Similarly, [12] showed that adding global context pooled from a feature map can help smooth the <span class="word_hot" title="[ˌsegmenˈteɪʃn]">segmentation</span> results.<span class="chs" title="同样，[12]表明，从特征映射上添加全局上下文池化可以有助于平滑分割结果。"><span></div>
    <div class="eng">Motivated by these methods, we use both the lower and upper feature maps for detection.<span class="chs" title="受这些方法的启发，我们使用较低和较高的特征映射进行检测。"><span></div>
    <div class="eng">Figure 1 shows two <span class="word_hot" title="[ɪgˈzemplɑ:(r)]">exemplar</span> feature maps (8 × 8 and 4 × 4) which are used in the framework.<span class="chs" title="图1显示了框架中使用的两个示例性特征映射（8×8和4×4）。"><span></div>
    <div class="eng">In practice, we can use many more with small <span class="word_hot" title="[ˌkɒmpjuˈteɪʃənl]">computational</span> overhead.<span class="chs" title="在实践中，我们可以使用更多的具有很少计算开支的特征映射。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Feature maps from different levels within a network are known to have different (empirical) <span class="word_hot" title="[rɪˈseptɪv]">receptive</span> field sizes [13].<span class="chs" title="已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。"><span></div>
    <div class="eng">Fortunately, within the <span class="word_hot_rare">SSD</span> framework, the default boxes do not necessary need to correspond to the actual <span class="word_hot" title="[rɪˈseptɪv]">receptive</span> fields of each layer.<span class="chs" title="幸运的是，在SSD框架内，默认边界框不需要对应于每层的实际感受野。"><span></div>
    <div class="eng">We design the <span class="word_hot" title="[taɪl]">til</span>ing of default boxes so that specific feature maps learn to be responsive to particular scales of the objects.<span class="chs" title="我们设计平铺默认边界框，以便特定的特征映射学习响应目标的特定尺度。"><span></div>
    <div class="eng">Suppose we want to use m feature maps for prediction.<span class="chs" title="假设我们要使用m个特征映射进行预测。"><span></div>
    <div class="eng">The scale of the default boxes for each feature map is computed as:<span class="chs" title="每个特征映射默认边界框的尺度计算如下："><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">sk=smin+smax−sminm−1(k−1),k∈[1,m]<span class="chs" title="sk=smin+smax−sminm−1(k−1),k∈[1,m]"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">where <span class="word_hot_rare">smin</span> is 0.2 and <span class="word_hot_rare">smax</span> is 0.9, meaning the lowest layer has a scale of 0.2 and the highest layer has a scale of 0.9, and all layers in between are regularly spaced.<span class="chs" title="其中smin为0.2，smax为0.9，意味着最低层具有0.2的尺度，最高层具有0.9的尺度，并且在它们之间的所有层是规则间隔的。"><span></div>
    <div class="eng">We impose different aspect ratios for the default boxes, and denote them as ar∈1,2,3,12,13.<span class="chs" title="我们为默认边界框添加不同的长宽比，并将它们表示为ar∈1,2,3,12,13。"><span></div>
    <div class="eng">We can compute the width (wak=sk√−−ar) and height (hak=sk/√−−ar) for each default box.<span class="chs" title="我们可以计算每个边界框的宽度(wak=sk√−−ar)和高度(hak=sk/√−−ar)。"><span></div>
    <div class="eng">For the aspect ratio of 1, we also add a default box whose scale is s′k=√−−−−−sksk+1, resulting in 6 default boxes per feature map location.<span class="chs" title="对于长宽比为1，我们还添加了一个默认边界框，其尺度为s′k=√−−−−−sksk+1，在每个特征映射位置得到6个默认边界框。"><span></div>
    <div class="eng">We set the center of each default box to (i+0.5|fk|,j+0.5|fk|), where |fk| is the size of the k-th square feature map, i,j∈[0,|fk|).<span class="chs" title="我们将每个默认边界框的中心设置为(i+0.5|fk|,j+0.5|fk|)，其中|fk|是第k个平方特征映射的大小，i,j∈[0,|fk|)。"><span></div>
    <div class="eng">In practice, one can also design a distribution of default boxes to best fit a specific dataset.<span class="chs" title="在实践中，也可以设计默认边界框的分布以最适合特定的数据集。"><span></div>
    <div class="eng">How to design the optimal <span class="word_hot" title="[taɪl]">til</span>ing is an open question as well.<span class="chs" title="如何设计最佳平铺也是一个悬而未决的问题。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">By combining predictions for all default boxes with different scales and aspect ratios from all locations of many feature maps, we have a diverse set of predictions, covering various input object sizes and shapes. For example, in Fig.<span class="chs" title="通过将所有默认边界框的预测与许多特征映射所有位置的不同尺度和高宽比相结合，我们有不同的预测集合，涵盖各种输入目标大小和形状。"><span></div>
    <div class="eng">1, the dog is matched to a default box in the 4 × 4 feature map, but not to any default boxes in the 8 × 8 feature map.<span class="chs" title="例如，在图1中，狗被匹配到4×4特征映射中的默认边界框，而不是8×8特征映射中的任何默认框。"><span></div>
    <div class="eng">This is because those boxes have different scales and do not match the dog box, and therefore are considered as negatives during training.<span class="chs" title="这是因为那些边界框有不同的尺度，不匹配狗的边界框，因此在训练期间被认为是负例。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Hard negative <span class="word_hot" title="[ˈmaɪnɪŋ]">mining</span> After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large.<span class="chs" title="难例挖掘。在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。"><span></div>
    <div class="eng">This introduces a significant <span class="word_hot" title="[ɪmˈbæləns]">imbalance</span> between the positive and negative training examples.<span class="chs" title="这在正的训练实例和负的训练实例之间引入了显著的不平衡。"><span></div>
    <div class="eng">Instead of using all the negative examples, we sort them using the highest confidence loss for each default box and pick the top ones so that the ratio between the negatives and positives is at most 3:1.<span class="chs" title="我们不使用所有负例，而是使用每个默认边界框的最高置信度损失来排序它们，并挑选最高的置信度，以便负例和正例之间的比例至多为3:1。"><span></div>
    <div class="eng">We found that this leads to faster optimization and a more stable training.<span class="chs" title="我们发现这会导致更快的优化和更稳定的训练。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span><span class="chs" title="数据增强。"><span></div>
    <div class="eng">To make the model more robust to various input object sizes and shapes, each training image is randomly sampled by one of the following options:<span class="chs" title="为了使模型对各种输入目标大小和形状更鲁棒，每张训练图像都是通过以下选项之一进行随机采样的："><span></div>
    <div class="eng">Use the entire original input image.<span class="chs" title="使用整个原始输入图像。"><span></div>
    <div class="eng">Sample a patch so that the minimum <span class="word_hot_rare">jaccard</span> overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9.<span class="chs" title="采样一个图像块，使得与目标之间的最小Jaccard重叠为0.1，0.3，0.5，0.7或0.9。"><span></div>
    <div class="eng">Randomly sample a patch.<span class="chs" title="随机采样一个图像块。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The size of each sampled patch is [0.1, 1] of the original image size, and the aspect ratio is between 12 and 2.<span class="chs" title="每个采样图像块的大小是原始图像大小的[0.1，1]，长宽比在12和2之间。"><span></div>
    <div class="eng">We keep the overlapped part of the ground truth box if the center of it is in the sampled patch.<span class="chs" title="如果实际边界框的中心在采用的图像块中，我们保留实际边界框与采样图像块的重叠部分。"><span></div>
    <div class="eng">After the <span class="word_hot" title="[əˌfɔ:ˈmenʃənd]">aforementioned</span> sampling step, each sampled patch is <span class="word_hot" title="[ˌri:ˈsaɪz]">resize</span>d to fixed size and is <span class="word_hot" title="[ˌhɒrɪ'zɒntəlɪ]">horizontally</span> <span class="word_hot" title="[flɪp]">flip</span>ped with probability of 0.5, in addition to applying some <span class="word_hot_rare">photo-metric</span> distortions similar to those described in [14].<span class="chs" title="在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3. Experimental Results<span class="chs" title="3. 实验结果"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Base network<span class="chs" title="基础网络。"><span></div>
    <div class="eng">Our experiments are all based on <span class="word_hot_rare">VGG</span>16[15], which is pre-trained on the <span class="word_hot_rare">ILSVRC</span> <span class="word_hot_rare">CLS-LOC</span> dataset[16].<span class="chs" title="我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。"><span></div>
    <div class="eng">Similar to <span class="word_hot_rare">DeepLab-LargeFOV</span>[17], we convert <span class="word_hot_rare">fc</span>6 and <span class="word_hot_rare">fc</span>7 to convolutional layers, <span class="word_hot" title="['sʌbsɑ:mpl]">subsample</span> parameters from <span class="word_hot_rare">fc</span>6 and <span class="word_hot_rare">fc</span>7, change pool5 from 2×2−s2 to 3×3−s1, and use the <span class="word_hot" title="['eitrәs]">atrous</span> algorithm[18] to fill the “holes”.<span class="chs" title="类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从2×2−s2更改为3×3−s1，并使用空洞算法[18]来填补这个“小洞”。"><span></div>
    <div class="eng">We remove all the dropout layers and the <span class="word_hot_rare">fc</span>8 layer.<span class="chs" title="我们删除所有的丢弃层和fc8层。"><span></div>
    <div class="eng">We <span class="word_hot" title="[faɪn tju:n]">fine-tune</span> the resulting model using <span class="word_hot" title="['esdʒ'i:d'i:]">SGD</span> with initial learning rate 10−3, 0.9 <span class="word_hot" title="[məˈmentəm]">momentum</span>, 0.0005 weight decay, and batch size 32.<span class="chs" title="我们使用SGD对得到的模型进行微调，初始学习率为10−3，动量为0.9，权重衰减为0.0005，批数据大小为32。"><span></div>
    <div class="eng">The learning rate decay policy is slightly different for each dataset, and we will describe details later.<span class="chs" title="每个数据集的学习速率衰减策略略有不同，我们将在后面详细描述。"><span></div>
    <div class="eng">The full training and testing code is built on <span class="word_hot_rare">Caffe</span>[19] and is open source at: https://github.com/weiliu89/<span class="word_hot_rare">caffe</span>/tree/ssd.<span class="chs" title="完整的训练和测试代码建立在Caffe[19]上并开源：https://github.com/weiliu89/caffe/tree/ssd。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.1 <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>2007<span class="chs" title="3.1 PASCAL VOC2007"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on <span class="word_hot_rare">VOC</span>2007 test (4952 images).<span class="chs" title="在这个数据集上，我们在VOC2007 test（4952张图像）上比较了Fast R-CNN[6]和FAST R-CNN[2]。"><span></div>
    <div class="eng">All methods <span class="word_hot" title="[faɪn tju:n]">fine-tune</span> on the same pre-trained <span class="word_hot_rare">VGG</span>16 network.<span class="chs" title="所有的方法都在相同的预训练好的VGG16网络上进行微调。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Figure 2 shows the architecture details of the <span class="word_hot_rare">SSD</span>300 model.<span class="chs" title="图2显示了SSD300模型的架构细节。"><span></div>
    <div class="eng">We use conv4_3, conv7 (<span class="word_hot_rare">fc</span>7), conv8_2, conv9_2, conv10_2, and conv11_2 to predict both location and confidences.<span class="chs" title="我们使用conv4_3，conv7（fc7），conv8_2，conv9_2，conv10_2和conv11_2来预测位置和置信度。"><span></div>
    <div class="eng">We set default box with scale 0.1 on conv4_3.<span class="chs" title="我们在conv4_3上设置了尺度为0.1的默认边界框。"><span></div>
    <div class="eng">We <span class="word_hot" title="[ɪˈnɪʃəlaɪz]">initialize</span> the parameters for all the newly added convolutional layers with the “<span class="word_hot" title="['zʌvɪə]">xavier</span>” method [20].<span class="chs" title="我们使用“xavier”方法[20]初始化所有新添加的卷积层的参数。"><span></div>
    <div class="eng">For conv4_3, conv10_2 and conv11_2, we only associate 4 default boxes at each feature map location —— omitting aspect ratios of 13 and 3.<span class="chs" title="对于conv4_3，conv10_2和conv11_2，我们只在每个特征映射位置上关联了4个默认边界框——忽略13和3的长宽比。"><span></div>
    <div class="eng">For all other layers, we put 6 default boxes as described in <span class="word_hot" title="[sek]">Sec</span>. 2.2.<span class="chs" title="对于所有其它层，我们像2.2节描述的那样放置了6个默认边界框。"><span></div>
    <div class="eng">Since, as pointed out in [12], conv4_3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back <span class="word_hot" title="[ˌprɒpə'ɡeɪʃn]">propagation</span>.<span class="chs" title="如[12]所指出的，与其它层相比，由于conv4_3具有不同的特征尺度，所以我们使用[12]中引入的L2正则化技术将特征映射中每个位置的特征标准缩放到20，在反向传播过程中学习尺度。"><span></div>
    <div class="eng">We use the 10−3 learning rate for 40k iterations, then continue training for 10k iterations with 10−4 and 10−5.<span class="chs" title="对于40k次迭代，我们使用10−3的学习率，然后继续用10−4和10−5的学习率训练10k迭代。"><span></div>
    <div class="eng">When training on <span class="word_hot_rare">VOC</span>2007 <span class="word_hot_rare">trainval</span>, Table 1 shows that our low resolution <span class="word_hot_rare">SSD</span>300 model is already more accurate than Fast R-CNN.<span class="chs" title="当对VOC2007 trainval进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。"><span></div>
    <div class="eng">When we train <span class="word_hot_rare">SSD</span> on a larger 512×512 input image, it is even more accurate, <span class="word_hot" title="[səˈpɑ:s]">surpass</span>ing Faster R-CNN by 1.7% mAP.<span class="chs" title="当我们用更大的512×512输入图像上训练SSD时，它更加准确，超过了Faster R-CNN 1.7%的mAP。"><span></div>
    <div class="eng">If we train <span class="word_hot_rare">SSD</span> with more (i.e. 07+12) data, we see that <span class="word_hot_rare">SSD</span>300 is already better than Faster R-CNN by 1.1% and that <span class="word_hot_rare">SSD</span>512 is 3.6% better.<span class="chs" title="如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好1.1%，SSD512比Faster R-CNN好3.6%。"><span></div>
    <div class="eng">If we take models trained on <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">trainval35k</span> as described in <span class="word_hot" title="[sek]">Sec</span>. 3.4 and <span class="word_hot" title="[faɪn tju:n]">fine-tun</span>ing them on the 07+12 dataset with <span class="word_hot_rare">SSD</span>512, we achieve the best results: 81.6% mAP.<span class="chs" title="如果我们将SSD512用3.4节描述的COCO trainval35k来训练模型并在07+12数据集上进行微调，我们获得了最好的结果：81.6%的mAP。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 1: <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>2007 test detection results.<span class="chs" title="表1：PASCAL VOC2007 test检测结果。"><span></div>
    <div class="eng">Both Fast and Faster R-CNN use input images whose minimum dimension is 600.<span class="chs" title="Fast和Faster R-CNN都使用最小维度为600的输入图像。"><span></div>
    <div class="eng">The two <span class="word_hot_rare">SSD</span> models have exactly the same settings except that they have different input sizes (300×300 <span class="word_hot_rare">vs</span>.<span class="chs" title="两个SSD模型使用完全相同的设置除了它们有不同的输入大小(300×300和512×512)。"><span></div>
    <div class="eng">512×512).<span class="chs" title="很明显更大的输入尺寸会导致更好的结果，并且更大的数据同样有帮助。"><span></div>
    <div class="eng">It is obvious that larger input size leads to better results, and more data always helps.<span class="chs" title="数据：“07”：VOC2007 trainval，“07+12”：VOC2007和VOC2012 trainval的联合。"><span></div>
    <div class="eng">Data: ”07”: <span class="word_hot_rare">VOC</span>2007 <span class="word_hot_rare">trainval</span>, ”07+12”: union of <span class="word_hot_rare">VOC</span>2007 and <span class="word_hot_rare">VOC</span>2012 <span class="word_hot_rare">trainval</span>. ”07+12+<span class="word_hot" title="['kəʊkəʊ]">COCO</span>”: first train on <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">trainval</span>35k then <span class="word_hot" title="[faɪn tju:n]">fine-tune</span> on 07+12.<span class="chs" title="“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">To understand the performance of our two <span class="word_hot_rare">SSD</span> models in more details, we used the detection analysis tool from [21].<span class="chs" title="为了更详细地了解我们两个SSD模型的性能，我们使用了[21]中的检测分析工具。"><span></div>
    <div class="eng">Figure 3 shows that <span class="word_hot_rare">SSD</span> can detect various object categories with high quality (large white area).<span class="chs" title="图3显示了SSD可以检测到高质量（大白色区域）的各种目标类别。"><span></div>
    <div class="eng">The majority of its confident detections are correct.<span class="chs" title="它大部分的确信检测是正确的。"><span></div>
    <div class="eng">The recall is around 85−90%, and is much higher with “weak” (0.1 <span class="word_hot_rare">jaccard</span> overlap) criteria.<span class="chs" title="召回约为85−90%，而“弱”（0.1 Jaccard重叠）标准则要高得多。"><span></div>
    <div class="eng">Compared to R-CNN [22], <span class="word_hot_rare">SSD</span> has less <span class="word_hot" title="[ˌləʊkəlaɪ'zeɪʃn]">localization</span> error, indicating that <span class="word_hot_rare">SSD</span> can <span class="word_hot" title="[ˈləʊkəlaɪz]">localize</span> objects better because it directly learns to <span class="word_hot" title="[rɪˈgres]">regress</span> the object shape and classify object categories instead of using two <span class="word_hot" title="[di:ˈkʌpl]">decouple</span>d steps.<span class="chs" title="与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。"><span></div>
    <div class="eng">However, <span class="word_hot_rare">SSD</span> has more confusions with similar object categories (especially for animals), partly because we share locations for multiple categories.<span class="chs" title="然而，SSD对类似的目标类别（特别是对于动物）有更多的混淆，部分原因是我们共享多个类别的位置。"><span></div>
    <div class="eng">Figure 4 shows that <span class="word_hot_rare">SSD</span> is very sensitive to the <span class="word_hot" title="[baundɪŋ]">bounding</span> box size.<span class="chs" title="图4显示SSD对边界框大小非常敏感。"><span></div>
    <div class="eng">In other words, it has much worse performance on smaller objects than bigger objects.<span class="chs" title="换句话说，它在较小目标上比在较大目标上的性能要差得多。"><span></div>
    <div class="eng">This is not surprising because those small objects may not even have any information at the very top layers.<span class="chs" title="这并不奇怪，因为这些小目标甚至可能在顶层没有任何信息。"><span></div>
    <div class="eng">Increasing the input size (e.g. from 300 × 300 to 512 × 512) can help improve detecting small objects, but there is still a lot of room to improve.<span class="chs" title="增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。"><span></div>
    <div class="eng">On the positive side, we can clearly see that <span class="word_hot_rare">SSD</span> performs really well on large objects.<span class="chs" title="积极的一面，我们可以清楚地看到SSD在大型目标上的表现非常好。"><span></div>
    <div class="eng">And it is very robust to different object aspect ratios because we use default boxes of various aspect ratios per feature map location.<span class="chs" title="而且对于不同长宽比的目标，它是非常鲁棒的，因为我们使用每个特征映射位置的各种长宽比的默认框。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 3: <span class="word_hot" title="[ˌvɪʒʊəlaɪ'zeɪʃn]">Visualization</span> of performance for <span class="word_hot_rare">SSD</span>512 on animals, vehicles, and furniture from <span class="word_hot_rare">VOC</span>2007 test.<span class="chs" title="图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。"><span></div>
    <div class="eng">The top row shows the <span class="word_hot" title="[ˈkju:mjələtɪv]">cumulative</span> fraction of detections that are <span class="word_hot" title="[kɔ:(r)]">cor</span>rect (<span class="word_hot" title="[kɔ:(r)]">Cor</span>) or false positive due to poor <span class="word_hot" title="[ˌləʊkəlaɪ'zeɪʃn]"><span class="word_hot_rare">loc</span>alization</span> (<span class="word_hot_rare">Loc</span>), confusion with <span class="word_hot" title="[sɪm]">sim</span>ilar categories (<span class="word_hot" title="[sɪm]">Sim</span>), with <span class="word_hot_rare">oth</span>ers (<span class="word_hot_rare">Oth</span>), or with background (<span class="word_hot_rare">BG</span>).<span class="chs" title="第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。"><span></div>
    <div class="eng">The solid red line reflects the change of recall with strong criteria (0.5 <span class="word_hot_rare">jaccard</span> overlap) as the number of detections increases.<span class="chs" title="红色的实线表示随着检测次数的增加，强标准（0.5 Jaccard重叠）下的召回变化。"><span></div>
    <div class="eng">The <span class="word_hot" title="[dæʃ]">dash</span>ed red line is using the weak criteria (0.1 <span class="word_hot_rare">jaccard</span> overlap).<span class="chs" title="红色虚线是使用弱标准（0.1 Jaccard重叠）。"><span></div>
    <div class="eng">The bottom row shows the distribution of <span class="word_hot" title="['tɒpr'æŋkt]">top-ranked</span> false positive types.<span class="chs" title="最下面一行显示了排名靠前的假阳性类型的分布。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 4: <span class="word_hot" title="[ˌsensəˈtɪvəti]">Sensitivity</span> and impact of different object characteristics on <span class="word_hot_rare">VOC</span>2007 test set using [21].<span class="chs" title="图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。"><span></div>
    <div class="eng">The plot on the left shows the effects of <span class="word_hot_rare">BBox</span> Area per category, and the right plot shows the effect of Aspect Ratio.<span class="chs" title="左边的图显示了BBox面积对每个类别的影响，右边的图显示了长宽比的影响。"><span></div>
    <div class="eng">Key: <span class="word_hot_rare">BBox</span> Area: XS=extra-small; S=small; M=medium; L=large; XL=extra-large.<span class="chs" title="关键：BBox区域：XS=超小；S=小；M=中等；L=大；XL=超大。"><span></div>
    <div class="eng">Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; <span class="word_hot_rare">XW</span> =<span class="word_hot_rare">extra-wide</span>.<span class="chs" title="长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.2 Model analysis<span class="chs" title="3.2 模型分析"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">To understand <span class="word_hot_rare">SSD</span> better, we carried out controlled experiments to examine how each component affects performance.<span class="chs" title="为了更好地了解SSD，我们进行了控制实验，以检查每个组件如何影响性能。"><span></div>
    <div class="eng">For all the experiments, we use the same settings and input size (300 × 300), except for specified changes to the settings or component(s).<span class="chs" title="对于所有的实验，我们使用相同的设置和输入大小（300×300），除了指定的设置或组件的更改。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> is crucial.<span class="chs" title="数据增强至关重要。"><span></div>
    <div class="eng">Fast and Faster R-CNN use the original image and the horizontal <span class="word_hot" title="[flɪp]">flip</span> to train.<span class="chs" title="Fast和Faster R-CNN使用原始图像和水平翻转来训练。"><span></div>
    <div class="eng">We use a more extensive sampling strategy, similar to <span class="word_hot_rare">YOLO</span> [5].<span class="chs" title="我们使用更广泛的抽样策略，类似于YOLO[5]。"><span></div>
    <div class="eng">Table 2 shows that we can improve 8.8% mAP with this sampling strategy.<span class="chs" title="从表2可以看出，采样策略可以提高8.8%的mAP。"><span></div>
    <div class="eng">We do not know how much our sampling strategy will benefit Fast and Faster R-CNN, but they are likely to benefit less because they use a feature pooling step during classification that is relatively robust to object translation by design.<span class="chs" title="我们不知道我们的采样策略将会使Fast和Faster R-CNN受益多少，但是他们可能从中受益较少，因为他们在分类过程中使用了一个特征池化步骤，这对通过设计的目标变换来说相对鲁棒。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 2: Effects of various design choices and components on <span class="word_hot_rare">SSD</span> performance.<span class="chs" title="表2：各种设计选择和组件对SSD性能的影响。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">More default box shapes is better.<span class="chs" title="更多的默认边界框形状会更好。"><span></div>
    <div class="eng">As described in <span class="word_hot" title="[sek]">Sec</span>. 2.2, by default we use 6 default boxes per location.<span class="chs" title="如2.2节所述，默认情况下，我们每个位置使用6个默认边界框。"><span></div>
    <div class="eng">If we remove the boxes with 13 and 3 aspect ratios, the performance drops by 0.6%.<span class="chs" title="如果我们删除长宽比为13和3的边界框，性能下降了0.6%。"><span></div>
    <div class="eng">By further removing the boxes with 12 and 2 aspect ratios, the performance drops another 2.1%.<span class="chs" title="通过进一步去除12和2长宽比的盒子，性能再下降2.1%。"><span></div>
    <div class="eng">Using a variety of default box shapes seems to make the task of predicting boxes easier for the network.<span class="chs" title="使用各种默认边界框形状似乎使网络预测边界框的任务更容易。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng"><span class="word_hot" title="['eitrәs]">Atrous</span> is faster.<span class="chs" title="Atrous更快。"><span></div>
    <div class="eng">As described in <span class="word_hot" title="[sek]">Sec</span>. 3, we used the <span class="word_hot" title="['eitrәs]">atrous</span> version of a <span class="word_hot_rare">subsampled</span> <span class="word_hot_rare">VGG</span>16, following <span class="word_hot_rare">DeepLab-LargeFOV</span> [17].<span class="chs" title="如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。"><span></div>
    <div class="eng">If we use the full <span class="word_hot_rare">VGG</span>16, keeping pool5 with 2×2−s2 and not <span class="word_hot_rare">subsampling</span> parameters from <span class="word_hot_rare">fc</span>6 and <span class="word_hot_rare">fc</span>7, and add conv5 3 for prediction, the result is about the same while the speed is about 20% slower.<span class="chs" title="如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Multiple output layers at different resolutions is better.<span class="chs" title="多个不同分辨率的输出层更好。"><span></div>
    <div class="eng">A major contribution of <span class="word_hot_rare">SSD</span> is using default boxes of different scales on different output layers.<span class="chs" title="SSD的主要贡献是在不同的输出层上使用不同尺度的默认边界框。"><span></div>
    <div class="eng">To measure the advantage gained, we <span class="word_hot" title="[prəˈgresɪvli]">progressively</span> remove layers and compare results.<span class="chs" title="为了衡量所获得的优势，我们逐步删除层并比较结果。"><span></div>
    <div class="eng">For a fair comparison, every time we remove a layer, we adjust the default box <span class="word_hot" title="[taɪl]">til</span>ing to keep the total number of boxes similar to the original (8732).<span class="chs" title="为了公平比较，每次我们删除一层，我们调整默认边界框平铺，以保持类似于最初的边界框的总数（8732）。"><span></div>
    <div class="eng">This is done by stacking more scales of boxes on remaining layers and adjusting scales of boxes if needed.<span class="chs" title="这是通过在剩余层上堆叠更多尺度的盒子并根据需要调整边界框的尺度来完成的。"><span></div>
    <div class="eng">We do not <span class="word_hot" title="[ɪɡ'zɔ:stɪvlɪ]">exhaustively</span> optimize the <span class="word_hot" title="[taɪl]">til</span>ing for each setting.<span class="chs" title="我们没有详尽地优化每个设置的平铺。"><span></div>
    <div class="eng">Table 3 shows a decrease in accuracy with fewer layers, dropping <span class="word_hot" title="[mɒnə'tɒnɪklɪ]">monotonically</span> from 74.3 to 62.4.<span class="chs" title="表3显示层数较少，精度降低，从74.3单调递减至62.4。"><span></div>
    <div class="eng">When we stack boxes of multiple scales on a layer, many are on the image boundary and need to be handled carefully.<span class="chs" title="当我们在一层上堆叠多尺度的边界框时，很多边界框在图像边界上需要小心处理。"><span></div>
    <div class="eng">We tried the strategy used in Faster R-CNN [2], ignoring boxes which are on the boundary.<span class="chs" title="我们尝试了在Faster R-CNN[2]中使用这个策略，忽略在边界上的边界框。"><span></div>
    <div class="eng">We observe some interesting trends.<span class="chs" title="我们观察到了一些有趣的趋势。"><span></div>
    <div class="eng">For example, it hurts the performance by a large margin if we use very coarse feature maps (e.g. conv11_2 (1 × 1) or conv10_2 (3 × 3)).<span class="chs" title="例如，如果我们使用非常粗糙的特征映射（例如conv11_2（1×1）或conv10_2（3×3）），它会大大伤害性能。"><span></div>
    <div class="eng">The reason might be that we do not have enough large boxes to cover large objects after the <span class="word_hot" title="[pru:n]">prun</span>ing.<span class="chs" title="原因可能是修剪后我们没有足够大的边界框来覆盖大的目标。"><span></div>
    <div class="eng">When we use primarily finer resolution maps, the performance starts increasing again because even after <span class="word_hot" title="[pru:n]">prun</span>ing a sufficient number of large boxes remains.<span class="chs" title="当我们主要使用更高分辨率的特征映射时，性能开始再次上升，因为即使在修剪之后仍然有足够数量的大边界框。"><span></div>
    <div class="eng">If we only use conv7 for prediction, the performance is the worst, reinforcing the message that it is critical to spread boxes of different scales over different layers.<span class="chs" title="如果我们只使用conv7进行预测，那么性能是最糟糕的，这就强化了在不同层上扩展不同尺度的边界框是非常关键的信息。"><span></div>
    <div class="eng">Besides, since our predictions do not rely on <span class="word_hot" title="[rwɑ:]">ROI</span> pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23].<span class="chs" title="此外，由于我们的预测不像[6]那样依赖于ROI池化，所以我们在低分辨率特征映射中没有折叠组块的问题[23]。"><span></div>
    <div class="eng">The <span class="word_hot_rare">SSD</span> architecture combines predictions from feature maps of various resolutions to achieve <span class="word_hot" title="[ˈkɒmpərəbl]">comparable</span> accuracy to Faster R-CNN, while using lower resolution input images.<span class="chs" title="SSD架构将来自各种分辨率的特征映射的预测结合起来，以达到与Faster R-CNN相当的精确度，同时使用较低分辨率的输入图像。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 3: Effects of using multiple output layers.<span class="chs" title="表3：使用多个输出层的影响。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.3 <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>2012<span class="chs" title="3.3 PASCAL VOC2012"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We use the same settings as those used for our basic <span class="word_hot_rare">VOC</span>2007 experiments above, except that we use <span class="word_hot_rare">VOC</span>2012 <span class="word_hot_rare">trainval</span> and <span class="word_hot_rare">VOC</span>2007 <span class="word_hot_rare">trainval</span> and test (21503 images) for training, and test on <span class="word_hot_rare">VOC</span>2012 test (10991 images).<span class="chs" title="除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。"><span></div>
    <div class="eng">We train the models with 10−3 learning rate for 60k iterations, then 10−4 for 20k iterations.<span class="chs" title="我们用10−3的学习率对模型进行60k次的迭代训练，然后使用10−4的学习率进行20k次迭代训练。"><span></div>
    <div class="eng">Table 4 shows the results of our <span class="word_hot_rare">SSD</span>300 and <span class="word_hot_rare">SSD</span>512 model.<span class="chs" title="表4显示了我们的SSD300和SSD512模型的结果。"><span></div>
    <div class="eng">We see the same performance trend as we observed on <span class="word_hot_rare">VOC</span>2007 test.<span class="chs" title="我们看到了与我们在VOC2007 test中观察到的相同的性能趋势。"><span></div>
    <div class="eng">Our <span class="word_hot_rare">SSD</span>300 improves accuracy over Fast/Faster R-CNN.<span class="chs" title="我们的SSD300比Fast/Faster R-CNN提高了准确性。"><span></div>
    <div class="eng">By increasing the training and testing image size to 512 × 512, we are 4.5% more accurate than Faster R-CNN.<span class="chs" title="通过将训练和测试图像大小增加到512×512，我们比Faster R-CNN的准确率提高了4.5%。"><span></div>
    <div class="eng">Compared to <span class="word_hot_rare">YOLO</span>, <span class="word_hot_rare">SSD</span> is significantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training.<span class="chs" title="与YOLO相比，SSD更精确，可能是由于使用了来自多个特征映射的卷积默认边界框和我们在训练期间的匹配策略。"><span></div>
    <div class="eng">When <span class="word_hot" title="[faɪn tju:n]">fine-tune</span>d from models trained on <span class="word_hot" title="['kəʊkəʊ]">COCO</span>, our <span class="word_hot_rare">SSD</span>512 achieves 80.0% mAP, which is 4.1% higher than Faster R-CNN.<span class="chs" title="当对从COCO上训练的模型进行微调后，我们的SSD512达到了80.0%的mAP，比Faster R-CNN高了4.1%。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 4: <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>2012 test detection results.<span class="chs" title="表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 48。"><span></div>
    <div class="eng">Fast and Faster R-CNN use images with minimum dimension 600, while the image size for <span class="word_hot_rare">YOLO</span> is 448 × 448. data: ”07++12”: union of <span class="word_hot_rare">VOC</span>2007 <span class="word_hot_rare">trainval</span> and test and <span class="word_hot_rare">VOC</span>2012 <span class="word_hot_rare">trainval</span>.<span class="chs" title="数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。"><span></div>
    <div class="eng">”07++12+<span class="word_hot" title="['kəʊkəʊ]">COCO</span>”: first train on <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">trainval35k</span> then <span class="word_hot" title="[faɪn tju:n]">fine-tune</span> on 07++12.<span class="chs" title="“07++12+COCO”：先在COCO trainval135k上训练然后在07++12上微调。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.4 <span class="word_hot" title="['kəʊkəʊ]">COCO</span><span class="chs" title="3.4 COCO"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">To further <span class="word_hot" title="[ˈvælɪdeɪt]">validate</span> the <span class="word_hot_rare">SSD</span> framework, we trained our <span class="word_hot_rare">SSD</span>300 and <span class="word_hot_rare">SSD</span>512 architectures on the <span class="word_hot" title="['kəʊkəʊ]">COCO</span> dataset.<span class="chs" title="为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。"><span></div>
    <div class="eng">Since objects in <span class="word_hot" title="['kəʊkəʊ]">COCO</span> tend to be smaller than <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>, we use smaller default boxes for all layers. We follow the strategy mentioned in <span class="word_hot" title="[sek]">Sec</span>.<span class="chs" title="由于COCO中的目标往往比PASCAL VOC中的更小，因此我们对所有层使用较小的默认边界框。"><span></div>
    <div class="eng">2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4_3 is 0.07 (e.g. 21 pixels for a 300 × 300 image).<span class="chs" title="我们遵循2.2节中提到的策略，但是现在我们最小的默认边界框尺度是0.15而不是0.2，并且conv4_3上的默认边界框尺度是0.07（例如，300×300图像中的21个像素）。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We use the <span class="word_hot_rare">trainval35k</span>[24] for training.<span class="chs" title="我们使用trainval35k[24]进行训练。"><span></div>
    <div class="eng">We first train the model with 10−3 learning rate for 160k iterations, and then continue training for 40k iterations with 10−4 and 40k iterations with 10−5.<span class="chs" title="我们首先用10−3的学习率对模型进行训练，进行160k次迭代，然后继续以10−4和10−5的学习率各进行40k次迭代。"><span></div>
    <div class="eng">Table 5 shows the results on <span class="word_hot_rare">test-dev</span>2015.<span class="chs" title="表5显示了test-dev2015的结果。"><span></div>
    <div class="eng">Similar to what we observed on the <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span> dataset, <span class="word_hot_rare">SSD</span>300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].<span class="chs" title="与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。"><span></div>
    <div class="eng"><span class="word_hot_rare">SSD</span>300 has a similar mAP@0.75 as <span class="word_hot" title="[ˈaɪən]">ION</span> [24] and Faster R-CNN [25], but is worse in mAP@0.5.<span class="chs" title="SSD300与ION 24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。"><span></div>
    <div class="eng">By increasing the image size to 512 × 512, our <span class="word_hot_rare">SSD</span>512 is better than Faster R-CNN [25] in both criteria.<span class="chs" title="通过将图像尺寸增加到512×512，我们的SSD512在这两个标准中都优于Faster R-CNN[25]。"><span></div>
    <div class="eng">Interestingly, we observe that <span class="word_hot_rare">SSD</span>512 is 5.3% better in mAP@0.75, but is only 1.2% better in mAP@0.5.<span class="chs" title="有趣的是，我们观察到SSD512在mAP@0.75中要好5.3%，但是在mAP@0.5中只好1.2%。"><span></div>
    <div class="eng">We also observe that it has much better AP (4.8%) and AR (4.6%) for large objects, but has relatively less improvement in AP (1.3%) and AR (2.0%) for small objects.<span class="chs" title="我们也观察到，对于大型目标，AP（4.8%）和AR（4.6%）的效果要好得多，但对于小目标，AP（1.3%）和AR（2.0%）有相对更少的改进。"><span></div>
    <div class="eng">Compared to <span class="word_hot" title="[ˈaɪən]">ION</span>, the improvement in AR for large and small objects is more similar (5.4% <span class="word_hot_rare">vs</span>.<span class="chs" title="与ION相比，大型和小型目标的AR改进更为相似（5.4%和3.9%）。"><span></div>
    <div class="eng">3.9%). We <span class="word_hot" title="[kənˈdʒektʃə(r)]">conjecture</span> that Faster R-CNN is more competitive on smaller objects with <span class="word_hot_rare">SSD</span> because it performs two box <span class="word_hot" title="[rɪˈfaɪnmənt]">refinement</span> steps, in both the <span class="word_hot_rare">RPN</span> part and in the Fast R-CNN part.<span class="chs" title="我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。"><span></div>
    <div class="eng">In Fig. 5, we show some detection examples on <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">test-dev</span> with the <span class="word_hot_rare">SSD</span>512 model.<span class="chs" title="在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 5: <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">test-dev</span>2015 detection results.<span class="chs" title="表5：COCO test-dev2015检测结果。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 5: Detection examples on <span class="word_hot" title="['kəʊkəʊ]">COCO</span> <span class="word_hot_rare">test-dev</span> with <span class="word_hot_rare">SSD</span>512 model.<span class="chs" title="图5：SSD512模型在COCO test-dev上的检测实例。"><span></div>
    <div class="eng">We show detections with scores higher than 0.6.<span class="chs" title="我们展示了分数高于0.6的检测。"><span></div>
    <div class="eng">Each color corresponds to an object category.<span class="chs" title="每种颜色对应一种目标类别。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.5 <span class="word_hot" title="[prɪˈlɪmɪnəri]">Preliminary</span> <span class="word_hot_rare">ILSVRC</span> results<span class="chs" title="3.5 初步的ILSVRC结果"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">We applied the same network architecture we used for <span class="word_hot" title="['kəʊkəʊ]">COCO</span> to the <span class="word_hot_rare">ILSVRC</span> <span class="word_hot_rare">DET</span> dataset [16].<span class="chs" title="我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。"><span></div>
    <div class="eng">We train a <span class="word_hot_rare">SSD</span>300 model using the <span class="word_hot_rare">ILSVRC</span>2014 <span class="word_hot_rare">DET</span> train and <span class="word_hot" title="[væl]">val</span>1 as used in [22].<span class="chs" title="我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。"><span></div>
    <div class="eng">We first train the model with 10−3 learning rate for 320k iterations, and then continue training for 80k iterations with 10−4 and 40k iterations with 10−5.<span class="chs" title="我们首先用10−3的学习率对模型进行训练，进行了320k次的迭代，然后以10−4继续迭代80k次，以10−5迭代40k次。"><span></div>
    <div class="eng">We can achieve 43.4 mAP on the <span class="word_hot" title="[væl]">val</span>2 set [22].<span class="chs" title="我们可以在val2数据集上[22]实现43.4 mAP。"><span></div>
    <div class="eng">Again, it <span class="word_hot" title="[ˈvælɪdeɪt]">validate</span>s that <span class="word_hot_rare">SSD</span> is a general framework for high quality real-time detection.<span class="chs" title="再一次证明了SSD是用于高质量实时检测的通用框架。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.6 Data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">Augmentation</span> for Small Object Accuracy<span class="chs" title="3.6 为小目标准确率进行数据增强"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Without a <span class="word_hot" title="['fɒləʊ ʌp]">follow-up</span> feature <span class="word_hot" title="[re'sɑ:mplɪŋ]">resampling</span> step as in Faster R-CNN, the classification task for small objects is relatively hard for <span class="word_hot_rare">SSD</span>, as demonstrated in our analysis (see Fig. 4).<span class="chs" title="SSD没有如Faster R-CNN中后续的特征重采样步骤，小目标的分类任务对SSD来说相对困难，正如我们的分析（见图4）所示。"><span></div>
    <div class="eng">The data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> strategy described in <span class="word_hot" title="[sek]">Sec</span>. 2.2 helps to improve the performance dramatically, especially on small datasets such as <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span>.<span class="chs" title="2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。"><span></div>
    <div class="eng">The random crops generated by the strategy can be thought of as a “<span class="word_hot" title="[zu:m]">zoom</span> in” operation and can generate many larger training examples.<span class="chs" title="策略产生的随机裁剪可以被认为是“放大”操作，并且可以产生许多更大的训练样本。"><span></div>
    <div class="eng">To implement a “<span class="word_hot" title="[zu:m]">zoom</span> out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation.<span class="chs" title="为了实现创建更多小型训练样本的“缩小”操作，我们首先将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。"><span></div>
    <div class="eng">Because we have more training images by introducing this new “expansion” data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> trick, we have to double the training iterations.<span class="chs" title="因为通过引入这个新的“扩展”数据增强技巧，我们有更多的训练图像，所以我们必须将训练迭代次数加倍。"><span></div>
    <div class="eng">We have seen a consistent increase of 2%−3% mAP across multiple datasets, as shown in Table 6.<span class="chs" title="我们已经在多个数据集上看到了一致的2%−3%的mAP增长，如表6所示。"><span></div>
    <div class="eng">In specific, Figure 6 shows that the new <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> trick significantly improves the performance on small objects.<span class="chs" title="具体来说，图6显示新的增强技巧显著提高了模型在小目标上的性能。"><span></div>
    <div class="eng">This result <span class="word_hot" title="[ˌʌndəˈskɔ:(r)]">underscore</span>s the importance of the data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> strategy for the final model accuracy.<span class="chs" title="这个结果强调了数据增强策略对最终模型精度的重要性。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 6: Results on multiple datasets when we add the image expansion data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> trick.<span class="chs" title="表6：我们使用图像扩展数据增强技巧在多个数据集上的结果。"><span></div>
    <div class="eng">SSD300∗ and SSD512∗ are the models that are trained with the new data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span>.<span class="chs" title="SSD300∗和SSD512∗是用新的数据增强训练的模型。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Fig. 6: <span class="word_hot" title="[ˌsensəˈtɪvəti]">Sensitivity</span> and impact of object size with new data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> on <span class="word_hot_rare">VOC</span>2007 test set using [21].<span class="chs" title="图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。"><span></div>
    <div class="eng">The top row shows the effects of <span class="word_hot_rare">BBox</span> Area per category for the original <span class="word_hot_rare">SSD</span>300 and <span class="word_hot_rare">SSD</span>512 model, and the bottom row corresponds to the <span class="word_hot_rare">SSD</span>300∗ and <span class="word_hot_rare">SSD</span>512∗ model trained with the new data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> trick.<span class="chs" title="最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的SSD300∗和SSD512∗模型。"><span></div>
    <div class="eng">It is obvious that the new data <span class="word_hot" title="[ˌɔ:ɡmen'teɪʃn]">augmentation</span> trick helps detecting small objects significantly.<span class="chs" title="新的数据增强技巧显然有助于显著检测小目标。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">An alternative way of improving <span class="word_hot_rare">SSD</span> is to design a better <span class="word_hot" title="[taɪl]">til</span>ing of default boxes so that its position and scale are better <span class="word_hot" title="[əˈlaɪn]">align</span>ed with the <span class="word_hot" title="[rɪˈseptɪv]">receptive</span> field of each position on a feature map.<span class="chs" title="改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。"><span></div>
    <div class="eng">We leave this for future work.<span class="chs" title="我们将这个留给未来工作。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">3.7 Inference time<span class="chs" title="3.7 推断时间"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Considering the large number of boxes generated from our method, it is essential to perform non-maximum <span class="word_hot" title="[səˈpreʃn]">suppression</span> (<span class="word_hot_rare">nms</span>) efficiently during inference.<span class="chs" title="考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。"><span></div>
    <div class="eng">By using a confidence threshold of 0.01, we can filter out most boxes.<span class="chs" title="通过使用0.01的置信度阈值，我们可以过滤大部分边界框。"><span></div>
    <div class="eng">We then apply <span class="word_hot_rare">nms</span> with <span class="word_hot_rare">jaccard</span> overlap of 0.45 per class and keep the top 200 detections per image.<span class="chs" title="然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。"><span></div>
    <div class="eng">This step costs about 1.7 <span class="word_hot" title="[m'zek]">msec</span> per image for <span class="word_hot_rare">SSD</span>300 and 20 <span class="word_hot_rare">VOC</span> classes, which is close to the total time (2.4 <span class="word_hot" title="[m'zek]">msec</span>) spent on all newly added layers.<span class="chs" title="对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。"><span></div>
    <div class="eng">We measure the speed with batch size 8 using <span class="word_hot" title="[ˈtaɪtn]">Titan</span> X and <span class="word_hot_rare">cuDNN</span> v4 with Intel <span class="word_hot_rare">Xeon</span> E5-2667v3@3.20GHz.<span class="chs" title="我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 7 shows the comparison between <span class="word_hot_rare">SSD</span>, Faster R-CNN[2], and <span class="word_hot_rare">YOLO</span>[5].<span class="chs" title="表7显示了SSD，Faster R-CNN[2]和YOLO[5]之间的比较。"><span></div>
    <div class="eng">Both our <span class="word_hot_rare">SSD</span>300 and <span class="word_hot_rare">SSD</span>512 method <span class="word_hot" title="[ˌaʊtpəˈfɔ:m]">outperform</span>s Faster R-CNN in both speed and accuracy.<span class="chs" title="我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。"><span></div>
    <div class="eng">Although Fast <span class="word_hot_rare">YOLO</span>[5] can run at 155 <span class="word_hot" title="['efp'i:'es]">FPS</span>, it has lower accuracy by almost 22% mAP.<span class="chs" title="虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。"><span></div>
    <div class="eng">To the best of our knowledge, <span class="word_hot_rare">SSD</span>300 is the first real-time method to achieve above 70% mAP.<span class="chs" title="就我们所知，SSD300是第一个实现70%以上mAP的实时方法。"><span></div>
    <div class="eng">Note that about 80% of the forward time is spent on the base network (<span class="word_hot_rare">VGG</span>16 in our case).<span class="chs" title="请注意，大约80%前馈时间花费在基础网络上（本例中为VGG16）。"><span></div>
    <div class="eng">Therefore, using a faster base network could even further improve the speed, which can possibly make the <span class="word_hot_rare">SSD</span>512 model real-time as well.<span class="chs" title="因此，使用更快的基础网络可以进一步提高速度，这也可能使SSD512模型达到实时。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Table 7: Results on <span class="word_hot" title="['pæskәl]">Pascal</span> <span class="word_hot_rare">VOC</span>2007 test.<span class="chs" title="表7：Pascal VOC2007 test上的结果。"><span></div>
    <div class="eng"><span class="word_hot_rare">SSD</span>300 is the only real-time detection method that can achieve above 70% mAP.<span class="chs" title="SSD300是唯一可以取得70%以上mAP的实现检测方法。"><span></div>
    <div class="eng">By using a larger input image, <span class="word_hot_rare">SSD</span>512 <span class="word_hot" title="[ˌaʊtpəˈfɔ:m]">outperform</span>s all methods on accuracy while maintaining a close to real-time speed.<span class="chs" title="通过使用更大的输入图像，SSD512在精度上超过了所有方法同时保持近似实时的速度。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">4. Related Work<span class="chs" title="4. 相关工作"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">There are two established classes of methods for object detection in images, one based on sliding windows and the other based on region proposal classification.<span class="chs" title="在图像中有两种建立的用于目标检测的方法，一种基于滑动窗口，另一种基于区域提出分类。"><span></div>
    <div class="eng">Before the <span class="word_hot" title="[ˈædvent]">advent</span> of convolutional neural networks, the state of the art for those two approaches —— <span class="word_hot" title="[dɪ'fɔ:məbl]">Deformable</span> Part Model (<span class="word_hot_rare">DPM</span>) [26] and <span class="word_hot" title="[sɪˈlektɪv]">Selective</span> Search [1] —— had <span class="word_hot" title="[ˈkɒmpərəbl]">comparable</span> performance.<span class="chs" title="在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。"><span></div>
    <div class="eng">However, after the dramatic improvement brought on by R-CNN [22], which combines <span class="word_hot" title="[sɪˈlektɪv]">selective</span> search region proposals and convolutional network based post-classification, region proposal object detection methods became <span class="word_hot" title="[ˈprevələnt]">prevalent</span>.<span class="chs" title="然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The original R-CNN approach has been improved in a variety of ways.<span class="chs" title="最初的R-CNN方法已经以各种方式进行了改进。"><span></div>
    <div class="eng">The first set of approaches improve the quality and speed of post-classification, since it requires the classification of thousands of image crops, which is expensive and <span class="word_hot" title="[taɪm kən'sju:mɪŋ]">time-consuming</span>.<span class="chs" title="第一套方法提高了后分类的质量和速度，因为它需要对成千上万的裁剪图像进行分类，这是昂贵和耗时的。"><span></div>
    <div class="eng"><span class="word_hot_rare">SPPnet</span> [9] speeds up the original R-CNN approach significantly.<span class="chs" title="SPPnet[9]显著加快了原有的R-CNN方法。"><span></div>
    <div class="eng">It introduces a spatial <span class="word_hot" title="[ˈpɪrəmɪd]">pyramid</span> pooling layer that is more robust to region size and scale and allows the classification layers to <span class="word_hot" title="[ˌri:ˈju:z]">reuse</span> features computed over feature maps generated at several image resolutions.<span class="chs" title="它引入了一个空间金字塔池化层，该层对区域大小和尺度更鲁棒，并允许分类层重用多个图像分辨率下生成的特征映射上计算的特征。"><span></div>
    <div class="eng">Fast R-CNN [6] extends <span class="word_hot_rare">SPPnet</span> so that it can <span class="word_hot" title="[faɪn tju:n]">fine-tune</span> all layers end-to-end by minimizing a loss for both confidences and <span class="word_hot" title="[baundɪŋ]">bounding</span> box regression, which was first introduced in <span class="word_hot_rare">MultiBox</span> [7] for learning <span class="word_hot_rare">objectness</span>.<span class="chs" title="Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">The second set of approaches improve the quality of proposal generation using deep neural networks.<span class="chs" title="第二套方法使用深度神经网络提高了提出生成的质量。"><span></div>
    <div class="eng">In the most recent works like <span class="word_hot_rare">MultiBox</span> [7,8], the <span class="word_hot" title="[sɪˈlektɪv]">Selective</span> Search region proposals, which are based on low-level image features, are replaced by proposals generated directly from a separate deep neural network.<span class="chs" title="在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。"><span></div>
    <div class="eng">This further improves the detection accuracy but results in a somewhat complex <span class="word_hot" title="['setʌp]">setup</span>, requiring the training of two neural networks with a dependency between them.<span class="chs" title="这进一步提高了检测精度，但是导致了一些复杂的设置，需要训练两个具有依赖关系的神经网络。"><span></div>
    <div class="eng">Faster R-CNN [2] replaces <span class="word_hot" title="[sɪˈlektɪv]">selective</span> search proposals by ones learned from a region proposal network (<span class="word_hot_rare">RPN</span>), and introduces a method to integrate the <span class="word_hot_rare">RPN</span> with Fast R-CNN by <span class="word_hot" title="[ɔ:lˈtɜ:nət]">alternat</span>ing between <span class="word_hot" title="[faɪn tju:n]">fine-tun</span>ing shared convolutional layers and prediction layers for these two networks.<span class="chs" title="Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。"><span></div>
    <div class="eng">This way region proposals are used to pool mid-level features and the final classification step is less expensive.<span class="chs" title="通过这种方式，使用区域提出池化中级特征，并且最后的分类步骤比较便宜。"><span></div>
    <div class="eng">Our <span class="word_hot_rare">SSD</span> is very similar to the region proposal network (<span class="word_hot_rare">RPN</span>) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the <span class="word_hot_rare">RPN</span>.<span class="chs" title="我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。"><span></div>
    <div class="eng">But instead of using these to pool features and evaluate another classifier, we simultaneously produce a score for each object category in each box.<span class="chs" title="但是，我们不是使用这些来池化特征并评估另一个分类器，而是为每个目标类别在每个边界框中同时生成一个分数。"><span></div>
    <div class="eng">Thus, our approach avoids the <span class="word_hot" title="[ˌkɒmplɪˈkeɪʃn]">complication</span> of merging <span class="word_hot_rare">RPN</span> with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.<span class="chs" title="因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes and confidences for multiple categories directly.<span class="chs" title="与我们的方法直接相关的另一组方法，完全跳过提出步骤，直接预测多个类别的边界框和置信度。"><span></div>
    <div class="eng"><span class="word_hot_rare">OverFeat</span> [4], a deep version of the sliding window method, predicts a <span class="word_hot" title="[baundɪŋ]">bounding</span> box directly from each location of the <span class="word_hot" title="[ˈtɒpməʊst]">topmost</span> feature map after knowing the confidences of the underlying object categories.<span class="chs" title="OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。"><span></div>
    <div class="eng"><span class="word_hot_rare">YOLO</span> [5] uses the whole <span class="word_hot" title="[ˈtɒpməʊst]">topmost</span> feature map to predict both confidences for multiple categories and <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes (which are shared for these categories).<span class="chs" title="YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。"><span></div>
    <div class="eng">Our <span class="word_hot_rare">SSD</span> method falls in this category because we do not have the proposal step but use the default boxes.<span class="chs" title="我们的SSD方法属于这一类，因为我们没有提出步骤，但使用默认边界框。"><span></div>
    <div class="eng">However, our approach is more flexible than the existing methods because we can use default boxes of different aspect ratios on each feature location from multiple feature maps at different scales.<span class="chs" title="然而，我们的方法比现有方法更灵活，因为我们可以在不同尺度的多个特征映射的每个特征位置上使用不同长宽比的默认边界框。"><span></div>
    <div class="eng">If we only use one default box per location from the <span class="word_hot" title="[ˈtɒpməʊst]">topmost</span> feature map, our <span class="word_hot_rare">SSD</span> would have similar architecture to <span class="word_hot_rare">OverFeat</span> [4]; if we use the whole <span class="word_hot" title="[ˈtɒpməʊst]">topmost</span> feature map and add a fully connected layer for predictions instead of our convolutional <span class="word_hot" title="[prɪˈdɪktə(r)]">predictor</span>s, and do not explicitly consider multiple aspect ratios, we can approximately reproduce <span class="word_hot_rare">YOLO</span> [5].<span class="chs" title="如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">5. Conclusions<span class="chs" title="5. 结论"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">This paper introduces <span class="word_hot_rare">SSD</span>, a fast <span class="word_hot" title="[ˈsiŋɡl ʃɔt]">single-shot</span> object detector for multiple categories.<span class="chs" title="本文介绍了SSD，一种快速的单次多类别目标检测器。"><span></div>
    <div class="eng">A key feature of our model is the use of multi-scale convolutional <span class="word_hot" title="[baundɪŋ]">bounding</span> box outputs attached to multiple feature maps at the top of the network.<span class="chs" title="我们模型的一个关键特性是使用网络顶部多个特征映射的多尺度卷积边界框输出。"><span></div>
    <div class="eng">This representation allows us to efficiently model the space of possible box shapes.<span class="chs" title="这种表示使我们能够高效地建模可能的边界框形状空间。"><span></div>
    <div class="eng">We <span class="word_hot" title="[ɪkˌsperɪ'mentəlɪ]">experimentally</span> <span class="word_hot" title="[ˈvælɪdeɪt]">validate</span> that given appropriate training strategies, a larger number of carefully chosen default <span class="word_hot" title="[baundɪŋ]">bounding</span> boxes results in improved performance.<span class="chs" title="我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。"><span></div>
    <div class="eng">We build <span class="word_hot_rare">SSD</span> models with at least an order of magnitude more box predictions sampling location, scale, and aspect ratio, than existing methods [5,7].<span class="chs" title="我们构建的SSD模型比现有的方法至少要多一个数量级的边界框预测采样位置，尺度和长宽比[5,7]。"><span></div>
    <div class="eng">We demonstrate that given the same <span class="word_hot_rare">VGG</span>-16 base architecture, <span class="word_hot_rare">SSD</span> compares <span class="word_hot" title="['feɪvərəblɪ]">favorably</span> to its state-of-the-art object detector counterparts in terms of both accuracy and speed.<span class="chs" title="我们证明了给定相同的VGG-16基础架构，SSD在准确性和速度方面与其对应的最先进的目标检测器相比毫不逊色。"><span></div>
    <div class="eng">Our <span class="word_hot_rare">SSD</span>512 model significantly <span class="word_hot" title="[ˌaʊtpəˈfɔ:m]">outperform</span>s the state-of-the-art Faster R-CNN [2] in terms of accuracy on <span class="word_hot" title="['pæskәl]">PASCAL</span> <span class="word_hot_rare">VOC</span> and <span class="word_hot" title="['kəʊkəʊ]">COCO</span>, while being 3× faster.<span class="chs" title="在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。"><span></div>
    <div class="eng">Our real time <span class="word_hot_rare">SSD</span>300 model runs at 59 <span class="word_hot" title="['efp'i:'es]">FPS</span>, which is faster than the current real time <span class="word_hot_rare">YOLO</span> [5] alternative, while producing markedly superior detection accuracy.<span class="chs" title="我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">Apart from its <span class="word_hot" title="['stændəˌləʊn]">standalone</span> utility, we believe that our <span class="word_hot" title="[ˌmɒnə'lɪθɪk]">monolithic</span> and relatively simple <span class="word_hot_rare">SSD</span> model provides a useful building block for larger systems that employ an object detection component.<span class="chs" title="除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。"><span></div>
    <div class="eng">A promising future direction is to explore its use as part of a system using <span class="word_hot" title="[rɪˈkʌrənt]">recurrent</span> neural networks to detect and track objects in video simultaneously.<span class="chs" title="一个有前景的未来方向是探索它作为系统的一部分，使用循环神经网络来同时检测和跟踪视频中的目标。"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">6. <span class="word_hot" title="[ək'nɒlɪdʒmənt]">Acknowledgment</span><span class="chs" title="6. 致谢"><span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="eng">This work was started as an <span class="word_hot" title="[ˈɪntɜ:nʃɪp]">internship</span> project at Google and continued at <span class="word_hot" title="[ʌŋk]">UNC</span>.<span class="chs" title="这项工作是在谷歌的一个实习项目开始的，并在UNC继续。"><span></div>
    <div class="eng">We would like to thank <span class="word_hot" title="['ælɪkʃ]">Alex</span> <span class="word_hot_rare">Toshev</span> for helpful discussions and are <span class="word_hot" title="[ɪnˈdetɪd]">indebted</span> to the Image Understanding and <span class="word_hot_rare">DistBelief</span> teams at Google.<span class="chs" title="我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。"><span></div>
    <div class="eng">We also thank Philip <span class="word_hot_rare">Ammirato</span> and <span class="word_hot" title="[ˈpætrik]">Patrick</span> <span class="word_hot_rare">Poirson</span> for helpful comments.<span class="chs" title="我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。"><span></div>
    <div class="eng">We thank <span class="word_hot" title="[ɪn'vɪdɪə]">NVIDIA</span> for providing GPUs and acknowledge support from <span class="word_hot_rare">NSF</span> 1452851, 1446631, 1526367, 1533771.<span class="chs" title="我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。"><span></div>
</div>
    <br>

</div>
<div class="panel">
<div class="panel-btn" title="切换展开中文" onclick="display_chs()">➽</div>
<div class="panel-btn" title="切换按句显示" onclick="display_lines()">✿</div>
</div>
</body>

<script type="text/javascript">
font_default_color = "#666"
function display_chs(){  
    var $cnLines = document.getElementsByClassName("chs");
    for (var i = 0; i < $cnLines.length; i++) {
        var line = $cnLines[i];
        if(line.title) { line.innerHTML = line.title; line.removeAttribute("title"); line.className="chs off"; }
        else if(line.innerText!="") {line.title=line.innerHTML; line.innerHTML = ""; line.className="chs on"; }
        line.style.color=font_default_color;
    }
}
function display_lines(){  
    var $cnLines = document.getElementsByClassName("eng");
    for (var i = $cnLines.length - 1; i >= 0; i--) {
        if ($cnLines[i].className == "eng line")
            $cnLines[i].className = "eng para";
        else
            $cnLines[i].className = "eng line";
    }
}
var handler = function(event) {
    line = event.target;
    switch (event.type){
        case "click":
            var txt = "";
            if (window.getSelection) { txt = window.getSelection().toString();}
            else if (window.document.getSelection) { txt = window.document.getSelection().toString();}
            else if (window.document.selection) { txt = window.document.selection.createRange().text;}
            if (txt != '')    break;
            if(line.title) { line.innerHTML = line.title; line.removeAttribute("title"); line.className="chs off"; line.style.color=font_default_color }
            else {line.title=line.innerHTML;
                line.innerHTML = "";
                line.className="chs on";
                line.style.color="red";}
            break;
        case "mouseover":
            if(line.title)    line.style.color="red";
            break;
        case "mouseout":
            if(line.title)    line.style.color=font_default_color
            break;
    }
}
var $cnLines = document.getElementsByClassName("chs");
if ($cnLines.length==0){
   var btns = document.getElementsByClassName("panel")[0];
    btns.removeChild(btns.children[0])
} else
for (var i = 0; i < $cnLines.length; i++) {
    var line = $cnLines[i];
    line.style.marginLeft="0.5em";line.style.marginRight="0.5em";
    if (line.title=="") {line.className="chs off"; continue;}
    line.style.fontSize = "90%"; line.style.color=font_default_color;
    line.addEventListener("click",handler,false);
    line.addEventListener("mouseover",handler,false);
    line.addEventListener("mouseout",handler,false);
}
</script>
</html>