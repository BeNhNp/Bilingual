<html>
<head>
<meta charset="utf-8">
<title> SSD - Single Shot MultiBox Detector </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> MultiBox </td> <td>  </td> <td> 
<ul><li>SSD: Single Shot <font color=forestgreen>MultiBox</font> Detector<span style="font-size:80%;opacity:0.8"> SSD：单发多盒检测器</span></li><li>Some version of this is also required for training in YOLO[5] and for the region proposal stage of Faster R-CNN[2] and <font color=forestgreen>MultiBox</font>[7].<span style="font-size:80%;opacity:0.8"> 在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。</span></li><li>We begin by matching each ground truth box to the default box with the best jaccard overlap (as in <font color=forestgreen>MultiBox</font> [7]).<span style="font-size:80%;opacity:0.8"> 我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。</span></li><li>Unlike <font color=forestgreen>MultiBox</font>, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5).<span style="font-size:80%;opacity:0.8"> 与MultiBox不同的是，我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框。</span></li><li>The SSD training objective is derived from the <font color=forestgreen>MultiBox</font> objective[7,8] but is extended to handle multiple object categories.<span style="font-size:80%;opacity:0.8"> SSD训练目标函数来自于MultiBox目标[7,8]，但扩展到处理多个目标类别。</span></li><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in <font color=forestgreen>MultiBox</font> [7] for learning objectness.<span style="font-size:80%;opacity:0.8"> Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li><li>In the most recent works like <font color=forestgreen>MultiBox</font> [7,8], the Selective Search region proposals, which are based on low-level image features, are replaced by proposals generated directly from a separate deep neural network.<span style="font-size:80%;opacity:0.8"> 在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> discretize </td> <td> ['diskri:taiz] </td> <td> 
<ul><li>Our approach, named SSD, <font color=orangered>discretizes</font> the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.<span style="font-size:80%;opacity:0.8"> 我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。</span></li><li>Allowing different default box shapes in several feature maps let us efficiently <font color=orangered>discretize</font> the space of possible output box shapes.<span style="font-size:80%;opacity:0.8"> 在几个特征映射中允许不同的默认边界框形状让我们有效地离散可能的输出框形状的空间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>Our approach, named SSD, discretizes the output space of <font color=orangered>bounding</font> boxes into a set of default boxes over different aspect ratios and scales per feature map location.<span style="font-size:80%;opacity:0.8"> 我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。</span></li><li>Current state-of-the-art object detection systems are variants of the following approach: hypothesize <font color=orangered>bounding</font> boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8"> 目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li><li>This paper presents the first deep network based object detector that does not resample pixels or features for <font color=orangered>bounding</font> box hypotheses and and is as accurate as approaches that do.<span style="font-size:80%;opacity:0.8"> 本文提出了第一个基于深度网络的目标检测器，它不对边界框假设的像素或特征进行重采样，并且与其它方法有一样精确度。</span></li><li>The fundamental improvement in speed comes from eliminating <font color=orangered>bounding</font> box proposals and the subsequent pixel or feature resampling stage.<span style="font-size:80%;opacity:0.8"> 速度的根本改进来自消除边界框提出和随后的像素或特征重采样阶段。</span></li><li>Our improvements include using a small convolutional filter to predict object categories and offsets in <font color=orangered>bounding</font> box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.<span style="font-size:80%;opacity:0.8"> 我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。</span></li><li>The core of SSD is predicting category scores and box offsets for a fixed set of default <font color=orangered>bounding</font> boxes using small convolutional filters applied to feature maps.<span style="font-size:80%;opacity:0.8"> SSD的核心是预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上。</span></li><li>The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of <font color=orangered>bounding</font> boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections.<span style="font-size:80%;opacity:0.8"> SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li><li>The <font color=orangered>bounding</font> box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8"> 边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li><li>Default boxes and aspect ratios We associate a set of default <font color=orangered>bounding</font> boxes with each feature map cell, for multiple feature maps at the top of the network.<span style="font-size:80%;opacity:0.8"> 默认边界框和长宽比。对于网络顶部的多个特征映射，我们将一组默认边界框与每个特征映射单元相关联。</span></li><li>Similar to Faster R-CNN[2], we regress to offsets for the center (cx, cy) of the default <font color=orangered>bounding</font> box (d) and for its width (w) and height (h).<span style="font-size:80%;opacity:0.8"> 类似于Faster R-CNN[2]，我们回归默认边界框(d)的中心偏移量(cx, cy)和其宽度(w)、高度(h)的偏移量。</span></li><li>Figure 4 shows that SSD is very sensitive to the <font color=orangered>bounding</font> box size.<span style="font-size:80%;opacity:0.8"> 图4显示SSD对边界框大小非常敏感。</span></li><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and <font color=orangered>bounding</font> box regression, which was first introduced in MultiBox [7] for learning objectness.<span style="font-size:80%;opacity:0.8"> Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li><li>Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict <font color=orangered>bounding</font> boxes and confidences for multiple categories directly.<span style="font-size:80%;opacity:0.8"> 与我们的方法直接相关的另一组方法，完全跳过提出步骤，直接预测多个类别的边界框和置信度。</span></li><li>OverFeat [4], a deep version of the sliding window method, predicts a <font color=orangered>bounding</font> box directly from each location of the topmost feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8"> OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>YOLO [5] uses the whole topmost feature map to predict both confidences for multiple categories and <font color=orangered>bounding</font> boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8"> YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>A key feature of our model is the use of multi-scale convolutional <font color=orangered>bounding</font> box outputs attached to multiple feature maps at the top of the network.<span style="font-size:80%;opacity:0.8"> 我们模型的一个关键特性是使用网络顶部多个特征映射的多尺度卷积边界框输出。</span></li><li>We experimentally validate that given appropriate training strategies, a larger number of carefully chosen default <font color=orangered>bounding</font> boxes results in improved performance.<span style="font-size:80%;opacity:0.8"> 我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> Additionally </td> <td> [ə'dɪʃənəlɪ] </td> <td> 
<ul><li><font color=orangered>Additionally</font>, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.<span style="font-size:80%;opacity:0.8"> 此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> encapsulate </td> <td> [ɪnˈkæpsjuleɪt] </td> <td> 
<ul><li>SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and <font color=orangered>encapsulates</font> all computation in a single network.<span style="font-size:80%;opacity:0.8"> 相对于需要目标提出的方法，SSD非常简单，因为它完全消除了提出生成和随后的像素或特征重新采样阶段，并将所有计算封装到单个网络中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> Pascal </td> <td> ['pæskәl] </td> <td> 
<ul><li>Experimental results on the <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.<span style="font-size:80%;opacity:0.8"> PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。</span></li><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8"> 自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for <font color=orangered>PASCAL</font> VOC from $63.4\%$ mAP for YOLO to $74.3\%$ mAP for our SSD.<span style="font-size:80%;opacity:0.8"> 虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。</span></li><li>Experiments include timing and accuracy analysis on models with varying input size evaluated on <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.<span style="font-size:80%;opacity:0.8"> 实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。</span></li><li>3.1 <font color=orangered>PASCAL</font> VOC2007<span style="font-size:80%;opacity:0.8"> 3.1 PASCAL VOC2007</span></li><li>Table 1: <font color=orangered>PASCAL</font> VOC2007 test detection results.<span style="font-size:80%;opacity:0.8"> 表1：PASCAL VOC2007 test检测结果。</span></li><li>3.3 <font color=orangered>PASCAL</font> VOC2012<span style="font-size:80%;opacity:0.8"> 3.3 PASCAL VOC2012</span></li><li>Table 4: <font color=orangered>PASCAL</font> VOC2012 test detection results.<span style="font-size:80%;opacity:0.8"> 表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 48。</span></li><li>Since objects in COCO tend to be smaller than <font color=orangered>PASCAL</font> VOC, we use smaller default boxes for all layers.<span style="font-size:80%;opacity:0.8"> 由于COCO中的目标往往比PASCAL VOC中的更小，因此我们对所有层使用较小的默认边界框。</span></li><li>Similar to what we observed on the <font color=orangered>PASCAL</font> VOC dataset, SSD300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].<span style="font-size:80%;opacity:0.8"> 与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。</span></li><li>The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as <font color=orangered>PASCAL</font> VOC.<span style="font-size:80%;opacity:0.8"> 2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。</span></li><li>Table 7: Results on <font color=orangered>Pascal</font> VOC2007 test.<span style="font-size:80%;opacity:0.8"> 表7：Pascal VOC2007 test上的结果。</span></li><li>Our SSD512 model significantly outperforms the state-of-the-art Faster R-CNN [2] in terms of accuracy on <font color=orangered>PASCAL</font> VOC and COCO, while being 3× faster.<span style="font-size:80%;opacity:0.8"> 在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> ILSVRC </td> <td> [!≈ aɪ el es vi: ɑ:(r) si:] </td> <td> 
<ul><li>Experimental results on the PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.<span style="font-size:80%;opacity:0.8"> PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。</span></li><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8"> 自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> and are compared to a range of recent state-of-the-art approaches.<span style="font-size:80%;opacity:0.8"> 实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。</span></li><li>Base network Our experiments are all based on VGG16[15], which is pre-trained on the <font color=orangered>ILSVRC</font> CLS-LOC dataset[16].<span style="font-size:80%;opacity:0.8"> 基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li><li>3.5 Preliminary <font color=orangered>ILSVRC</font> results<span style="font-size:80%;opacity:0.8"> 3.5 初步的ILSVRC结果</span></li><li>We applied the same network architecture we used for COCO to the <font color=orangered>ILSVRC</font> DET dataset [16].<span style="font-size:80%;opacity:0.8"> 我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> VOC2007 </td> <td>  </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on <font color=forestgreen>VOC2007</font> test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8"> 对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on <font color=forestgreen>VOC2007</font> test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8"> 这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>SSD with a 300 × 300 input size significantly outperforms its 448 × 448 YOLO counterpart in accuracy on <font color=forestgreen>VOC2007</font> test while also improving the speed.<span style="font-size:80%;opacity:0.8"> 300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。</span></li><li>3.1 PASCAL <font color=forestgreen>VOC2007</font><span style="font-size:80%;opacity:0.8"> 3.1 PASCAL VOC2007</span></li><li>On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on <font color=forestgreen>VOC2007</font> test (4952 images).<span style="font-size:80%;opacity:0.8"> 在这个数据集上，我们在VOC2007 test（4952张图像）上比较了Fast R-CNN[6]和FAST R-CNN[2]。</span></li><li>When training on <font color=forestgreen>VOC2007</font> $\texttt{trainval}$, Table 1 shows that our low resolution SSD300 model is already more accurate than Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 当对VOC2007 $\texttt{trainval}$进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。</span></li><li>Table 1: PASCAL <font color=forestgreen>VOC2007</font> test detection results.<span style="font-size:80%;opacity:0.8"> 表1：PASCAL VOC2007 test检测结果。</span></li><li>Data: ”07”: <font color=forestgreen>VOC2007</font> trainval, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of <font color=forestgreen>VOC2007</font> and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Fig. 3: Visualization of performance for SSD512 on animals, vehicles, and furniture from <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8"> 图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li><li>Fig. 4: Sensitivity and impact of different object characteristics on <font color=forestgreen>VOC2007</font> test set using [21].<span style="font-size:80%;opacity:0.8"> 图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。</span></li><li>We use the same settings as those used for our basic <font color=forestgreen>VOC2007</font> experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and <font color=forestgreen>VOC2007</font> trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We see the same performance trend as we observed on <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8"> 我们看到了与我们在VOC2007 test中观察到的相同的性能趋势。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of <font color=forestgreen>VOC2007</font> trainval and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8"> 数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Fig. 6: Sensitivity and impact of object size with new data augmentation on <font color=forestgreen>VOC2007</font> test set using [21].<span style="font-size:80%;opacity:0.8"> 图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li><li>Table 7: Results on Pascal <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8"> 表7：Pascal VOC2007 test上的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> FPS </td> <td> ['efp'i:'es] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 <font color=orangered>FPS</font> on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8"> 对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>Often detection speed for these approaches is measured in seconds per frame (SPF), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (<font color=orangered>FPS</font>).<span style="font-size:80%;opacity:0.8"> 通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 <font color=orangered>FPS</font> with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8"> 这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 <font color=orangered>FPS</font> with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8"> 这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 <font color=orangered>FPS</font> with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8"> 这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>Although Fast YOLO[5] can run at 155 <font color=orangered>FPS</font>, it has lower accuracy by almost $22\%$ mAP.<span style="font-size:80%;opacity:0.8"> 虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。</span></li><li>Our real time SSD300 model runs at 59 <font color=orangered>FPS</font>, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8"> 我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> Nvidia </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a <font color=orangered>Nvidia</font> Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8"> 对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>We thank <font color=orangered>NVIDIA</font> for providing GPUs and acknowledge support from NSF 1452851, 1446631, 1526367, 1533771.<span style="font-size:80%;opacity:0.8"> 我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> Titan </td> <td> [ˈtaɪtn] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a Nvidia <font color=orangered>Titan</font> X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8"> 对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>We measure the speed with batch size 8 using <font color=orangered>Titan</font> X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8"> 我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> comparable </td> <td> [ˈkɒmpərəbl] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a <font color=orangered>comparable</font> state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8"> 对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>The SSD architecture combines predictions from feature maps of various resolutions to achieve <font color=orangered>comparable</font> accuracy to Faster R-CNN, while using lower resolution input images.<span style="font-size:80%;opacity:0.8"> SSD架构将来自各种分辨率的特征映射的预测结合起来，以达到与Faster R-CNN相当的精确度，同时使用较低分辨率的输入图像。</span></li><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and Selective Search [1] —— had <font color=orangered>comparable</font> performance.<span style="font-size:80%;opacity:0.8"> 在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> variant </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Current state-of-the-art object detection systems are <font color=orangered>variants</font> of the following approach: hypothesize bounding boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8"> 目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> hypothesize </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>Current state-of-the-art object detection systems are variants of the following approach: <font color=orangered>hypothesize</font> bounding boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8"> 目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> prevail </td> <td> [prɪˈveɪl] </td> <td> 
<ul><li>This pipeline has <font color=orangered>prevailed</font> on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8"> 自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> selective </td> <td> [sɪˈlektɪv] </td> <td> 
<ul><li>This pipeline has prevailed on detection benchmarks since the <font color=orangered>Selective</font> Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8"> 自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and <font color=orangered>Selective</font> Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8"> 在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li><li>However, after the dramatic improvement brought on by R-CNN [22], which combines <font color=orangered>selective</font> search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent.<span style="font-size:80%;opacity:0.8"> 然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。</span></li><li>In the most recent works like MultiBox [7,8], the <font color=orangered>Selective</font> Search region proposals, which are based on low-level image features, are replaced by proposals generated directly from a separate deep neural network.<span style="font-size:80%;opacity:0.8"> 在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。</span></li><li>Faster R-CNN [2] replaces <font color=orangered>selective</font> search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8"> Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> albeit </td> <td> [ˌɔ:lˈbi:ɪt] </td> <td> 
<ul><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] <font color=orangered>albeit</font> with deeper features such as [3].<span style="font-size:80%;opacity:0.8"> 自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> computationally </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>While accurate, these approaches have been too <font color=orangered>computationally</font> intensive for embedded systems and, even with high-end hardware, too slow for real-time applications.<span style="font-size:80%;opacity:0.8"> 尽管这些方法准确，但对于嵌入式系统而言，这些方法的计算量过大，即使是高端硬件，对于实时应用而言也太慢。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> SPF </td> <td> [.es piː 'ef] </td> <td> 
<ul><li>Often detection speed for these approaches is measured in seconds per frame (<font color=orangered>SPF</font>), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (FPS).<span style="font-size:80%;opacity:0.8"> 通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> YOLO </td> <td> [!≈ wai əu el əu] </td> <td> 
<ul><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or <font color=orangered>YOLO</font> 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8"> 这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from $63.4\%$ mAP for <font color=orangered>YOLO</font> to $74.3\%$ mAP for our SSD.<span style="font-size:80%;opacity:0.8"> 虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。</span></li><li>We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (<font color=orangered>YOLO</font>), and significantly more accurate,<span style="font-size:80%;opacity:0.8"> 我们引入了SSD，这是一种针对多个类别的单次检测器，比先前的先进的单次检测器（YOLO）更快，并且准确得多，</span></li><li>The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and <font color=orangered>YOLO</font>[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8"> 用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li>The bounding box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of <font color=orangered>YOLO</font>[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8"> 边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li><li>Fig. 2: A comparison between two single shot detection models: SSD and <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8"> 图2：两个单次检测模型的比较：SSD和YOLO[5]。</span></li><li>SSD with a 300 × 300 input size significantly outperforms its 448 × 448 <font color=orangered>YOLO</font> counterpart in accuracy on VOC2007 test while also improving the speed.<span style="font-size:80%;opacity:0.8"> 300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。</span></li><li>Some version of this is also required for training in <font color=orangered>YOLO</font>[5] and for the region proposal stage of Faster R-CNN[2] and MultiBox[7].<span style="font-size:80%;opacity:0.8"> 在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。</span></li><li>We use a more extensive sampling strategy, similar to <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8"> 我们使用更广泛的抽样策略，类似于YOLO[5]。</span></li><li>Compared to <font color=orangered>YOLO</font>, SSD is significantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training.<span style="font-size:80%;opacity:0.8"> 与YOLO相比，SSD更精确，可能是由于使用了来自多个特征映射的卷积默认边界框和我们在训练期间的匹配策略。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for <font color=orangered>YOLO</font> is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8"> 数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Table 7 shows the comparison between SSD, Faster R-CNN[2], and <font color=orangered>YOLO</font>[5].<span style="font-size:80%;opacity:0.8"> 表7显示了SSD，Faster R-CNN[2]和YOLO[5]之间的比较。</span></li><li>Although Fast <font color=orangered>YOLO</font>[5] can run at 155 FPS, it has lower accuracy by almost $22\%$ mAP.<span style="font-size:80%;opacity:0.8"> 虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。</span></li><li><font color=orangered>YOLO</font> [5] uses the whole topmost feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8"> YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8"> 如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li><li>Our real time SSD300 model runs at 59 FPS, which is faster than the current real time <font color=orangered>YOLO</font> [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8"> 我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> cf </td> <td>  </td> <td> 
<ul><li>We are not the first to do this (<font color=forestgreen>cf</font> [4,5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts.<span style="font-size:80%;opacity:0.8"> 我们并不是第一个这样做的人（查阅[4,5]），但是通过增加一系列改进，我们设法比以前的尝试显著提高了准确性。</span></li><li>The convolutional model for predicting detections is different for each feature layer (<font color=forestgreen>cf</font> Overfeat[4] and YOLO[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8"> 用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li>The bounding box offset output values are measured relative to a default box position relative to each feature map location (<font color=forestgreen>cf</font> the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8"> 边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> predictor </td> <td> [prɪˈdɪktə(r)] </td> <td> 
<ul><li>Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate <font color=orangered>predictors</font> (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.<span style="font-size:80%;opacity:0.8"> 我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。</span></li><li>Convolutional <font color=orangered>predictors</font> for detection Each added feature layer (or optionally an existing feature layer from the base network) can produce a fixed set of detection predictions using a set of convolutional filters.<span style="font-size:80%;opacity:0.8"> 用于检测的卷积预测器。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional <font color=orangered>predictors</font>, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8"> 如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> residual </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on <font color=orangered>residual</font> networks [3].<span style="font-size:80%;opacity:0.8"> 相比于最近备受瞩目的残差网络方面的工作[3]，在检测精度上这是相对更大的提高。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> trade-off </td> <td> [ˈtreɪdˌɔ:f, -ˌɔf] </td> <td> 
<ul><li>These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy <font color=orangered>trade-off</font>.<span style="font-size:80%;opacity:0.8"> 这些设计功能使得即使在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> dataset-specific </td> <td> [!≈ 'deɪtəset spəˈsɪfɪk] </td> <td> 
<ul><li>Afterwards, Sec. 2.3 presents <font color=orangered>dataset-specific</font> model details and experimental results.<span style="font-size:80%;opacity:0.8"> 之后，2.3节介绍了数据集特有的模型细节和实验结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> feed-forward </td> <td> ['fi:df'ɔ:wəd] </td> <td> 
<ul><li>The SSD approach is based on a <font color=orangered>feed-forward</font> convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections.<span style="font-size:80%;opacity:0.8"> SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> suppression </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum <font color=orangered>suppression</font> step to produce the final detections.<span style="font-size:80%;opacity:0.8"> SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li><li>Considering the large number of boxes generated from our method, it is essential to perform non-maximum <font color=orangered>suppression</font> (nms) efficiently during inference.<span style="font-size:80%;opacity:0.8"> 考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> truncated </td> <td> ['trʌŋkeɪtɪd] </td> <td> 
<ul><li>The early network layers are based on a standard architecture used for high quality image classification (<font color=orangered>truncated</font> before any classification layers), which we will call the base network.<span style="font-size:80%;opacity:0.8"> 早期的网络层基于用于高质量图像分类的标准架构（在任何分类层之前被截断），我们将其称为基础网络。</span></li><li>Multi-scale feature maps for detection We add convolutional feature layers to the end of the <font color=orangered>truncated</font> base network.<span style="font-size:80%;opacity:0.8"> 用于检测的多尺度特征映射。我们将卷积特征层添加到截取的基础网络的末端。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> auxiliary </td> <td> [ɔ:gˈzɪliəri] </td> <td> 
<ul><li>We then add <font color=orangered>auxiliary</font> structure to the network to produce detections with the following key features:<span style="font-size:80%;opacity:0.8"> 然后，我们将辅助结构添加到网络中以产生具有以下关键特征的检测：</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> progressively </td> <td> [prəˈgresɪvli] </td> <td> 
<ul><li>These layers decrease in size <font color=orangered>progressively</font> and allow predictions of detections at multiple scales.<span style="font-size:80%;opacity:0.8"> 这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。</span></li><li>To measure the advantage gained, we <font color=orangered>progressively</font> remove layers and compare results.<span style="font-size:80%;opacity:0.8"> 为了衡量所获得的优势，我们逐步删除层并比较结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> Overfeat </td> <td>  </td> <td> 
<ul><li>The convolutional model for predicting detections is different for each feature layer (cf <font color=forestgreen>Overfeat</font>[4] and YOLO[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8"> 用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li><font color=forestgreen>OverFeat</font> [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the topmost feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8"> OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to <font color=forestgreen>OverFeat</font> [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8"> 如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> optionally </td> <td> ['ɒpʃənəlɪ] </td> <td> 
<ul><li>Convolutional predictors for detection Each added feature layer (or <font color=orangered>optionally</font> an existing feature layer from the base network) can produce a fixed set of detection predictions using a set of convolutional filters.<span style="font-size:80%;opacity:0.8"> 用于检测的卷积预测器。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> tile </td> <td> [taɪl] </td> <td> 
<ul><li>The default boxes <font color=orangered>tile</font> the feature map in a convolutional manner, so that the position of each box relative to its corresponding cell is fixed.<span style="font-size:80%;opacity:0.8"> 默认边界框以卷积的方式平铺特征映射，以便每个边界框相对于其对应单元的位置是固定的。</span></li><li>We design the <font color=orangered>tiling</font> of default boxes so that specific feature maps learn to be responsive to particular scales of the objects.<span style="font-size:80%;opacity:0.8"> 我们设计平铺默认边界框，以便特定的特征映射学习响应目标的特定尺度。</span></li><li>How to design the optimal <font color=orangered>tiling</font> is an open question as well.<span style="font-size:80%;opacity:0.8"> 如何设计最佳平铺也是一个悬而未决的问题。</span></li><li>For a fair comparison, every time we remove a layer, we adjust the default box <font color=orangered>tiling</font> to keep the total number of boxes similar to the original (8732).<span style="font-size:80%;opacity:0.8"> 为了公平比较，每次我们删除一层，我们调整默认边界框平铺，以保持类似于最初的边界框的总数（8732）。</span></li><li>We do not exhaustively optimize the <font color=orangered>tiling</font> for each setting.<span style="font-size:80%;opacity:0.8"> 我们没有详尽地优化每个设置的平铺。</span></li><li>An alternative way of improving SSD is to design a better <font color=orangered>tiling</font> of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map.<span style="font-size:80%;opacity:0.8"> 改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> kmn </td> <td>  </td> <td> 
<ul><li>This results in a total of (c+4)k filters that are applied around each location in the feature map, yielding (c+4)<font color=forestgreen>kmn</font> outputs for a $m\times n$ feature map.<span style="font-size:80%;opacity:0.8"> 这导致在特征映射中的每个位置周围应用总共(c+4)k个滤波器，对于$m\times n$的特征映射取得(c+4)kmn个输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> Fig.1. </td> <td>  </td> <td> 
<ul><li>For an illustration of default boxes, please refer to <font color=forestgreen>Fig.1.</font><span style="font-size:80%;opacity:0.8"> 有关默认边界框的说明，请参见图1。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> e.g. </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>(a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (<font color=orangered>e.g.</font> 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 × 8 and 4 × 4 in (b) and (c)).<span style="font-size:80%;opacity:0.8"> 以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。</span></li><li>(a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (<font color=orangered>e.g.</font> 8 × 8 and 4 × 4 in (b) and (c)).<span style="font-size:80%;opacity:0.8"> 以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。</span></li><li>The model loss is a weighted sum between localization loss (<font color=orangered>e.g.</font> Smooth L1 [6]) and confidence loss (e.g. Softmax).<span style="font-size:80%;opacity:0.8"> 模型损失是定位损失（例如，Smooth L1[6]）和置信度损失（例如Softmax）之间的加权和。</span></li><li>The model loss is a weighted sum between localization loss (e.g. Smooth L1 [6]) and confidence loss (<font color=orangered>e.g.</font> Softmax).<span style="font-size:80%;opacity:0.8"> 模型损失是定位损失（例如，Smooth L1[6]）和置信度损失（例如Softmax）之间的加权和。</span></li><li>Increasing the input size (<font color=orangered>e.g.</font> from 300 × 300 to 512 × 512) can help improve detecting small objects, but there is still a lot of room to improve.<span style="font-size:80%;opacity:0.8"> 增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。</span></li><li>For example, it hurts the performance by a large margin if we use very coarse feature maps (<font color=orangered>e.g.</font> conv11_2 (1 × 1) or conv10_2 (3 × 3)).<span style="font-size:80%;opacity:0.8"> 例如，如果我们使用非常粗糙的特征映射（例如conv11_2（1×1）或conv10_2（3×3）），它会大大伤害性能。</span></li><li>We follow the strategy mentioned in Sec. 2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4_3 is 0.07 (<font color=orangered>e.g.</font> 21 pixels for a 300 × 300 image).<span style="font-size:80%;opacity:0.8"> 我们遵循2.2节中提到的策略，但是现在我们最小的默认边界框尺度是0.15而不是0.2，并且conv4_3上的默认边界框尺度是0.07（例如，300×300图像中的21个像素）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> propagation </td> <td> [ˌprɒpə'ɡeɪʃn] </td> <td> 
<ul><li>Once this assignment is determined, the loss function and back <font color=orangered>propagation</font> are applied end-to-end.<span style="font-size:80%;opacity:0.8"> 一旦确定了这个分配，损失函数和反向传播就可以应用端到端了。</span></li><li>Since, as pointed out in [12], conv4_3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back <font color=orangered>propagation</font>.<span style="font-size:80%;opacity:0.8"> 如[12]所指出的，与其它层相比，由于conv4_3具有不同的特征尺度，所以我们使用[12]中引入的L2正则化技术将特征映射中每个位置的特征标准缩放到20，在反向传播过程中学习尺度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> mining </td> <td> [ˈmaɪnɪŋ] </td> <td> 
<ul><li>Training also involves choosing the set of default boxes and scales for detection as well as the hard negative <font color=orangered>mining</font> and data augmentation strategies.<span style="font-size:80%;opacity:0.8"> 训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。</span></li><li>Hard negative <font color=orangered>mining</font> After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large.<span style="font-size:80%;opacity:0.8"> 难例挖掘。在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> augmentation </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>Training also involves choosing the set of default boxes and scales for detection as well as the hard negative mining and data <font color=orangered>augmentation</font> strategies.<span style="font-size:80%;opacity:0.8"> 训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。</span></li><li>Data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8"> 数据增强。</span></li><li>Data <font color=orangered>augmentation</font> is crucial.<span style="font-size:80%;opacity:0.8"> 数据增强至关重要。</span></li><li>3.6 Data <font color=orangered>Augmentation</font> for Small Object Accuracy<span style="font-size:80%;opacity:0.8"> 3.6 为小目标准确率进行数据增强</span></li><li>The data <font color=orangered>augmentation</font> strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC.<span style="font-size:80%;opacity:0.8"> 2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。</span></li><li>Because we have more training images by introducing this new “expansion” data <font color=orangered>augmentation</font> trick, we have to double the training iterations.<span style="font-size:80%;opacity:0.8"> 因为通过引入这个新的“扩展”数据增强技巧，我们有更多的训练图像，所以我们必须将训练迭代次数加倍。</span></li><li>In specific, Figure 6 shows that the new <font color=orangered>augmentation</font> trick significantly improves the performance on small objects.<span style="font-size:80%;opacity:0.8"> 具体来说，图6显示新的增强技巧显著提高了模型在小目标上的性能。</span></li><li>This result underscores the importance of the data <font color=orangered>augmentation</font> strategy for the final model accuracy.<span style="font-size:80%;opacity:0.8"> 这个结果强调了数据增强策略对最终模型精度的重要性。</span></li><li>Table 6: Results on multiple datasets when we add the image expansion data <font color=orangered>augmentation</font> trick.<span style="font-size:80%;opacity:0.8"> 表6：我们使用图像扩展数据增强技巧在多个数据集上的结果。</span></li><li>$SSD300^{*}$ and $SSD512^{*}$ are the models that are trained with the new data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8"> $SSD300^{*}$和$SSD512^{*}$是用新的数据增强训练的模型。</span></li><li>Fig. 6: Sensitivity and impact of object size with new data <font color=orangered>augmentation</font> on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8"> 图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li><li>The top row shows the effects of BBox Area per category for the original SSD300 and SSD512 model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data <font color=orangered>augmentation</font> trick.<span style="font-size:80%;opacity:0.8"> 最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>It is obvious that the new data <font color=orangered>augmentation</font> trick helps detecting small objects significantly.<span style="font-size:80%;opacity:0.8"> 新的数据增强技巧显然有助于显著检测小目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> jaccard </td> <td>  </td> <td> 
<ul><li>We begin by matching each ground truth box to the default box with the best <font color=forestgreen>jaccard</font> overlap (as in MultiBox [7]).<span style="font-size:80%;opacity:0.8"> 我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。</span></li><li>Unlike MultiBox, we then match default boxes to any ground truth with <font color=forestgreen>jaccard</font> overlap higher than a threshold (0.5).<span style="font-size:80%;opacity:0.8"> 与MultiBox不同的是，我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框。</span></li><li>Sample a patch so that the minimum <font color=forestgreen>jaccard</font> overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9.<span style="font-size:80%;opacity:0.8"> 采样一个图像块，使得与目标之间的最小Jaccard重叠为0.1，0.3，0.5，0.7或0.9。</span></li><li>The recall is around 85-90\%, and is much higher with “weak” (0.1 <font color=forestgreen>jaccard</font> overlap) criteria.<span style="font-size:80%;opacity:0.8"> 召回约为85-90\%，而“弱”（0.1 Jaccard重叠）标准则要高得多。</span></li><li>The solid red line reflects the change of recall with strong criteria (0.5 <font color=forestgreen>jaccard</font> overlap) as the number of detections increases.<span style="font-size:80%;opacity:0.8"> 红色的实线表示随着检测次数的增加，强标准（0.5 Jaccard重叠）下的召回变化。</span></li><li>The dashed red line is using the weak criteria (0.1 <font color=forestgreen>jaccard</font> overlap).<span style="font-size:80%;opacity:0.8"> 红色虚线是使用弱标准（0.1 Jaccard重叠）。</span></li><li>We then apply nms with <font color=forestgreen>jaccard</font> overlap of 0.45 per class and keep the top 200 detections per image.<span style="font-size:80%;opacity:0.8"> 然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> indicator </td> <td> [ˈɪndɪkeɪtə(r)] </td> <td> 
<ul><li>Let $x_{ij}^p = \lbrace 1,0 \rbrace$ be an <font color=orangered>indicator</font> for matching the i-th default box to the j-th ground truth box of category p.<span style="font-size:80%;opacity:0.8"> 设$x_{ij}^p = \lbrace 1,0 \rbrace$是第i个默认边界框匹配到类别p的第j个实际边界框的指示器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> regress </td> <td> [rɪˈgres] </td> <td> 
<ul><li>Similar to Faster R-CNN[2], we <font color=orangered>regress</font> to offsets for the center (cx, cy) of the default bounding box (d) and for its width (w) and height (h).<span style="font-size:80%;opacity:0.8"> 类似于Faster R-CNN[2]，我们回归默认边界框(d)的中心偏移量(cx, cy)和其宽度(w)、高度(h)的偏移量。</span></li><li>Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to <font color=orangered>regress</font> the object shape and classify object categories instead of using two decoupled steps.<span style="font-size:80%;opacity:0.8"> 与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> mimic </td> <td> [ˈmɪmɪk] </td> <td> 
<ul><li>However, by utilizing feature maps from several different layers in a single network for prediction we can <font color=orangered>mimic</font> the same effect, while also sharing parameters across all object scales.<span style="font-size:80%;opacity:0.8"> 然而，通过利用单个网络中几个不同层的特征映射进行预测，我们可以模拟相同的效果，同时还可以跨所有目标尺度共享参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> semantic </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>Previous works [10,11] have shown that using feature maps from the lower layers can improve <font color=orangered>semantic</font> segmentation quality because the lower layers capture more fine details of the input objects.<span style="font-size:80%;opacity:0.8"> 以前的工作[10,11]已经表明，使用低层的特征映射可以提高语义分割的质量，因为低层会捕获输入目标的更多细节。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> exemplar </td> <td> [ɪgˈzemplɑ:(r)] </td> <td> 
<ul><li>Figure 1 shows two <font color=orangered>exemplar</font> feature maps (8 × 8 and 4 × 4) which are used in the framework.<span style="font-size:80%;opacity:0.8"> 图1显示了框架中使用的两个示例性特征映射（8×8和4×4）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> empirical </td> <td> [ɪmˈpɪrɪkl] </td> <td> 
<ul><li>Feature maps from different levels within a network are known to have different (<font color=orangered>empirical</font>) receptive field sizes [13].<span style="font-size:80%;opacity:0.8"> 已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> receptive </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>Feature maps from different levels within a network are known to have different (empirical) <font color=orangered>receptive</font> field sizes [13].<span style="font-size:80%;opacity:0.8"> 已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。</span></li><li>Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual <font color=orangered>receptive</font> fields of each layer.<span style="font-size:80%;opacity:0.8"> 幸运的是，在SSD框架内，默认边界框不需要对应于每层的实际感受野。</span></li><li>An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the <font color=orangered>receptive</font> field of each position on a feature map.<span style="font-size:80%;opacity:0.8"> 改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> imbalance </td> <td> [ɪmˈbæləns] </td> <td> 
<ul><li>This introduces a significant <font color=orangered>imbalance</font> between the positive and negative training examples.<span style="font-size:80%;opacity:0.8"> 这在正的训练实例和负的训练实例之间引入了显著的不平衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> aforementioned </td> <td> [əˌfɔ:ˈmenʃənd] </td> <td> 
<ul><li>After the <font color=orangered>aforementioned</font> sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8"> 在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> resize </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is <font color=orangered>resized</font> to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8"> 在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> horizontally </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is <font color=orangered>horizontally</font> flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8"> 在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> flip </td> <td> [flɪp] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally <font color=orangered>flipped</font> with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8"> 在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li><li>Fast and Faster R-CNN use the original image and the horizontal <font color=orangered>flip</font> to train.<span style="font-size:80%;opacity:0.8"> Fast和Faster R-CNN使用原始图像和水平翻转来训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> photo-metric </td> <td> [!≈ ˈfəʊtəʊ ˈmetrɪk] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some <font color=orangered>photo-metric</font> distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8"> 在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> VGG16[15 </td> <td>  </td> <td> 
<ul><li>Base network Our experiments are all based on <font color=forestgreen>VGG16[15</font>], which is pre-trained on the ILSVRC CLS-LOC dataset[16].<span style="font-size:80%;opacity:0.8"> 基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> CLS-LOC </td> <td>  </td> <td> 
<ul><li>Base network Our experiments are all based on VGG16[15], which is pre-trained on the ILSVRC <font color=forestgreen>CLS-LOC</font> dataset[16].<span style="font-size:80%;opacity:0.8"> 基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> DeepLab-LargeFOV </td> <td>  </td> <td> 
<ul><li>Similar to <font color=forestgreen>DeepLab-LargeFOV</font>[17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the atrous algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8"> 类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li><li>As described in Sec. 3, we used the atrous version of a subsampled VGG16, following <font color=forestgreen>DeepLab-LargeFOV</font> [17].<span style="font-size:80%;opacity:0.8"> 如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> subsample </td> <td> ['sʌbsɑ:mpl] </td> <td> 
<ul><li>Similar to DeepLab-LargeFOV[17], we convert fc6 and fc7 to convolutional layers, <font color=orangered>subsample</font> parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the atrous algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8"> 类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> atrous </td> <td> ['eitrәs] </td> <td> 
<ul><li>Similar to DeepLab-LargeFOV[17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the <font color=orangered>atrous</font> algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8"> 类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li><li><font color=orangered>Atrous</font> is faster.<span style="font-size:80%;opacity:0.8"> Atrous更快。</span></li><li>As described in Sec. 3, we used the <font color=orangered>atrous</font> version of a subsampled VGG16, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8"> 如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>We fine-tune the resulting model using <font color=orangered>SGD</font> with initial learning rate $10^{-3}$, 0.9 momentum, 0.0005 weight decay, and batch size 32.<span style="font-size:80%;opacity:0.8"> 我们使用SGD对得到的模型进行微调，初始学习率为$10^{-3}$，动量为0.9，权重衰减为0.0005，批数据大小为32。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We fine-tune the resulting model using SGD with initial learning rate $10^{-3}$, 0.9 <font color=orangered>momentum</font>, 0.0005 weight decay, and batch size 32.<span style="font-size:80%;opacity:0.8"> 我们使用SGD对得到的模型进行微调，初始学习率为$10^{-3}$，动量为0.9，权重衰减为0.0005，批数据大小为32。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> Caffe </td> <td>  </td> <td> 
<ul><li>The full training and testing code is built on <font color=forestgreen>Caffe</font>[19] and is open source at: https://github.com/weiliu89/caffe/tree/ssd.<span style="font-size:80%;opacity:0.8"> 完整的训练和测试代码建立在Caffe[19]上并开源：https://github.com/weiliu89/caffe/tree/ssd。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> VGG16 </td> <td>  </td> <td> 
<ul><li>All methods fine-tune on the same pre-trained <font color=forestgreen>VGG16</font> network.<span style="font-size:80%;opacity:0.8"> 所有的方法都在相同的预训练好的VGG16网络上进行微调。</span></li><li>As described in Sec. 3, we used the atrous version of a subsampled <font color=forestgreen>VGG16</font>, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8"> 如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li><li>If we use the full <font color=forestgreen>VGG16</font>, keeping pool5 with 2×2−s2 and not subsampling parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about $20\%$ slower.<span style="font-size:80%;opacity:0.8"> 如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。</span></li><li>Note that about $80\%$ of the forward time is spent on the base network (<font color=forestgreen>VGG16</font> in our case).<span style="font-size:80%;opacity:0.8"> 请注意，大约80%前馈时间花费在基础网络上（本例中为VGG16）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> SSD300 </td> <td>  </td> <td> 
<ul><li>Figure 2 shows the architecture details of the <font color=forestgreen>SSD300</font> model.<span style="font-size:80%;opacity:0.8"> 图2显示了SSD300模型的架构细节。</span></li><li>When training on VOC2007 $\texttt{trainval}$, Table 1 shows that our low resolution <font color=forestgreen>SSD300</font> model is already more accurate than Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 当对VOC2007 $\texttt{trainval}$进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。</span></li><li>If we train SSD with more (i.e. 07+12) data, we see that <font color=forestgreen>SSD300</font> is already better than Faster R-CNN by 1.1\% and that SSD512 is $3.6\%$ better.<span style="font-size:80%;opacity:0.8"> 如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li><li>Table 4 shows the results of our <font color=forestgreen>SSD300</font> and SSD512 model.<span style="font-size:80%;opacity:0.8"> 表4显示了我们的SSD300和SSD512模型的结果。</span></li><li>Our <font color=forestgreen>SSD300</font> improves accuracy over Fast/Faster R-CNN.<span style="font-size:80%;opacity:0.8"> 我们的SSD300比Fast/Faster R-CNN提高了准确性。</span></li><li>To further validate the SSD framework, we trained our <font color=forestgreen>SSD300</font> and SSD512 architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8"> 为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>Similar to what we observed on the PASCAL VOC dataset, <font color=forestgreen>SSD300</font> is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].<span style="font-size:80%;opacity:0.8"> 与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。</span></li><li><font color=forestgreen>SSD300</font> has a similar mAP@0.75 as ION [24] and Faster R-CNN [25], but is worse in mAP@0.5.<span style="font-size:80%;opacity:0.8"> SSD300与ION 24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。</span></li><li>We train a <font color=forestgreen>SSD300</font> model using the ILSVRC2014 DET train and val1 as used in [22].<span style="font-size:80%;opacity:0.8"> 我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li><li>The top row shows the effects of BBox Area per category for the original <font color=forestgreen>SSD300</font> and SSD512 model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data augmentation trick.<span style="font-size:80%;opacity:0.8"> 最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>This step costs about 1.7 msec per image for <font color=forestgreen>SSD300</font> and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers.<span style="font-size:80%;opacity:0.8"> 对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li><li>Both our <font color=forestgreen>SSD300</font> and SSD512 method outperforms Faster R-CNN in both speed and accuracy.<span style="font-size:80%;opacity:0.8"> 我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。</span></li><li>To the best of our knowledge, <font color=forestgreen>SSD300</font> is the first real-time method to achieve above $70\%$ mAP.<span style="font-size:80%;opacity:0.8"> 就我们所知，SSD300是第一个实现70%以上mAP的实时方法。</span></li><li><font color=forestgreen>SSD300</font> is the only real-time detection method that can achieve above 70\% mAP.<span style="font-size:80%;opacity:0.8"> SSD300是唯一可以取得70\%以上mAP的实现检测方法。</span></li><li>Our real time <font color=forestgreen>SSD300</font> model runs at 59 FPS, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8"> 我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> xavier </td> <td> ['zʌvɪə] </td> <td> 
<ul><li>We initialize the parameters for all the newly added convolutional layers with the “<font color=orangered>xavier</font>” method [20].<span style="font-size:80%;opacity:0.8"> 我们使用“xavier”方法[20]初始化所有新添加的卷积层的参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> surpass </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>When we train SSD on a larger $512\times 512$ input image, it is even more accurate, <font color=orangered>surpassing</font> Faster R-CNN by $1.7\%$ mAP.<span style="font-size:80%;opacity:0.8"> 当我们用更大的$512\times 512$输入图像上训练SSD时，它更加准确，超过了Faster R-CNN $1.7\%$的mAP。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> i.e. </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>If we train SSD with more (<font color=orangered>i.e.</font> 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1\% and that SSD512 is $3.6\%$ better.<span style="font-size:80%;opacity:0.8"> 如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> SSD512 </td> <td>  </td> <td> 
<ul><li>If we train SSD with more (i.e. 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1\% and that <font color=forestgreen>SSD512</font> is $3.6\%$ better.<span style="font-size:80%;opacity:0.8"> 如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li><li>If we take models trained on COCO $\texttt{trainval35k}$ as described in Sec. 3.4 and fine-tuning them on the 07+12 dataset with <font color=forestgreen>SSD512</font>, we achieve the best results: 81.6\% mAP.<span style="font-size:80%;opacity:0.8"> 如果我们将SSD512用3.4节描述的COCO $\texttt{trainval35k}$来训练模型并在07+12数据集上进行微调，我们获得了最好的结果：$81.6\%$的mAP。</span></li><li>Fig. 3: Visualization of performance for <font color=forestgreen>SSD512</font> on animals, vehicles, and furniture from VOC2007 test.<span style="font-size:80%;opacity:0.8"> 图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li><li>Table 4 shows the results of our SSD300 and <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8"> 表4显示了我们的SSD300和SSD512模型的结果。</span></li><li>When fine-tuned from models trained on COCO, our <font color=forestgreen>SSD512</font> achieves $80.0\%$ mAP, which is $4.1\%$ higher than Faster R-CNN.<span style="font-size:80%;opacity:0.8"> 当对从COCO上训练的模型进行微调后，我们的SSD512达到了80.0%的mAP，比Faster R-CNN高了4.1%。</span></li><li>To further validate the SSD framework, we trained our SSD300 and <font color=forestgreen>SSD512</font> architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8"> 为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>By increasing the image size to 512 × 512, our <font color=forestgreen>SSD512</font> is better than Faster R-CNN [25] in both criteria.<span style="font-size:80%;opacity:0.8"> 通过将图像尺寸增加到512×512，我们的SSD512在这两个标准中都优于Faster R-CNN[25]。</span></li><li>Interestingly, we observe that <font color=forestgreen>SSD512</font> is 5.3\% better in mAP@0.75, but is only $1.2\%$ better in mAP@0.5.<span style="font-size:80%;opacity:0.8"> 有趣的是，我们观察到SSD512在mAP@0.75中要好5.3%，但是在mAP@0.5中只好1.2%。</span></li><li>In Fig. 5, we show some detection examples on COCO test-dev with the <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8"> 在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。</span></li><li>Fig. 5: Detection examples on COCO test-dev with <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8"> 图5：SSD512模型在COCO test-dev上的检测实例。</span></li><li>The top row shows the effects of BBox Area per category for the original SSD300 and <font color=forestgreen>SSD512</font> model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data augmentation trick.<span style="font-size:80%;opacity:0.8"> 最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>Both our SSD300 and <font color=forestgreen>SSD512</font> method outperforms Faster R-CNN in both speed and accuracy.<span style="font-size:80%;opacity:0.8"> 我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。</span></li><li>Therefore, using a faster base network could even further improve the speed, which can possibly make the <font color=forestgreen>SSD512</font> model real-time as well.<span style="font-size:80%;opacity:0.8"> 因此，使用更快的基础网络可以进一步提高速度，这也可能使SSD512模型达到实时。</span></li><li>By using a larger input image, <font color=forestgreen>SSD512</font> outperforms all methods on accuracy while maintaining a close to real-time speed.<span style="font-size:80%;opacity:0.8"> 通过使用更大的输入图像，SSD512在精度上超过了所有方法同时保持近似实时的速度。</span></li><li>Our <font color=forestgreen>SSD512</font> model significantly outperforms the state-of-the-art Faster R-CNN [2] in terms of accuracy on PASCAL VOC and COCO, while being 3× faster.<span style="font-size:80%;opacity:0.8"> 在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> trainval </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 <font color=forestgreen>trainval</font>, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 <font color=forestgreen>trainval</font>. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 <font color=forestgreen>trainval</font> and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 <font color=forestgreen>trainval</font> and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 <font color=forestgreen>trainval</font> and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8"> 数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and VOC2012 <font color=forestgreen>trainval</font>.<span style="font-size:80%;opacity:0.8"> 数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> VOC2012 </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and <font color=forestgreen>VOC2012</font> trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>3.3 PASCAL <font color=forestgreen>VOC2012</font><span style="font-size:80%;opacity:0.8"> 3.3 PASCAL VOC2012</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use <font color=forestgreen>VOC2012</font> trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on <font color=forestgreen>VOC2012</font> test (10991 images).<span style="font-size:80%;opacity:0.8"> 除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>Table 4: PASCAL <font color=forestgreen>VOC2012</font> test detection results.<span style="font-size:80%;opacity:0.8"> 表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 48。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and <font color=forestgreen>VOC2012</font> trainval.<span style="font-size:80%;opacity:0.8"> 数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> trainval35k </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO <font color=forestgreen>trainval35k</font> then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8"> “07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>”07++12+COCO”: first train on COCO <font color=forestgreen>trainval35k</font> then fine-tune on 07++12.<span style="font-size:80%;opacity:0.8"> “07++12+COCO”：先在COCO trainval135k上训练然后在07++12上微调。</span></li><li>We use the <font color=forestgreen>trainval35k</font>[24] for training.<span style="font-size:80%;opacity:0.8"> 我们使用trainval35k[24]进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> decouple </td> <td> [di:ˈkʌpl] </td> <td> 
<ul><li>Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two <font color=orangered>decoupled</font> steps.<span style="font-size:80%;opacity:0.8"> 与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> Visualization </td> <td> [ˌvɪʒʊəlaɪ'zeɪʃn] </td> <td> 
<ul><li>Fig. 3: <font color=orangered>Visualization</font> of performance for SSD512 on animals, vehicles, and furniture from VOC2007 test.<span style="font-size:80%;opacity:0.8"> 图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> cumulative </td> <td> [ˈkju:mjələtɪv] </td> <td> 
<ul><li>The top row shows the <font color=orangered>cumulative</font> fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8"> 第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> Cor </td> <td> [kɔ:(r)] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (<font color=orangered>Cor</font>) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8"> 第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> Sim </td> <td> [sɪm] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (<font color=orangered>Sim</font>), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8"> 第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> Oth </td> <td>  </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (<font color=forestgreen>Oth</font>), or with background (BG).<span style="font-size:80%;opacity:0.8"> 第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> BG </td> <td> [!≈ bi: dʒi:] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (<font color=orangered>BG</font>).<span style="font-size:80%;opacity:0.8"> 第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> dash </td> <td> [dæʃ] </td> <td> 
<ul><li>The <font color=orangered>dashed</font> red line is using the weak criteria (0.1 jaccard overlap).<span style="font-size:80%;opacity:0.8"> 红色虚线是使用弱标准（0.1 Jaccard重叠）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> top-ranked </td> <td> ['tɒpr'æŋkt] </td> <td> 
<ul><li>The bottom row shows the distribution of <font color=orangered>top-ranked</font> false positive types.<span style="font-size:80%;opacity:0.8"> 最下面一行显示了排名靠前的假阳性类型的分布。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> Sensitivity </td> <td> [ˌsensəˈtɪvəti] </td> <td> 
<ul><li>Fig. 4: <font color=orangered>Sensitivity</font> and impact of different object characteristics on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8"> 图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。</span></li><li>Fig. 6: <font color=orangered>Sensitivity</font> and impact of object size with new data augmentation on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8"> 图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> XW </td> <td> [!≈ eks 'dʌblju:] </td> <td> 
<ul><li>Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; <font color=orangered>XW</font> =extra-wide.<span style="font-size:80%;opacity:0.8"> 长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> extra-wide </td> <td> [!≈ ˈekstrə waɪd] </td> <td> 
<ul><li>Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =<font color=orangered>extra-wide</font>.<span style="font-size:80%;opacity:0.8"> 长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> subsampled </td> <td>  </td> <td> 
<ul><li>As described in Sec. 3, we used the atrous version of a <font color=forestgreen>subsampled</font> VGG16, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8"> 如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> subsampling </td> <td>  </td> <td> 
<ul><li>If we use the full VGG16, keeping pool5 with 2×2−s2 and not <font color=forestgreen>subsampling</font> parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about $20\%$ slower.<span style="font-size:80%;opacity:0.8"> 如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> exhaustively </td> <td> [ɪɡ'zɔ:stɪvlɪ] </td> <td> 
<ul><li>We do not <font color=orangered>exhaustively</font> optimize the tiling for each setting.<span style="font-size:80%;opacity:0.8"> 我们没有详尽地优化每个设置的平铺。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> monotonically </td> <td> [mɒnə'tɒnɪklɪ] </td> <td> 
<ul><li>Table 3 shows a decrease in accuracy with fewer layers, dropping <font color=orangered>monotonically</font> from 74.3 to 62.4.<span style="font-size:80%;opacity:0.8"> 表3显示层数较少，精度降低，从74.3单调递减至62.4。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> prune </td> <td> [pru:n] </td> <td> 
<ul><li>The reason might be that we do not have enough large boxes to cover large objects after the <font color=orangered>pruning</font>.<span style="font-size:80%;opacity:0.8"> 原因可能是修剪后我们没有足够大的边界框来覆盖大的目标。</span></li><li>When we use primarily finer resolution maps, the performance starts increasing again because even after <font color=orangered>pruning</font> a sufficient number of large boxes remains.<span style="font-size:80%;opacity:0.8"> 当我们主要使用更高分辨率的特征映射时，性能开始再次上升，因为即使在修剪之后仍然有足够数量的大边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> ROI </td> <td> [rwɑ:] </td> <td> 
<ul><li>Besides, since our predictions do not rely on <font color=orangered>ROI</font> pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23].<span style="font-size:80%;opacity:0.8"> 此外，由于我们的预测不像[6]那样依赖于ROI池化，所以我们在低分辨率特征映射中没有折叠组块的问题[23]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> validate </td> <td> [ˈvælɪdeɪt] </td> <td> 
<ul><li>To further <font color=orangered>validate</font> the SSD framework, we trained our SSD300 and SSD512 architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8"> 为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>Again, it <font color=orangered>validates</font> that SSD is a general framework for high quality real-time detection.<span style="font-size:80%;opacity:0.8"> 再一次证明了SSD是用于高质量实时检测的通用框架。</span></li><li>We experimentally <font color=orangered>validate</font> that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance.<span style="font-size:80%;opacity:0.8"> 我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> test-dev </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Table 5 shows the results on <font color=orangered>test-dev</font>2015.<span style="font-size:80%;opacity:0.8"> 表5显示了test-dev2015的结果。</span></li><li>In Fig. 5, we show some detection examples on COCO <font color=orangered>test-dev</font> with the SSD512 model.<span style="font-size:80%;opacity:0.8"> 在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。</span></li><li>Table 5: COCO <font color=orangered>test-dev</font>2015 detection results.<span style="font-size:80%;opacity:0.8"> 表5：COCO test-dev2015检测结果。</span></li><li>Fig. 5: Detection examples on COCO <font color=orangered>test-dev</font> with SSD512 model.<span style="font-size:80%;opacity:0.8"> 图5：SSD512模型在COCO test-dev上的检测实例。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> ION </td> <td> [ˈaɪən] </td> <td> 
<ul><li>SSD300 has a similar mAP@0.75 as <font color=orangered>ION</font> [24] and Faster R-CNN [25], but is worse in mAP@0.5.<span style="font-size:80%;opacity:0.8"> SSD300与ION 24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。</span></li><li>Compared to <font color=orangered>ION</font>, the improvement in AR for large and small objects is more similar ($5.4\%$ vs.<span style="font-size:80%;opacity:0.8"> 与ION相比，大型和小型目标的AR改进更为相似（5.4%和3.9%）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> conjecture </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>$3.9\%$). We <font color=orangered>conjecture</font> that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the RPN part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8"> 我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> refinement </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>$3.9\%$). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box <font color=orangered>refinement</font> steps, in both the RPN part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8"> 我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> RPN </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>$3.9\%$). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the <font color=orangered>RPN</font> part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8"> 我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li><li>Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (<font color=orangered>RPN</font>), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8"> Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li><li>Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the <font color=orangered>RPN</font> with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8"> Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li><li>Our SSD is very similar to the region proposal network (<font color=orangered>RPN</font>) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the RPN.<span style="font-size:80%;opacity:0.8"> 我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。</span></li><li>Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8"> 我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。</span></li><li>Thus, our approach avoids the complication of merging <font color=orangered>RPN</font> with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.<span style="font-size:80%;opacity:0.8"> 因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> Preliminary </td> <td> [prɪˈlɪmɪnəri] </td> <td> 
<ul><li>3.5 <font color=orangered>Preliminary</font> ILSVRC results<span style="font-size:80%;opacity:0.8"> 3.5 初步的ILSVRC结果</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> DET </td> <td> [!≈ di: i: ti:] </td> <td> 
<ul><li>We applied the same network architecture we used for COCO to the ILSVRC <font color=orangered>DET</font> dataset [16].<span style="font-size:80%;opacity:0.8"> 我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。</span></li><li>We train a SSD300 model using the ILSVRC2014 <font color=orangered>DET</font> train and val1 as used in [22].<span style="font-size:80%;opacity:0.8"> 我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> ILSVRC2014 </td> <td>  </td> <td> 
<ul><li>We train a SSD300 model using the <font color=forestgreen>ILSVRC2014</font> DET train and val1 as used in [22].<span style="font-size:80%;opacity:0.8"> 我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> follow-up </td> <td> ['fɒləʊ ʌp] </td> <td> 
<ul><li>Without a <font color=orangered>follow-up</font> feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4).<span style="font-size:80%;opacity:0.8"> SSD没有如Faster R-CNN中后续的特征重采样步骤，小目标的分类任务对SSD来说相对困难，正如我们的分析（见图4）所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> zoom </td> <td> [zu:m] </td> <td> 
<ul><li>The random crops generated by the strategy can be thought of as a “<font color=orangered>zoom</font> in” operation and can generate many larger training examples.<span style="font-size:80%;opacity:0.8"> 策略产生的随机裁剪可以被认为是“放大”操作，并且可以产生许多更大的训练样本。</span></li><li>To implement a “<font color=orangered>zoom</font> out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation.<span style="font-size:80%;opacity:0.8"> 为了实现创建更多小型训练样本的“缩小”操作，我们首先将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> underscore </td> <td> [ˌʌndəˈskɔ:(r)] </td> <td> 
<ul><li>This result <font color=orangered>underscores</font> the importance of the data augmentation strategy for the final model accuracy.<span style="font-size:80%;opacity:0.8"> 这个结果强调了数据增强策略对最终模型精度的重要性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> align </td> <td> [əˈlaɪn] </td> <td> 
<ul><li>An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better <font color=orangered>aligned</font> with the receptive field of each position on a feature map.<span style="font-size:80%;opacity:0.8"> 改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> nms </td> <td>  </td> <td> 
<ul><li>Considering the large number of boxes generated from our method, it is essential to perform non-maximum suppression (<font color=forestgreen>nms</font>) efficiently during inference.<span style="font-size:80%;opacity:0.8"> 考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。</span></li><li>We then apply <font color=forestgreen>nms</font> with jaccard overlap of 0.45 per class and keep the top 200 detections per image.<span style="font-size:80%;opacity:0.8"> 然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> msec </td> <td> [m'zek] </td> <td> 
<ul><li>This step costs about 1.7 <font color=orangered>msec</font> per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers.<span style="font-size:80%;opacity:0.8"> 对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li><li>This step costs about 1.7 msec per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 <font color=orangered>msec</font>) spent on all newly added layers.<span style="font-size:80%;opacity:0.8"> 对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> cuDNN </td> <td>  </td> <td> 
<ul><li>We measure the speed with batch size 8 using Titan X and <font color=forestgreen>cuDNN</font> v4 with Intel Xeon E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8"> 我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> Xeon </td> <td>  </td> <td> 
<ul><li>We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel <font color=forestgreen>Xeon</font> E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8"> 我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> advent </td> <td> [ˈædvent] </td> <td> 
<ul><li>Before the <font color=orangered>advent</font> of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8"> 在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> Deformable </td> <td> [dɪ'fɔ:məbl] </td> <td> 
<ul><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— <font color=orangered>Deformable</font> Part Model (DPM) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8"> 在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> DPM </td> <td> [!≈ di: pi: em] </td> <td> 
<ul><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (<font color=orangered>DPM</font>) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8"> 在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> prevalent </td> <td> [ˈprevələnt] </td> <td> 
<ul><li>However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became <font color=orangered>prevalent</font>.<span style="font-size:80%;opacity:0.8"> 然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> time-consuming </td> <td> [taɪm kən'sju:mɪŋ] </td> <td> 
<ul><li>The first set of approaches improve the quality and speed of post-classification, since it requires the classification of thousands of image crops, which is expensive and <font color=orangered>time-consuming</font>.<span style="font-size:80%;opacity:0.8"> 第一套方法提高了后分类的质量和速度，因为它需要对成千上万的裁剪图像进行分类，这是昂贵和耗时的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> SPPnet </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SPPnet</font> [9] speeds up the original R-CNN approach significantly.<span style="font-size:80%;opacity:0.8"> SPPnet[9]显著加快了原有的R-CNN方法。</span></li><li>Fast R-CNN [6] extends <font color=forestgreen>SPPnet</font> so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning objectness.<span style="font-size:80%;opacity:0.8"> Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> objectness </td> <td> [!≈ ˈɒbdʒɪktnəs] </td> <td> 
<ul><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning <font color=orangered>objectness</font>.<span style="font-size:80%;opacity:0.8"> Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> setup </td> <td> ['setʌp] </td> <td> 
<ul><li>This further improves the detection accuracy but results in a somewhat complex <font color=orangered>setup</font>, requiring the training of two neural networks with a dependency between them.<span style="font-size:80%;opacity:0.8"> 这进一步提高了检测精度，但是导致了一些复杂的设置，需要训练两个具有依赖关系的神经网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> complication </td> <td> [ˌkɒmplɪˈkeɪʃn] </td> <td> 
<ul><li>Thus, our approach avoids the <font color=orangered>complication</font> of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.<span style="font-size:80%;opacity:0.8"> 因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> topmost </td> <td> [ˈtɒpməʊst] </td> <td> 
<ul><li>OverFeat [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the <font color=orangered>topmost</font> feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8"> OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>YOLO [5] uses the whole <font color=orangered>topmost</font> feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8"> YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>If we only use one default box per location from the <font color=orangered>topmost</font> feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8"> 如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole <font color=orangered>topmost</font> feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8"> 如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> experimentally </td> <td> [ɪkˌsperɪ'mentəlɪ] </td> <td> 
<ul><li>We <font color=orangered>experimentally</font> validate that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance.<span style="font-size:80%;opacity:0.8"> 我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> favorably </td> <td> ['feɪvərəblɪ] </td> <td> 
<ul><li>We demonstrate that given the same VGG-16 base architecture, SSD compares <font color=orangered>favorably</font> to its state-of-the-art object detector counterparts in terms of both accuracy and speed.<span style="font-size:80%;opacity:0.8"> 我们证明了给定相同的VGG-16基础架构，SSD在准确性和速度方面与其对应的最先进的目标检测器相比毫不逊色。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> standalone </td> <td> ['stændəˌləʊn] </td> <td> 
<ul><li>Apart from its <font color=orangered>standalone</font> utility, we believe that our monolithic and relatively simple SSD model provides a useful building block for larger systems that employ an object detection component.<span style="font-size:80%;opacity:0.8"> 除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> monolithic </td> <td> [ˌmɒnə'lɪθɪk] </td> <td> 
<ul><li>Apart from its standalone utility, we believe that our <font color=orangered>monolithic</font> and relatively simple SSD model provides a useful building block for larger systems that employ an object detection component.<span style="font-size:80%;opacity:0.8"> 除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> recurrent </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>A promising future direction is to explore its use as part of a system using <font color=orangered>recurrent</font> neural networks to detect and track objects in video simultaneously.<span style="font-size:80%;opacity:0.8"> 一个有前景的未来方向是探索它作为系统的一部分，使用循环神经网络来同时检测和跟踪视频中的目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> Acknowledgment </td> <td> [ək'nɒlɪdʒmənt] </td> <td> 
<ul><li>6. <font color=orangered>Acknowledgment</font><span style="font-size:80%;opacity:0.8"> 6. 致谢</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> internship </td> <td> [ˈɪntɜ:nʃɪp] </td> <td> 
<ul><li>This work was started as an <font color=orangered>internship</font> project at Google and continued at UNC.<span style="font-size:80%;opacity:0.8"> 这项工作是在谷歌的一个实习项目开始的，并在UNC继续。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> UNC </td> <td> [ʌŋk] </td> <td> 
<ul><li>This work was started as an internship project at Google and continued at <font color=orangered>UNC</font>.<span style="font-size:80%;opacity:0.8"> 这项工作是在谷歌的一个实习项目开始的，并在UNC继续。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> Alex </td> <td> ['ælɪkʃ] </td> <td> 
<ul><li>We would like to thank <font color=orangered>Alex</font> Toshev for helpful discussions and are indebted to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8"> 我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> Toshev </td> <td>  </td> <td> 
<ul><li>We would like to thank Alex <font color=forestgreen>Toshev</font> for helpful discussions and are indebted to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8"> 我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> indebted </td> <td> [ɪnˈdetɪd] </td> <td> 
<ul><li>We would like to thank Alex Toshev for helpful discussions and are <font color=orangered>indebted</font> to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8"> 我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> DistBelief </td> <td>  </td> <td> 
<ul><li>We would like to thank Alex Toshev for helpful discussions and are indebted to the Image Understanding and <font color=forestgreen>DistBelief</font> teams at Google.<span style="font-size:80%;opacity:0.8"> 我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> Ammirato </td> <td>  </td> <td> 
<ul><li>We also thank Philip <font color=forestgreen>Ammirato</font> and Patrick Poirson for helpful comments.<span style="font-size:80%;opacity:0.8"> 我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> Patrick </td> <td> [ˈpætrik] </td> <td> 
<ul><li>We also thank Philip Ammirato and <font color=orangered>Patrick</font> Poirson for helpful comments.<span style="font-size:80%;opacity:0.8"> 我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> Poirson </td> <td>  </td> <td> 
<ul><li>We also thank Philip Ammirato and Patrick <font color=forestgreen>Poirson</font> for helpful comments.<span style="font-size:80%;opacity:0.8"> 我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> NSF </td> <td> [!≈ en es ef] </td> <td> 
<ul><li>We thank NVIDIA for providing GPUs and acknowledge support from <font color=orangered>NSF</font> 1452851, 1446631, 1526367, 1533771.<span style="font-size:80%;opacity:0.8"> 我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> bounding<br>(17) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>Our approach, named SSD, discretizes the output space of <font color=orangered>bounding</font> boxes into a set of default boxes over different aspect ratios and scales per feature map location.<span style="font-size:80%;opacity:0.8">我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。</span></li><li>Current state-of-the-art object detection systems are variants of the following approach: hypothesize <font color=orangered>bounding</font> boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8">目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li><li>This paper presents the first deep network based object detector that does not resample pixels or features for <font color=orangered>bounding</font> box hypotheses and and is as accurate as approaches that do.<span style="font-size:80%;opacity:0.8">本文提出了第一个基于深度网络的目标检测器，它不对边界框假设的像素或特征进行重采样，并且与其它方法有一样精确度。</span></li><li>The fundamental improvement in speed comes from eliminating <font color=orangered>bounding</font> box proposals and the subsequent pixel or feature resampling stage.<span style="font-size:80%;opacity:0.8">速度的根本改进来自消除边界框提出和随后的像素或特征重采样阶段。</span></li><li>Our improvements include using a small convolutional filter to predict object categories and offsets in <font color=orangered>bounding</font> box locations, using separate predictors (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.<span style="font-size:80%;opacity:0.8">我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。</span></li><li>The core of SSD is predicting category scores and box offsets for a fixed set of default <font color=orangered>bounding</font> boxes using small convolutional filters applied to feature maps.<span style="font-size:80%;opacity:0.8">SSD的核心是预测固定的一系列默认边界框的类别分数和边界框偏移，使用更小的卷积滤波器应用到特征映射上。</span></li><li>The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of <font color=orangered>bounding</font> boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections.<span style="font-size:80%;opacity:0.8">SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li><li>The <font color=orangered>bounding</font> box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8">边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li><li>Default boxes and aspect ratios We associate a set of default <font color=orangered>bounding</font> boxes with each feature map cell, for multiple feature maps at the top of the network.<span style="font-size:80%;opacity:0.8">默认边界框和长宽比。对于网络顶部的多个特征映射，我们将一组默认边界框与每个特征映射单元相关联。</span></li><li>Similar to Faster R-CNN[2], we regress to offsets for the center (cx, cy) of the default <font color=orangered>bounding</font> box (d) and for its width (w) and height (h).<span style="font-size:80%;opacity:0.8">类似于Faster R-CNN[2]，我们回归默认边界框(d)的中心偏移量(cx, cy)和其宽度(w)、高度(h)的偏移量。</span></li><li>Figure 4 shows that SSD is very sensitive to the <font color=orangered>bounding</font> box size.<span style="font-size:80%;opacity:0.8">图4显示SSD对边界框大小非常敏感。</span></li><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and <font color=orangered>bounding</font> box regression, which was first introduced in MultiBox [7] for learning objectness.<span style="font-size:80%;opacity:0.8">Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li><li>Another set of methods, which are directly related to our approach, skip the proposal step altogether and predict <font color=orangered>bounding</font> boxes and confidences for multiple categories directly.<span style="font-size:80%;opacity:0.8">与我们的方法直接相关的另一组方法，完全跳过提出步骤，直接预测多个类别的边界框和置信度。</span></li><li>OverFeat [4], a deep version of the sliding window method, predicts a <font color=orangered>bounding</font> box directly from each location of the topmost feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8">OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>YOLO [5] uses the whole topmost feature map to predict both confidences for multiple categories and <font color=orangered>bounding</font> boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8">YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>A key feature of our model is the use of multi-scale convolutional <font color=orangered>bounding</font> box outputs attached to multiple feature maps at the top of the network.<span style="font-size:80%;opacity:0.8">我们模型的一个关键特性是使用网络顶部多个特征映射的多尺度卷积边界框输出。</span></li><li>We experimentally validate that given appropriate training strategies, a larger number of carefully chosen default <font color=orangered>bounding</font> boxes results in improved performance.<span style="font-size:80%;opacity:0.8">我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> VOC2007<br>(17) </td> <td>  </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on <font color=forestgreen>VOC2007</font> test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8">对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on <font color=forestgreen>VOC2007</font> test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8">这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>SSD with a 300 × 300 input size significantly outperforms its 448 × 448 YOLO counterpart in accuracy on <font color=forestgreen>VOC2007</font> test while also improving the speed.<span style="font-size:80%;opacity:0.8">300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。</span></li><li>3.1 PASCAL <font color=forestgreen>VOC2007</font><span style="font-size:80%;opacity:0.8">3.1 PASCAL VOC2007</span></li><li>On this dataset, we compare against Fast R-CNN [6] and Faster R-CNN [2] on <font color=forestgreen>VOC2007</font> test (4952 images).<span style="font-size:80%;opacity:0.8">在这个数据集上，我们在VOC2007 test（4952张图像）上比较了Fast R-CNN[6]和FAST R-CNN[2]。</span></li><li>When training on <font color=forestgreen>VOC2007</font> $\texttt{trainval}$, Table 1 shows that our low resolution SSD300 model is already more accurate than Fast R-CNN.<span style="font-size:80%;opacity:0.8">当对VOC2007 $\texttt{trainval}$进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。</span></li><li>Table 1: PASCAL <font color=forestgreen>VOC2007</font> test detection results.<span style="font-size:80%;opacity:0.8">表1：PASCAL VOC2007 test检测结果。</span></li><li>Data: ”07”: <font color=forestgreen>VOC2007</font> trainval, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of <font color=forestgreen>VOC2007</font> and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Fig. 3: Visualization of performance for SSD512 on animals, vehicles, and furniture from <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8">图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li><li>Fig. 4: Sensitivity and impact of different object characteristics on <font color=forestgreen>VOC2007</font> test set using [21].<span style="font-size:80%;opacity:0.8">图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。</span></li><li>We use the same settings as those used for our basic <font color=forestgreen>VOC2007</font> experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and <font color=forestgreen>VOC2007</font> trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We see the same performance trend as we observed on <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8">我们看到了与我们在VOC2007 test中观察到的相同的性能趋势。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of <font color=forestgreen>VOC2007</font> trainval and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8">数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Fig. 6: Sensitivity and impact of object size with new data augmentation on <font color=forestgreen>VOC2007</font> test set using [21].<span style="font-size:80%;opacity:0.8">图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li><li>Table 7: Results on Pascal <font color=forestgreen>VOC2007</font> test.<span style="font-size:80%;opacity:0.8">表7：Pascal VOC2007 test上的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> YOLO<br>(16) </td> <td> [!≈ wai əu el əu] </td> <td> 
<ul><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or <font color=orangered>YOLO</font> 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8">这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for PASCAL VOC from $63.4\%$ mAP for <font color=orangered>YOLO</font> to $74.3\%$ mAP for our SSD.<span style="font-size:80%;opacity:0.8">虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。</span></li><li>We introduce SSD, a single-shot detector for multiple categories that is faster than the previous state-of-the-art for single shot detectors (<font color=orangered>YOLO</font>), and significantly more accurate,<span style="font-size:80%;opacity:0.8">我们引入了SSD，这是一种针对多个类别的单次检测器，比先前的先进的单次检测器（YOLO）更快，并且准确得多，</span></li><li>The convolutional model for predicting detections is different for each feature layer (cf Overfeat[4] and <font color=orangered>YOLO</font>[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8">用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li>The bounding box offset output values are measured relative to a default box position relative to each feature map location (cf the architecture of <font color=orangered>YOLO</font>[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8">边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li><li>Fig. 2: A comparison between two single shot detection models: SSD and <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8">图2：两个单次检测模型的比较：SSD和YOLO[5]。</span></li><li>SSD with a 300 × 300 input size significantly outperforms its 448 × 448 <font color=orangered>YOLO</font> counterpart in accuracy on VOC2007 test while also improving the speed.<span style="font-size:80%;opacity:0.8">300×300输入尺寸的SSD在VOC2007 test上的准确度上明显优于448×448的YOLO的准确度，同时也提高了速度。</span></li><li>Some version of this is also required for training in <font color=orangered>YOLO</font>[5] and for the region proposal stage of Faster R-CNN[2] and MultiBox[7].<span style="font-size:80%;opacity:0.8">在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。</span></li><li>We use a more extensive sampling strategy, similar to <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8">我们使用更广泛的抽样策略，类似于YOLO[5]。</span></li><li>Compared to <font color=orangered>YOLO</font>, SSD is significantly more accurate, likely due to the use of convolutional default boxes from multiple feature maps and our matching strategy during training.<span style="font-size:80%;opacity:0.8">与YOLO相比，SSD更精确，可能是由于使用了来自多个特征映射的卷积默认边界框和我们在训练期间的匹配策略。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for <font color=orangered>YOLO</font> is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8">数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Table 7 shows the comparison between SSD, Faster R-CNN[2], and <font color=orangered>YOLO</font>[5].<span style="font-size:80%;opacity:0.8">表7显示了SSD，Faster R-CNN[2]和YOLO[5]之间的比较。</span></li><li>Although Fast <font color=orangered>YOLO</font>[5] can run at 155 FPS, it has lower accuracy by almost $22\%$ mAP.<span style="font-size:80%;opacity:0.8">虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。</span></li><li><font color=orangered>YOLO</font> [5] uses the whole topmost feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8">YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce <font color=orangered>YOLO</font> [5].<span style="font-size:80%;opacity:0.8">如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li><li>Our real time SSD300 model runs at 59 FPS, which is faster than the current real time <font color=orangered>YOLO</font> [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8">我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> SSD300<br>(15) </td> <td>  </td> <td> 
<ul><li>Figure 2 shows the architecture details of the <font color=forestgreen>SSD300</font> model.<span style="font-size:80%;opacity:0.8">图2显示了SSD300模型的架构细节。</span></li><li>When training on VOC2007 $\texttt{trainval}$, Table 1 shows that our low resolution <font color=forestgreen>SSD300</font> model is already more accurate than Fast R-CNN.<span style="font-size:80%;opacity:0.8">当对VOC2007 $\texttt{trainval}$进行训练时，表1显示了我们的低分辨率SSD300模型已经比Fast R-CNN更准确。</span></li><li>If we train SSD with more (i.e. 07+12) data, we see that <font color=forestgreen>SSD300</font> is already better than Faster R-CNN by 1.1\% and that SSD512 is $3.6\%$ better.<span style="font-size:80%;opacity:0.8">如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li><li>Table 4 shows the results of our <font color=forestgreen>SSD300</font> and SSD512 model.<span style="font-size:80%;opacity:0.8">表4显示了我们的SSD300和SSD512模型的结果。</span></li><li>Our <font color=forestgreen>SSD300</font> improves accuracy over Fast/Faster R-CNN.<span style="font-size:80%;opacity:0.8">我们的SSD300比Fast/Faster R-CNN提高了准确性。</span></li><li>To further validate the SSD framework, we trained our <font color=forestgreen>SSD300</font> and SSD512 architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8">为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>Similar to what we observed on the PASCAL VOC dataset, <font color=forestgreen>SSD300</font> is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].<span style="font-size:80%;opacity:0.8">与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。</span></li><li><font color=forestgreen>SSD300</font> has a similar mAP@0.75 as ION [24] and Faster R-CNN [25], but is worse in mAP@0.5.<span style="font-size:80%;opacity:0.8">SSD300与ION 24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。</span></li><li>We train a <font color=forestgreen>SSD300</font> model using the ILSVRC2014 DET train and val1 as used in [22].<span style="font-size:80%;opacity:0.8">我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li><li>The top row shows the effects of BBox Area per category for the original <font color=forestgreen>SSD300</font> and SSD512 model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data augmentation trick.<span style="font-size:80%;opacity:0.8">最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>This step costs about 1.7 msec per image for <font color=forestgreen>SSD300</font> and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers.<span style="font-size:80%;opacity:0.8">对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li><li>Both our <font color=forestgreen>SSD300</font> and SSD512 method outperforms Faster R-CNN in both speed and accuracy.<span style="font-size:80%;opacity:0.8">我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。</span></li><li>To the best of our knowledge, <font color=forestgreen>SSD300</font> is the first real-time method to achieve above $70\%$ mAP.<span style="font-size:80%;opacity:0.8">就我们所知，SSD300是第一个实现70%以上mAP的实时方法。</span></li><li><font color=forestgreen>SSD300</font> is the only real-time detection method that can achieve above 70\% mAP.<span style="font-size:80%;opacity:0.8">SSD300是唯一可以取得70\%以上mAP的实现检测方法。</span></li><li>Our real time <font color=forestgreen>SSD300</font> model runs at 59 FPS, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8">我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> SSD512<br>(15) </td> <td>  </td> <td> 
<ul><li>If we train SSD with more (i.e. 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1\% and that <font color=forestgreen>SSD512</font> is $3.6\%$ better.<span style="font-size:80%;opacity:0.8">如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li><li>If we take models trained on COCO $\texttt{trainval35k}$ as described in Sec. 3.4 and fine-tuning them on the 07+12 dataset with <font color=forestgreen>SSD512</font>, we achieve the best results: 81.6\% mAP.<span style="font-size:80%;opacity:0.8">如果我们将SSD512用3.4节描述的COCO $\texttt{trainval35k}$来训练模型并在07+12数据集上进行微调，我们获得了最好的结果：$81.6\%$的mAP。</span></li><li>Fig. 3: Visualization of performance for <font color=forestgreen>SSD512</font> on animals, vehicles, and furniture from VOC2007 test.<span style="font-size:80%;opacity:0.8">图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li><li>Table 4 shows the results of our SSD300 and <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8">表4显示了我们的SSD300和SSD512模型的结果。</span></li><li>When fine-tuned from models trained on COCO, our <font color=forestgreen>SSD512</font> achieves $80.0\%$ mAP, which is $4.1\%$ higher than Faster R-CNN.<span style="font-size:80%;opacity:0.8">当对从COCO上训练的模型进行微调后，我们的SSD512达到了80.0%的mAP，比Faster R-CNN高了4.1%。</span></li><li>To further validate the SSD framework, we trained our SSD300 and <font color=forestgreen>SSD512</font> architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8">为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>By increasing the image size to 512 × 512, our <font color=forestgreen>SSD512</font> is better than Faster R-CNN [25] in both criteria.<span style="font-size:80%;opacity:0.8">通过将图像尺寸增加到512×512，我们的SSD512在这两个标准中都优于Faster R-CNN[25]。</span></li><li>Interestingly, we observe that <font color=forestgreen>SSD512</font> is 5.3\% better in mAP@0.75, but is only $1.2\%$ better in mAP@0.5.<span style="font-size:80%;opacity:0.8">有趣的是，我们观察到SSD512在mAP@0.75中要好5.3%，但是在mAP@0.5中只好1.2%。</span></li><li>In Fig. 5, we show some detection examples on COCO test-dev with the <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8">在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。</span></li><li>Fig. 5: Detection examples on COCO test-dev with <font color=forestgreen>SSD512</font> model.<span style="font-size:80%;opacity:0.8">图5：SSD512模型在COCO test-dev上的检测实例。</span></li><li>The top row shows the effects of BBox Area per category for the original SSD300 and <font color=forestgreen>SSD512</font> model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data augmentation trick.<span style="font-size:80%;opacity:0.8">最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>Both our SSD300 and <font color=forestgreen>SSD512</font> method outperforms Faster R-CNN in both speed and accuracy.<span style="font-size:80%;opacity:0.8">我们的SSD300和SSD512的速度和精度均优于Faster R-CNN。</span></li><li>Therefore, using a faster base network could even further improve the speed, which can possibly make the <font color=forestgreen>SSD512</font> model real-time as well.<span style="font-size:80%;opacity:0.8">因此，使用更快的基础网络可以进一步提高速度，这也可能使SSD512模型达到实时。</span></li><li>By using a larger input image, <font color=forestgreen>SSD512</font> outperforms all methods on accuracy while maintaining a close to real-time speed.<span style="font-size:80%;opacity:0.8">通过使用更大的输入图像，SSD512在精度上超过了所有方法同时保持近似实时的速度。</span></li><li>Our <font color=forestgreen>SSD512</font> model significantly outperforms the state-of-the-art Faster R-CNN [2] in terms of accuracy on PASCAL VOC and COCO, while being 3× faster.<span style="font-size:80%;opacity:0.8">在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> Pascal<br>(13) </td> <td> ['pæskәl] </td> <td> 
<ul><li>Experimental results on the <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.<span style="font-size:80%;opacity:0.8">PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。</span></li><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8">自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>While these contributions may seem small independently, we note that the resulting system improves accuracy on real-time detection for <font color=orangered>PASCAL</font> VOC from $63.4\%$ mAP for YOLO to $74.3\%$ mAP for our SSD.<span style="font-size:80%;opacity:0.8">虽然这些贡献可能单独看起来很小，但是我们注意到由此产生的系统将PASCAL VOC实时检测的准确度从YOLO的63.4%的mAP提高到我们的SSD的74.3%的mAP。</span></li><li>Experiments include timing and accuracy analysis on models with varying input size evaluated on <font color=orangered>PASCAL</font> VOC, COCO, and ILSVRC and are compared to a range of recent state-of-the-art approaches.<span style="font-size:80%;opacity:0.8">实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。</span></li><li>3.1 <font color=orangered>PASCAL</font> VOC2007<span style="font-size:80%;opacity:0.8">3.1 PASCAL VOC2007</span></li><li>Table 1: <font color=orangered>PASCAL</font> VOC2007 test detection results.<span style="font-size:80%;opacity:0.8">表1：PASCAL VOC2007 test检测结果。</span></li><li>3.3 <font color=orangered>PASCAL</font> VOC2012<span style="font-size:80%;opacity:0.8">3.3 PASCAL VOC2012</span></li><li>Table 4: <font color=orangered>PASCAL</font> VOC2012 test detection results.<span style="font-size:80%;opacity:0.8">表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 48。</span></li><li>Since objects in COCO tend to be smaller than <font color=orangered>PASCAL</font> VOC, we use smaller default boxes for all layers.<span style="font-size:80%;opacity:0.8">由于COCO中的目标往往比PASCAL VOC中的更小，因此我们对所有层使用较小的默认边界框。</span></li><li>Similar to what we observed on the <font color=orangered>PASCAL</font> VOC dataset, SSD300 is better than Fast R-CNN in both mAP@0.5 and mAP@[0.5:0.95].<span style="font-size:80%;opacity:0.8">与我们在PASCAL VOC数据集中观察到的结果类似，SSD300在mAP@0.5和mAP@[0.5:0.95]中都优于Fast R-CNN。</span></li><li>The data augmentation strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as <font color=orangered>PASCAL</font> VOC.<span style="font-size:80%;opacity:0.8">2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。</span></li><li>Table 7: Results on <font color=orangered>Pascal</font> VOC2007 test.<span style="font-size:80%;opacity:0.8">表7：Pascal VOC2007 test上的结果。</span></li><li>Our SSD512 model significantly outperforms the state-of-the-art Faster R-CNN [2] in terms of accuracy on <font color=orangered>PASCAL</font> VOC and COCO, while being 3× faster.<span style="font-size:80%;opacity:0.8">在PASCAL VOC和COCO上，我们的SSD512模型的性能明显优于最先进的Faster R-CNN[2]，而速度提高了3倍。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> augmentation<br>(13) </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>Training also involves choosing the set of default boxes and scales for detection as well as the hard negative mining and data <font color=orangered>augmentation</font> strategies.<span style="font-size:80%;opacity:0.8">训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。</span></li><li>Data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8">数据增强。</span></li><li>Data <font color=orangered>augmentation</font> is crucial.<span style="font-size:80%;opacity:0.8">数据增强至关重要。</span></li><li>3.6 Data <font color=orangered>Augmentation</font> for Small Object Accuracy<span style="font-size:80%;opacity:0.8">3.6 为小目标准确率进行数据增强</span></li><li>The data <font color=orangered>augmentation</font> strategy described in Sec. 2.2 helps to improve the performance dramatically, especially on small datasets such as PASCAL VOC.<span style="font-size:80%;opacity:0.8">2.2描述的数据增强有助于显著提高性能，特别是在PASCAL VOC等小数据集上。</span></li><li>Because we have more training images by introducing this new “expansion” data <font color=orangered>augmentation</font> trick, we have to double the training iterations.<span style="font-size:80%;opacity:0.8">因为通过引入这个新的“扩展”数据增强技巧，我们有更多的训练图像，所以我们必须将训练迭代次数加倍。</span></li><li>In specific, Figure 6 shows that the new <font color=orangered>augmentation</font> trick significantly improves the performance on small objects.<span style="font-size:80%;opacity:0.8">具体来说，图6显示新的增强技巧显著提高了模型在小目标上的性能。</span></li><li>This result underscores the importance of the data <font color=orangered>augmentation</font> strategy for the final model accuracy.<span style="font-size:80%;opacity:0.8">这个结果强调了数据增强策略对最终模型精度的重要性。</span></li><li>Table 6: Results on multiple datasets when we add the image expansion data <font color=orangered>augmentation</font> trick.<span style="font-size:80%;opacity:0.8">表6：我们使用图像扩展数据增强技巧在多个数据集上的结果。</span></li><li>$SSD300^{*}$ and $SSD512^{*}$ are the models that are trained with the new data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8">$SSD300^{*}$和$SSD512^{*}$是用新的数据增强训练的模型。</span></li><li>Fig. 6: Sensitivity and impact of object size with new data <font color=orangered>augmentation</font> on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8">图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li><li>The top row shows the effects of BBox Area per category for the original SSD300 and SSD512 model, and the bottom row corresponds to the $SSD300^{*}$ and $SSD512^{*}$ model trained with the new data <font color=orangered>augmentation</font> trick.<span style="font-size:80%;opacity:0.8">最上一行显示了原始SSD300和SSD512模型上每个类别的BBox面积的影响，最下面一行对应使用新的数据增强训练技巧的$SSD300^{*}$和$SSD512^{*}$模型。</span></li><li>It is obvious that the new data <font color=orangered>augmentation</font> trick helps detecting small objects significantly.<span style="font-size:80%;opacity:0.8">新的数据增强技巧显然有助于显著检测小目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> MultiBox<br>(7) </td> <td>  </td> <td> 
<ul><li>SSD: Single Shot <font color=forestgreen>MultiBox</font> Detector<span style="font-size:80%;opacity:0.8">SSD：单发多盒检测器</span></li><li>Some version of this is also required for training in YOLO[5] and for the region proposal stage of Faster R-CNN[2] and <font color=forestgreen>MultiBox</font>[7].<span style="font-size:80%;opacity:0.8">在YOLO[5]的训练中、Faster R-CNN[2]和MultiBox[7]的区域提出阶段，一些版本也需要这样的操作。</span></li><li>We begin by matching each ground truth box to the default box with the best jaccard overlap (as in <font color=forestgreen>MultiBox</font> [7]).<span style="font-size:80%;opacity:0.8">我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。</span></li><li>Unlike <font color=forestgreen>MultiBox</font>, we then match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5).<span style="font-size:80%;opacity:0.8">与MultiBox不同的是，我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框。</span></li><li>The SSD training objective is derived from the <font color=forestgreen>MultiBox</font> objective[7,8] but is extended to handle multiple object categories.<span style="font-size:80%;opacity:0.8">SSD训练目标函数来自于MultiBox目标[7,8]，但扩展到处理多个目标类别。</span></li><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in <font color=forestgreen>MultiBox</font> [7] for learning objectness.<span style="font-size:80%;opacity:0.8">Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li><li>In the most recent works like <font color=forestgreen>MultiBox</font> [7,8], the Selective Search region proposals, which are based on low-level image features, are replaced by proposals generated directly from a separate deep neural network.<span style="font-size:80%;opacity:0.8">在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> FPS<br>(7) </td> <td> ['efp'i:'es] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 <font color=orangered>FPS</font> on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8">对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>Often detection speed for these approaches is measured in seconds per frame (SPF), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (<font color=orangered>FPS</font>).<span style="font-size:80%;opacity:0.8">通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 <font color=orangered>FPS</font> with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8">这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 <font color=orangered>FPS</font> with mAP $73.2\%$ or YOLO 45 FPS with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8">这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>This results in a significant improvement in speed for high-accuracy detection (59 FPS with mAP $74.3\%$ on VOC2007 test, vs. Faster R-CNN 7 FPS with mAP $73.2\%$ or YOLO 45 <font color=orangered>FPS</font> with mAP $63.4\%$).<span style="font-size:80%;opacity:0.8">这对高精度检测在速度上有显著提高（在VOC2007测试中，59FPS和74.3%的mAP，与Faster R-CNN 7FPS和73.2%的mAP或者YOLO 45 FPS和63.4%的mAP相比）。</span></li><li>Although Fast YOLO[5] can run at 155 <font color=orangered>FPS</font>, it has lower accuracy by almost $22\%$ mAP.<span style="font-size:80%;opacity:0.8">虽然Fast YOLO[5]可以以155FPS的速度运行，但其准确性却降低了近22%的mAP。</span></li><li>Our real time SSD300 model runs at 59 <font color=orangered>FPS</font>, which is faster than the current real time YOLO [5] alternative, while producing markedly superior detection accuracy.<span style="font-size:80%;opacity:0.8">我们的实时SSD300模型运行速度为59FPS，比目前的实时YOLO[5]更快，同时显著提高了检测精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> e.g.<br>(7) </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>(a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (<font color=orangered>e.g.</font> 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 × 8 and 4 × 4 in (b) and (c)).<span style="font-size:80%;opacity:0.8">以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。</span></li><li>(a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set (e.g. 4) of default boxes of different aspect ratios at each location in several feature maps with different scales (<font color=orangered>e.g.</font> 8 × 8 and 4 × 4 in (b) and (c)).<span style="font-size:80%;opacity:0.8">以卷积方式，我们评估具有不同尺度（例如（b）和（c）中的8×8和4×4）的几个特征映射中每个位置处不同长宽比的默认框的小集合（例如4个）。</span></li><li>The model loss is a weighted sum between localization loss (<font color=orangered>e.g.</font> Smooth L1 [6]) and confidence loss (e.g. Softmax).<span style="font-size:80%;opacity:0.8">模型损失是定位损失（例如，Smooth L1[6]）和置信度损失（例如Softmax）之间的加权和。</span></li><li>The model loss is a weighted sum between localization loss (e.g. Smooth L1 [6]) and confidence loss (<font color=orangered>e.g.</font> Softmax).<span style="font-size:80%;opacity:0.8">模型损失是定位损失（例如，Smooth L1[6]）和置信度损失（例如Softmax）之间的加权和。</span></li><li>Increasing the input size (<font color=orangered>e.g.</font> from 300 × 300 to 512 × 512) can help improve detecting small objects, but there is still a lot of room to improve.<span style="font-size:80%;opacity:0.8">增加输入尺寸（例如从300×300到512×512）可以帮助改进检测小目标，但仍然有很大的改进空间。</span></li><li>For example, it hurts the performance by a large margin if we use very coarse feature maps (<font color=orangered>e.g.</font> conv11_2 (1 × 1) or conv10_2 (3 × 3)).<span style="font-size:80%;opacity:0.8">例如，如果我们使用非常粗糙的特征映射（例如conv11_2（1×1）或conv10_2（3×3）），它会大大伤害性能。</span></li><li>We follow the strategy mentioned in Sec. 2.2, but now our smallest default box has a scale of 0.15 instead of 0.2, and the scale of the default box on conv4_3 is 0.07 (<font color=orangered>e.g.</font> 21 pixels for a 300 × 300 image).<span style="font-size:80%;opacity:0.8">我们遵循2.2节中提到的策略，但是现在我们最小的默认边界框尺度是0.15而不是0.2，并且conv4_3上的默认边界框尺度是0.07（例如，300×300图像中的21个像素）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> jaccard<br>(7) </td> <td>  </td> <td> 
<ul><li>We begin by matching each ground truth box to the default box with the best <font color=forestgreen>jaccard</font> overlap (as in MultiBox [7]).<span style="font-size:80%;opacity:0.8">我们首先将每个实际边界框与具有最好的Jaccard重叠（如MultiBox[7]）的边界框相匹配。</span></li><li>Unlike MultiBox, we then match default boxes to any ground truth with <font color=forestgreen>jaccard</font> overlap higher than a threshold (0.5).<span style="font-size:80%;opacity:0.8">与MultiBox不同的是，我们将默认边界框匹配到Jaccard重叠高于阈值（0.5）的任何实际边界框。</span></li><li>Sample a patch so that the minimum <font color=forestgreen>jaccard</font> overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9.<span style="font-size:80%;opacity:0.8">采样一个图像块，使得与目标之间的最小Jaccard重叠为0.1，0.3，0.5，0.7或0.9。</span></li><li>The recall is around 85-90\%, and is much higher with “weak” (0.1 <font color=forestgreen>jaccard</font> overlap) criteria.<span style="font-size:80%;opacity:0.8">召回约为85-90\%，而“弱”（0.1 Jaccard重叠）标准则要高得多。</span></li><li>The solid red line reflects the change of recall with strong criteria (0.5 <font color=forestgreen>jaccard</font> overlap) as the number of detections increases.<span style="font-size:80%;opacity:0.8">红色的实线表示随着检测次数的增加，强标准（0.5 Jaccard重叠）下的召回变化。</span></li><li>The dashed red line is using the weak criteria (0.1 <font color=forestgreen>jaccard</font> overlap).<span style="font-size:80%;opacity:0.8">红色虚线是使用弱标准（0.1 Jaccard重叠）。</span></li><li>We then apply nms with <font color=forestgreen>jaccard</font> overlap of 0.45 per class and keep the top 200 detections per image.<span style="font-size:80%;opacity:0.8">然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> ILSVRC<br>(6) </td> <td> [!≈ aɪ el es vi: ɑ:(r) si:] </td> <td> 
<ul><li>Experimental results on the PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference.<span style="font-size:80%;opacity:0.8">PASCAL VOC，COCO和ILSVRC数据集上的实验结果证实，SSD对于利用额外的目标提出步骤的方法具有竞争性的准确性，并且速度更快，同时为训练和推断提供了统一的框架。</span></li><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8">自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>Experiments include timing and accuracy analysis on models with varying input size evaluated on PASCAL VOC, COCO, and <font color=orangered>ILSVRC</font> and are compared to a range of recent state-of-the-art approaches.<span style="font-size:80%;opacity:0.8">实验包括在PASCAL VOC，COCO和ILSVRC上评估具有不同输入大小的模型的时间和精度分析，并与最近的一系列最新方法进行比较。</span></li><li>Base network Our experiments are all based on VGG16[15], which is pre-trained on the <font color=orangered>ILSVRC</font> CLS-LOC dataset[16].<span style="font-size:80%;opacity:0.8">基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li><li>3.5 Preliminary <font color=orangered>ILSVRC</font> results<span style="font-size:80%;opacity:0.8">3.5 初步的ILSVRC结果</span></li><li>We applied the same network architecture we used for COCO to the <font color=orangered>ILSVRC</font> DET dataset [16].<span style="font-size:80%;opacity:0.8">我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> tile<br>(6) </td> <td> [taɪl] </td> <td> 
<ul><li>The default boxes <font color=orangered>tile</font> the feature map in a convolutional manner, so that the position of each box relative to its corresponding cell is fixed.<span style="font-size:80%;opacity:0.8">默认边界框以卷积的方式平铺特征映射，以便每个边界框相对于其对应单元的位置是固定的。</span></li><li>We design the <font color=orangered>tiling</font> of default boxes so that specific feature maps learn to be responsive to particular scales of the objects.<span style="font-size:80%;opacity:0.8">我们设计平铺默认边界框，以便特定的特征映射学习响应目标的特定尺度。</span></li><li>How to design the optimal <font color=orangered>tiling</font> is an open question as well.<span style="font-size:80%;opacity:0.8">如何设计最佳平铺也是一个悬而未决的问题。</span></li><li>For a fair comparison, every time we remove a layer, we adjust the default box <font color=orangered>tiling</font> to keep the total number of boxes similar to the original (8732).<span style="font-size:80%;opacity:0.8">为了公平比较，每次我们删除一层，我们调整默认边界框平铺，以保持类似于最初的边界框的总数（8732）。</span></li><li>We do not exhaustively optimize the <font color=orangered>tiling</font> for each setting.<span style="font-size:80%;opacity:0.8">我们没有详尽地优化每个设置的平铺。</span></li><li>An alternative way of improving SSD is to design a better <font color=orangered>tiling</font> of default boxes so that its position and scale are better aligned with the receptive field of each position on a feature map.<span style="font-size:80%;opacity:0.8">改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> trainval<br>(6) </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 <font color=forestgreen>trainval</font>, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 <font color=forestgreen>trainval</font>. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 <font color=forestgreen>trainval</font> and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 <font color=forestgreen>trainval</font> and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 <font color=forestgreen>trainval</font> and test and VOC2012 trainval.<span style="font-size:80%;opacity:0.8">数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and VOC2012 <font color=forestgreen>trainval</font>.<span style="font-size:80%;opacity:0.8">数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> VOC2012<br>(6) </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and <font color=forestgreen>VOC2012</font> trainval. ”07+12+COCO”: first train on COCO trainval35k then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>3.3 PASCAL <font color=forestgreen>VOC2012</font><span style="font-size:80%;opacity:0.8">3.3 PASCAL VOC2012</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use <font color=forestgreen>VOC2012</font> trainval and VOC2007 trainval and test (21503 images) for training, and test on VOC2012 test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>We use the same settings as those used for our basic VOC2007 experiments above, except that we use VOC2012 trainval and VOC2007 trainval and test (21503 images) for training, and test on <font color=forestgreen>VOC2012</font> test (10991 images).<span style="font-size:80%;opacity:0.8">除了我们使用VOC2012 trainval和VOC2007 trainval，test（21503张图像）进行训练，以及在VOC2012 test（10991张图像）上进行测试之外，我们使用与上述基本的VOC2007实验相同的设置。</span></li><li>Table 4: PASCAL <font color=forestgreen>VOC2012</font> test detection results.<span style="font-size:80%;opacity:0.8">表4： PASCAL VOC2012 test上的检测结果. Fast和Faster R-CNN使用最小维度为600的图像，而YOLO的图像大小为448× 48。</span></li><li>Fast and Faster R-CNN use images with minimum dimension 600, while the image size for YOLO is 448 × 448. data: ”07++12”: union of VOC2007 trainval and test and <font color=forestgreen>VOC2012</font> trainval.<span style="font-size:80%;opacity:0.8">数据：“07++12”：VOC2007 trainval，test和VOC2012 trainval。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> RPN<br>(6) </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>$3.9\%$). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the <font color=orangered>RPN</font> part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8">我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li><li>Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (<font color=orangered>RPN</font>), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8">Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li><li>Faster R-CNN [2] replaces selective search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the <font color=orangered>RPN</font> with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8">Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li><li>Our SSD is very similar to the region proposal network (<font color=orangered>RPN</font>) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the RPN.<span style="font-size:80%;opacity:0.8">我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。</span></li><li>Our SSD is very similar to the region proposal network (RPN) in Faster R-CNN in that we also use a fixed set of (default) boxes for prediction, similar to the anchor boxes in the <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8">我们的SSD与Faster R-CNN中的区域提出网络（RPN）非常相似，因为我们也使用一组固定的（默认）边界框进行预测，类似于RPN中的锚边界框。</span></li><li>Thus, our approach avoids the complication of merging <font color=orangered>RPN</font> with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.<span style="font-size:80%;opacity:0.8">因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> selective<br>(5) </td> <td> [sɪˈlektɪv] </td> <td> 
<ul><li>This pipeline has prevailed on detection benchmarks since the <font color=orangered>Selective</font> Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8">自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and <font color=orangered>Selective</font> Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8">在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li><li>However, after the dramatic improvement brought on by R-CNN [22], which combines <font color=orangered>selective</font> search region proposals and convolutional network based post-classification, region proposal object detection methods became prevalent.<span style="font-size:80%;opacity:0.8">然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。</span></li><li>In the most recent works like MultiBox [7,8], the <font color=orangered>Selective</font> Search region proposals, which are based on low-level image features, are replaced by proposals generated directly from a separate deep neural network.<span style="font-size:80%;opacity:0.8">在最近的工作MultiBox[7,8]中，基于低级图像特征的选择性搜索区域提出直接被单独的深度神经网络生成的提出所取代。</span></li><li>Faster R-CNN [2] replaces <font color=orangered>selective</font> search proposals by ones learned from a region proposal network (RPN), and introduces a method to integrate the RPN with Fast R-CNN by alternating between fine-tuning shared convolutional layers and prediction layers for these two networks.<span style="font-size:80%;opacity:0.8">Faster R-CNN[2]将选择性搜索提出替换为区域提出网络（RPN）学习到的区域提出，并引入了一种方法，通过交替两个网络之间的微调共享卷积层和预测层将RPN和Fast R-CNN结合在一起。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> VGG16<br>(4) </td> <td>  </td> <td> 
<ul><li>All methods fine-tune on the same pre-trained <font color=forestgreen>VGG16</font> network.<span style="font-size:80%;opacity:0.8">所有的方法都在相同的预训练好的VGG16网络上进行微调。</span></li><li>As described in Sec. 3, we used the atrous version of a subsampled <font color=forestgreen>VGG16</font>, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8">如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li><li>If we use the full <font color=forestgreen>VGG16</font>, keeping pool5 with 2×2−s2 and not subsampling parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about $20\%$ slower.<span style="font-size:80%;opacity:0.8">如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。</span></li><li>Note that about $80\%$ of the forward time is spent on the base network (<font color=forestgreen>VGG16</font> in our case).<span style="font-size:80%;opacity:0.8">请注意，大约80%前馈时间花费在基础网络上（本例中为VGG16）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> test-dev<br>(4) </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Table 5 shows the results on <font color=orangered>test-dev</font>2015.<span style="font-size:80%;opacity:0.8">表5显示了test-dev2015的结果。</span></li><li>In Fig. 5, we show some detection examples on COCO <font color=orangered>test-dev</font> with the SSD512 model.<span style="font-size:80%;opacity:0.8">在图5中，我们展示了SSD512模型在COCO test-dev上的一些检测实例。</span></li><li>Table 5: COCO <font color=orangered>test-dev</font>2015 detection results.<span style="font-size:80%;opacity:0.8">表5：COCO test-dev2015检测结果。</span></li><li>Fig. 5: Detection examples on COCO <font color=orangered>test-dev</font> with SSD512 model.<span style="font-size:80%;opacity:0.8">图5：SSD512模型在COCO test-dev上的检测实例。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> topmost<br>(4) </td> <td> [ˈtɒpməʊst] </td> <td> 
<ul><li>OverFeat [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the <font color=orangered>topmost</font> feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8">OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>YOLO [5] uses the whole <font color=orangered>topmost</font> feature map to predict both confidences for multiple categories and bounding boxes (which are shared for these categories).<span style="font-size:80%;opacity:0.8">YOLO[5]使用整个最顶层的特征映射来预测多个类别和边界框（这些类别共享）的置信度。</span></li><li>If we only use one default box per location from the <font color=orangered>topmost</font> feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8">如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole <font color=orangered>topmost</font> feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8">如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> comparable<br>(3) </td> <td> [ˈkɒmpərəbl] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a <font color=orangered>comparable</font> state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8">对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>The SSD architecture combines predictions from feature maps of various resolutions to achieve <font color=orangered>comparable</font> accuracy to Faster R-CNN, while using lower resolution input images.<span style="font-size:80%;opacity:0.8">SSD架构将来自各种分辨率的特征映射的预测结合起来，以达到与Faster R-CNN相当的精确度，同时使用较低分辨率的输入图像。</span></li><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and Selective Search [1] —— had <font color=orangered>comparable</font> performance.<span style="font-size:80%;opacity:0.8">在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> cf<br>(3) </td> <td>  </td> <td> 
<ul><li>We are not the first to do this (<font color=forestgreen>cf</font> [4,5]), but by adding a series of improvements, we manage to increase the accuracy significantly over previous attempts.<span style="font-size:80%;opacity:0.8">我们并不是第一个这样做的人（查阅[4,5]），但是通过增加一系列改进，我们设法比以前的尝试显著提高了准确性。</span></li><li>The convolutional model for predicting detections is different for each feature layer (<font color=forestgreen>cf</font> Overfeat[4] and YOLO[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8">用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li>The bounding box offset output values are measured relative to a default box position relative to each feature map location (<font color=forestgreen>cf</font> the architecture of YOLO[5] that uses an intermediate fully connected layer instead of a convolutional filter for this step).<span style="font-size:80%;opacity:0.8">边界框偏移输出值是相对每个特征映射位置的相对默认框位置来度量的（查阅YOLO[5]的架构，该步骤使用中间全连接层而不是卷积滤波器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> predictor<br>(3) </td> <td> [prɪˈdɪktə(r)] </td> <td> 
<ul><li>Our improvements include using a small convolutional filter to predict object categories and offsets in bounding box locations, using separate <font color=orangered>predictors</font> (filters) for different aspect ratio detections, and applying these filters to multiple feature maps from the later stages of a network in order to perform detection at multiple scales.<span style="font-size:80%;opacity:0.8">我们的改进包括使用小型卷积滤波器来预测边界框位置中的目标类别和偏移量，使用不同长宽比检测的单独预测器（滤波器），并将这些滤波器应用于网络后期的多个特征映射中，以执行多尺度检测。</span></li><li>Convolutional <font color=orangered>predictors</font> for detection Each added feature layer (or optionally an existing feature layer from the base network) can produce a fixed set of detection predictions using a set of convolutional filters.<span style="font-size:80%;opacity:0.8">用于检测的卷积预测器。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to OverFeat [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional <font color=orangered>predictors</font>, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8">如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> Overfeat<br>(3) </td> <td>  </td> <td> 
<ul><li>The convolutional model for predicting detections is different for each feature layer (cf <font color=forestgreen>Overfeat</font>[4] and YOLO[5] that operate on a single scale feature map).<span style="font-size:80%;opacity:0.8">用于预测检测的卷积模型对于每个特征层都是不同的（查阅Overfeat[4]和YOLO[5]在单尺度特征映射上的操作）。</span></li><li><font color=forestgreen>OverFeat</font> [4], a deep version of the sliding window method, predicts a bounding box directly from each location of the topmost feature map after knowing the confidences of the underlying object categories.<span style="font-size:80%;opacity:0.8">OverFeat[4]是滑动窗口方法的深度版本，在知道了底层目标类别的置信度之后，直接从最顶层的特征映射的每个位置预测边界框。</span></li><li>If we only use one default box per location from the topmost feature map, our SSD would have similar architecture to <font color=forestgreen>OverFeat</font> [4]; if we use the whole topmost feature map and add a fully connected layer for predictions instead of our convolutional predictors, and do not explicitly consider multiple aspect ratios, we can approximately reproduce YOLO [5].<span style="font-size:80%;opacity:0.8">如果我们只从最顶层的特征映射的每个位置使用一个默认框，我们的SSD将具有与OverFeat[4]相似的架构；如果我们使用整个最顶层的特征映射，并添加一个全连接层进行预测来代替我们的卷积预测器，并且没有明确地考虑多个长宽比，我们可以近似地再现YOLO[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> receptive<br>(3) </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>Feature maps from different levels within a network are known to have different (empirical) <font color=orangered>receptive</font> field sizes [13].<span style="font-size:80%;opacity:0.8">已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。</span></li><li>Fortunately, within the SSD framework, the default boxes do not necessary need to correspond to the actual <font color=orangered>receptive</font> fields of each layer.<span style="font-size:80%;opacity:0.8">幸运的是，在SSD框架内，默认边界框不需要对应于每层的实际感受野。</span></li><li>An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better aligned with the <font color=orangered>receptive</font> field of each position on a feature map.<span style="font-size:80%;opacity:0.8">改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> atrous<br>(3) </td> <td> ['eitrәs] </td> <td> 
<ul><li>Similar to DeepLab-LargeFOV[17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the <font color=orangered>atrous</font> algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8">类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li><li><font color=orangered>Atrous</font> is faster.<span style="font-size:80%;opacity:0.8">Atrous更快。</span></li><li>As described in Sec. 3, we used the <font color=orangered>atrous</font> version of a subsampled VGG16, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8">如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> trainval35k<br>(3) </td> <td>  </td> <td> 
<ul><li>Data: ”07”: VOC2007 trainval, ”07+12”: union of VOC2007 and VOC2012 trainval. ”07+12+COCO”: first train on COCO <font color=forestgreen>trainval35k</font> then fine-tune on 07+12.<span style="font-size:80%;opacity:0.8">“07+12+COCO”：首先在COCO trainval35k上训练然后在07+12上微调。</span></li><li>”07++12+COCO”: first train on COCO <font color=forestgreen>trainval35k</font> then fine-tune on 07++12.<span style="font-size:80%;opacity:0.8">“07++12+COCO”：先在COCO trainval135k上训练然后在07++12上微调。</span></li><li>We use the <font color=forestgreen>trainval35k</font>[24] for training.<span style="font-size:80%;opacity:0.8">我们使用trainval35k[24]进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> validate<br>(3) </td> <td> [ˈvælɪdeɪt] </td> <td> 
<ul><li>To further <font color=orangered>validate</font> the SSD framework, we trained our SSD300 and SSD512 architectures on the COCO dataset.<span style="font-size:80%;opacity:0.8">为了进一步验证SSD框架，我们在COCO数据集上对SSD300和SSD512架构进行了训练。</span></li><li>Again, it <font color=orangered>validates</font> that SSD is a general framework for high quality real-time detection.<span style="font-size:80%;opacity:0.8">再一次证明了SSD是用于高质量实时检测的通用框架。</span></li><li>We experimentally <font color=orangered>validate</font> that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance.<span style="font-size:80%;opacity:0.8">我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> discretize<br>(2) </td> <td> ['diskri:taiz] </td> <td> 
<ul><li>Our approach, named SSD, <font color=orangered>discretizes</font> the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location.<span style="font-size:80%;opacity:0.8">我们的方法命名为SSD，将边界框的输出空间离散化为不同长宽比的一组默认框和并缩放每个特征映射的位置。</span></li><li>Allowing different default box shapes in several feature maps let us efficiently <font color=orangered>discretize</font> the space of possible output box shapes.<span style="font-size:80%;opacity:0.8">在几个特征映射中允许不同的默认边界框形状让我们有效地离散可能的输出框形状的空间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> Nvidia<br>(2) </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a <font color=orangered>Nvidia</font> Titan X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8">对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>We thank <font color=orangered>NVIDIA</font> for providing GPUs and acknowledge support from NSF 1452851, 1446631, 1526367, 1533771.<span style="font-size:80%;opacity:0.8">我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> Titan<br>(2) </td> <td> [ˈtaɪtn] </td> <td> 
<ul><li>For 300 × 300 input, SSD achieves $74.3\%$ mAP on VOC2007 test at 59 FPS on a Nvidia <font color=orangered>Titan</font> X and for 512 × 512 input, SSD achieves $76.9\%$ mAP, outperforming a comparable state-of-the-art Faster R-CNN model.<span style="font-size:80%;opacity:0.8">对于300×300的输入，SSD在VOC2007测试中以59FPS的速度在Nvidia Titan X上达到$74.3\%$的mAP，对于512×512的输入，SSD达到了$76.9\%$的mAP，优于参照的最先进的Faster R-CNN模型。</span></li><li>We measure the speed with batch size 8 using <font color=orangered>Titan</font> X and cuDNN v4 with Intel Xeon E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8">我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> suppression<br>(2) </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>The SSD approach is based on a feed-forward convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum <font color=orangered>suppression</font> step to produce the final detections.<span style="font-size:80%;opacity:0.8">SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li><li>Considering the large number of boxes generated from our method, it is essential to perform non-maximum <font color=orangered>suppression</font> (nms) efficiently during inference.<span style="font-size:80%;opacity:0.8">考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> truncated<br>(2) </td> <td> ['trʌŋkeɪtɪd] </td> <td> 
<ul><li>The early network layers are based on a standard architecture used for high quality image classification (<font color=orangered>truncated</font> before any classification layers), which we will call the base network.<span style="font-size:80%;opacity:0.8">早期的网络层基于用于高质量图像分类的标准架构（在任何分类层之前被截断），我们将其称为基础网络。</span></li><li>Multi-scale feature maps for detection We add convolutional feature layers to the end of the <font color=orangered>truncated</font> base network.<span style="font-size:80%;opacity:0.8">用于检测的多尺度特征映射。我们将卷积特征层添加到截取的基础网络的末端。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> progressively<br>(2) </td> <td> [prəˈgresɪvli] </td> <td> 
<ul><li>These layers decrease in size <font color=orangered>progressively</font> and allow predictions of detections at multiple scales.<span style="font-size:80%;opacity:0.8">这些层在尺寸上逐渐减小，并允许在多个尺度上对检测结果进行预测。</span></li><li>To measure the advantage gained, we <font color=orangered>progressively</font> remove layers and compare results.<span style="font-size:80%;opacity:0.8">为了衡量所获得的优势，我们逐步删除层并比较结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> propagation<br>(2) </td> <td> [ˌprɒpə'ɡeɪʃn] </td> <td> 
<ul><li>Once this assignment is determined, the loss function and back <font color=orangered>propagation</font> are applied end-to-end.<span style="font-size:80%;opacity:0.8">一旦确定了这个分配，损失函数和反向传播就可以应用端到端了。</span></li><li>Since, as pointed out in [12], conv4_3 has a different feature scale compared to the other layers, we use the L2 normalization technique introduced in [12] to scale the feature norm at each location in the feature map to 20 and learn the scale during back <font color=orangered>propagation</font>.<span style="font-size:80%;opacity:0.8">如[12]所指出的，与其它层相比，由于conv4_3具有不同的特征尺度，所以我们使用[12]中引入的L2正则化技术将特征映射中每个位置的特征标准缩放到20，在反向传播过程中学习尺度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> mining<br>(2) </td> <td> [ˈmaɪnɪŋ] </td> <td> 
<ul><li>Training also involves choosing the set of default boxes and scales for detection as well as the hard negative <font color=orangered>mining</font> and data augmentation strategies.<span style="font-size:80%;opacity:0.8">训练也涉及选择默认边界框集合和缩放进行检测，以及难例挖掘和数据增强策略。</span></li><li>Hard negative <font color=orangered>mining</font> After the matching step, most of the default boxes are negatives, especially when the number of possible default boxes is large.<span style="font-size:80%;opacity:0.8">难例挖掘。在匹配步骤之后，大多数默认边界框为负例，尤其是当可能的默认边界框数量较多时。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> regress<br>(2) </td> <td> [rɪˈgres] </td> <td> 
<ul><li>Similar to Faster R-CNN[2], we <font color=orangered>regress</font> to offsets for the center (cx, cy) of the default bounding box (d) and for its width (w) and height (h).<span style="font-size:80%;opacity:0.8">类似于Faster R-CNN[2]，我们回归默认边界框(d)的中心偏移量(cx, cy)和其宽度(w)、高度(h)的偏移量。</span></li><li>Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to <font color=orangered>regress</font> the object shape and classify object categories instead of using two decoupled steps.<span style="font-size:80%;opacity:0.8">与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> flip<br>(2) </td> <td> [flɪp] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally <font color=orangered>flipped</font> with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8">在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li><li>Fast and Faster R-CNN use the original image and the horizontal <font color=orangered>flip</font> to train.<span style="font-size:80%;opacity:0.8">Fast和Faster R-CNN使用原始图像和水平翻转来训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> DeepLab-LargeFOV<br>(2) </td> <td>  </td> <td> 
<ul><li>Similar to <font color=forestgreen>DeepLab-LargeFOV</font>[17], we convert fc6 and fc7 to convolutional layers, subsample parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the atrous algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8">类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li><li>As described in Sec. 3, we used the atrous version of a subsampled VGG16, following <font color=forestgreen>DeepLab-LargeFOV</font> [17].<span style="font-size:80%;opacity:0.8">如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> Sensitivity<br>(2) </td> <td> [ˌsensəˈtɪvəti] </td> <td> 
<ul><li>Fig. 4: <font color=orangered>Sensitivity</font> and impact of different object characteristics on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8">图4：使用[21]在VOC2007 test设置上不同目标特性的灵敏度和影响。</span></li><li>Fig. 6: <font color=orangered>Sensitivity</font> and impact of object size with new data augmentation on VOC2007 test set using [21].<span style="font-size:80%;opacity:0.8">图6：具有新的数据增强的目标尺寸在[21]中使用的VOC2007test数据集上灵敏度及影响。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> prune<br>(2) </td> <td> [pru:n] </td> <td> 
<ul><li>The reason might be that we do not have enough large boxes to cover large objects after the <font color=orangered>pruning</font>.<span style="font-size:80%;opacity:0.8">原因可能是修剪后我们没有足够大的边界框来覆盖大的目标。</span></li><li>When we use primarily finer resolution maps, the performance starts increasing again because even after <font color=orangered>pruning</font> a sufficient number of large boxes remains.<span style="font-size:80%;opacity:0.8">当我们主要使用更高分辨率的特征映射时，性能开始再次上升，因为即使在修剪之后仍然有足够数量的大边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> ION<br>(2) </td> <td> [ˈaɪən] </td> <td> 
<ul><li>SSD300 has a similar mAP@0.75 as <font color=orangered>ION</font> [24] and Faster R-CNN [25], but is worse in mAP@0.5.<span style="font-size:80%;opacity:0.8">SSD300与ION 24]和Faster R-CNN[25]具有相似的mAP@0.75，但是mAP@0.5更差。</span></li><li>Compared to <font color=orangered>ION</font>, the improvement in AR for large and small objects is more similar ($5.4\%$ vs.<span style="font-size:80%;opacity:0.8">与ION相比，大型和小型目标的AR改进更为相似（5.4%和3.9%）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> DET<br>(2) </td> <td> [!≈ di: i: ti:] </td> <td> 
<ul><li>We applied the same network architecture we used for COCO to the ILSVRC <font color=orangered>DET</font> dataset [16].<span style="font-size:80%;opacity:0.8">我们将在COCO上应用的相同网络架构应用于ILSVRC DET数据集[16]。</span></li><li>We train a SSD300 model using the ILSVRC2014 <font color=orangered>DET</font> train and val1 as used in [22].<span style="font-size:80%;opacity:0.8">我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> zoom<br>(2) </td> <td> [zu:m] </td> <td> 
<ul><li>The random crops generated by the strategy can be thought of as a “<font color=orangered>zoom</font> in” operation and can generate many larger training examples.<span style="font-size:80%;opacity:0.8">策略产生的随机裁剪可以被认为是“放大”操作，并且可以产生许多更大的训练样本。</span></li><li>To implement a “<font color=orangered>zoom</font> out” operation that creates more small training examples, we first randomly place an image on a canvas of 16× of the original image size filled with mean values before we do any random crop operation.<span style="font-size:80%;opacity:0.8">为了实现创建更多小型训练样本的“缩小”操作，我们首先将图像随机放置在填充了平均值的原始图像大小为16x的画布上，然后再进行任意的随机裁剪操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> nms<br>(2) </td> <td>  </td> <td> 
<ul><li>Considering the large number of boxes generated from our method, it is essential to perform non-maximum suppression (<font color=forestgreen>nms</font>) efficiently during inference.<span style="font-size:80%;opacity:0.8">考虑到我们的方法产生大量边界框，在推断期间执行非最大值抑制（nms）是必要的。</span></li><li>We then apply <font color=forestgreen>nms</font> with jaccard overlap of 0.45 per class and keep the top 200 detections per image.<span style="font-size:80%;opacity:0.8">然后，我们应用nms，每个类别0.45的Jaccard重叠，并保留每张图像的前200个检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> msec<br>(2) </td> <td> [m'zek] </td> <td> 
<ul><li>This step costs about 1.7 <font color=orangered>msec</font> per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 msec) spent on all newly added layers.<span style="font-size:80%;opacity:0.8">对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li><li>This step costs about 1.7 msec per image for SSD300 and 20 VOC classes, which is close to the total time (2.4 <font color=orangered>msec</font>) spent on all newly added layers.<span style="font-size:80%;opacity:0.8">对于SSD300和20个VOC类别，这个步骤每张图像花费大约1.7毫秒，接近在所有新增层上花费的总时间（2.4毫秒）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> SPPnet<br>(2) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SPPnet</font> [9] speeds up the original R-CNN approach significantly.<span style="font-size:80%;opacity:0.8">SPPnet[9]显著加快了原有的R-CNN方法。</span></li><li>Fast R-CNN [6] extends <font color=forestgreen>SPPnet</font> so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning objectness.<span style="font-size:80%;opacity:0.8">Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> Additionally<br>(1) </td> <td> [ə'dɪʃənəlɪ] </td> <td> 
<ul><li><font color=orangered>Additionally</font>, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes.<span style="font-size:80%;opacity:0.8">此外，网络还结合了不同分辨率的多个特征映射的预测，自然地处理各种尺寸的目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> encapsulate<br>(1) </td> <td> [ɪnˈkæpsjuleɪt] </td> <td> 
<ul><li>SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and <font color=orangered>encapsulates</font> all computation in a single network.<span style="font-size:80%;opacity:0.8">相对于需要目标提出的方法，SSD非常简单，因为它完全消除了提出生成和随后的像素或特征重新采样阶段，并将所有计算封装到单个网络中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> variant<br>(1) </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Current state-of-the-art object detection systems are <font color=orangered>variants</font> of the following approach: hypothesize bounding boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8">目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> hypothesize<br>(1) </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>Current state-of-the-art object detection systems are variants of the following approach: <font color=orangered>hypothesize</font> bounding boxes, resample pixels or features for each box, and apply a high-quality classifier.<span style="font-size:80%;opacity:0.8">目前最先进的目标检测系统是以下方法的变种：假设边界框，每个框重采样像素或特征，并应用一个高质量的分类器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> prevail<br>(1) </td> <td> [prɪˈveɪl] </td> <td> 
<ul><li>This pipeline has <font color=orangered>prevailed</font> on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] albeit with deeper features such as [3].<span style="font-size:80%;opacity:0.8">自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> albeit<br>(1) </td> <td> [ˌɔ:lˈbi:ɪt] </td> <td> 
<ul><li>This pipeline has prevailed on detection benchmarks since the Selective Search work [1] through the current leading results on PASCAL VOC, COCO, and ILSVRC detection all based on Faster R-CNN[2] <font color=orangered>albeit</font> with deeper features such as [3].<span style="font-size:80%;opacity:0.8">自从选择性搜索[1]通过在PASCAL VOC，COCO和ILSVRC上所有基于Faster R-CNN[2]的检测都取得了当前领先的结果（尽管具有更深的特征如[3]），这种流程在检测基准数据上流行开来。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> computationally<br>(1) </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>While accurate, these approaches have been too <font color=orangered>computationally</font> intensive for embedded systems and, even with high-end hardware, too slow for real-time applications.<span style="font-size:80%;opacity:0.8">尽管这些方法准确，但对于嵌入式系统而言，这些方法的计算量过大，即使是高端硬件，对于实时应用而言也太慢。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> SPF<br>(1) </td> <td> [.es piː 'ef] </td> <td> 
<ul><li>Often detection speed for these approaches is measured in seconds per frame (<font color=orangered>SPF</font>), and even the fastest high-accuracy detector, Faster R-CNN, operates at only 7 frames per second (FPS).<span style="font-size:80%;opacity:0.8">通常，这些方法的检测速度是以每帧秒（SPF）度量，甚至最快的高精度检测器，Faster R-CNN，仅以每秒7帧（FPS）的速度运行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> residual<br>(1) </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>This is a larger relative improvement in detection accuracy than that from the recent, very high-profile work on <font color=orangered>residual</font> networks [3].<span style="font-size:80%;opacity:0.8">相比于最近备受瞩目的残差网络方面的工作[3]，在检测精度上这是相对更大的提高。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> trade-off<br>(1) </td> <td> [ˈtreɪdˌɔ:f, -ˌɔf] </td> <td> 
<ul><li>These design features lead to simple end-to-end training and high accuracy, even on low resolution input images, further improving the speed vs accuracy <font color=orangered>trade-off</font>.<span style="font-size:80%;opacity:0.8">这些设计功能使得即使在低分辨率输入图像上也能实现简单的端到端训练和高精度，从而进一步提高速度与精度之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> dataset-specific<br>(1) </td> <td> [!≈ 'deɪtəset spəˈsɪfɪk] </td> <td> 
<ul><li>Afterwards, Sec. 2.3 presents <font color=orangered>dataset-specific</font> model details and experimental results.<span style="font-size:80%;opacity:0.8">之后，2.3节介绍了数据集特有的模型细节和实验结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> feed-forward<br>(1) </td> <td> ['fi:df'ɔ:wəd] </td> <td> 
<ul><li>The SSD approach is based on a <font color=orangered>feed-forward</font> convolutional network that produces a fixed-size collection of bounding boxes and scores for the presence of object class instances in those boxes, followed by a non-maximum suppression step to produce the final detections.<span style="font-size:80%;opacity:0.8">SSD方法基于前馈卷积网络，该网络产生固定大小的边界框集合，并对这些边界框中存在的目标类别实例进行评分，然后进行非极大值抑制步骤来产生最终的检测结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> auxiliary<br>(1) </td> <td> [ɔ:gˈzɪliəri] </td> <td> 
<ul><li>We then add <font color=orangered>auxiliary</font> structure to the network to produce detections with the following key features:<span style="font-size:80%;opacity:0.8">然后，我们将辅助结构添加到网络中以产生具有以下关键特征的检测：</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> optionally<br>(1) </td> <td> ['ɒpʃənəlɪ] </td> <td> 
<ul><li>Convolutional predictors for detection Each added feature layer (or <font color=orangered>optionally</font> an existing feature layer from the base network) can produce a fixed set of detection predictions using a set of convolutional filters.<span style="font-size:80%;opacity:0.8">用于检测的卷积预测器。每个添加的特征层（或者任选的来自基础网络的现有特征层）可以使用一组卷积滤波器产生固定的检测预测集合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> kmn<br>(1) </td> <td>  </td> <td> 
<ul><li>This results in a total of (c+4)k filters that are applied around each location in the feature map, yielding (c+4)<font color=forestgreen>kmn</font> outputs for a $m\times n$ feature map.<span style="font-size:80%;opacity:0.8">这导致在特征映射中的每个位置周围应用总共(c+4)k个滤波器，对于$m\times n$的特征映射取得(c+4)kmn个输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> Fig.1.<br>(1) </td> <td>  </td> <td> 
<ul><li>For an illustration of default boxes, please refer to <font color=forestgreen>Fig.1.</font><span style="font-size:80%;opacity:0.8">有关默认边界框的说明，请参见图1。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> indicator<br>(1) </td> <td> [ˈɪndɪkeɪtə(r)] </td> <td> 
<ul><li>Let $x_{ij}^p = \lbrace 1,0 \rbrace$ be an <font color=orangered>indicator</font> for matching the i-th default box to the j-th ground truth box of category p.<span style="font-size:80%;opacity:0.8">设$x_{ij}^p = \lbrace 1,0 \rbrace$是第i个默认边界框匹配到类别p的第j个实际边界框的指示器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> mimic<br>(1) </td> <td> [ˈmɪmɪk] </td> <td> 
<ul><li>However, by utilizing feature maps from several different layers in a single network for prediction we can <font color=orangered>mimic</font> the same effect, while also sharing parameters across all object scales.<span style="font-size:80%;opacity:0.8">然而，通过利用单个网络中几个不同层的特征映射进行预测，我们可以模拟相同的效果，同时还可以跨所有目标尺度共享参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> semantic<br>(1) </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>Previous works [10,11] have shown that using feature maps from the lower layers can improve <font color=orangered>semantic</font> segmentation quality because the lower layers capture more fine details of the input objects.<span style="font-size:80%;opacity:0.8">以前的工作[10,11]已经表明，使用低层的特征映射可以提高语义分割的质量，因为低层会捕获输入目标的更多细节。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> exemplar<br>(1) </td> <td> [ɪgˈzemplɑ:(r)] </td> <td> 
<ul><li>Figure 1 shows two <font color=orangered>exemplar</font> feature maps (8 × 8 and 4 × 4) which are used in the framework.<span style="font-size:80%;opacity:0.8">图1显示了框架中使用的两个示例性特征映射（8×8和4×4）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> empirical<br>(1) </td> <td> [ɪmˈpɪrɪkl] </td> <td> 
<ul><li>Feature maps from different levels within a network are known to have different (<font color=orangered>empirical</font>) receptive field sizes [13].<span style="font-size:80%;opacity:0.8">已知网络中不同层的特征映射具有不同的（经验的）感受野大小[13]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> imbalance<br>(1) </td> <td> [ɪmˈbæləns] </td> <td> 
<ul><li>This introduces a significant <font color=orangered>imbalance</font> between the positive and negative training examples.<span style="font-size:80%;opacity:0.8">这在正的训练实例和负的训练实例之间引入了显著的不平衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> aforementioned<br>(1) </td> <td> [əˌfɔ:ˈmenʃənd] </td> <td> 
<ul><li>After the <font color=orangered>aforementioned</font> sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8">在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> resize<br>(1) </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is <font color=orangered>resized</font> to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8">在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> horizontally<br>(1) </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is <font color=orangered>horizontally</font> flipped with probability of 0.5, in addition to applying some photo-metric distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8">在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> photo-metric<br>(1) </td> <td> [!≈ ˈfəʊtəʊ ˈmetrɪk] </td> <td> 
<ul><li>After the aforementioned sampling step, each sampled patch is resized to fixed size and is horizontally flipped with probability of 0.5, in addition to applying some <font color=orangered>photo-metric</font> distortions similar to those described in [14].<span style="font-size:80%;opacity:0.8">在上述采样步骤之后，除了应用类似于文献[14]中描述的一些光度变形之外，将每个采样图像块调整到固定尺寸并以0.5的概率进行水平翻转。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> VGG16[15<br>(1) </td> <td>  </td> <td> 
<ul><li>Base network Our experiments are all based on <font color=forestgreen>VGG16[15</font>], which is pre-trained on the ILSVRC CLS-LOC dataset[16].<span style="font-size:80%;opacity:0.8">基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> CLS-LOC<br>(1) </td> <td>  </td> <td> 
<ul><li>Base network Our experiments are all based on VGG16[15], which is pre-trained on the ILSVRC <font color=forestgreen>CLS-LOC</font> dataset[16].<span style="font-size:80%;opacity:0.8">基础网络。我们的实验全部基于VGG16[15]，它是在ILSVRC CLS-LOC数据集[16]上预先训练的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> subsample<br>(1) </td> <td> ['sʌbsɑ:mpl] </td> <td> 
<ul><li>Similar to DeepLab-LargeFOV[17], we convert fc6 and fc7 to convolutional layers, <font color=orangered>subsample</font> parameters from fc6 and fc7, change pool5 from $2\times 2$-s2 to $3\times 3$-s1, and use the atrous algorithm[18] to fill the “holes”.<span style="font-size:80%;opacity:0.8">类似于DeepLab-LargeFOV[17]，我们将fc6和fc7转换为卷积层，从fc6和fc7中重采样参数，将pool5从$2\times 2$-s2更改为$3\times 3$-s1，并使用空洞算法[18]来填补这个“小洞”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> SGD<br>(1) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>We fine-tune the resulting model using <font color=orangered>SGD</font> with initial learning rate $10^{-3}$, 0.9 momentum, 0.0005 weight decay, and batch size 32.<span style="font-size:80%;opacity:0.8">我们使用SGD对得到的模型进行微调，初始学习率为$10^{-3}$，动量为0.9，权重衰减为0.0005，批数据大小为32。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> momentum<br>(1) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We fine-tune the resulting model using SGD with initial learning rate $10^{-3}$, 0.9 <font color=orangered>momentum</font>, 0.0005 weight decay, and batch size 32.<span style="font-size:80%;opacity:0.8">我们使用SGD对得到的模型进行微调，初始学习率为$10^{-3}$，动量为0.9，权重衰减为0.0005，批数据大小为32。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> Caffe<br>(1) </td> <td>  </td> <td> 
<ul><li>The full training and testing code is built on <font color=forestgreen>Caffe</font>[19] and is open source at: https://github.com/weiliu89/caffe/tree/ssd.<span style="font-size:80%;opacity:0.8">完整的训练和测试代码建立在Caffe[19]上并开源：https://github.com/weiliu89/caffe/tree/ssd。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> xavier<br>(1) </td> <td> ['zʌvɪə] </td> <td> 
<ul><li>We initialize the parameters for all the newly added convolutional layers with the “<font color=orangered>xavier</font>” method [20].<span style="font-size:80%;opacity:0.8">我们使用“xavier”方法[20]初始化所有新添加的卷积层的参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> surpass<br>(1) </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>When we train SSD on a larger $512\times 512$ input image, it is even more accurate, <font color=orangered>surpassing</font> Faster R-CNN by $1.7\%$ mAP.<span style="font-size:80%;opacity:0.8">当我们用更大的$512\times 512$输入图像上训练SSD时，它更加准确，超过了Faster R-CNN $1.7\%$的mAP。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> i.e.<br>(1) </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>If we train SSD with more (<font color=orangered>i.e.</font> 07+12) data, we see that SSD300 is already better than Faster R-CNN by 1.1\% and that SSD512 is $3.6\%$ better.<span style="font-size:80%;opacity:0.8">如果我们用更多的（即07+12）数据来训练SSD，我们看到SSD300已经比Faster R-CNN好$1.1\%$，SSD512比Faster R-CNN好$3.6\%$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> decouple<br>(1) </td> <td> [di:ˈkʌpl] </td> <td> 
<ul><li>Compared to R-CNN [22], SSD has less localization error, indicating that SSD can localize objects better because it directly learns to regress the object shape and classify object categories instead of using two <font color=orangered>decoupled</font> steps.<span style="font-size:80%;opacity:0.8">与R-CNN[22]相比，SSD具有更小的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归目标形状和分类目标类别，而不是使用两个解耦步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> Visualization<br>(1) </td> <td> [ˌvɪʒʊəlaɪ'zeɪʃn] </td> <td> 
<ul><li>Fig. 3: <font color=orangered>Visualization</font> of performance for SSD512 on animals, vehicles, and furniture from VOC2007 test.<span style="font-size:80%;opacity:0.8">图3：SSD512在VOC2007 test中的动物，车辆和家具上的性能可视化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> cumulative<br>(1) </td> <td> [ˈkju:mjələtɪv] </td> <td> 
<ul><li>The top row shows the <font color=orangered>cumulative</font> fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8">第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Cor<br>(1) </td> <td> [kɔ:(r)] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (<font color=orangered>Cor</font>) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8">第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> Sim<br>(1) </td> <td> [sɪm] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (<font color=orangered>Sim</font>), with others (Oth), or with background (BG).<span style="font-size:80%;opacity:0.8">第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> Oth<br>(1) </td> <td>  </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (<font color=forestgreen>Oth</font>), or with background (BG).<span style="font-size:80%;opacity:0.8">第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> BG<br>(1) </td> <td> [!≈ bi: dʒi:] </td> <td> 
<ul><li>The top row shows the cumulative fraction of detections that are correct (Cor) or false positive due to poor localization (Loc), confusion with similar categories (Sim), with others (Oth), or with background (<font color=orangered>BG</font>).<span style="font-size:80%;opacity:0.8">第一行显示由于定位不佳（Loc），与相似类别（Sim）混淆，与其它（Oth）或背景（BG）相关的正确检测（Cor）或假阳性的累积分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> dash<br>(1) </td> <td> [dæʃ] </td> <td> 
<ul><li>The <font color=orangered>dashed</font> red line is using the weak criteria (0.1 jaccard overlap).<span style="font-size:80%;opacity:0.8">红色虚线是使用弱标准（0.1 Jaccard重叠）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> top-ranked<br>(1) </td> <td> ['tɒpr'æŋkt] </td> <td> 
<ul><li>The bottom row shows the distribution of <font color=orangered>top-ranked</font> false positive types.<span style="font-size:80%;opacity:0.8">最下面一行显示了排名靠前的假阳性类型的分布。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> XW<br>(1) </td> <td> [!≈ eks 'dʌblju:] </td> <td> 
<ul><li>Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; <font color=orangered>XW</font> =extra-wide.<span style="font-size:80%;opacity:0.8">长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> extra-wide<br>(1) </td> <td> [!≈ ˈekstrə waɪd] </td> <td> 
<ul><li>Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =<font color=orangered>extra-wide</font>.<span style="font-size:80%;opacity:0.8">长宽比：XT=超高/窄；T=高；M=中等；W=宽；XW =超宽。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> subsampled<br>(1) </td> <td>  </td> <td> 
<ul><li>As described in Sec. 3, we used the atrous version of a <font color=forestgreen>subsampled</font> VGG16, following DeepLab-LargeFOV [17].<span style="font-size:80%;opacity:0.8">如第3节所述，我们根据DeepLab-LargeFOV[17]使用子采样的VGG16的空洞版本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> subsampling<br>(1) </td> <td>  </td> <td> 
<ul><li>If we use the full VGG16, keeping pool5 with 2×2−s2 and not <font color=forestgreen>subsampling</font> parameters from fc6 and fc7, and add conv5 3 for prediction, the result is about the same while the speed is about $20\%$ slower.<span style="font-size:80%;opacity:0.8">如果我们使用完整的VGG16，保持pool5为2×2-s2，并且不从fc6和fc7中子采样参数，并添加conv5_3进行预测，结果大致相同，而速度慢了大约20%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> exhaustively<br>(1) </td> <td> [ɪɡ'zɔ:stɪvlɪ] </td> <td> 
<ul><li>We do not <font color=orangered>exhaustively</font> optimize the tiling for each setting.<span style="font-size:80%;opacity:0.8">我们没有详尽地优化每个设置的平铺。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> monotonically<br>(1) </td> <td> [mɒnə'tɒnɪklɪ] </td> <td> 
<ul><li>Table 3 shows a decrease in accuracy with fewer layers, dropping <font color=orangered>monotonically</font> from 74.3 to 62.4.<span style="font-size:80%;opacity:0.8">表3显示层数较少，精度降低，从74.3单调递减至62.4。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> ROI<br>(1) </td> <td> [rwɑ:] </td> <td> 
<ul><li>Besides, since our predictions do not rely on <font color=orangered>ROI</font> pooling as in [6], we do not have the collapsing bins problem in low-resolution feature maps [23].<span style="font-size:80%;opacity:0.8">此外，由于我们的预测不像[6]那样依赖于ROI池化，所以我们在低分辨率特征映射中没有折叠组块的问题[23]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> conjecture<br>(1) </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>$3.9\%$). We <font color=orangered>conjecture</font> that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box refinement steps, in both the RPN part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8">我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> refinement<br>(1) </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>$3.9\%$). We conjecture that Faster R-CNN is more competitive on smaller objects with SSD because it performs two box <font color=orangered>refinement</font> steps, in both the RPN part and in the Fast R-CNN part.<span style="font-size:80%;opacity:0.8">我们推测Faster R-CNN在较小的目标上比SSD更具竞争力，因为它在RPN部分和Fast R-CNN部分都执行了两个边界框细化步骤。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> Preliminary<br>(1) </td> <td> [prɪˈlɪmɪnəri] </td> <td> 
<ul><li>3.5 <font color=orangered>Preliminary</font> ILSVRC results<span style="font-size:80%;opacity:0.8">3.5 初步的ILSVRC结果</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> ILSVRC2014<br>(1) </td> <td>  </td> <td> 
<ul><li>We train a SSD300 model using the <font color=forestgreen>ILSVRC2014</font> DET train and val1 as used in [22].<span style="font-size:80%;opacity:0.8">我们使用[22]中使用的ILSVRC2014 DETtrain和val1来训练SSD300模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> follow-up<br>(1) </td> <td> ['fɒləʊ ʌp] </td> <td> 
<ul><li>Without a <font color=orangered>follow-up</font> feature resampling step as in Faster R-CNN, the classification task for small objects is relatively hard for SSD, as demonstrated in our analysis (see Fig. 4).<span style="font-size:80%;opacity:0.8">SSD没有如Faster R-CNN中后续的特征重采样步骤，小目标的分类任务对SSD来说相对困难，正如我们的分析（见图4）所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> underscore<br>(1) </td> <td> [ˌʌndəˈskɔ:(r)] </td> <td> 
<ul><li>This result <font color=orangered>underscores</font> the importance of the data augmentation strategy for the final model accuracy.<span style="font-size:80%;opacity:0.8">这个结果强调了数据增强策略对最终模型精度的重要性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> align<br>(1) </td> <td> [əˈlaɪn] </td> <td> 
<ul><li>An alternative way of improving SSD is to design a better tiling of default boxes so that its position and scale are better <font color=orangered>aligned</font> with the receptive field of each position on a feature map.<span style="font-size:80%;opacity:0.8">改进SSD的另一种方法是设计一个更好的默认边界框平铺，使其位置和尺度与特征映射上每个位置的感受野更好地对齐。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> cuDNN<br>(1) </td> <td>  </td> <td> 
<ul><li>We measure the speed with batch size 8 using Titan X and <font color=forestgreen>cuDNN</font> v4 with Intel Xeon E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8">我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> Xeon<br>(1) </td> <td>  </td> <td> 
<ul><li>We measure the speed with batch size 8 using Titan X and cuDNN v4 with Intel <font color=forestgreen>Xeon</font> E5-2667v3@3.20GHz.<span style="font-size:80%;opacity:0.8">我们使用Titan X、cuDNN v4、Intel Xeon E5-2667v3@3.20GHz以及批大小为8来测量速度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> advent<br>(1) </td> <td> [ˈædvent] </td> <td> 
<ul><li>Before the <font color=orangered>advent</font> of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (DPM) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8">在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> Deformable<br>(1) </td> <td> [dɪ'fɔ:məbl] </td> <td> 
<ul><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— <font color=orangered>Deformable</font> Part Model (DPM) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8">在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> DPM<br>(1) </td> <td> [!≈ di: pi: em] </td> <td> 
<ul><li>Before the advent of convolutional neural networks, the state of the art for those two approaches —— Deformable Part Model (<font color=orangered>DPM</font>) [26] and Selective Search [1] —— had comparable performance.<span style="font-size:80%;opacity:0.8">在卷积神经网络出现之前，这两种方法的最新技术——可变形部件模型（DPM）[26]和选择性搜索[1]——具有相当的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> prevalent<br>(1) </td> <td> [ˈprevələnt] </td> <td> 
<ul><li>However, after the dramatic improvement brought on by R-CNN [22], which combines selective search region proposals and convolutional network based post-classification, region proposal object detection methods became <font color=orangered>prevalent</font>.<span style="font-size:80%;opacity:0.8">然而，在R-CNN[22]结合选择性搜索区域提出和基于后分类的卷积网络带来的显著改进后，区域提出目标检测方法变得流行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> time-consuming<br>(1) </td> <td> [taɪm kən'sju:mɪŋ] </td> <td> 
<ul><li>The first set of approaches improve the quality and speed of post-classification, since it requires the classification of thousands of image crops, which is expensive and <font color=orangered>time-consuming</font>.<span style="font-size:80%;opacity:0.8">第一套方法提高了后分类的质量和速度，因为它需要对成千上万的裁剪图像进行分类，这是昂贵和耗时的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> objectness<br>(1) </td> <td> [!≈ ˈɒbdʒɪktnəs] </td> <td> 
<ul><li>Fast R-CNN [6] extends SPPnet so that it can fine-tune all layers end-to-end by minimizing a loss for both confidences and bounding box regression, which was first introduced in MultiBox [7] for learning <font color=orangered>objectness</font>.<span style="font-size:80%;opacity:0.8">Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端的微调，最初在MultiBox[7]中引入用于学习目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> setup<br>(1) </td> <td> ['setʌp] </td> <td> 
<ul><li>This further improves the detection accuracy but results in a somewhat complex <font color=orangered>setup</font>, requiring the training of two neural networks with a dependency between them.<span style="font-size:80%;opacity:0.8">这进一步提高了检测精度，但是导致了一些复杂的设置，需要训练两个具有依赖关系的神经网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> complication<br>(1) </td> <td> [ˌkɒmplɪˈkeɪʃn] </td> <td> 
<ul><li>Thus, our approach avoids the <font color=orangered>complication</font> of merging RPN with Fast R-CNN and is easier to train, faster, and straightforward to integrate in other tasks.<span style="font-size:80%;opacity:0.8">因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快且更直接地集成到其它任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> experimentally<br>(1) </td> <td> [ɪkˌsperɪ'mentəlɪ] </td> <td> 
<ul><li>We <font color=orangered>experimentally</font> validate that given appropriate training strategies, a larger number of carefully chosen default bounding boxes results in improved performance.<span style="font-size:80%;opacity:0.8">我们通过实验验证，在给定合适训练策略的情况下，大量仔细选择的默认边界框会提高性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> favorably<br>(1) </td> <td> ['feɪvərəblɪ] </td> <td> 
<ul><li>We demonstrate that given the same VGG-16 base architecture, SSD compares <font color=orangered>favorably</font> to its state-of-the-art object detector counterparts in terms of both accuracy and speed.<span style="font-size:80%;opacity:0.8">我们证明了给定相同的VGG-16基础架构，SSD在准确性和速度方面与其对应的最先进的目标检测器相比毫不逊色。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> standalone<br>(1) </td> <td> ['stændəˌləʊn] </td> <td> 
<ul><li>Apart from its <font color=orangered>standalone</font> utility, we believe that our monolithic and relatively simple SSD model provides a useful building block for larger systems that employ an object detection component.<span style="font-size:80%;opacity:0.8">除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> monolithic<br>(1) </td> <td> [ˌmɒnə'lɪθɪk] </td> <td> 
<ul><li>Apart from its standalone utility, we believe that our <font color=orangered>monolithic</font> and relatively simple SSD model provides a useful building block for larger systems that employ an object detection component.<span style="font-size:80%;opacity:0.8">除了单独使用之外，我们相信我们的整体和相对简单的SSD模型为采用目标检测组件的大型系统提供了有用的构建模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> recurrent<br>(1) </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>A promising future direction is to explore its use as part of a system using <font color=orangered>recurrent</font> neural networks to detect and track objects in video simultaneously.<span style="font-size:80%;opacity:0.8">一个有前景的未来方向是探索它作为系统的一部分，使用循环神经网络来同时检测和跟踪视频中的目标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> Acknowledgment<br>(1) </td> <td> [ək'nɒlɪdʒmənt] </td> <td> 
<ul><li>6. <font color=orangered>Acknowledgment</font><span style="font-size:80%;opacity:0.8">6. 致谢</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> internship<br>(1) </td> <td> [ˈɪntɜ:nʃɪp] </td> <td> 
<ul><li>This work was started as an <font color=orangered>internship</font> project at Google and continued at UNC.<span style="font-size:80%;opacity:0.8">这项工作是在谷歌的一个实习项目开始的，并在UNC继续。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> UNC<br>(1) </td> <td> [ʌŋk] </td> <td> 
<ul><li>This work was started as an internship project at Google and continued at <font color=orangered>UNC</font>.<span style="font-size:80%;opacity:0.8">这项工作是在谷歌的一个实习项目开始的，并在UNC继续。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> Alex<br>(1) </td> <td> ['ælɪkʃ] </td> <td> 
<ul><li>We would like to thank <font color=orangered>Alex</font> Toshev for helpful discussions and are indebted to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8">我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> Toshev<br>(1) </td> <td>  </td> <td> 
<ul><li>We would like to thank Alex <font color=forestgreen>Toshev</font> for helpful discussions and are indebted to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8">我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> indebted<br>(1) </td> <td> [ɪnˈdetɪd] </td> <td> 
<ul><li>We would like to thank Alex Toshev for helpful discussions and are <font color=orangered>indebted</font> to the Image Understanding and DistBelief teams at Google.<span style="font-size:80%;opacity:0.8">我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> DistBelief<br>(1) </td> <td>  </td> <td> 
<ul><li>We would like to thank Alex Toshev for helpful discussions and are indebted to the Image Understanding and <font color=forestgreen>DistBelief</font> teams at Google.<span style="font-size:80%;opacity:0.8">我们要感谢Alex Toshev进行有益的讨论，并感谢Google的Image Understanding和DistBelief团队。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> Ammirato<br>(1) </td> <td>  </td> <td> 
<ul><li>We also thank Philip <font color=forestgreen>Ammirato</font> and Patrick Poirson for helpful comments.<span style="font-size:80%;opacity:0.8">我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> Patrick<br>(1) </td> <td> [ˈpætrik] </td> <td> 
<ul><li>We also thank Philip Ammirato and <font color=orangered>Patrick</font> Poirson for helpful comments.<span style="font-size:80%;opacity:0.8">我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> Poirson<br>(1) </td> <td>  </td> <td> 
<ul><li>We also thank Philip Ammirato and Patrick <font color=forestgreen>Poirson</font> for helpful comments.<span style="font-size:80%;opacity:0.8">我们也感谢Philip Ammirato和Patrick Poirson提供有用的意见。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> NSF<br>(1) </td> <td> [!≈ en es ef] </td> <td> 
<ul><li>We thank NVIDIA for providing GPUs and acknowledge support from <font color=orangered>NSF</font> 1452851, 1446631, 1526367, 1533771.<span style="font-size:80%;opacity:0.8">我们感谢NVIDIA提供的GPU，并对NSF 1452851,1446631,1526367,1533771的支持表示感谢。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>