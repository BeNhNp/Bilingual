<html>
<head>
<meta charset="utf-8">
<title> RARE - Robust Scene Text Recognition with Automatic Rectification </title>
<link href="../../../configs/common.css" rel="stylesheet" type="text/css"/>
<script src="../../../configs/common.js" type="text/javascript"></script>
<script type="text/javascript"
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none", CommonHTML: { scale: 50 }
    });
</script>
</head>
<body>
<div class="chapter_part">
<div class="paragraph_part">
    <div class="src">Robust Scene Text Recognition with Automatic <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">Rectification</span><span class="des" title="具有自动校正的可靠场景文本识别器"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Shi, Baoguang (School of Electronic Information and Communications, Huazhong University of Science and Technology, China); Wang, Xinggang; Lyu, Pengyuan; Yao, Cong; Bai, Xiang</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Source:  Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, v 2016-December, p 4168-4176, December 9, 2016, Proceedings - 29th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">url: https://github.com/tongpi/basicOCR/blob/master/docs/yushan/RARE.md<span class="des" title="原文链接"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Abstract<span class="des" title="摘要"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Recognizing text in natural images is a challenging task with many <span class="word_hot" title="unsolved [ˌʌnˈsɒlvd]">unsolved</span> problems.<span class="des" title="识别自然图像中的文本是一项具有挑战性的任务，存在许多未解决的问题。"></span></div>
    <div class="src">Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character <span class="word_hot" title="placement [ˈpleɪsmənt]">placement</span>, etc. We propose RARE (Robust text <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizer</span> with Automatic <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">REctification</span>), a recognition model that is robust to irregular text.<span class="des" title="与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。"></span></div>
    <div class="src">RARE is a specially designed deep neural network, which consists of a Spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">Transformer</span> Network (<span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>) and a Sequence Recognition Network (<span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>).<span class="des" title="RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。"></span></div>
    <div class="src">In testing, an image is firstly <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> via a predicted <span class="word_hot_synth" title="Thin-Plate-Spline [!≈ θɪn pleɪt splaɪn]">Thin-Plate-Spline</span> (<span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span>) transformation, into a more “<span class="word_hot" title="readable [ˈri:dəbl]">readable</span>” image for the following <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>, which recognizes text through a sequence recognition approach.<span class="des" title="在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。"></span></div>
    <div class="src">We show that the model is able to recognize several types of irregular text, including perspective text and curved text.<span class="des" title="我们证明该模型能够识别几种类型的不规则文本，包括透视文本和弯曲文本。"></span></div>
    <div class="src">RARE is end-to-end <span class="word_hot" title="trainable [t'reɪnəbl]">trainable</span>, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems.<span class="des" title="RARE是端到端的可训练的，只需要图像和相关的文本标签，便于在实际系统中训练和部署模型。"></span></div>
    <div class="src">State-of-the-art or highly-competitive performance achieved on several benchmarks well demonstrates the effectiveness of the proposed model.<span class="des" title="在若干基准测试中取得的最先进或极具竞争力的性能很好地证明了所提出模型的有效性。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">1. Introduction<span class="des" title="1. 简介"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In natural scenes, text appears on various kinds of objects, <span class="word_hot" title="e.g. [ˌi: ˈdʒi:]">e.g.</span> road signs, <span class="word_hot" title="billboard [ˈbɪlbɔ:d]">billboards</span>, and product packaging.<span class="des" title="在自然场景中，文本出现在各种对象上，例如道路标志，广告牌和产品包装。"></span></div>
    <div class="src">It carries rich and high-level <span class="word_hot" title="semantic [sɪˈmæntɪk]">semantic</span> information that is important for image understanding.<span class="des" title="它携带丰富的高级语义信息，这对于图像理解非常重要。"></span></div>
    <div class="src">Recognizing text in images facilitates many <span class="word_hot_synth" title="real-world [!≈ ˈri:əl wɜ:ld]">real-world</span> applications, such as <span class="word_hot" title="geolocation [dʒɪɒləʊ'keɪʃn]">geolocation</span>, <span class="word_hot" title="driverless [d'raɪvərles]">driverless</span> car, and image-based machine translation.<span class="des" title="识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。"></span></div>
    <div class="src">For these reasons, scene text recognition has attracted great interest from the community [28, 37, 15].<span class="des" title="由于这些原因，场景文本识别引起了社区的极大兴趣[28,37,15]。"></span></div>
    <div class="src">Despite the maturity of the research on Optical Character Recognition (OCR) [26], recognizing text in natural images, rather than scanned documents, is still challenging.<span class="des" title="尽管光学字符识别（OCR）研究成熟[26]，但识别自然图像中的文本而不是扫描文档仍然具有挑战性。"></span></div>
    <div class="src">Scene text images exhibit large variations in the aspects of illumination, motion blur, text font, color, etc. Moreover, text in the wild may have irregular shape.<span class="des" title="场景文本图像在照明，移动模糊，文本字体，颜色等方面表现出很大的变化。此外，野外文本可能具有不规则的形状。"></span></div>
    <div class="src">For example, some scene text is perspective text [29], which is caused by <span class="word_hot" title="side-view ['saɪdvj'u:]">side-view</span> camera angles; some has curved shapes, meaning that its characters are placed along curves rather than straight lines.<span class="des" title="例如，一些场景文本是透视文本[29]，它是由侧视摄像机角度引起的;有些具有弯曲的形状，这意味着它的角色沿着曲线而不是直线放置。"></span></div>
    <div class="src">We call such text irregular text, in contrast to regular text which is horizontal and <span class="word_hot" title="frontal [ˈfrʌntl]">frontal</span>.<span class="des" title="我们将此类文本称为不规则文本，与常规文本（水平和正面）形成对比。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig01.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 1</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 1. <span class="word_hot" title="Schematic [ski:ˈmætɪk]">Schematic</span> overview of RARE, which consists a spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">transformer</span> network (<span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>) and a sequence recognition network (<span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>).<span class="des" title="图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> transforms an input image to a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image, while the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> recognizes text.<span class="des" title="STN将输入图像变换为矫正图像，而SRN识别文本。"></span></div>
    <div class="src">The two networks are <span class="word_hot" title="jointly [dʒɔɪntlɪ]">jointly</span> trained by the back-propagation algorithm [22].<span class="des" title="这两个网络由反向传播算法共同训练[22]。"></span></div>
    <div class="src">The <span class="word_hot" title="dash [dæʃ]">dashed</span> lines represent the ﬂ<span class="word_hot" title="ow [aʊ]">ows</span> of the <span class="word_hot_synth" title="back-propagated [!≈ bæk ˈprɔpəɡeitid]">back-propagated</span> gradients.<span class="des" title="虚线表示反向传播的梯度的流动。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Usually, a text <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizer</span> works best when its input images contain <span class="word_hot_synth" title="tightly-bounded [!≈ ˈtaɪtli 'baʊndɪd]">tightly-bounded</span> regular text.<span class="des" title="通常，文本识别器在其输入图像包含紧密有界的常规文本时效果最佳。"></span></div>
    <div class="src">This motivates us to apply a spatial transformation prior to recognition, in order to <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectify</span> input images into ones that are more “<span class="word_hot" title="readable [ˈri:dəbl]">readable</span>” by <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizers</span>.<span class="des" title="这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。"></span></div>
    <div class="src">In this paper, we propose a recognition method that is robust to irregular text.<span class="des" title="在本文中，我们提出了一种对不规则文本具有鲁棒性的识别方法。"></span></div>
    <div class="src">Specifically, we construct a deep neural network that combines a Spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">Transformer</span> Network [18] (<span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>) and a Sequence Recognition Network (<span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>).<span class="des" title="具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。"></span></div>
    <div class="src">An overview of the model is given in fig. 1.<span class="des" title="该模型的概述如图1所示。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>, an input image is <span class="word_hot" title="spatially ['speɪʃəlɪ]">spatially</span> transformed into a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image.<span class="des" title="在STN中，输入图像在空间上变换成校正后的图像。"></span></div>
    <div class="src"><span class="word_hot" title="ideally [aɪ'di:əlɪ]">Ideally</span>, the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> produces an image that contains regular text, which is a more appropriate input for the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> than the original one.<span class="des" title="在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。"></span></div>
    <div class="src">The transformation is a <span class="word_hot_rare">thinplate-spline</span> [6] (<span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span>) transformation, whose <span class="word_hot" title="nonlinearity [nɒnlɪnɪ'ærɪtɪ]">nonlinearity</span> allows us to <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectify</span> various types of irregular text, including perspective and curved text.<span class="des" title="STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation is <span class="word_hot" title="configure [kənˈfɪgə(r)]">configured</span> by a set of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points, whose coordinates are <span class="word_hot" title="regress [rɪˈgres]">regressed</span> by a convolutional neural network.<span class="des" title="TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In an image that contains regular text, characters are arranged along a horizontal line.<span class="des" title="常规文字的图像中，字符沿水平线排列。"></span></div>
    <div class="src">It bares some resemblance to a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> signal.<span class="des" title="它有点类似于顺序信号。"></span></div>
    <div class="src">Motivated by this, for the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> we construct an attention-based model [4] that recognizes text in a sequence recognition approach.<span class="des" title="受此启发，我们构建了SRN，这是一种在序列识别中采用了注意力的模型。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> consists of an encoder and a decoder.<span class="des" title="SRN由编码器和解码器构成。"></span></div>
    <div class="src">Given an input image, the encoder generates a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> feature representation, which is a sequence of feature vectors.<span class="des" title="编码器将输入的图像表示成序列的特征，即一系列的特征向量。"></span></div>
    <div class="src">The decoder <span class="word_hot" title="recurrently [rɪ'kʌrəntlɪ]">recurrently</span> generates a character sequence conditioning on the input sequence, by <span class="word_hot" title="decode [ˌdi:ˈkəʊd]">decoding</span> the relevant contents which are determined by its attention mechanism at each step.<span class="des" title="解码器会根据注意力机制进行解码，循序地生成识别出的字符序列。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We show that, with proper initialization, the whole model can be trained end-to-end.<span class="des" title="如果模型能够正确的初始化，那么整个模型是可以端到端训练的。"></span></div>
    <div class="src">Consequently, for the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>, we do not need to label any <span class="word_hot" title="geometric [ˌdʒi:əˈmetrɪk]">geometric</span> ground truth, <span class="word_hot" title="i.e. [ˌaɪ ˈi:]">i.e.</span> the positions of the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points, but let its training be supervised by the error <span class="word_hot" title="differential [ˌdɪfəˈrenʃl]">differentials</span> <span class="word_hot_synth" title="back-propagated [!≈ bæk ˈprɔpəɡeitid]">back-propagated</span> by the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>.<span class="des" title="因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。"></span></div>
    <div class="src">In practice, the training eventually makes the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> tend to produce images that contain regular text, which are desirable inputs for the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>.<span class="des" title="在实践中，训练最终会使STN倾向于产生包含常规文本的图像，这些图像正是SRN的理想输入。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The contributions of this paper are three-fold: first, we propose a novel scene text recognition method that is robust to irregular text.<span class="des" title="本文的贡献有三个方面：首先，我们提出了一种新颖的自然场景的文本识别方法，该方法对不规则文字十分鲁棒。"></span></div>
    <div class="src">Second, our model extends the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> framework [18] with an attention-based model.<span class="des" title="第二，我们的模型扩展了以注意为基础的STN框架的模型。"></span></div>
    <div class="src">The original <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is only tested on plain convolutional neural networks.<span class="des" title="原本的STN仅在普通卷积神经网络上进行测试。"></span></div>
    <div class="src">Third, our model adopts a <span class="word_hot_synth" title="convolutional-recurrent [!≈ kɒnvə'lu:ʃənəl rɪˈkʌrənt]">convolutional-recurrent</span> structure in the encoder of the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>, thus is a novel <span class="word_hot" title="variant [ˈveəriənt]">variant</span> of the attention-based model [4].<span class="des" title="第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">2. Related Work<span class="des" title="2. 相关工作"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In recent years, a rich body of literature concerning scene text recognition has been published.<span class="des" title="近年来，出版了大量关于场景文本识别的文献。"></span></div>
    <div class="src">Comprehensive surveys have been given in [40, 44].<span class="des" title="在[40,44]中给出了综合调查。"></span></div>
    <div class="src">Among the traditional methods, many adopt bottom-up approaches, where individual characters are firstly detected using sliding window [36, 35], connected components [28], or <span class="word_hot" title="Hough [hɒk]">Hough</span> voting [39].<span class="des" title="在传统方法中，许多方法采用自下而上的方法，其中首先使用滑动窗口[36,35]，连通组件[28]或霍夫投票[39]来检测单个字符。"></span></div>
    <div class="src">Following that, the detected characters are integrated into words by means of dynamic programming, <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> search [35], etc. Other work adopts top-down approaches, where text is directly recognized from entire input images, rather than detecting and recognizing individual characters.<span class="des" title="之后，通过动态编程，词典搜索[35]等将检测到的字符集成到单词中。其他工作采用自上而下的方法，其中文本直接从整个输入图像中识别，而不是检测和识别单个字符。"></span></div>
    <div class="src">For example, <span class="word_hot_rare">Alm´azan</span> et al. [2] propose to predict label embedding vectors from input images.<span class="des" title="例如，Alm'azan等人[2]建议从输入图像预测标签嵌入向量。"></span></div>
    <div class="src"><span class="word_hot_rare">Jaderberg</span> et al. [17] address text recognition with a 90<span class="word_hot_synth" title="k-class [!≈ keɪ klɑ:s]">k-class</span> convolutional neural network, where each class corresponds to an English word.<span class="des" title="Jaderberg等人[17]使用90k级卷积神经网络进行文本识别，其中每个类对应一个英语单词。"></span></div>
    <div class="src">In [16], a CNN with a structured output layer is constructed for <span class="word_hot" title="unconstrained [ˌʌnkən'streɪnd]">unconstrained</span> text recognition.<span class="des" title="在[16]中，构造具有结构化输出层的CNN用于无约束文本识别。"></span></div>
    <div class="src">Some recent work models the problem as a sequence recognition problem, where text is represented by character sequence.<span class="des" title="最近的一些工作将问题建模为序列识别问题，其中文本由字符序列表示。"></span></div>
    <div class="src">Su and Lu [34] extract <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> image representation, which is a sequence of <span class="word_hot" title="HOG [hɒg]">HOG</span> [10] <span class="word_hot" title="descriptor [dɪˈskrɪptə(r)]">descriptors</span>, and predict the corresponding character sequence with a <span class="word_hot" title="recurrent [rɪˈkʌrənt]">recurrent</span> neural network (RNN).<span class="des" title="Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。"></span></div>
    <div class="src">Shi et al. [32] propose an end-to-end sequence recognition network which combines CNN and RNN.<span class="des" title="施等人[32]提出了一种组合CNN和RNN的端到端序列识别网络。"></span></div>
    <div class="src">Our method also adopts the sequence prediction scheme, but we further take the problem of irregular text into account.<span class="des" title="我们的方法也采用序列预测方案，但我们进一步考虑了不规则文本的问题。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Although being common in the tasks of scene text detection and recognition, the issue of irregular text is relatively less addressed in explicit ways.<span class="des" title="虽然在场景文本检测和识别的任务中很常见，但是不规则文本的问题相对较少以明确的方式解决。"></span></div>
    <div class="src">Yao et al. [38] firstly propose the multi-oriented text detection problem, and deal with it by carefully designing <span class="word_hot_synth" title="rotation-invariant [!≈ rəʊˈteɪʃn ɪnˈveəriənt]">rotation-invariant</span> region <span class="word_hot" title="descriptor [dɪˈskrɪptə(r)]">descriptors</span>.<span class="des" title="姚等人[38]首先提出了多方向文本检测问题，并通过仔细设计旋转不变区域描述符来处理它。"></span></div>
    <div class="src">Zhang et al. [42] propose a character <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> method that <span class="word_hot" title="leverage [ˈli:vərɪdʒ]">leverages</span> the low-rank structures of text.<span class="des" title="张等人 [42]提出了一种利用文本的低等级结构的字符整理方法。"></span></div>
    <div class="src"><span class="word_hot" title="Phan [fæn]">Phan</span> et al. propose to explicitly <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectify</span> perspective distortions via <span class="word_hot" title="SIFT [sɪft]">SIFT</span> [23] <span class="word_hot" title="descriptor [dɪˈskrɪptə(r)]">descriptor</span> matching.<span class="des" title="潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。"></span></div>
    <div class="src">The <span class="word_hot" title="above-mentioned [ə'bʌv 'menʃnd]">above-mentioned</span> work brings <span class="word_hot" title="insightful [ˈɪnsaɪtfʊl]">insightful</span> ideas into this issue.<span class="des" title="上述工作为这个问题带来了深刻的见解。"></span></div>
    <div class="src">However, most methods deal with only one type of irregular text with specifically designed schemes.<span class="des" title="但是，大多数方法只处理一种具有特定设计方案的不规则文本。"></span></div>
    <div class="src">Our method <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectifies</span> several types of irregular text in a unified way.<span class="des" title="我们的方法以统一的方式重新定义了几种不规则文本。"></span></div>
    <div class="src">Moreover, it does not require extra <span class="word_hot" title="annotation [ˌænə'teɪʃn]">annotations</span> for the <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> process, since the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is supervised by the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> during training.<span class="des" title="此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3. Proposed Model<span class="des" title="3. 提出模型"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In this section we formulate our model.<span class="des" title="在本节中，我们将模型形式化。"></span></div>
    <div class="src">Overall, the model takes an input image I and outputs a sequence $I=(l_1,\cdots,l_T)$, where $l_t$ is the t-th character, $T$ is the variable string length.<span class="des" title="总的来说，模型采用输入图像I并输出序列 $I=(l_1,\cdots,l_T)$，其中是第t个字符，$T$是变量字符串长度。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.1. Spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">Transformer</span> Network<span class="des" title="3.1 空间变换网络"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> transforms an input image I to a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$ with a predicted <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation.<span class="des" title="STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。"></span></div>
    <div class="src">It follows the framework proposed in [18].<span class="des" title="它遵循[18]中提出的框架。"></span></div>
    <div class="src">As illustrated in fig. 2, it first predicts a set of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points via its localization network.<span class="des" title="如图2所示，它首先通过其定位网络预测一组特定点。"></span></div>
    <div class="src">Then, inside the grid generator, it calculates the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation parameters from the <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points, and generates a sampling grid on I.<span class="des" title="然后，在网格生成器内部，它从各个点计算TPS变换参数，并在I上生成采样网格。"></span></div>
    <div class="src">The <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">sampler</span> takes both the grid and the input image, it produces a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$ by sampling on the grid points.<span class="des" title="采样器同时采用网格和输入图像，通过对网格点进行采样，生成一个矫正的图像$I^\prime$。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">A distinctive property of <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is that its <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">sampler</span> is <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span>.<span class="des" title="STN的一个独特属性是其采样器是可微分的。"></span></div>
    <div class="src">Therefore, once we have a <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span> localization network and a <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span> grid generator, the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> can <span class="word_hot_synth" title="back-propagate [!≈ bæk ˈprɒpəgeɪt]">back-propagate</span> error <span class="word_hot" title="differential [ˌdɪfəˈrenʃl]">differentials</span> and gets trained.<span class="des" title="因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig02.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 2</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 2.<span class="des" title="图2. STN的结构。"></span></div>
    <div class="src">Structure of the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>. The localization network localizes a set of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points C, with which the grid generator generates a sampling grid P. The <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">sampler</span> produces a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$ , given I and P.<span class="des" title="定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.1.1 Localization Network<span class="des" title="3.1.1本地化网络"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The localization network localizes K <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points by directly <span class="word_hot" title="regress [rɪˈgres]">regressing</span> their $x,y$ -coordinates.<span class="des" title="本地化网络通过直接回归其 $x,y$-coordinates来定位K个点。"></span></div>
    <div class="src">Here, constant K is an even number.<span class="des" title="这里，常数K是偶数。"></span></div>
    <div class="src">The coordinates are denoted by $C=[c_1,\cdots, c_K] \in R^{2 \times K}$ , whose k-th column $c_k=[x_k,y_k]^T$ contains the coordinates of the k-th <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> point.<span class="des" title="坐标由 $C=[c_1,\cdots, c_K] \in R^{2 \times K}$表示，其第k列包含第k个特定点的坐标。"></span></div>
    <div class="src">We use a <span class="word_hot" title="normalize [ˈnɔ:məlaɪz]">normalized</span> coordinate system whose origin is the image center, so that $x_k, y_k$ are within the range of $[-1, 1]$ .<span class="des" title="我们使用归一化坐标系，其原点是图像中心，因此$x_k, y_k$在$[-1, 1]$的范围内。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We use a convolutional neural network (CNN) for the regression.<span class="des" title="我们使用卷积神经网络（CNN）进行回归。"></span></div>
    <div class="src">Similar to the conventional structures [33, 21], the CNN contains convolutional layers, pooling layers and fully-connected layers.<span class="des" title="与传统结构[33,21]类似，CNN包含卷积层，汇集层和完全连接的层。"></span></div>
    <div class="src">However, we use it for regression instead of classification.<span class="des" title="但是，我们将其用于回归而不是分类。"></span></div>
    <div class="src">For the output layer, which is the last fully-connected layer, we set the number of output nodes to 2K and the activation function to $\tanh(\cdot)$ , so that its output vectors have values that are within the range of $(-1,1)$.<span class="des" title="对于输出层（最后一个完全连接的层），我们将输出节点的数量设置为2K，将激活函数设置为$\tanh(\cdot)$，以使其输出向量的值在$(-1,1)$的范围内。"></span></div>
    <div class="src">Last, the output vector is reshaped into C.<span class="des" title="最后，输出向量重新转换为C."></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The network localizes <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points based on global image contexts.<span class="des" title="网络基于全局图像上下文定位各个点。"></span></div>
    <div class="src">It is expected to capture the overall text shape of an input image, and localizes <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points accordingly.<span class="des" title="期望捕获输入图像的整体文本形状，并相应地定位各个点。"></span></div>
    <div class="src">It should be emphasized that we do not annotate coordinates of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points for any sample.<span class="des" title="应该强调的是，我们不会为任何样本注释各个点的坐标。"></span></div>
    <div class="src">Instead, the training of the localization network is completely supervised by the gradients <span class="word_hot" title="propagate [ˈprɒpəgeɪt]">propagated</span> by the other parts of the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span>, following the back-propagation algorithm [22].<span class="des" title="相反，定位网络的训练完全受到STN其他部分传播的梯度的监督，遵循反向传播算法[22]。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.1.2 Grid Generator<span class="des" title="3.1.2网格生成器"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The grid generator estimates the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation parameters, and generates a sampling grid.<span class="des" title="网格生成器估计TPS变换参数，并生成采样网格。"></span></div>
    <div class="src">We first define another set of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points, called the base <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points, denoted by $C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$.<span class="des" title="我们首先定义了另一组特殊点，称为基本点，由$C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$表示。"></span></div>
    <div class="src">As illustrated in fig. 3, the base <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points are evenly distributed along the top and bottom edge of a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$.<span class="des" title="如图3所示，基本金属点沿着矫正图像$I^\prime$的顶部和底部边缘均匀分布。"></span></div>
    <div class="src">Since K is a constant and the coordinate system is <span class="word_hot" title="normalize [ˈnɔ:məlaɪz]">normalized</span>, $C^\prime$ is always a constant.<span class="des" title="由于K是常数并且坐标系被归一化，因此$C^\prime$始终是常数。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig03.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 3</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 3. <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points and the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation.<span class="des" title="图3.基准点和TPS转换。"></span></div>
    <div class="src">Green <span class="word_hot" title="marker [ˈmɑ:kə(r)]">markers</span> on the left image are the <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points C.<span class="des" title="左侧图像上的绿色标记是C点。"></span></div>
    <div class="src"><span class="word_hot" title="Cyan [ˈsaɪən]">Cyan</span> <span class="word_hot" title="marker [ˈmɑ:kə(r)]">markers</span> on the right image are the base <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points $C^\prime$.<span class="des" title="右侧图像上的青色标记是$C^\prime$的基本特征点。"></span></div>
    <div class="src">The transformation T is represented by the pink arrow.<span class="des" title="变换T由粉红色箭头表示。"></span></div>
    <div class="src">For a point $(x_i^\prime, y_i^\prime)$ on $I^\prime$ , the transformation T finds the corresponding point $(x_i, y_i)$ on I.<span class="des" title="对于$I^\prime$上的点$(x_i^\prime, y_i^\prime)$，转换T在I上找到对应点$(x_i, y_i)$。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The parameters of the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation is represented by a matrix $T \in \Re^{2 \times (K+3)}$ , which is computed by<span class="des" title="TPS变换的参数由矩阵表示，其由下式计算"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$T=\left( \Delta_{C^\prime}^{-1} \left[ \begin{array}{c} C^T \\ 0^{3\times 2} \\ \end{array} \right] \right)^T \tag{1}$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where $\Delta_{C^\prime} \in \Re^{(K+3) \times (K+3)}$ is a matrix determined only by $C^\prime$ , thus also a constant:<span class="des" title="其中$\Delta_{C^\prime} \in \Re^{(K+3) \times (K+3)}$是仅由$C^\prime$确定的矩阵，因此也是常量："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$ \Delta_{C^\prime} = \left[ \begin{array}{c} 1^{K \times 1} & C^{\prime T} & R\\ 0 & 0 & 1^{K \times 1}\\ 0 & 0 & C^\prime\\ \end{array} \right],$$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where the element on the i-th row and j-th column of R is $r_{i,j}=d_{i,j}^2$ , $d_{i,j}$ is the <span class="word_hot" title="euclidean [ju:ˈklidiən]">euclidean</span> distance between $c_i^\prime$ and $c_j^\prime$ .<span class="des" title="其中R的第i行和第j列的元素是$r_{i,j}=d_{i,j}^2$，$d_{i,j}$是$c_i^\prime$和$c_j^\prime$之间的欧氏距离。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The grid of pixels on a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$ is denoted by $P^\prime = {p_i^\prime}_{i=1,\cdots,N}$ , where $p_i^\prime = {x_i^\prime, y_i^\prime}^T$ is the x,y coordinates of the i-th pixel, N is the number of pixels.<span class="des" title="矫正图像上的像素网格由$P^\prime = {p_i^\prime}_{i=1,\cdots,N}$表示，其中$p_i^\prime = {x_i^\prime, y_i^\prime}^T$是第i个像素的x，y坐标，N是像素数。"></span></div>
    <div class="src">As illustrated in fig. 3, for every point $p_i^\prime$ on $I^\prime$ , we find the corresponding point $p_i=[x_i,y_i]^T$ on $I$, by applying the transformation:<span class="des" title="如图3所示，对于$I^\prime$上的每个$p_i^\prime$点，我们通过应用转换在$I$上找到对应点$p_i=[x_i,y_i]^T$："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$r^\prime_{i,k}=d^2_{i,k}\ln d^2_{i,k} \tag{2}$$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$\hat p^\prime_i = [ 1, x^\prime_i, y^\prime_i, r^\prime_{i,1}, \cdots, r^\prime_{i,K}] \tag{3}$$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$p_i = T \hat p^\prime_i \tag{4}$$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where $d_{i,k}$ is the <span class="word_hot" title="euclidean [ju:ˈklidiən]">euclidean</span> distance between $p^\prime_i$ and the $k$-th base <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> point $c^\prime_k$ .<span class="des" title="其中$d_{i,k}$是$p^\prime_i$和第k个基本点$c^\prime_k$之间的欧氏距离。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">By <span class="word_hot" title="iterate [ˈɪtəreɪt]">iterating</span> over all points in $P^\prime$ , we generate a grid $P=\{p_i\}_{i=1,\cdots,N}$ on the input image $I$.<span class="des" title="通过迭代$P^\prime$中的所有点，我们在输入图像$I$上生成网格$P=\{p_i\}_{i=1,\cdots,N}$。"></span></div>
    <div class="src">The grid generator can <span class="word_hot_synth" title="back-propagate [!≈ bæk ˈprɒpəgeɪt]">back-propagate</span> gradients, since its two matrix <span class="word_hot" title="multiplication [ˌmʌltɪplɪˈkeɪʃn]">multiplications</span>, <span class="word_hot_rare">Eq</span>. 1 and <span class="word_hot_rare">Eq</span>. 4, are both <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span>.<span class="des" title="网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.1.3 <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">Sampler</span><span class="des" title="3.1.3 采样器"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src"><span class="word_hot" title="Lastly [ˈlɑ:stli]">Lastly</span>, in the <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">sampler</span>, the pixel value of $p^\prime_i$ is <span class="word_hot_synth" title="bilinearly [!≈ baɪ'lɪnɪəli]">bilinearly</span> <span class="word_hot" title="interpolate [ɪnˈtɜ:pəleɪt]">interpolated</span> from the pixels near $p_i$ on the input image.<span class="des" title="最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。"></span></div>
    <div class="src">By setting all pixel values, we get the <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $T^\prime$ :<span class="des" title="通过设置所有像素值，我们得到了矫正的图像$T^\prime$："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$T^\prime = V(P, I) \tag{5}$$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where $V$ represents the <span class="word_hot" title="bilinear [baɪ'lɪnɪə]">bilinear</span> <span class="word_hot" title="sampler [ˈsɑ:mplə(r)]">sampler</span> [18], which is also a <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span> module.<span class="des" title="其中V代表双线性采样器[18]，它也是一个可微分的模块。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The ﬂ<span class="word_hot_rare">exibility</span> of the <span class="word_hot_synth" title="TPS [!≈ ti: pi: es]">TPS</span> transformation allows us to transform irregular text images into <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> images that contain regular text.<span class="des" title="TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。"></span></div>
    <div class="src">In fig. 4, we show some common types of irregular text, including a) <span class="word_hot_synth" title="loosely-bounded [!≈ ˈlu:sli 'baʊndɪd]">loosely-bounded</span> text, which resulted by <span class="word_hot" title="imperfect [ɪmˈpɜ:fɪkt]">imperfect</span> text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by <span class="word_hot" title="side-view ['saɪdvj'u:]">side-view</span> camera angles; d) curved text, a commonly seen artistic style.<span class="des" title="在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is able to <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectify</span> images that contain these types of irregular text, making them more <span class="word_hot" title="readable [ˈri:dəbl]">readable</span> for the following <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizer</span>.<span class="des" title="STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.2. Sequence Recognition Network<span class="des" title="3.2 序列识别网络"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Since target words are <span class="word_hot" title="inherently [ɪnˈhɪərəntlɪ]">inherently</span> sequences of characters, we model the recognition problem as a sequence recognition problem, and address it with a sequence recognition network.<span class="des" title="由于目标词本质上是字符序列，我们将识别问题建模为序列识别问题，并用序列识别网络对其进行处理。"></span></div>
    <div class="src">The input to the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> is a <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> image $I^\prime$ , which <span class="word_hot" title="ideally [aɪ'di:əlɪ]">ideally</span> contains a word that is written <span class="word_hot" title="horizontally [ˌhɒrɪ'zɒntəlɪ]">horizontally</span> from left to right.<span class="des" title="SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。"></span></div>
    <div class="src">We extract a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> representation from $I^\prime$ , and recognize a word from it.<span class="des" title="我们从$I^\prime$中提取顺序表示，并从中识别出一个单词。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig04.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 4</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 4. The <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectifies</span> images that contain several types of irregular text.<span class="des" title="图4. STN重新构建包含多种不规则文本的图像。"></span></div>
    <div class="src">Green <span class="word_hot" title="marker [ˈmɑ:kə(r)]">markers</span> are the predicted <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points on the input images.<span class="des" title="绿色标记是输入图像上预测的特定点。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> can deal with several types of irregular text, including (a) <span class="word_hot_synth" title="loosely-bounded [!≈ ˈlu:sli 'baʊndɪd]">loosely-bounded</span> text; (b) multi-oriented text; (c) perspective text; (d) curved text.<span class="des" title="STN可以处理几种类型的不规则文本，包括（a）松散有界的文本; （b）多方面文本; （c）透视文本; （d）弯曲文本。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In our model, the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> is an attention-based model [4, 8], which directly recognizes a sequence from an input image.<span class="des" title="在我们的模型中，SRN是一种基于注意力的模型[4,8]，它直接识别来自输入图像的序列。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> consists of an encoder and a decoder.<span class="des" title=" SRN由编码器和解码器组成。"></span></div>
    <div class="src">The encoder extracts a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> representation from the input image $I^\prime$.<span class="des" title="编码器从输入图像$I^\prime$中提取顺序表示。"></span></div>
    <div class="src">The decoder <span class="word_hot" title="recurrently [rɪ'kʌrəntlɪ]">recurrently</span> generates a sequence conditioned on the <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> representation, by <span class="word_hot" title="decode [ˌdi:ˈkəʊd]">decoding</span> the relevant contents it attends to at each step.<span class="des" title="解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.2.1 Encoder: <span class="word_hot_synth" title="convolutional-recurrent [!≈ kɒnvə'lu:ʃənəl rɪˈkʌrənt]">Convolutional-Recurrent</span> Network<span class="des" title="3.2.1编码器：卷积 - 循环网络"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">A <span class="word_hot_rare">na¨ıve</span> approach for extracting a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> representation for $I^\prime$ is to take local image patches from left to right, and describe each of them with a CNN.<span class="des" title="用于提取$I^\prime$的顺序表示的一种简单方法是从左到右获取图像中的局部图像块，并用CNN描述每个图像块。"></span></div>
    <div class="src">However, this approach does not share the computation among overlapping patches, thus inefficient.<span class="des" title="然而，这种方法不共享重叠图像块之间的计算，因此效率低下。"></span></div>
    <div class="src">Besides, the spatial dependencies between the patches are not exploited and <span class="word_hot" title="leverage [ˈli:vərɪdʒ]">leveraged</span>.<span class="des" title="此外，图像块之间的空间依赖性未被利用和利用。"></span></div>
    <div class="src">Instead, following [32], we build a network that combines convolutional layers and <span class="word_hot" title="recurrent [rɪˈkʌrənt]">recurrent</span> networks.<span class="des" title="相反，在[32]之后，我们构建了一个结合了卷积层和循环网络的网络。"></span></div>
    <div class="src">The network extracts a sequence of feature vectors, given an input image of <span class="word_hot" title="arbitrary [ˈɑ:bɪtrəri]">arbitrary</span> size.<span class="des" title="给定任意大小的输入图像，网络提取特征向量序列。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">As illustrated in fig. 5, at the bottom of the encoder is several convolutional layers.<span class="des" title="如图5所示，在编码器的底部是几个卷积层。"></span></div>
    <div class="src">They produce feature maps that are robust and high-level descriptions of an input image.<span class="des" title="它们生成的特征映射是输入图像的强大和高级描述。"></span></div>
    <div class="src">Suppose the feature maps have the size $D_{conv} \times H_{conv} \times W_{conv}$ , where $D_{conv}$ is the depth, and $H_{conv}$ , $W_{conv}$ are the height and width respectively.<span class="des" title="假设要素图的大小为 $D_{conv} \times H_{conv} \times W_{conv}$ ，其中为深度$D_{conv}$，$H_{conv}$，$W_{conv}$分别为高度和宽度。"></span></div>
    <div class="src">The next operation is to convert the maps into a sequence of $W_{conv}$ vectors, each has $D_{conv}H_{conv}$ dimensions.<span class="des" title="下一步操作是将地图转换为$W_{conv}$个向量序列，每个向量都具有维度$D_{conv}H_{conv}$。"></span></div>
    <div class="src">Specifically, the “map-to-sequence” operation takes out the columns of the maps in the left-to-right order, and ﬂ<span class="word_hot_rare">attens</span> them into vectors.<span class="des" title="具体而言，“map-to-sequence”操作以从左到右的顺序取出地图的列，并将fl视为向量。"></span></div>
    <div class="src">According to the translation <span class="word_hot" title="invariance [ɪn'veərɪəns]">invariance</span> property of CNN, each vector corresponds to a local image region, <span class="word_hot" title="i.e. [ˌaɪ ˈi:]">i.e.</span> <span class="word_hot" title="receptive [rɪˈseptɪv]">receptive</span> field, and is a <span class="word_hot" title="descriptor [dɪˈskrɪptə(r)]">descriptor</span> for that region.<span class="des" title="根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig05.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 5</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 5.<span class="des" title="图5. SRN的结构，包括编码器和解码器。"></span></div>
    <div class="src">Structure of the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>, which consists of an encoder and a decoder. The encoder uses several convolution layers (<span class="word_hot_rare">ConvNet</span>) and a two-layer <span class="word_hot_synth" title="BLSTM [!≈ bi: el es ti: em]">BLSTM</span> network to extract a <span class="word_hot" title="sequential [sɪˈkwenʃl]">sequential</span> representation (h) for the input image.<span class="des" title="编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。"></span></div>
    <div class="src">The decoder generates a character sequence (including the <span class="word_hot_rare">EOS</span> token) conditioned on h.<span class="des" title="解码器生成以h为条件的字符序列（包括EOS令牌）。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Restricted by the sizes of the <span class="word_hot" title="receptive [rɪˈseptɪv]">receptive</span> fields, the feature sequence <span class="word_hot" title="leverage [ˈli:vərɪdʒ]">leverages</span> limited image contexts.<span class="des" title="受接收场的大小限制，特征序列利用有限的图像上下文。"></span></div>
    <div class="src">We further apply a two-layer Bidirectional <span class="word_hot_synth" title="Long-Short [!≈ lɒŋ ʃɔ:t]">Long-Short</span> Term Memory (<span class="word_hot_synth" title="BLSTM [!≈ bi: el es ti: em]">BLSTM</span>) [14, 13] network to the sequence, in order to model the long-term dependencies within the sequence.<span class="des" title="我们进一步将两层双向长短期记忆（BLSTM）[14,13]网络应用于序列，以模拟序列内的长期依赖性。"></span></div>
    <div class="src">The <span class="word_hot_synth" title="BLSTM [!≈ bi: el es ti: em]">BLSTM</span> is a <span class="word_hot" title="recurrent [rɪˈkʌrənt]">recurrent</span> network that can <span class="word_hot" title="analyze ['ænəlaɪz]">analyze</span> the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span class="des" title="BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。"></span></div>
    <div class="src">The output sequence is $h=(h_1,\cdots,h_L)$, where $L=W_{conv}$ .<span class="des" title="输出序列为$h=(h_1,\cdots,h_L)$，其中$L=W_{conv}$。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.2.2 Decoder: <span class="word_hot" title="recurrent [rɪˈkʌrənt]">Recurrent</span> Character Generator<span class="des" title="3.2.2解码器：循环字符发生器"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The decoder <span class="word_hot" title="recurrently [rɪ'kʌrəntlɪ]">recurrently</span> generates a sequence of characters, conditioned on the sequence produced by the encoder.<span class="des" title="解码器以编码器产生的序列为条件，反复生成一系列字符。"></span></div>
    <div class="src">It is a <span class="word_hot" title="recurrent [rɪˈkʌrənt]">recurrent</span> neural network with the attention structure proposed in [4, 8].<span class="des" title="它是一种递归神经网络，具有[4,8]中提出的注意结构。"></span></div>
    <div class="src">In the <span class="word_hot_rare">recurrency</span> part, we adopt the Gated <span class="word_hot" title="recurrent [rɪˈkʌrənt]">Recurrent</span> Unit (<span class="word_hot_synth" title="GRU [!≈ dʒi: ɑ:(r) ju:]">GRU</span>) [7] as the cell.<span class="des" title="在重发部分，我们采用门控循环单元（GRU）[7]作为单元。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The generation is a T -step process, at step $t$, the decoder computes a vector of attention weights $\alpha_t \in R^L$ via the attention process described in [8]:<span class="des" title="生成是T阶段过程，在步骤$t$，解码器通过[8]中描述的关注过程计算注意权重$\alpha_t \in R^L$的向量："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$\alpha_t = \text{Attend}(s_{t-1}, \alpha_{t-1},h), \tag{6}$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where $s_{t-1}$ is the state variable of the <span class="word_hot_synth" title="GRU [!≈ dʒi: ɑ:(r) ju:]">GRU</span> cell at the last step.<span class="des" title="其中$s_{t-1}$是最后一步GRU单元的状态变量。"></span></div>
    <div class="src">For $t=1$ , both $s_0$ and $\alpha_0$ are zero vectors.<span class="des" title="对于，和都是零向量。"></span></div>
    <div class="src">Then, a glimpse $g_t$ is computed by <span class="word_hot" title="linearly [ˈliniəli]">linearly</span> combining the vectors in $h$: $g_t=\sum_{i=1}^L\alpha_{ti}h_i$.<span class="des" title="然后，通过线性组合$h$：$g_t=\sum_{i=1}^L\alpha_{ti}h_i$中的向量来计算一瞥$g_t$。"></span></div>
    <div class="src">Since $\alpha_t$ has non-negative values that sum to one, it effectively controls where the decoder focuses on.<span class="des" title="由于$\alpha_t$具有总和为1的非负值，因此它有效地控制了解码器所关注的位置。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The state $s_{t-1}$ is updated via the <span class="word_hot" title="recurrent [rɪˈkʌrənt]">recurrent</span> process of <span class="word_hot_synth" title="GRU [!≈ dʒi: ɑ:(r) ju:]">GRU</span> [7, 8]:<span class="des" title="状态$s_{t-1}$通过GRU [7,8]的循环过程更新："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$s_t=\text{GRU}(l_{t-1},g_t,s_{t-1}) \tag{7}$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where $l_{t-1}$ is the $t-1$-th ground-truth label in training, while in testing, it is the label predicted in the previous step, <span class="word_hot" title="i.e. [ˌaɪ ˈi:]">i.e.</span> $\hat l_{t-1}$ .<span class="des" title="其中$l_{t-1}$是训练中第$t-1$个真实标签，而在测试中，它是上一步中预测的标签，即$\hat l_{t-1}$。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The probability distribution over the label space is estimated by:<span class="des" title="标签空间上的概率分布通过以下方式估算："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$\hat y_t = \text{softmax}(W^Ts_t). \tag{8}$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Following that, a character $\hat l_t$ is predicted by taking the class with the highest probability.<span class="des" title="之后，通过获取具有最高概率的类来预测字符$\hat l_t$。"></span></div>
    <div class="src">The label space includes all English <span class="word_hot" title="alphanumeric [ˌælfənju:ˈmerɪk]">alphanumeric</span> characters, plus a special “<span class="word_hot_rare">end-ofsequence</span>” (<span class="word_hot_rare">EOS</span>) token, which ends the generation process.<span class="des" title="标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span> directly maps a input sequence to another sequence.<span class="des" title="SRN直接将输入序列映射到另一个序列。"></span></div>
    <div class="src">Both input and output sequences may have <span class="word_hot" title="arbitrary [ˈɑ:bɪtrəri]">arbitrary</span> lengths.<span class="des" title="输入和输出序列都可以具有任意长度。"></span></div>
    <div class="src">It can be trained with only word images and associated text.<span class="des" title="它只能用单词图像和相关文本进行训练。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.3. Model Training<span class="des" title="3.3 模特训练"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We denote the training set by $X=\{(I^{(i)},l^{(i)})\}_{i=1,\cdots,N}$.<span class="des" title="我们表示由设置的训练。"></span></div>
    <div class="src">To train the model, we minimize the negative <span class="word_hot_synth" title="log-likelihood [!≈ lɒg ˈlaɪklihʊd]">log-likelihood</span> over X :<span class="des" title="为了训练模型，我们最小化X上的负对数似然："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$$ L=\sum_{i=1}^N {\log{\Pi_{t=1}^{|l^{(i)}|} p(l_t^{(i)}|I^{(i)};\theta})} \tag{9} $$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">where the probability $p(\cdot)$ is computed by <span class="word_hot_rare">Eq</span>. 8, $\theta$ is the parameters of both <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> and <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>.<span class="des" title="其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。"></span></div>
    <div class="src">The optimization algorithm is the <span class="word_hot_synth" title="ADADELTA [!≈ eɪ di: eɪ di: i: el ti: eɪ]">ADADELTA</span> [41], which we find fast in <span class="word_hot" title="convergence [kən'vɜ:dʒəns]">convergence</span> speed.<span class="des" title="优化算法是ADADELTA [41]，我们发现其收敛速度很快。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig06.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 6</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 6. Some initialization patterns for the <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points.<span class="des" title="图6.各个点的一些初始化模式。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">The model parameters are randomly initialized, except the localization network, whose output fully-connected layer is initialized by setting weights to zero.<span class="des" title="除定位网络外，模型参数是随机初始化的，定位网络的输出全连接层通过将权重设置为零来初始化。"></span></div>
    <div class="src">The initial biases are set to such values that yield the <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points pattern displayed in fig. 6. a.<span class="des" title="初始偏差设置为这样的值，产生图6.a中显示的金属点图案。"></span></div>
    <div class="src"><span class="word_hot" title="Empirically [ɪm'pɪrɪklɪ]">Empirically</span>, we also find that the patterns displayed fig. 6. b and fig. 6. c yield relatively poorer performance.<span class="des" title="根据经验，我们还发现图6.b和图6.c所示的模式产生相对较差的性能。"></span></div>
    <div class="src">Randomly initializing the localization network results in failure of <span class="word_hot" title="convergence [kən'vɜ:dʒəns]">convergence</span> during training.<span class="des" title="随机初始化定位网络导致训练期间收敛失败。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">figure 7.<span class="des" title="图7.三个单词的预制树：“ten”，“tea”和“to”。"></span></div>
    <div class="src">A <span class="word_hot" title="prefix [ˈpri:fɪks]">prefix</span> tree of three words: “ten”, “tea”, and “to”. $\epsilon$ and $\Omega$ are the tree root and the <span class="word_hot_rare">EOS</span> token respectively.<span class="des" title=" $\epsilon$和$\Omega$分别是树根和EOS令牌。"></span></div>
    <div class="src">The recognition starts from the tree root.<span class="des" title="识别从树根开始。"></span></div>
    <div class="src">At each step the <span class="word_hot" title="posterior [pɒˈstɪəriə(r)]">posterior</span> probabilities of all child nodes are computed.<span class="des" title="在每个步骤中，计算所有子节点的后验概率。"></span></div>
    <div class="src">The child node with the highest probability is selected as the next node.<span class="des" title="选择概率最高的子节点作为下一个节点。"></span></div>
    <div class="src">The process <span class="word_hot" title="iterate [ˈɪtəreɪt]">iterates</span> until a leaf node is reached.<span class="des" title="该过程将迭代，直到到达叶节点。"></span></div>
    <div class="src">Numbers on the edges are the <span class="word_hot" title="posterior [pɒˈstɪəriə(r)]">posterior</span> probabilities.<span class="des" title="边缘上的数字是后验概率。"></span></div>
    <div class="src">Blue nodes are the selected nodes.<span class="des" title="蓝色节点是选定的节点。"></span></div>
    <div class="src">In this case, the predicted word is “tea”.<span class="des" title="在这种情况下，预测的单词是“tea”。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">3.4. Recognizing With a <span class="word_hot" title="lexicon [ˈleksɪkən]">Lexicon</span><span class="des" title="3.4 用词典识别"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">When a test image is associated with a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, <span class="word_hot" title="i.e. [ˌaɪ ˈi:]">i.e.</span> a set of words for selection, the recognition process is to pick the word with the highest <span class="word_hot" title="posterior [pɒˈstɪəriə(r)]">posterior</span> <span class="word_hot" title="conditional [kənˈdɪʃənl]">conditional</span> probability:<span class="des" title="当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">$I^*=\arg \max_l \log \Pi_{t=1}^{|l|} p(l_t|I;\theta) \tag{10}$</div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">However, on very large <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span>, <span class="word_hot" title="e.g. [ˌi: ˈdʒi:]">e.g.</span> the <span class="word_hot_rare">Hunspell</span> [1] which contains more than 50k words, computing <span class="word_hot_rare">Eq</span>. 10 is time consuming, as it requires <span class="word_hot" title="iterate [ˈɪtəreɪt]">iterating</span> over all <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> words.<span class="des" title="但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。"></span></div>
    <div class="src">We adopt an efficient approximate search scheme on large <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span>.<span class="des" title="我们在大词典上采用了有效的近似搜索方案。"></span></div>
    <div class="src">The motivation is that computation can be shared among words that share the same <span class="word_hot" title="prefix [ˈpri:fɪks]">prefix</span>.<span class="des" title="动机是计算可以在共享相同前缀的单词之间共享。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We first construct a <span class="word_hot" title="prefix [ˈpri:fɪks]">prefix</span> tree over a given <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title="我们首先在给定的词典上构建一个pre fi x树。"></span></div>
    <div class="src">As illustrated in fig. 7, each node of the tree is a character label.<span class="des" title="如图7所示，树的每个节点是字符标签。"></span></div>
    <div class="src">Nodes on a path from the root to a leaf forms a word (including the <span class="word_hot_rare">EOS</span>).<span class="des" title="从根到叶子的路径上的节点形成一个单词（包括EOS）。"></span></div>
    <div class="src">In testing, we start from the root node, every time the model outputs a distribution $\hat y_t$ , the child node with the highest <span class="word_hot" title="posterior [pɒˈstɪəriə(r)]">posterior</span> probability is selected as the next node to move to.<span class="des" title="在测试中，我们从根节点开始，每次模型输出分布$\hat y_t$时，选择具有最高后验概率的子节点作为要移动到的下一个节点。"></span></div>
    <div class="src">The process repeats until a leaf node is reached, and a word is found on the path from the root to that leaf.<span class="des" title="重复该过程直到到达叶节点，并且在从根到该叶的路径上找到单词。"></span></div>
    <div class="src">Since the tree depth is at most the length of the longest word in the <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, this search process takes much less computation than the precise search.<span class="des" title="由于树深度最多是词典中最长单词的长度，因此该搜索过程比精确搜索所需的计算量少得多。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Recognition performance could be further improved by <span class="word_hot" title="incorporate [ɪnˈkɔ:pəreɪt]">incorporating</span> beam search.<span class="des" title="通过结合波束搜索可以进一步提高识别性能。"></span></div>
    <div class="src">A list of nodes is maintained, and the above search process is repeated on each of them.<span class="des" title="维护节点列表，并对每个节点重复上述搜索过程。"></span></div>
    <div class="src">After each step, the list is updated to store the nodes with <span class="word_hot_synth" title="top-B [!≈ tɒp bi:]">top-B</span> accumulated <span class="word_hot_synth" title="log-likelihood [!≈ lɒg ˈlaɪklihʊd]">log-likelihoods</span>, where B is the beam width.<span class="des" title="在每个步骤之后，更新列表以存储具有前B累积对数似然的节点，其中B是波束宽度。"></span></div>
    <div class="src">Larger beam width usually results in better performance, but lower search speed.<span class="des" title="较大的波束宽度通常会带来更好的性能，但搜索速度会降低。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Table 1. Recognition accuracies on general recognition benchmarks.<span class="des" title="表1.一般认可基准的识别准确度。"></span></div>
    <div class="src">The titles “50”, “1k” and “50k” are <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> sizes.<span class="des" title="标题“50”，“1k”和“50k”是词典大小。"></span></div>
    <div class="src">The “Full” <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> contains all per-image <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> words.<span class="des" title="“完整”词典包含所有每个图像的词典单词。"></span></div>
    <div class="src">“None” means recognition without a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title=" “无”表示没有词典的识别。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/tab01.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Table 1</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">4. Experiments<span class="des" title="4. 实验"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In this section we evaluate our model on a number of standard scene text recognition benchmarks, paying special attention to recognition performance on irregular text.<span class="des" title="在本节中，我们在许多标准场景文本识别基准上评估我们的模型，特别注意不规则文本的识别性能。"></span></div>
    <div class="src">first we evaluate our model on some general recognition benchmarks, which mainly consist of regular text, but irregular text also exists.<span class="des" title="首先，我们在一些通用识别基准上评估我们的模型，这些基准主要由常规文本组成，但也存在不规则文本。"></span></div>
    <div class="src">Next, we perform evaluations on benchmarks that are specially designed for irregular text recognition.<span class="des" title="接下来，我们对专门为不规则文本识别设计的基准测试进行评估。"></span></div>
    <div class="src">For all benchmarks, performance is measured by word accuracy.<span class="des" title="对于所有基准测试，性能都是通过字准确度来衡量的。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">4.1. Implementation Details<span class="des" title="4.1 实施细节"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">Transformer</span> Network The localization network of <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> has 4 convolution layers, each followed by a $2 \times 2$ max-pooling layer.<span class="des" title="空间变换器网络STN的定位网络有4个卷积层，每个卷层都有一个$2 \times 2$最大池层。"></span></div>
    <div class="src">The filter size, padding size and stride are 3, 1, 1 respectively, for all convolutional layers.<span class="des" title="对于所有卷积层，滤波器尺寸，填充尺寸和步幅分别为3,1,1。"></span></div>
    <div class="src">The number of filters are respectively 64, 128, 256 and 512.<span class="des" title="过滤器的数量分别为64,128,256和512。"></span></div>
    <div class="src">Following the convolutional and the max-pooling layers is two fully-connected layers with 1024 hidden units.<span class="des" title="在卷积和最大池之后是两个完全连接的层，具有1024个隐藏单元。"></span></div>
    <div class="src">We set the number of <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points to $K=20$ , meaning that the localization network outputs a 40-dimensional vector.<span class="des" title="我们将多个点的数量设置为$K=20$，这意味着定位网络输出一个40维向量。"></span></div>
    <div class="src">Activation functions for all layers are the ReLU [27], except the output layer which uses $\tanh (\cdot)$ .<span class="des" title="除了使用$\tanh (\cdot)$的输出层外，所有层的激活功能都是ReLU [27]。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Sequence Recognition Network In the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>, the encoder has 7 convolutional layers, whose {filter size, number of filters, stride, padding size} are respectively {3,64,1,1}, {3,128,1,1}, {3,256,1,1}, {3,256,1,1,}, {3,512,1,1}, {3,512,1,1}, and {2,512,1,0}.<span class="des" title="序列识别网络在SRN中，编码器有7个卷积层，其{滤波器大小，滤波器数量，步幅，填充大小}分别为{3,64,1,1}，{3,128,1,1}，{3,256 ，1,1}，{3,256,1,1，}，{3,512,1,1}，{3,512,1,1}和{2,512,1,0}。"></span></div>
    <div class="src">The 1st, 2nd, 4th, 6th convolutional layers are each followed by a $2 \times 2$ max-pooling layer.<span class="des" title="第1，第2，第4，第6卷积层每个都后接$2 \times 2$最大池层。"></span></div>
    <div class="src">On the top of the convolutional layers is a two-layer <span class="word_hot_synth" title="BLSTM [!≈ bi: el es ti: em]">BLSTM</span> network, each LSTM has 256 hidden units.<span class="des" title="在卷积层的顶部是两层BLSTM网络，每个LSTM具有256个隐藏单元。"></span></div>
    <div class="src">For the decoder, we use a <span class="word_hot_synth" title="GRU [!≈ dʒi: ɑ:(r) ju:]">GRU</span> cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 <span class="word_hot_rare">EOS</span> token).<span class="des" title="对于解码器，我们使用具有256个存储器块和37个输出单元（26个字母，10个数字和1个EOS令牌）的GRU单元。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Model Training<span class="des" title="模型训练"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Our model is trained on the 8-million <span class="word_hot" title="synthetic [sɪnˈθetɪk]">synthetic</span> samples released by <span class="word_hot_rare">Jaderberg</span> et al. [15].<span class="des" title="我们的模型在Jaderberg等人[15]发布的800万个合成样本上进行训练。"></span></div>
    <div class="src">No extra data is used.<span class="des" title="没有使用额外的数据。"></span></div>
    <div class="src">The batch size is set to 64 in training.<span class="des" title="在训练中批量大小设置为64。"></span></div>
    <div class="src">Following [17, 16], images are <span class="word_hot" title="resize [ˌri:ˈsaɪz]">resized</span> to $100 \times 32$ in both training and testing.<span class="des" title="在[17,16]之后，在训练和测试中将图像调整为$100 \times 32$。"></span></div>
    <div class="src">The output size of the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is also $100 \times 32$.<span class="des" title="STN的输出大小也是$100 \times 32$。"></span></div>
    <div class="src">Our model processes ~160 samples per second during training, and converges in 2 days after ~3 <span class="word_hot" title="epoch [ˈi:pɒk]">epochs</span> over the training dataset.<span class="des" title="我们的模型在训练期间每秒处理~160个样本，并且在训练数据集的~3个时期之后的2天内收敛。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Implementation We implement our model under the Torch7 framework [9].<span class="des" title="实现我们在Torch7框架下实现我们的模型[9]。"></span></div>
    <div class="src">Most parts of the model are <span class="word_hot_rare">GPUaccelerated</span>.<span class="des" title="该模型的大多数部分都是GPU加速的。"></span></div>
    <div class="src">All our experiments are carried out on a workstation which has one Intel <span class="word_hot_rare">Xeon</span>(R) E5-2620 2.40GHz CPU, an <span class="word_hot" title="NVIDIA [ɪn'vɪdɪə]">NVIDIA</span> <span class="word_hot_rare">GTX-Titan</span> GPU, and 64GB <span class="word_hot" title="RAM [ræm]">RAM</span>.<span class="des" title="我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Without a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, the model takes less than 2ms recognizing an image.<span class="des" title="没有词典，模型识别图像所需的时间不到2毫秒。"></span></div>
    <div class="src">With a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, recognition speed depends on the <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> size.<span class="des" title="使用词典，识别速度取决于词典大小。"></span></div>
    <div class="src">We adopt the precise search (Sec. 3.4) when <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> size ≤ 1k.<span class="des" title="当词典大小≤1k时，我们采用精确搜索（第3.4节）。"></span></div>
    <div class="src">On larger <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span>, we adopt the approximate beam search (Sec. 3.4) with a beam width of 7.<span class="des" title="在较大的词典中，我们采用近似beam搜索（第3.4节），光束宽度为7。"></span></div>
    <div class="src">With a 50<span class="word_hot_synth" title="k-word [!≈ keɪ wɜ:d]">k-word</span> <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, the search takes ~200ms per image.<span class="des" title="使用50k字的词典，每张图像搜索大约需要200毫秒。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">4.2. Results on General Benchmarks<span class="des" title="4.2 一般基准的结果"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Our model is firstly evaluated on benchmarks that are designed for general scene text recognition tasks.<span class="des" title="我们的模型首先在为一般场景文本识别任务设计的基准上进行评估。"></span></div>
    <div class="src">Samples in these benchmarks mostly contain regular text, but irregular text also exists.<span class="des" title="这些基准测试中的样本大多包含常规文本，但也存在不规则文本。"></span></div>
    <div class="src">The benchmark datasets are:<span class="des" title="基准数据集是："></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">• <span class="word_hot_synth" title="IIIT [!≈ aɪ aɪ aɪ ti:]">IIIT</span> 5<span class="word_hot_synth" title="K-Words [!≈ keɪ wɜ:dz]">K-Words</span> [25] (<span class="word_hot_rare">IIIT5K</span>) contains 3000 cropped word images for testing.<span class="des" title="•IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。"></span></div>
    <div class="src">The images are collected from the Internet.<span class="des" title="图像是从互联网上收集的。"></span></div>
    <div class="src">For each image, there is a 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> and a 1000-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title="对于每个图像，有一个50字的词典和一个1000字的词典。"></span></div>
    <div class="src">All <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span> consist of a ground truth word and some randomly picked words.<span class="des" title="所有词典都包含一个真实词和一些随机选择的词。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">• Street View Text [35] (<span class="word_hot_synth" title="SVT [!≈ es vi: ti:]">SVT</span>) is collected from Google Street View.<span class="des" title="•街景文字[35]（SVT）是从Google街景中收集的。"></span></div>
    <div class="src">Its test dataset consists of 647 word images.<span class="des" title="其测试数据集由647个单词图像组成。"></span></div>
    <div class="src">Many images in <span class="word_hot_synth" title="SVT [!≈ es vi: ti:]">SVT</span> are severely corrupted by noise and blur, or have very low resolutions.<span class="des" title=" SVT中的许多图像受到噪声和模糊的严重破坏，或者具有非常低的分辨率。"></span></div>
    <div class="src">Each sample is associated with a 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title="每个样本与50个单词的词典相关联。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">• <span class="word_hot_synth" title="ICDAR [!≈ aɪ si: di: eɪ ɑ:(r)]">ICDAR</span> 2003 [24] (<span class="word_hot_rare">IC03</span>) contains 860 cropped word images, each associated with a 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> defined by Wang et al. [35].<span class="des" title="•ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。"></span></div>
    <div class="src">Following [35], we discard images that contain <span class="word_hot_synth" title="non-alphanumeric [!≈ nɒn ˌælfənju:ˈmerɪk]">non-alphanumeric</span> characters or have less than three characters.<span class="des" title="按照[35]，我们丢弃包含非字母数字字符或少于三个字符的图像。"></span></div>
    <div class="src">Besides, there is a “full <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>” which contains all <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> words, and the <span class="word_hot_rare">Hunspell</span> [1] <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> which has 50k words.<span class="des" title="此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">• <span class="word_hot_synth" title="ICDAR [!≈ aɪ si: di: eɪ ɑ:(r)]">ICDAR</span> 2013 [20] (<span class="word_hot_rare">IC13</span>) inherits most of its samples from <span class="word_hot_rare">IC03</span>.<span class="des" title="•ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。"></span></div>
    <div class="src">After filtering samples as done in <span class="word_hot_rare">IC03</span>, the dataset contains 857 samples.<span class="des" title="在IC03中完成过滤样品后，数据集包含857个样品。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In <span class="word_hot" title="Tab [tæb]">Tab</span>. 1 we report our results, and compare them with other methods.<span class="des" title="在表1中，我们报告实验结果，并与其他方法进行比较。"></span></div>
    <div class="src">On <span class="word_hot" title="unconstrained [ˌʌnkən'streɪnd]">unconstrained</span> recognition tasks (recognizing without a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>), our model outperforms all the other methods in comparison.<span class="des" title="在无约束的识别任务（没有词典识别）的情况下，我们的模型在比较中优于所有其他方法。"></span></div>
    <div class="src">On <span class="word_hot_rare">IIIT5K</span>, RARE outperforms prior art CRNN [32] by nearly 4 percentages, indicating a clear improvement in performance.<span class="des" title="在IIIT5K上，RARE的性能比现有技术CRNN [32]高出近4个百分点，表明性能明显提高。"></span></div>
    <div class="src">We observe that <span class="word_hot_rare">IIIT5K</span> contains a lot of irregular text, especially curved text, while RARE has an advantage in dealing with irregular text.<span class="des" title="我们观察到IIIT5K包含大量不规则文本，尤其是弯曲文本，而RARE在处理不规则文本方面具有优势。"></span></div>
    <div class="src">Note that, although our model falls behind [17] on some datasets, our model differs from [17] in that it is able recognize random strings such as telephone numbers, while<span class="des" title="请注意，尽管我们的模型在某些数据集上落后于[17]，但我们的模型与[17]的不同之处在于它能够识别随机字符串，如电话号码，而"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">[17] only recognizes words that are in its 90<span class="word_hot_synth" title="k-dictionary [!≈ keɪ ˈdɪkʃənri]">k-dictionary</span>.<span class="des" title="[17]只识别其90k字典中的单词。"></span></div>
    <div class="src">On constrained recognition tasks (recognizing with a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>), RARE achieves state-of-the-art or highly competitive accuracies.<span class="des" title="在受约束的识别任务（用词典识别）中，RARE实现了最先进或极具竞争力的准确性。"></span></div>
    <div class="src">On <span class="word_hot_rare">IIIT5K</span>, <span class="word_hot_synth" title="SVT [!≈ es vi: ti:]">SVT</span> and <span class="word_hot_rare">IC03</span>, constrained recognition accuracies are on <span class="word_hot" title="par [pɑ:(r)]">par</span> with [17], and slightly lower than [32].<span class="des" title="在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We also train and test a model that contains only the <span class="word_hot_synth" title="SRN [!≈ es ɑ:(r) en]">SRN</span>.<span class="des" title="我们还训练和测试仅包含SRN的模型。"></span></div>
    <div class="src">As reported in the last row of <span class="word_hot" title="Tab [tæb]">Tab</span>.<span class="des" title="正如Tab的最后一行所报道的那样。"></span></div>
    <div class="src">1, we see that the <span class="word_hot_rare">SRN-only</span> model is also a very competitive <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizer</span>, achieving higher or competitive performance on most of the benchmarks.<span class="des" title=" 1，我们看到仅SRN模型也是一个非常有竞争力的识别器，在大多数基准测试中实现了更高或更具竞争力的性能。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">4.3. Recognizing Perspective Text<span class="des" title="4.3 认识透视文本"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">To <span class="word_hot" title="validate [ˈvælɪdeɪt]">validate</span> the effectiveness of the <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> scheme, we evaluate RARE on the task of perspective text recognition.<span class="des" title="为了验证整合方案的有效性，我们评估了RARE对透视文本识别的任务。"></span></div>
    <div class="src"><span class="word_hot_rare">SVT-Perspective</span> [29] is specifically designed for evaluating performance of perspective text recognition algorithms.<span class="des" title="SVT-Perspective [29]专门用于评估透视文本识别算法的性能。"></span></div>
    <div class="src">Text samples in <span class="word_hot_rare">SVT-Perspective</span> are picked from side view angles in Google Street View, thus most of them are heavily <span class="word_hot" title="deformed [dɪˈfɔ:md]">deformed</span> by perspective distortion.<span class="des" title="SVT-Perspective中的文本样本是从Google街景中的侧视角中选取的，因此大多数文本样本都因透视变形而严重变形。"></span></div>
    <div class="src">Some examples are shown in fig. 8.<span class="des" title="一些例子如图8.a所示。"></span></div>
    <div class="src">a. <span class="word_hot_rare">SVT-Perspective</span> consists of 639 cropped images for testing.<span class="des" title=" SVT-Perspective由639个裁剪图像组成，用于测试。"></span></div>
    <div class="src">Each image is associated with a 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>, which is inherited from the <span class="word_hot_synth" title="SVT [!≈ es vi: ti:]">SVT</span> [35] dataset.<span class="des" title="每个图像都与一个50字的词典相关联，该词典继承自SVT [35]数据集。"></span></div>
    <div class="src">In addition, there is a “Full” <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> which contains all the per-image <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> words.<span class="des" title="此外，还有一个“Full”词典，其中包含所有每个图像的词典单词。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig08.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 8</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 8.<span class="des" title="图8.不规则文本的示例。"></span></div>
    <div class="src">Examples of irregular text.<span class="des" title=" a）透视文本。"></span></div>
    <div class="src">a) Perspective text.<span class="des" title="样本取自SVT-Perspective [29]数据集; b）弯曲文本。"></span></div>
    <div class="src">Samples are taken from the <span class="word_hot_rare">SVT-Perspective</span> [29] dataset; b) Curved text. Samples are taken from the <span class="word_hot_rare">CUTE80</span> [30] dataset.<span class="des" title="样本取自CUTE80 [30]数据集。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We use the same model trained on the <span class="word_hot" title="synthetic [sɪnˈθetɪk]">synthetic</span> dataset without fine-tuning.<span class="des" title="我们使用在合成数据集上训练的相同模型而不进行微调。"></span></div>
    <div class="src">For comparison, we test the CRNN model [32] on <span class="word_hot_rare">SVT-Perspective</span>.<span class="des" title="为了比较，我们在SVT-Perspective上测试CRNN模型[32]。"></span></div>
    <div class="src">We also compare RARE with [35, 25, 37, 29], whose recognition accuracies are reported in [29].<span class="des" title="我们还将RARE与[35,25,37,29]进行了比较，其识别精度在[29]中有所报道。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Table 2. Recognition accuracies on <span class="word_hot_rare">SVT-Perspective</span> [29].<span class="des" title="表2. SVT-Perspective的识别准确度[29]。"></span></div>
    <div class="src">“50” and “Full” represent recognition with 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span> and the full <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> respectively.<span class="des" title=" “50”和“Full”分别表示50字词典和完整词典的识别。"></span></div>
    <div class="src">“None” represents recognition without a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title=" “无”表示没有词典的识别。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/tab02.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Table 2</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src"><span class="word_hot" title="Tab [tæb]">Tab</span>. 2 summarizes the results.<span class="des" title="表 2总结了结果。"></span></div>
    <div class="src">In the second and third columns, we compare the accuracies of recognition with the 50-word <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> and the full <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title="在第二和第三列中，我们将识别的准确性与50字词汇和完整词典进行比较。"></span></div>
    <div class="src">Our method outperforms [29], which is a perspective text recognition method, by a large margin on both <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicons</span>.<span class="des" title="我们的方法优于[29]，这是一种透视文本识别方法，在两个词典上都有很大的余地。"></span></div>
    <div class="src">However, this gap is partially due to that we use a much larger training set than [29].<span class="des" title="然而，这种差距部分是由于我们使用了比[29]更大的训练集。"></span></div>
    <div class="src">In the comparisons with [32], which uses the same training set as RARE, we still observe significant improvements in both the Full <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span> and the lexicon-free settings.<span class="des" title="在与使用与RARE相同的训练集的[32]的比较中，我们仍然观察到Full lexicon和Lexicon-free设置的显着改进。"></span></div>
    <div class="src">Furthermore, recall the results in <span class="word_hot" title="Tab [tæb]">Tab</span>. 1, on <span class="word_hot_rare">SVTPerspective</span> RARE outperforms [32] by a even larger margin.<span class="des" title="此外，回顾表1中的结果，在SVTP上，RARE的表现优于[32]达到更大的余地。"></span></div>
    <div class="src">The reason is that the <span class="word_hot_rare">SVT-perspective</span> dataset mainly consists of perspective text, which is inappropriate for direct recognition.<span class="des" title="原因是SVT透视数据集主要由透视文本组成，不适合直接识别。"></span></div>
    <div class="src">Our <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> scheme can significantly <span class="word_hot" title="alleviate [əˈli:vieɪt]">alleviate</span> this problem.<span class="des" title="我们的整改计划可以显着缓解这一问题。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/fig09.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Figure 9</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">Figure 9. Examples showing the <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectifications</span> our model makes and the recognition results.<span class="des" title="图9. 示例显示了我们的模型所做的纠正和识别结果。"></span></div>
    <div class="src">The left column is the input images, where green crosses are the predicted <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points.<span class="des" title="左列是输入图像，其中绿色十字是预测的基准点。"></span></div>
    <div class="src">The middle column is the <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectified</span> images (we use <span class="word_hot" title="gray-scale [ɡ'reɪsk'eɪl]">gray-scale</span> images for recognition).<span class="des" title="中间一列是校正后的图像(我们使用灰度图像进行识别)。"></span></div>
    <div class="src">The right column is the recognized text and the ground truth text.<span class="des" title="右列是识别的文本和真实文本。"></span></div>
    <div class="src">Green and red characters are correctly and <span class="word_hot" title="mistakenly [mɪ'steɪkənlɪ]">mistakenly</span> recognized characters, respectively.<span class="des" title="绿色和红色字符分别是正确和错误识别的字符。"></span></div>
    <div class="src">The first five rows are taken from <span class="word_hot_rare">SVT-Perspective</span> [29], the rest rows are taken from <span class="word_hot_rare">CUTE80</span> [30].<span class="des" title="前五行取自SVT-透视图[29]，其余行取自CUTE80[30]。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In fig. 9 we present some <span class="word_hot" title="qualitative [ˈkwɒlɪtətɪv]">qualitative</span> analysis.<span class="des" title="在图9中，我们提出了一些定性分析。"></span></div>
    <div class="src"><span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points predicted by the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> are plotted on input images in green crosses.<span class="des" title="由STN预测的基准点被绘制在绿色十字架的输入图像上。"></span></div>
    <div class="src">We see that the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> tends to place <span class="word_hot" title="fiducial [fɪ'dju:ʃjəl]">fiducial</span> points along upper and lower edges of scene text, and<span class="des" title="我们看到STN倾向于沿场景文本的上下边缘放置特定点，并且"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">4.4. Recognizing Curved Text<span class="des" title="4.4 识别弯曲文本"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Curved text is a commonly seen <span class="word_hot_synth" title="artistic-style [!≈ ɑ:ˈtɪstɪk staɪl]">artistic-style</span> text in natural scenes.<span class="des" title="弯曲文本是自然场景中常见的艺术风格文本。"></span></div>
    <div class="src">Due to its irregular character <span class="word_hot" title="placement [ˈpleɪsmənt]">placement</span>, recognizing curved text is very challenging.<span class="des" title="由于其不规则的字符放置，识别弯曲文本是非常具有挑战性的。"></span></div>
    <div class="src"><span class="word_hot_rare">CUTE80</span> [30] focuses on the recognition of curved text.<span class="des" title="CUTE80 [30]专注于弯曲文本的识别。"></span></div>
    <div class="src">The dataset contains 80 high-resolution images taken in natural scenes.<span class="des" title="该数据集包含在自然场景中拍摄的80张高分辨率图像。"></span></div>
    <div class="src">Originally, the dataset is proposed for detection tasks.<span class="des" title="最初，该数据集被提议用于检测任务。"></span></div>
    <div class="src">We crop the words, resulting in 288 word images for testing.<span class="des" title="我们裁剪单词，产生288个单词图像进行测试。"></span></div>
    <div class="src">For comparisons, we evaluate the trained models of [17] and [32].<span class="des" title="为了进行比较，我们评估了[17]和[32]的训练模型。"></span></div>
    <div class="src">All models are evaluated without a <span class="word_hot" title="lexicon [ˈleksɪkən]">lexicon</span>.<span class="des" title="所有模型都在没有词典的情况下进行评估。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">Table 3. Recognition accuracies on <span class="word_hot_rare">CUTE80</span> [29].<span class="des" title="表3.CUTE80上的识别准确度[29]。"></span></div>
</div>
    <br>
<figure style="text-align: center;">
    <img src="../../images/RARE/tab03.png"/>
    <figcaption style="font-size:75%;">
    <div class="src">Table 3</div>
</figcaption></figure>
<div class="paragraph_part">
    <div class="src">From the results summarized in <span class="word_hot" title="Tab [tæb]">Tab</span>. 3, we see that RARE outperforms the other two methods by a large margin.<span class="des" title="从表3中总结的结果，我们看到RARE的性能远远超过其他两种方法。"></span></div>
    <div class="src">[17] is a constrained recognition model, it cannot recognize words that are not in its dictionary.<span class="des" title="[17]是一个受约束的识别模型，它不能识别不在其词典中的单词。"></span></div>
    <div class="src">[32] is able to recognize <span class="word_hot" title="arbitrary [ˈɑ:bɪtrəri]">arbitrary</span> words, but it does not have a specific mechanism for handling curved text.<span class="des" title="[32]能够识别任意单词，但没有处理弯曲文本的特定机制。"></span></div>
    <div class="src">Our model <span class="word_hot" title="rectify [ˈrektɪfaɪ]">rectifies</span> images that contain curved text before recognizing them.<span class="des" title="我们的模型在识别包含弯曲文本的图像之前对其进行校正。"></span></div>
    <div class="src">Therefore, it is <span class="word_hot" title="advantageous [ˌædvənˈteɪdʒəs]">advantageous</span> on this task.<span class="des" title="因此，在这项任务上是有利的。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">In Fig. 9, we demonstrate the effect of <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> through some examples.<span class="des" title="在图9中，我们通过一些例子演示了整改的效果。"></span></div>
    <div class="src">Generally, the <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> made by the <span class="word_hot_synth" title="STN [!≈ es ti: en]">STN</span> is not perfect, but it <span class="word_hot" title="alleviate [əˈli:vieɪt]">alleviates</span> the recognition difficulty to some extent.<span class="des" title="一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。"></span></div>
    <div class="src">RARE tends to fail when curve angles are too large, as shown in the last two rows of Fig. 9.<span class="des" title="当曲线角度太大时，RARE倾向于失败，如图9的最后两行所示。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">5. Conclusion<span class="des" title="5. 结论"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">We study a common but difficult problem in scene text recognition, called the irregular text problem.<span class="des" title="本文研究了场景文本识别中的一个常见而又困难的问题，即不规则文本问题。"></span></div>
    <div class="src">Traditional solutions typically use a separate text <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> component.<span class="des" title="传统的解决方案通常使用单独的文本校正组件。"></span></div>
    <div class="src">We address this problem in a more feasible and elegant way by adopting a <span class="word_hot" title="differentiable [ˌdɪfə'renʃɪəbl]">differentiable</span> spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">transformer</span> network module.<span class="des" title="我们通过采用可区分的空间变换网络模块，以一种更可行和更优雅的方式解决了这个问题。"></span></div>
    <div class="src">In addition, the spatial <span class="word_hot" title="transformer [trænsˈfɔ:mə(r)]">transformer</span> network is connected to an attention-based sequence <span class="word_hot" title="recognizer ['rekəgnaɪzə]">recognizer</span>, allowing us to train the whole model end-to-end.<span class="des" title="此外，空间变换器网络连接到基于注意力的序列识别器，允许我们端到端地训练整个模型。"></span></div>
    <div class="src">The extensive experimental results show that 1) without <span class="word_hot" title="geometric [ˌdʒi:əˈmetrɪk]">geometric</span> supervision, the learned model can automatically generate more “<span class="word_hot" title="readable [ˈri:dəbl]">readable</span>” images for both human and the sequence recognition network; 2) the proposed text <span class="word_hot" title="rectification [ˌrektɪfɪ'keɪʃn]">rectification</span> method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span class="des" title="大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。"></span></div>
    <div class="src">In the future, we plan to address the end-to-end scene text reading problem through the combination of RARE with a scene text detection method, <span class="word_hot" title="e.g. [ˌi: ˈdʒi:]">e.g.</span> [43].<span class="des" title="未来，我们计划通过将RARE与场景文本检测方法相结合来解决端到端场景文本阅读问题，例如[43]。"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src"><span class="word_hot" title="acknowledgment [ək'nɒlɪdʒmənt]">Acknowledgments</span><span class="des" title="致谢"></span></div>
</div>
    <br>
<div class="paragraph_part">
    <div class="src">This work was primarily supported by National Natural Science Foundation of China (<span class="word_hot_synth" title="NSFC [!≈ en es ef si:]">NSFC</span>) (No. 61222308, No. 61573160 and No. 61503145), and Open Project Program of the State Key Laboratory of Digital Publishing Technology (No. <span class="word_hot_rare">F2016001</span>).<span class="des" title="本工作主要得到国家自然科学基金(61222308，61573160，61503145)和数字出版技术国家重点实验室开放项目(No. F2016001)的支持。"></span></div>
</div>
    <br>

</div>
<div class="panel">
<div class="panel-btn" title="Toggle unfolding Chinese" onclick="display_des()">➥</div>
<div class="panel-btn" title="Toggle display by sentences" onclick="display_lines()">☵</div>
<div class="panel-btn" title="Toggle display of word list" onclick="display_words()">ᙡ</div>
</div>
<div id="wordPage" style="visibility:hidden;">
<div style="position: fixed; width: 100%; height: 100%; top:0; left:0;background-color:#333; z-index:1; opacity:0.5; " onclick="display_words()"></div>
<iframe src="./RARE - Robust Scene Text Recognition with Automatic Rectification_words.html" scrolling="auto" frameborder="0"></iframe>
</div>
<script type="text/javascript">
font_default_color = "#666"
var cnLines = document.getElementsByClassName("des");
if (cnLines.length==0){
   var btns = document.getElementsByClassName("panel")[0];
    btns.removeChild(btns.children[0]); btns.removeChild(btns.children[2])
} else
for (var i = 0; i < cnLines.length; i++) {
    var line = cnLines[i];
    line.style.marginLeft="0.5em";line.style.marginRight="0.5em"; line.className="des off";
    if (line.title=="") { continue;}
    line.style.fontSize = "90%"; line.style.color=font_default_color;
    line.innerHTML = line.title; line.removeAttribute("title"); 
    line.addEventListener("click",handler,false);
    line.addEventListener("mouseover",handler,false);
    line.addEventListener("mouseout",handler,false);
}
</script>
</body>
</html>