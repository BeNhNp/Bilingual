<html>
<head>
<meta charset="utf-8">
<title> VGG - Very Deep Convolutional Networks for Large-Scale Image Recognition </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> Simonyan </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Simonyan</font>, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> Karen </td> <td> ['ka:rən] </td> <td> 
<ul><li>Simonyan, <font color=orangered>Karen</font> (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> Zisserman </td> <td>  </td> <td> 
<ul><li>Simonyan, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); <font color=forestgreen>Zisserman</font>, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> andrew </td> <td> [ˈændru:] </td> <td> 
<ul><li>Simonyan, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, <font color=orangered>Andrew</font><span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> ICLR </td> <td>  </td> <td> 
<ul><li>Source: 3rd International Conference on Learning Representations, <font color=forestgreen>ICLR</font> 2015 - Conference Track Proceedings<span style="font-size:80%;opacity:0.8"></span></li><li>v4 The paper is converted to <font color=forestgreen>ICLR</font>-2015 submission format.<span style="font-size:80%;opacity:0.8">V4将论文转换为ICLR-2015提交格式。</span></li><li>v6 Camera-ready <font color=forestgreen>ICLR</font>-2015 conference paper.<span style="font-size:80%;opacity:0.8">V6 复印就绪ICLR-2015会议论文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> thorough </td> <td> [ˈθʌrə] </td> <td> 
<ul><li>Our main contribution is a <font color=orangered>thorough</font> evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li><li>In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a <font color=orangered>thorough</font> evaluation of ConvNet architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> prior-art </td> <td>  </td> <td> 
<ul><li>Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the <font color=forestgreen>prior-art</font> configurations can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> configuration </td> <td> [kənˌfɪgəˈreɪʃn] </td> <td> 
<ul><li>Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art <font color=orangered>configurations</font> can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li><li>In Sect. 2, we describe our ConvNet <font color=orangered>configurations</font>.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>The details of the image classification training and evaluation are then presented in Sect. 3, and the <font color=orangered>configurations</font> are compared on the ILSVRC classification task in Sect. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li>2 CONVNET <font color=orangered>CONFIGURATIONS</font><span style="font-size:80%;opacity:0.8">2. ConvNet配置</span></li><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer <font color=orangered>configurations</font> are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>In this section, we first describe a generic layout of our ConvNet <font color=orangered>configurations</font> (Sect. 2.1) and then detail the specific <font color=orangered>configurations</font> used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>In one of the <font color=orangered>configurations</font> we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>The <font color=orangered>configuration</font> of the fully connected layers is the same in all networks.<span style="font-size:80%;opacity:0.8">所有网络中全连接层的配置是相同的。</span></li><li>2.2 <font color=orangered>CONFIGURATIONS</font><span style="font-size:80%;opacity:0.8">2.2 配置</span></li><li>The ConvNet <font color=orangered>configurations</font>, evaluated in this paper, are outlined in Table 1, one per column.<span style="font-size:80%;opacity:0.8">本文中评估的ConvNet配置在表1中列出，每列一个。</span></li><li>All <font color=orangered>configurations</font> follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>Table 1: ConvNet <font color=orangered>configurations</font> (shown in columns).<span style="font-size:80%;opacity:0.8">表1：ConvNet配置（以列显示）。</span></li><li>The depth of the <font color=orangered>configurations</font> increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold).<span style="font-size:80%;opacity:0.8">随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。</span></li><li>In Table 2 we report the number of parameters for each <font color=orangered>configuration</font>.<span style="font-size:80%;opacity:0.8">在表2中，我们报告了每个配置的参数数量。</span></li><li>Our ConvNet <font color=orangered>configurations</font> are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>The incorporation of 1 × 1 conv. layers (<font color=orangered>configuration</font> C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>In the previous section we presented the details of our network <font color=orangered>configurations</font>.<span style="font-size:80%;opacity:0.8">在上一节中，我们介绍了我们的网络配置的细节。</span></li><li>To circumvent this problem, we began with training the <font color=orangered>configuration</font> A (Table 1), shallow enough to be trained with random initialisation.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li><li>Given a ConvNet <font color=orangered>configuration</font>, we first trained the network using S = 256.<span style="font-size:80%;opacity:0.8">给定ConvNet配置，我们首先使用S=256来训练网络。</span></li><li>For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same <font color=orangered>configuration</font>, pre-trained with fixed S = 384.<span style="font-size:80%;opacity:0.8">为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的S = 384进行预训练。</span></li><li>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer <font color=orangered>configurations</font> described in Sect. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>Notably, in spite of the same depth, the <font color=orangered>configuration</font> C (which contains three 1 × 1 conv. layers), performs worse than the <font color=orangered>configuration</font> D, which uses 3 × 3 conv.<span style="font-size:80%;opacity:0.8">值得注意的是，尽管深度相同，配置C（包含三个1×1卷积层）比在整个网络层中使用3×3卷积的配置D更差。</span></li><li>As before, the deepest <font color=orangered>configurations</font> (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S.<span style="font-size:80%;opacity:0.8">如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。</span></li><li>On the test set, the <font color=orangered>configuration</font> E achieves 7.3% top-5 error.<span style="font-size:80%;opacity:0.8">在测试集上，配置E实现了7.3％ top-5的错误率。</span></li><li>After the submission, we considered an ensemble of only two best-performing multi-scale models (<font color=orangered>configurations</font> D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>All ConvNet layers (except for the last one) have the <font color=orangered>configuration</font> D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>In this evaluation, we consider two models with the best classification performance on ILSVRC(Sect. 4) – <font color=orangered>configurations</font> “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> submission </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>These findings were the basis of our ImageNet Challenge 2014 <font color=orangered>submission</font>, where our team secured the first and the second places in the localisation and classification tracks respectively.<span style="font-size:80%;opacity:0.8">这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。</span></li><li>For instance, the best-performing <font color=orangered>submissions</font> to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>It is worth noting that after the paper <font color=orangered>submission</font> we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC <font color=orangered>submissions</font> in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>By the time of ILSVRC <font color=orangered>submission</font> we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>After the <font color=orangered>submission</font>, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>After the <font color=orangered>submission</font>, we decreased the error rate to 6.8% using an ensemble of 2 models.<span style="font-size:80%;opacity:0.8">提交后，我们使用2个模型的组合将错误率降低到6.8％。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning <font color=orangered>submission</font> Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>This is remarkable, considering that our best result is achieved by combining just two models —— significantly less than used in most ILSVRC <font color=orangered>submissions</font>.<span style="font-size:80%;opacity:0.8">这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 <font color=orangered>submission</font>).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>Presents the experiments carried out before the ILSVRC <font color=orangered>submission</font>.<span style="font-size:80%;opacity:0.8">介绍在ILSVRC提交之前进行的实验。</span></li><li>v4 The paper is converted to ICLR-2015 <font color=orangered>submission</font> format.<span style="font-size:80%;opacity:0.8">V4将论文转换为ICLR-2015提交格式。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> localisation </td> <td> [,ləukəlai'zeiʃən, -li'z-] </td> <td> 
<ul><li>These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the <font color=orangered>localisation</font> and classification tracks respectively.<span style="font-size:80%;opacity:0.8">这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。</span></li><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and <font color=orangered>localisation</font> tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>For completeness, we also describe and assess our ILSVRC-2014 object <font color=orangered>localisation</font> system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>A <font color=orangered>LOCALISATION</font><span style="font-size:80%;opacity:0.8">A 定位</span></li><li>In this section, we turn to the <font color=orangered>localisation</font> task of the challenge, which we have won in 2014 with 25.3% error.<span style="font-size:80%;opacity:0.8">在本节中，我们将转向挑战的本地化任务，我们在2014年以25.3%的错误率赢得了这项任务。</span></li><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 <font color=orangered>localisation</font> challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>A.1 <font color=orangered>LOCALISATION</font> CONVNET<span style="font-size:80%;opacity:0.8">A.1 ConvNet定位</span></li><li>To perform object <font color=orangered>localisation</font>, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>Training of <font color=orangered>localisation</font> ConvNets is similar to that of the classification ConvNets(Sect. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>We trained two <font color=orangered>localisation</font> models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the <font color=orangered>localisation</font> ConvNet to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>When several <font color=orangered>localisation</font> ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>A.2 <font color=orangered>LOCALISATION</font> EXPERIMENTS<span style="font-size:80%;opacity:0.8">A.2 定位实验</span></li><li>In this section we first determine the best-performing <font color=orangered>localisation</font> setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>The <font color=orangered>localisation</font> error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>We also note that fine-tuning all layers for the <font color=orangered>localisation</font> task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Table 8: <font color=orangered>Localisation</font> error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used.<span style="font-size:80%;opacity:0.8">表8：使用简化测试协议的不同修改的定位误差：从单个中心图像裁剪预测边界框，并使用实际类别。</span></li><li>Having determined the best <font color=orangered>localisation</font> setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>As can be seen from Table 9, application of the <font color=orangered>localisation</font> ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li><li>Table 9: <font color=orangered>Localisation</font> error<span style="font-size:80%;opacity:0.8">表9：定位错误</span></li><li>We compare our best <font color=orangered>localisation</font> result with the state of the art in Table 10.<span style="font-size:80%;opacity:0.8">我们将我们最好的本地化结果与表10中的最新技术进行了比较。</span></li><li>With 25.3% test error, our “VGG” team won the <font color=orangered>localisation</font> challenge of ILSVRC-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>We envisage that better <font color=orangered>localisation</font> performance can be achieved if this technique is incorporated into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li><li>This indicates the performance advancement brought by our very deep ConvNets – we got better results with a simpler <font color=orangered>localisation</font> method, but a more powerful representation.<span style="font-size:80%;opacity:0.8">这表明我们非常深入的ConvNets带来的性能提升-我们使用更简单的定位方法获得了更好的结果，但更强大的表示。</span></li><li>Table 10: Comparison with the state of the art in ILSVRC <font color=orangered>localisation</font>.<span style="font-size:80%;opacity:0.8">表10：与ILSVRC本地化技术的比较。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> generalise </td> <td> ['dʒenərəlaɪz] </td> <td> 
<ul><li>We also show that our representations <font color=orangered>generalise</font> well to other datasets, where they achieve state-of-the-art results.<span style="font-size:80%;opacity:0.8">我们还表明，我们的表示对于其他数据集泛化的很好，在其它数据集上取得了最好的结果。</span></li><li>In the appendix, we also show that our models <font color=orangered>generalise</font> well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations.<span style="font-size:80%;opacity:0.8">在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, <font color=orangered>generalise</font> well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> best-performing </td> <td>  </td> <td> 
<ul><li>We have made our two <font color=forestgreen>best-performing</font> ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.<span style="font-size:80%;opacity:0.8">我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</span></li><li>For instance, the <font color=forestgreen>best-performing</font> submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>We have released our two <font color=forestgreen>best-performing</font> models to facilitate further research.<span style="font-size:80%;opacity:0.8">我们发布了两款表现最好的模型1，以便进一步研究。</span></li><li>After the submission, we considered an ensemble of only two <font color=forestgreen>best-performing</font> multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>For reference, our <font color=forestgreen>best-performing</font> single model achieves 7.1% error (model E, Table 5).<span style="font-size:80%;opacity:0.8">作为参考，我们表现最佳的单模型达到7.1％的误差（模型E，表5）。</span></li><li>Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the <font color=forestgreen>best-performing</font> in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>In this section we first determine the <font color=forestgreen>best-performing</font> localisation setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our <font color=forestgreen>best-performing</font> classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>We also evaluated our <font color=forestgreen>best-performing</font> image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> ConvNet </td> <td>  </td> <td> 
<ul><li>We have made our two best-performing <font color=forestgreen>ConvNet</font> models publicly available to facilitate further research on the use of deep visual representations in computer vision.<span style="font-size:80%;opacity:0.8">我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</span></li><li>In this paper, we address another important aspect of <font color=forestgreen>ConvNet</font> architecture design —— its depth.<span style="font-size:80%;opacity:0.8">在本文中，我们解决了ConvNet架构设计的另一个重要方面——其深度。</span></li><li>As a result, we come up with significantly more accurate <font color=forestgreen>ConvNet</font> architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>In Sect. 2, we describe our <font color=forestgreen>ConvNet</font> configurations.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>To measure the improvement brought by the increased <font color=forestgreen>ConvNet</font> depth in a fair setting, all our <font color=forestgreen>ConvNet</font> layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>In this section, we first describe a generic layout of our <font color=forestgreen>ConvNet</font> configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>The <font color=forestgreen>ConvNet</font> configurations, evaluated in this paper, are outlined in Table 1, one per column.<span style="font-size:80%;opacity:0.8">本文中评估的ConvNet配置在表1中列出，每列一个。</span></li><li>Table 1: <font color=forestgreen>ConvNet</font> configurations (shown in columns).<span style="font-size:80%;opacity:0.8">表1：ConvNet配置（以列显示）。</span></li><li>Our <font color=forestgreen>ConvNet</font> configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>In this section, we describe the details of classification <font color=forestgreen>ConvNet</font> training and evaluation.<span style="font-size:80%;opacity:0.8">在本节中，我们将介绍分类ConvNet训练和评估的细节。</span></li><li>The <font color=forestgreen>ConvNet</font> training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later).<span style="font-size:80%;opacity:0.8">ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。</span></li><li>To obtain the fixed-size 224×224 <font color=forestgreen>ConvNet</font> input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Let S be the smallest side of an isotropically-rescaled training image, from which the <font color=forestgreen>ConvNet</font> input is cropped (we also refer to S as the training scale).<span style="font-size:80%;opacity:0.8">令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。</span></li><li>Given a <font color=forestgreen>ConvNet</font> configuration, we first trained the network using S = 256.<span style="font-size:80%;opacity:0.8">给定ConvNet配置，我们首先使用S=256来训练网络。</span></li><li>At test time, given a trained <font color=forestgreen>ConvNet</font> and an input image, it is classified in the following way.<span style="font-size:80%;opacity:0.8">在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a <font color=forestgreen>ConvNet</font> to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>While more sophisticated methods of speeding up <font color=forestgreen>ConvNet</font> training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li><li>In this section, we present the image classification results achieved by the described <font color=forestgreen>ConvNet</font> architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges).<span style="font-size:80%;opacity:0.8">在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。</span></li><li>We begin with evaluating the performance of individual <font color=forestgreen>ConvNet</font> models at a single scale with the layer configurations described in Sect. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>Table 3: <font color=forestgreen>ConvNet</font> performance at a single test scale.<span style="font-size:80%;opacity:0.8">表3：在单测试尺度的ConvNet性能</span></li><li>Second, we observe that the classification error decreases with the increased <font color=forestgreen>ConvNet</font> depth: from 11 layers in A to 19 layers in E.<span style="font-size:80%;opacity:0.8">第二，我们观察到分类误差随着ConvNet深度的增加而减小：从A中的11层到E中的19层。</span></li><li>Having evaluated the <font color=forestgreen>ConvNet</font> models at a single scale, we now assess the effect of scale jittering at test time.<span style="font-size:80%;opacity:0.8">在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。</span></li><li>Table 4: <font color=forestgreen>ConvNet</font> performance at multiple test scales.<span style="font-size:80%;opacity:0.8">表4：在多个测试尺度上的ConvNet性能</span></li><li>In Table 5 we compare dense <font color=forestgreen>ConvNet</font> evaluation with mult-crop evaluation (see Sect. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li><li>Table 5: <font color=forestgreen>ConvNet</font> evaluation techniques comparison.<span style="font-size:80%;opacity:0.8">表5：ConvNet评估技术比较。</span></li><li>Up until now, we evaluated the performance of individual <font color=forestgreen>ConvNet</font> models.<span style="font-size:80%;opacity:0.8">到目前为止，我们评估了ConvNet模型的性能。</span></li><li>Table 6: Multiple <font color=forestgreen>ConvNet</font> fusion results.<span style="font-size:80%;opacity:0.8">表6：多个卷积网络融合结果</span></li><li>Notably, we did not depart from the classical <font color=forestgreen>ConvNet</font> architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional <font color=forestgreen>ConvNet</font> architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of <font color=forestgreen>ConvNet</font> architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li><li>To perform object localisation, we use a very deep <font color=forestgreen>ConvNet</font>, where the last fully connected layer predicts the bounding box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>Apart from the last bounding box prediction layer, we use the <font color=forestgreen>ConvNet</font> architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the localisation <font color=forestgreen>ConvNet</font> to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>All <font color=forestgreen>ConvNet</font> layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>As can be seen from Table 9, application of the localisation <font color=forestgreen>ConvNet</font> to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li><li>For simplicity, pre-trained <font color=forestgreen>ConvNet</font> weights are kept fixed (no fine-tuning is performed).<span style="font-size:80%;opacity:0.8">为简单起见，预先训练的ConvNet权重保持固定(不执行微调)。</span></li><li>We considered two training settings: (i) computing the <font color=forestgreen>ConvNet</font> features on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation.<span style="font-size:80%;opacity:0.8">我们考虑了两个训练设置：(I)计算整个图像上的凸网特征并忽略提供的边界框；(Ii)计算整个图像和提供的边界框上的特征，并将它们堆叠以获得最终表示。</span></li><li>For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the <font color=forestgreen>ConvNet</font> of Krizhevsky et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> ConvNets </td> <td>  </td> <td> 
<ul><li>With <font color=forestgreen>ConvNets</font> becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li><li>During training, the input to our <font color=forestgreen>ConvNets</font> is a fixed-size 224 × 224 RGB image.<span style="font-size:80%;opacity:0.8">在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。</span></li><li>Goodfellow et al. (2014) applied deep <font color=forestgreen>ConvNets</font> (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance.<span style="font-size:80%;opacity:0.8">Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep <font color=forestgreen>ConvNets</font>(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>As can be seen from Table 7, our very deep <font color=forestgreen>ConvNets</font> significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Training of localisation <font color=forestgreen>ConvNets</font> is similar to that of the classification <font color=forestgreen>ConvNets</font>(Sect. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>When several localisation <font color=forestgreen>ConvNets</font> are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>This indicates the performance advancement brought by our very deep <font color=forestgreen>ConvNets</font> – we got better results with a simpler localisation method, but a more powerful representation.<span style="font-size:80%;opacity:0.8">这表明我们非常深入的ConvNets带来的性能提升-我们使用更简单的定位方法获得了更好的结果，但更强大的表示。</span></li><li>In the previous sections we have discussed training and evaluation of very deep <font color=forestgreen>ConvNets</font> on the ILSVRC dataset.<span style="font-size:80%;opacity:0.8">在前面的部分中，我们讨论了ILSVRC数据集上非常深的ConvNet的训练和评估。</span></li><li>In this section, we evaluate our <font color=forestgreen>ConvNets</font>, pre-trained on ILSVRC, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li><li>To utilise the <font color=forestgreen>ConvNets</font>, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Results marked with * were achieved using <font color=forestgreen>ConvNets</font> pre-trained on the extended ILSVRC dataset (2000 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(2000个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Results marked with * were achieved using <font color=forestgreen>ConvNets</font> pre-trained on the extended ILSVRC dataset (1512 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(1512个类)上预训练的ConvNets获得标有*的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> commodity </td> <td> [kəˈmɒdəti] </td> <td> 
<ul><li>With ConvNets becoming more of a <font color=orangered>commodity</font> in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> Krizhevsky </td> <td>  </td> <td> 
<ul><li>With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of <font color=forestgreen>Krizhevsky</font> et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); <font color=forestgreen>Krizhevsky</font> et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>All hidden layers are equipped with the rectification (ReLU (<font color=forestgreen>Krizhevsky</font> et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (<font color=forestgreen>Krizhevsky</font> et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Where applicable, the parameters for the LRN layer are those of (<font color=forestgreen>Krizhevsky</font> et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (<font color=forestgreen>Krizhevsky</font> et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (<font color=forestgreen>Krizhevsky</font> et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>The ConvNet training procedure generally follows <font color=forestgreen>Krizhevsky</font> et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later).<span style="font-size:80%;opacity:0.8">ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (<font color=forestgreen>Krizhevsky</font> et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li><li>To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (<font color=forestgreen>Krizhevsky</font> et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (<font color=forestgreen>Krizhevsky</font> et al. , 2012; Zeiler & Fergus, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (<font color=forestgreen>Krizhevsky</font> et al. , 2012), which is less efficient as it requires network re-computation for each crop.<span style="font-size:80%;opacity:0.8">由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。</span></li><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (<font color=forestgreen>Krizhevsky</font>, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (<font color=forestgreen>Krizhevsky</font> et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al. , 1989; <font color=forestgreen>Krizhevsky</font> et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the ConvNet of <font color=forestgreen>Krizhevsky</font> et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li><li>Similar gains over a more shallow architecture of <font color=forestgreen>Krizhevsky</font> et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> ILSVRC </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the <font color=forestgreen>ILSVRC</font>-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on <font color=forestgreen>ILSVRC</font> classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the <font color=forestgreen>ILSVRC</font> classification task in Sect. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li>For completeness, we also describe and assess our <font color=forestgreen>ILSVRC</font>-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way <font color=forestgreen>ILSVRC</font> classification and thus contains 1000 channels (one for each class).<span style="font-size:80%;opacity:0.8">一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the <font color=forestgreen>ILSVRC</font> dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the <font color=forestgreen>ILSVRC</font>-2012 (Krizhevsky et al. , 2012) and <font color=forestgreen>ILSVRC</font>-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale <font color=forestgreen>ILSVRC</font> dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a top-performing entry of the <font color=forestgreen>ILSVRC</font>-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for <font color=forestgreen>ILSVRC</font> 2012–2014 challenges).<span style="font-size:80%;opacity:0.8">在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。</span></li><li>The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in <font color=forestgreen>ILSVRC</font>, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.<span style="font-size:80%;opacity:0.8">前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</span></li><li>Certain experiments were also carried out on the test set and submitted to the official <font color=forestgreen>ILSVRC</font> server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top <font color=forestgreen>ILSVRC</font> submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>By the time of <font color=forestgreen>ILSVRC</font> submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>The resulting ensemble of 7 networks has 7.3% <font color=forestgreen>ILSVRC</font> test error.<span style="font-size:80%;opacity:0.8">由此产生的7个网络组合具有7.3％的ILSVRC测试误差。</span></li><li>In the classification task of <font color=forestgreen>ILSVRC</font>-2014 challenge (Russakovsky et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>Table 7: Comparison with the state of the art in <font color=forestgreen>ILSVRC</font> classification.<span style="font-size:80%;opacity:0.8">表7：在ILSVRC分类中与最新技术比较。</span></li><li>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the <font color=forestgreen>ILSVRC</font>-2012 and <font color=forestgreen>ILSVRC</font>-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the <font color=forestgreen>ILSVRC</font>-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>This is remarkable, considering that our best result is achieved by combining just two models —— significantly less than used in most <font color=forestgreen>ILSVRC</font> submissions.<span style="font-size:80%;opacity:0.8">这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。</span></li><li>In the main body of the paper we have considered the classification task of the <font color=forestgreen>ILSVRC</font> challenge, and performed a thorough evaluation of ConvNet architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the <font color=forestgreen>ILSVRC</font>-2013 localisation challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our <font color=forestgreen>ILSVRC</font>-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>The localisation error is measured according to the <font color=forestgreen>ILSVRC</font> criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>With 25.3% test error, our “VGG” team won the localisation challenge of <font color=forestgreen>ILSVRC</font>-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>Notably, our results are considerably better than those of the <font color=forestgreen>ILSVRC</font>-2013 winner Overfeat (Sermanet et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li><li>Table 10: Comparison with the state of the art in <font color=forestgreen>ILSVRC</font> localisation.<span style="font-size:80%;opacity:0.8">表10：与ILSVRC本地化技术的比较。</span></li><li>In the previous sections we have discussed training and evaluation of very deep ConvNets on the <font color=forestgreen>ILSVRC</font> dataset.<span style="font-size:80%;opacity:0.8">在前面的部分中，我们讨论了ILSVRC数据集上非常深的ConvNet的训练和评估。</span></li><li>In this section, we evaluate our ConvNets, pre-trained on <font color=forestgreen>ILSVRC</font>, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on <font color=forestgreen>ILSVRC</font>, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>In this evaluation, we consider two models with the best classification performance on <font color=forestgreen>ILSVRC</font>(Sect. 4) – configurations “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li><li>To utilise the ConvNets, pre-trained on <font color=forestgreen>ILSVRC</font>, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way <font color=forestgreen>ILSVRC</font> classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Aggregation of features is carried out in a similar manner to our <font color=forestgreen>ILSVRC</font> evaluation procedure (Sect. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>Results marked with * were achieved using ConvNets pre-trained on the extended <font color=forestgreen>ILSVRC</font> dataset (2000 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(2000个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the <font color=forestgreen>ILSVRC</font> dataset, outperforming the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class <font color=forestgreen>ILSVRC</font> dataset, which includes additional 1000 categories, semantically close to those in VOC datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>Results marked with * were achieved using ConvNets pre-trained on the extended <font color=forestgreen>ILSVRC</font> dataset (1512 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(1512个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Presents the experiments carried out before the <font color=forestgreen>ILSVRC</font> submission.<span style="font-size:80%;opacity:0.8">介绍在ILSVRC提交之前进行的实验。</span></li><li>v2 Adds post-submission <font color=forestgreen>ILSVRC</font> experiments with training set augmentation using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> Zeiler </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; <font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>Recently, there has been a lot of interest in such a use case (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Following Chatfield et al. (2014); <font color=forestgreen>Zeiler</font> & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> Fergus </td> <td> ['fә:^әs] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & <font color=orangered>Fergus</font>, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Following Chatfield et al. (2014); Zeiler & <font color=orangered>Fergus</font> (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> Sermanet </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (<font color=forestgreen>Sermanet</font> et al. , 2014; Howard, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li><li>In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (<font color=forestgreen>Sermanet</font> et al. , 2014)).<span style="font-size:80%;opacity:0.8">尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>Then, the network is applied densely over the rescaled test image in a way similar to (<font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>For this we adopt the approach of <font color=forestgreen>Sermanet</font> et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (<font color=forestgreen>Sermanet</font> et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (<font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">我们探索了微调所有层和仅微调前两个完全连接的层，如(Sermanet等人，2014年)。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of <font color=forestgreen>Sermanet</font> et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>We did not use the multiple pooling offsets technique of <font color=forestgreen>Sermanet</font> et al. (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results.<span style="font-size:80%;opacity:0.8">我们没有使用Sermanet等人(2014)的多池补偿技术，它提高了边界框预测的空间分辨率，并可以进一步改进结果。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of <font color=forestgreen>Sermanet</font> et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>We also note that fine-tuning all layers for the localisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (<font color=forestgreen>Sermanet</font> et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of <font color=forestgreen>Sermanet</font> et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (<font color=forestgreen>Sermanet</font> et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> utilise </td> <td> ['ju:tɪlaɪz] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) <font color=orangered>utilised</font> smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>In one of the configurations we also <font color=orangered>utilise</font> 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>It should be noted that 1 × 1 conv. layers have recently been <font color=orangered>utilised</font> in the “Network in Network” architecture of Lin et al. (2014).<span style="font-size:80%;opacity:0.8">应该注意的是1×1卷积层最近在Lin等人(2014)的“Network in Network”架构中已经得到了使用。</span></li><li>To come up with the final prediction, we <font color=orangered>utilise</font> the greedy merging procedure of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>Following that line of work, we investigate if our models lead to better performance than more shallow models <font color=orangered>utilised</font> in the state-of-the-artmethods.<span style="font-size:80%;opacity:0.8">遵循这一工作路线，我们研究我们的模型是否比现有技术中使用的更浅的模型具有更好的性能。</span></li><li>To <font color=orangered>utilise</font> the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> receptive </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller <font color=orangered>receptive</font> window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>The image is passed through a stack of convolutional (conv. ) layers, where we use filters with a very small <font color=orangered>receptive</font> field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center).<span style="font-size:80%;opacity:0.8">图像通过一堆卷积（conv.）层，我们使用感受野很小的滤波器：3×3（这是捕获左/右，上/下，中心概念的最小尺寸）。</span></li><li>In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and <font color=orangered>receptive</font> fields (144M weights in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</span></li><li>Rather than using relatively large <font color=orangered>receptive</font> fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 <font color=orangered>receptive</font> fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective <font color=orangered>receptive</font> field of 5 × 5; three such layers have a 7 × 7 effective <font color=orangered>receptive</font> field.<span style="font-size:80%;opacity:0.8">很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野；三个这样的层具有7×7的有效感受野。</span></li><li>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the <font color=orangered>receptive</font> fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network <font color=orangered>receptive</font> field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial <font color=orangered>receptive</font> fields (D is better than C).<span style="font-size:80%;opacity:0.8">这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。</span></li><li>layer (which has the same <font color=orangered>receptive</font> field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> densely </td> <td> [denslɪ] </td> <td> 
<ul><li>Another line of improvements dealt with training and testing the networks <font color=orangered>densely</font> over the whole image and over multiple scales (Sermanet et al. , 2014; Howard, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li><li>Then, the network is applied <font color=orangered>densely</font> over the rescaled test image in a way similar to (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>Namely, an image is first rescaled so that its smallest side equals Q, and then the network is <font color=orangered>densely</font> applied over the image plane (which is possible when all weight layers are treated as convolutional).<span style="font-size:80%;opacity:0.8">即，首先重新缩放图像，使得其最小侧等于Q，然后在图像平面上密集地应用网络(当所有权重层都被视为卷积时，这是可能的)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> Howard </td> <td> [ˈhauəd] </td> <td> 
<ul><li>Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al. , 2014; <font color=orangered>Howard</font>, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> applicable </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also <font color=orangered>applicable</font> to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>Where <font color=orangered>applicable</font>, the parameters for the LRN layer are those of (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li><li>For random initialisation (where <font color=orangered>applicable</font>), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> SVM </td> <td>  </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear <font color=forestgreen>SVM</font> without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>The resulting image descriptor is L2-normalised and combined with a linear <font color=forestgreen>SVM</font> classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> fine-tune </td> <td> [faɪn tju:n] </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without <font color=orangered>fine-tuning</font>).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>For speed reasons, we trained multi-scale models by <font color=orangered>fine-tuning</font> all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384.<span style="font-size:80%;opacity:0.8">为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的S = 384进行预训练。</span></li><li>By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by <font color=orangered>fine-tuning</font> only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>We explored both <font color=orangered>fine-tuning</font> all layers and <font color=orangered>fine-tuning</font> only the first two fully-connected layers, as done in (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们探索了微调所有层和仅微调前两个完全连接的层，如(Sermanet等人，2014年)。</span></li><li>We also note that <font color=orangered>fine-tuning</font> all layers for the localisation task leads to noticeably better results than <font color=orangered>fine-tuning</font> only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Having determined the best localisation setting (PCR, <font color=orangered>fine-tuning</font> of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>For simplicity, pre-trained ConvNet weights are kept fixed (no <font color=orangered>fine-tuning</font> is performed).<span style="font-size:80%;opacity:0.8">为简单起见，预先训练的ConvNet权重保持固定(不执行微调)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> sect </td> <td> [sekt] </td> <td> 
<ul><li>In <font color=orangered>Sect</font>. 2, we describe our ConvNet configurations.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>The details of the image classification training and evaluation are then presented in <font color=orangered>Sect</font>. 3, and the configurations are compared on the ILSVRC classification task in <font color=orangered>Sect</font>. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li><font color=orangered>Sect</font>. 5 concludes the paper.<span style="font-size:80%;opacity:0.8">第5节总结了论文。</span></li><li>In this section, we first describe a generic layout of our ConvNet configurations (<font color=orangered>Sect</font>. 2.1) and then detail the specific configurations used in the evaluation (<font color=orangered>Sect</font>. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>Our design choices are then discussed and compared to the prior art in <font color=orangered>Sect</font>. 2.3.<span style="font-size:80%;opacity:0.8">最后，我们的设计选择将在2.3节进行讨论并与现有技术进行比较。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in <font color=orangered>Sect</font>. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>All configurations follow the generic design presented in <font color=orangered>Sect</font>. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>As will be shown in <font color=orangered>Sect</font>. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>We note that Q is not necessarily equal to the training scale S (as we will show in <font color=orangered>Sect</font>. 4, using several values of Q for each S leads to improved performance).<span style="font-size:80%;opacity:0.8">我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。</span></li><li>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in <font color=orangered>Sect</font>. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>layer (which has the same receptive field as explained in <font color=orangered>Sect</font>. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li><li>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see <font color=orangered>Sect</font>. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li><li>Our method is described in <font color=orangered>Sect</font>. A. 1 and evaluated in <font color=orangered>Sect</font>. A. 2.<span style="font-size:80%;opacity:0.8">我们的方法在A.1节中描述，并在A.2.节中进行评估。</span></li><li>Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (<font color=orangered>Sect</font>. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>Training of localisation ConvNets is similar to that of the classification ConvNets(<font color=orangered>Sect</font>. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (<font color=orangered>Sect</font>. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (<font color=orangered>Sect</font>. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>Similarly to the classification task (<font color=orangered>Sect</font>. 4), testing at several scales and combining the predictions of multiple networks further improves the performance.<span style="font-size:80%;opacity:0.8">类似于分类任务(Sect.4)在多个尺度上进行测试，并结合多个网络的预测进一步提高性能。</span></li><li>In this evaluation, we consider two models with the best classification performance on ILSVRC(<font color=orangered>Sect</font>. 4) – configurations “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li><li>Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (<font color=orangered>Sect</font>. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>As was shown in <font color=orangered>Sect</font>. 4.2, evaluation over multiple scales is beneficial, so we extract features over several scales Q.<span style="font-size:80%;opacity:0.8">如4.2节所示，在多个尺度上进行评估是有益的，因此我们在几个尺度Q上提取特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> completeness </td> <td> [kəm'pli:tnəs] </td> <td> 
<ul><li>For <font color=orangered>completeness</font>, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> generalisation </td> <td> [ˌdʒenərəlaɪ'zeɪʃən] </td> <td> 
<ul><li>For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the <font color=orangered>generalisation</font> of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>B <font color=orangered>GENERALISATION</font> OF VERY DEEP FEATURES<span style="font-size:80%;opacity:0.8">B 非常深层特征的概括</span></li><li>v3 Adds <font color=orangered>generalisation</font> experiments (Appendix B) on PASCAL VOC and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> revision </td> <td> [rɪˈvɪʒn] </td> <td> 
<ul><li>Finally, Appendix C contains the list of major paper <font color=orangered>revisions</font>.<span style="font-size:80%;opacity:0.8">最后，附录C包含了主要的论文修订列表。</span></li><li>C PAPER <font color=orangered>REVISIONS</font><span style="font-size:80%;opacity:0.8">C 论文修订</span></li><li>Here we present the list of major paper <font color=orangered>revisions</font>, outlining the substantial changes for the convenience of the reader.<span style="font-size:80%;opacity:0.8">为了方便读者，我们在这里列出了主要的论文修订列表，概述了重要的变化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> CONVNET </td> <td>  </td> <td> 
<ul><li>2 <font color=forestgreen>CONVNET</font> CONFIGURATIONS<span style="font-size:80%;opacity:0.8">2. ConvNet配置</span></li><li>4.4 <font color=forestgreen>CONVNET</font> FUSION<span style="font-size:80%;opacity:0.8">4.4 卷积网络融合</span></li><li>A.1 LOCALISATION <font color=forestgreen>CONVNET</font><span style="font-size:80%;opacity:0.8">A.1 ConvNet定位</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> Ciresan </td> <td>  </td> <td> 
<ul><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by <font color=forestgreen>Ciresan</font> et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>Small-size convolution filters have been previously used by <font color=forestgreen>Ciresan</font> et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> generic </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>In this section, we first describe a <font color=orangered>generic</font> layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>All configurations follow the <font color=orangered>generic</font> design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> fixed-size </td> <td>  </td> <td> 
<ul><li>During training, the input to our ConvNets is a <font color=forestgreen>fixed-size</font> 224 × 224 RGB image.<span style="font-size:80%;opacity:0.8">在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。</span></li><li>To obtain the <font color=forestgreen>fixed-size</font> 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Finally, to obtain a <font color=forestgreen>fixed-size</font> vector of class scores for the image, the class score map is spatially averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> non-linearity </td> <td> ['nɒnlaɪn'ərɪtɪ] </td> <td> 
<ul><li>In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by <font color=orangered>non-linearity</font>).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al. , 2012)) <font color=orangered>non-linearity</font>.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with <font color=orangered>non-linearity</font> injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li><li>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the <font color=orangered>non-linearity</font> of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional <font color=orangered>non-linearity</font> is introduced by the rectification function.<span style="font-size:80%;opacity:0.8">即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。</span></li><li>layers throughout the network. This indicates that while the additional <font color=orangered>non-linearity</font> does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C).<span style="font-size:80%;opacity:0.8">这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> max-pooling </td> <td>  </td> <td> 
<ul><li>layers. Spatial pooling is carried out by five <font color=forestgreen>max-pooling</font> layers, which follow some of the conv. layers (not all the conv.<span style="font-size:80%;opacity:0.8">空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。</span></li><li>layers are followed by <font color=forestgreen>max-pooling</font>). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.<span style="font-size:80%;opacity:0.8">在2×2像素窗口上进行最大池化，步长为2。</span></li><li>and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each <font color=forestgreen>max-pooling</font> layer, until it reaches 512.<span style="font-size:80%;opacity:0.8">卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</span></li><li>We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or <font color=forestgreen>max-pooling</font>.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>layers are followed by max-pooling). <font color=forestgreen>Max-pooling</font> is performed over a 2 × 2 pixel window, with stride 2.<span style="font-size:80%;opacity:0.8">在2×2像素窗口上进行最大池化，步长为2。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> FC </td> <td>  </td> <td> 
<ul><li>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (<font color=forestgreen>FC</font>) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class).<span style="font-size:80%;opacity:0.8">一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。</span></li><li>All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 <font color=forestgreen>FC</font> layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>and 3 <font color=forestgreen>FC</font> layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.<span style="font-size:80%;opacity:0.8">卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</span></li><li>Namely, the fully-connected layers are first converted to convolutional layers (the first <font color=forestgreen>FC</font> layer to a 7 × 7 conv. layer, the last two <font color=forestgreen>FC</font> layers to 1 × 1 conv.<span style="font-size:80%;opacity:0.8">即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> soft-max </td> <td>  </td> <td> 
<ul><li>The final layer is the <font color=forestgreen>soft-max</font> layer.<span style="font-size:80%;opacity:0.8">最后一层是soft-max层。</span></li><li>We also augment the test set by horizontal flipping of the images; the <font color=forestgreen>soft-max</font> class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>We also assess the complementarity of the two evaluation techniques by averaging their <font color=forestgreen>soft-max</font> outputs.<span style="font-size:80%;opacity:0.8">我们还通过平均其soft-max输出来评估两种评估技术的互补性。</span></li><li>In this part of the experiments, we combine the outputs of several models by averaging their <font color=forestgreen>soft-max</font> class posteriors.<span style="font-size:80%;opacity:0.8">在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> rectification </td> <td> [ˌrektɪfɪ'keɪʃn] </td> <td> 
<ul><li>All hidden layers are equipped with the <font color=orangered>rectification</font> (ReLU (Krizhevsky et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>First, we incorporate three non-linear <font color=orangered>rectification</font> layers instead of a single one, which makes the decision function more discriminative.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li><li>layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the <font color=orangered>rectification</font> function.<span style="font-size:80%;opacity:0.8">即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> ReLU </td> <td>  </td> <td> 
<ul><li>All hidden layers are equipped with the rectification (<font color=forestgreen>ReLU</font> (Krizhevsky et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>The <font color=forestgreen>ReLU</font> activation function is not shown for brevity.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> LRN </td> <td>  </td> <td> 
<ul><li>We note that none of our networks (except for one) contain Local Response Normalisation (<font color=forestgreen>LRN</font>) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Where applicable, the parameters for the <font color=forestgreen>LRN</font> layer are those of (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> normalisation </td> <td> [,nɔ:məlai'zeiʃən] </td> <td> 
<ul><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) <font color=orangered>normalisation</font> (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such <font color=orangered>normalisation</font> does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>First, we note that using local response <font color=orangered>normalisation</font> (A-LRN network) does not improve on the model A without any <font color=orangered>normalisation</font> layers.<span style="font-size:80%;opacity:0.8">首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。</span></li><li>We thus do not employ <font color=orangered>normalisation</font> in the deeper architectures (B–E).<span style="font-size:80%;opacity:0.8">因此，我们在较深的架构（B-E）中不采用归一化。</span></li><li>We note that none of our networks (except for one) contain Local Response <font color=orangered>Normalisation</font> (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> bold </td> <td> [bəʊld] </td> <td> 
<ul><li>The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in <font color=orangered>bold</font>).<span style="font-size:80%;opacity:0.8">随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。</span></li><li>Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in <font color=orangered>bold</font> in Table 4).<span style="font-size:80%;opacity:0.8">我们在验证集上的最佳单网络性能为24.8％/7.5％ top-1/top-5的错误率（在表4中用粗体突出显示）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> activation </td> <td> [ˌæktɪ'veɪʃn] </td> <td> 
<ul><li>The ReLU <font color=orangered>activation</font> function is not shown for brevity.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> brevity </td> <td> [ˈbrevəti] </td> <td> 
<ul><li>The ReLU activation function is not shown for <font color=orangered>brevity</font>.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li><li>In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for <font color=orangered>brevity</font>.<span style="font-size:80%;opacity:0.8">在这些实验中，最小图像侧被设置为S=384；S=256的结果表现出相同的行为，为了简洁起见不显示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> top-performing </td> <td>  </td> <td> 
<ul><li>Our ConvNet configurations are quite different from the ones used in the <font color=forestgreen>top-performing</font> entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a <font color=forestgreen>top-performing</font> entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> convolved </td> <td> [kənˈvɔlvd] </td> <td> 
<ul><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are <font color=orangered>convolved</font> with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the <font color=orangered>convolved</font> feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> incorporate </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>First, we <font color=orangered>incorporate</font> three non-linear rectification layers instead of a single one, which makes the decision function more discriminative.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li><li>We envisage that better localisation performance can be achieved if this technique is <font color=orangered>incorporated</font> into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li><li>Unlike other approaches, we did not <font color=orangered>incorporate</font> any task-specific heuristics, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> discriminative </td> <td> [dɪs'krɪmɪnətɪv] </td> <td> 
<ul><li>First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more <font color=orangered>discriminative</font>.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> parametrised </td> <td>  </td> <td> 
<ul><li>Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is <font color=forestgreen>parametrised</font> by $3(3^2C^2)=27C^2$ weights; at the same time, a single 7 × 7 conv. layer would require $7^2C^2=49C^2$ parameters, i.e. 81% more.<span style="font-size:80%;opacity:0.8">其次，我们减少参数的数量：假设三层3×3卷积堆叠的输入和输出有C个通道，堆叠卷积层的参数为$3(3^2C^2)=27C^2$个权重；同时，单个7×7卷积层将需要$7^2C^2=49C^2$个参数，即参数多81％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> regularisation </td> <td> [,reɡjulərai'zeiʃən] </td> <td> 
<ul><li>This can be seen as imposing a <font color=orangered>regularisation</font> on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li><li>The training was regularised by weight decay (the L2 penalty multiplier set to $5 \times 10^{−4}$) and dropout <font color=orangered>regularisation</font> for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit <font color=orangered>regularisation</font> imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> decomposition </td> <td> [ˌdi:kɒmpə'zɪʃn] </td> <td> 
<ul><li>This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a <font color=orangered>decomposition</font> through the 3 × 3 filters (with non-linearity injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> incorporation </td> <td> [ɪnˌkɔ:pə'reɪʃn] </td> <td> 
<ul><li>The <font color=orangered>incorporation</font> of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> Small-size </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Small-size</font> convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> Goodfellow </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Goodfellow</font> et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance.<span style="font-size:80%;opacity:0.8">Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> GoogLeNet </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>GoogLeNet</font> (Szegedy et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>Our result is also competitive with respect to the classification task winner (<font color=forestgreen>GoogLeNet</font> with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single <font color=forestgreen>GoogLeNet</font> by 0.9%.<span style="font-size:80%;opacity:0.8">在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> Szegedy </td> <td>  </td> <td> 
<ul><li>GoogLeNet (<font color=forestgreen>Szegedy</font> et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>As will be shown in Sect. 4.5, our model is outperforming that of <font color=forestgreen>Szegedy</font> et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>At the same time, using a large set of crops, as done by <font color=forestgreen>Szegedy</font> et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net.<span style="font-size:80%;opacity:0.8">同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。</span></li><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by <font color=forestgreen>Szegedy</font> et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> topology </td> <td> [tə'pɒlədʒɪ] </td> <td> 
<ul><li>Their network <font color=orangered>topology</font> is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation.<span style="font-size:80%;opacity:0.8">然而，它们的网络拓扑结构比我们的更复杂，并且在第一层中特征图的空间分辨率被更积极地减少，以减少计算量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> outperform </td> <td> [ˌaʊtpəˈfɔ:m] </td> <td> 
<ul><li>As will be shown in Sect. 4.5, our model is <font color=orangered>outperforming</font> that of Szegedy et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters <font color=orangered>outperforms</font> a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li><li>As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination <font color=orangered>outperforms</font> each of them.<span style="font-size:80%;opacity:0.8">可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。</span></li><li>As can be seen from Table 7, our very deep ConvNets significantly <font color=orangered>outperform</font> the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially <font color=orangered>outperforms</font> the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>In terms of the single-net performance, our architecture achieves the best result (7.0% test error), <font color=orangered>outperforming</font> a single GoogLeNet by 0.9%.<span style="font-size:80%;opacity:0.8">在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。</span></li><li>In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or <font color=orangered>outperforming</font> more complex recognition pipelines built around less deep image representations.<span style="font-size:80%;opacity:0.8">在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) <font color=orangered>outperforms</font> the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was <font color=orangered>outperformed</font> by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have <font color=orangered>outperformed</font> hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, <font color=orangered>outperforming</font> the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>On Caltech-256, our features <font color=orangered>outperform</font> the state of the art (Chatfield et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li><li>Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently <font color=orangered>outperforming</font> more shallow representations.<span style="font-size:80%;opacity:0.8">自从我们的模型公开发布以来，研究界一直在积极地使用它们来完成广泛的图像识别任务，始终优于更浅的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> optimise </td> <td> ['ɒptɪmaɪz] </td> <td> 
<ul><li>Namely, the training is carried out by <font color=orangered>optimising</font> the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> multinomial </td> <td> [ˌmʌltɪ'nəʊmɪəl] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the <font color=orangered>multinomial</font> logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> descent </td> <td> [dɪˈsent] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient <font color=orangered>descent</font> (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> back-propagation </td> <td>  </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on <font color=forestgreen>back-propagation</font> (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> LeCun </td> <td>  </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (<font color=forestgreen>LeCun</font> et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li><li>Notably, we did not depart from the classical ConvNet architecture of <font color=forestgreen>LeCun</font> et al. (1989), but improved it by substantially increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (<font color=forestgreen>LeCun</font> et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with <font color=orangered>momentum</font>.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li><li>The batch size was set to 256, <font color=orangered>momentum</font> to 0.9.<span style="font-size:80%;opacity:0.8">批量大小设为256，动量为0.9。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> regularise </td> <td> ['regjʊləraɪz] </td> <td> 
<ul><li>The training was <font color=orangered>regularised</font> by weight decay (the L2 penalty multiplier set to $5 \times 10^{−4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> multiplier </td> <td> [ˈmʌltɪplaɪə(r)] </td> <td> 
<ul><li>The training was regularised by weight decay (the L2 penalty <font color=orangered>multiplier</font> set to $5 \times 10^{−4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> epoch </td> <td> [ˈi:pɒk] </td> <td> 
<ul><li>In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 <font color=orangered>epochs</font>).<span style="font-size:80%;opacity:0.8">学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less <font color=orangered>epochs</font> to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> conjecture </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> pre-initialisation </td> <td>  </td> <td> 
<ul><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) <font color=forestgreen>pre-initialisation</font> of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> initialisation </td> <td> [ɪnɪʃəlaɪ'zeɪʃən] </td> <td> 
<ul><li>The <font color=orangered>initialisation</font> of the network weights is important, since bad <font color=orangered>initialisation</font> can stall learning due to the instability of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li><li>To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random <font color=orangered>initialisation</font>.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li><li>For random <font color=orangered>initialisation</font> (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random <font color=orangered>initialisation</font> procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> stall </td> <td> [stɔ:l] </td> <td> 
<ul><li>The initialisation of the network weights is important, since bad initialisation can <font color=orangered>stall</font> learning due to the instability of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> instability </td> <td> [ˌɪnstəˈbɪləti] </td> <td> 
<ul><li>The initialisation of the network weights is important, since bad initialisation can stall learning due to the <font color=orangered>instability</font> of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> circumvent </td> <td> [ˌsɜ:kəmˈvent] </td> <td> 
<ul><li>To <font color=orangered>circumvent</font> this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> initialise </td> <td> [ɪ'nɪʃəlaɪz] </td> <td> 
<ul><li>Then, when training deeper architectures, we <font color=orangered>initialised</font> the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were <font color=orangered>initialised</font> randomly).<span style="font-size:80%;opacity:0.8">然后，当训练更深的架构时，我们用网络A的层初始化前四个卷积层和最后三个全连接层（中间层被随机初始化）。</span></li><li>The biases were <font color=orangered>initialised</font> with zero.<span style="font-size:80%;opacity:0.8">偏置初始化为零。</span></li><li>It is worth noting that after the paper submission we found that it is possible to <font color=orangered>initialise</font> the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li><li>To speed-up training of the S = 384 network, it was <font color=orangered>initialised</font> with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}$.<span style="font-size:80%;opacity:0.8">为了加速S = 384网络的训练，用S = 256预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</span></li><li>Training was <font color=orangered>initialised</font> with the corresponding classification models (trained on the same scales), and the initial learning rate was set to $10^−3$.<span style="font-size:80%;opacity:0.8">使用相应的分类模型(在相同的规模上训练)初始化训练，并将初始学习率设置为$10^−3$。</span></li><li>The last fully-connected layer was <font color=orangered>initialised</font> randomly and trained from scratch.<span style="font-size:80%;opacity:0.8">最后一个完全连接的层是随机初始化的，并从头开始训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> pre-initialised </td> <td>  </td> <td> 
<ul><li>We did not decrease the learning rate for the <font color=forestgreen>pre-initialised</font> layers, allowing them to change during learning.<span style="font-size:80%;opacity:0.8">我们没有减少预初始化层的学习率，允许他们在学习过程中改变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> variance </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ <font color=orangered>variance</font>.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> Glorot </td> <td>  </td> <td> 
<ul><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of <font color=forestgreen>Glorot</font> & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> Bengio </td> <td>  </td> <td> 
<ul><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & <font color=forestgreen>Bengio</font> (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> rescale </td> <td> [ri:'skeɪl] </td> <td> 
<ul><li>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from <font color=orangered>rescaled</font> training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Training image <font color=orangered>rescaling</font> is explained below.<span style="font-size:80%;opacity:0.8">下面解释训练图像归一化。</span></li><li>The second approach to setting S is multi-scale training, where each training image is individually <font color=orangered>rescaled</font> by randomly sampling S from a certain range $[S_{min},S_{max}]$ (we used $S_{min} = 256$ and $S_{max} = 512$).<span style="font-size:80%;opacity:0.8">设置S的第二种方法是多尺度训练，其中每个训练图像通过从一定范围$[S_{min}，S_{max}]$（我们使用$S_{min} = 256$ 和 $S_{max} = 512$）随机采样S来单独进行归一化。</span></li><li>First, it is isotropically <font color=orangered>rescaled</font> to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale).<span style="font-size:80%;opacity:0.8">首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。</span></li><li>Then, the network is applied densely over the <font color=orangered>rescaled</font> test image in a way similar to (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>It consists of running a model over several <font color=orangered>rescaled</font> versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors.<span style="font-size:80%;opacity:0.8">它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。</span></li><li>Namely, an image is first <font color=orangered>rescaled</font> so that its smallest side equals Q, and then the network is densely applied over the image plane (which is possible when all weight layers are treated as convolutional).<span style="font-size:80%;opacity:0.8">即，首先重新缩放图像，使得其最小侧等于Q，然后在图像平面上密集地应用网络(当所有权重层都被视为卷积时，这是可能的)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per <font color=orangered>SGD</font> iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> augment </td> <td> [ɔ:gˈment] </td> <td> 
<ul><li>To further <font color=orangered>augment</font> the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>We also <font color=orangered>augment</font> the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> flip </td> <td> [flɪp] </td> <td> 
<ul><li>To further augment the training set, the crops underwent random horizontal <font color=orangered>flipping</font> and random RGB colour shift (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>We also augment the test set by horizontal <font color=orangered>flipping</font> of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and <font color=orangered>flipped</font> images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 <font color=orangered>flips</font>), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li><li>The descriptor is then averaged with the descriptor of a horizontally <font color=orangered>flipped</font> image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> isotropically-rescaled </td> <td>  </td> <td> 
<ul><li>Let S be the smallest side of an <font color=forestgreen>isotropically-rescaled</font> training image, from which the ConvNet input is cropped (we also refer to S as the training scale).<span style="font-size:80%;opacity:0.8">令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> whole-image </td> <td>  </td> <td> 
<ul><li>While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture <font color=forestgreen>whole-image</font> statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.<span style="font-size:80%;opacity:0.8">虽然裁剪尺寸固定为224×224，但原则上S可以是不小于224的任何值：对于S=224，裁剪图像将捕获整个图像的统计数据，完全扩展训练图像的最小边；对于S»224，裁剪图像将对应于图像的一小部分，包含小对象或对象的一部分。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> speed-up </td> <td> [s'pi:d'ʌp] </td> <td> 
<ul><li>To <font color=orangered>speed-up</font> training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}$.<span style="font-size:80%;opacity:0.8">为了加速S = 384网络的训练，用S = 256预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> augmentation </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>This can also be seen as training set <font color=orangered>augmentation</font> by scale jittering, where a single model is trained to recognise objects over a wide range of scales.<span style="font-size:80%;opacity:0.8">这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。</span></li><li>This confirms that training set <font color=orangered>augmentation</font> by scale jittering is indeed helpful for capturing multi-scale image statistics.<span style="font-size:80%;opacity:0.8">这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</span></li><li>v2 Adds post-submission ILSVRC experiments with training set <font color=orangered>augmentation</font> using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> jittering </td> <td> [ˈdʒitərɪŋ] </td> <td> 
<ul><li>This can also be seen as training set augmentation by scale <font color=orangered>jittering</font>, where a single model is trained to recognise objects over a wide range of scales.<span style="font-size:80%;opacity:0.8">这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。</span></li><li>Finally, scale <font color=orangered>jittering</font> at training time ($S \in [256; 512]$) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time.<span style="font-size:80%;opacity:0.8">最后，训练时的尺度抖动（$S \in [256; 512]$）得到了与固定最小边（S = 256或S = 384）的图像训练相比更好的结果，即使在测试时使用单尺度。</span></li><li>This confirms that training set augmentation by scale <font color=orangered>jittering</font> is indeed helpful for capturing multi-scale image statistics.<span style="font-size:80%;opacity:0.8">这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</span></li><li>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale <font color=orangered>jittering</font> at test time.<span style="font-size:80%;opacity:0.8">在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。</span></li><li>At the same time, scale <font color=orangered>jittering</font> at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable $ S \in [S_{min}; S_{max}] $ was evaluated over a larger range of sizes $ Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \} $.<span style="font-size:80%;opacity:0.8">同时，训练时的尺度抖动允许网络在测试时应用于更广的尺度范围，所以用变量$S \in [S_{min}; S_{max}]$训练的模型在更大的尺寸范围$ Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \} $上进行评估。</span></li><li>The results, presented in Table 4, indicate that scale <font color=orangered>jittering</font> at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3).<span style="font-size:80%;opacity:0.8">表4中给出的结果表明，测试时的尺度抖动导致了更好的性能（与在单一尺度上相同模型的评估相比，如表3所示）。</span></li><li>As before, the deepest configurations (D and E) perform the best, and scale <font color=orangered>jittering</font> is better than training with a fixed smallest side S.<span style="font-size:80%;opacity:0.8">如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale <font color=orangered>jittering</font> for our ILSVRC-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>v2 Adds post-submission ILSVRC experiments with training set augmentation using scale <font color=orangered>jittering</font>, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> isotropically </td> <td>  </td> <td> 
<ul><li>First, it is <font color=forestgreen>isotropically</font> rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale).<span style="font-size:80%;opacity:0.8">首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> uncropped </td> <td> [ʌn'krɒpt] </td> <td> 
<ul><li>layers). The resulting fully-convolutional net is then applied to the whole (<font color=orangered>uncropped</font>) image.<span style="font-size:80%;opacity:0.8">然后将所得到的全卷积网络应用于整个（未裁剪）图像上。</span></li><li>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (<font color=orangered>uncropped</font>) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> vector </td> <td> [ˈvektə(r)] </td> <td> 
<ul><li>Finally, to obtain a fixed-size <font color=orangered>vector</font> of class scores for the image, the class score map is spatially averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li><li>A bounding box is represented by a 4-D <font color=orangered>vector</font> storing its center coordinates, width, and height.<span style="font-size:80%;opacity:0.8">边界框由存储其中心坐标、宽度和高度的4维矢量表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> spatially </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>Finally, to obtain a fixed-size vector of class scores for the image, the class score map is <font color=orangered>spatially</font> averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which first merges <font color=orangered>spatially</font> close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> sum-pooled </td> <td>  </td> <td> 
<ul><li>Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (<font color=forestgreen>sum-pooled</font>).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> posterior </td> <td> [pɒˈstɪəriə(r)] </td> <td> 
<ul><li>We also augment the test set by horizontal flipping of the images; the soft-max class <font color=orangered>posteriors</font> of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class <font color=orangered>posteriors</font>.<span style="font-size:80%;opacity:0.8">它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。</span></li><li>In this part of the experiments, we combine the outputs of several models by averaging their soft-max class <font color=orangered>posteriors</font>.<span style="font-size:80%;opacity:0.8">在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> re-computation </td> <td>  </td> <td> 
<ul><li>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al. , 2012), which is less efficient as it requires network <font color=forestgreen>re-computation</font> for each crop.<span style="font-size:80%;opacity:0.8">由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> complementary </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>Also, multi-crop evaluation is <font color=orangered>complementary</font> to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed <font color=orangered>complementary</font>, as their combination outperforms each of them.<span style="font-size:80%;opacity:0.8">可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> substantially </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which <font color=orangered>substantially</font> increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and <font color=orangered>substantially</font> outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by <font color=orangered>substantially</font> increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with <font color=orangered>substantially</font> increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>As can be seen from Table 9, application of the localisation ConvNet to the whole image <font color=orangered>substantially</font> improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> comparable </td> <td> [ˈkɒmpərəbl] </td> <td> 
<ul><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is <font color=orangered>comparable</font> to 144 crops over 4 scales used by Szegedy et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> Caffe </td> <td>  </td> <td> 
<ul><li>Our implementation is derived from the publicly available C++ <font color=forestgreen>Caffe</font> toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> full-size </td> <td> [ˈfulˈsaiz] </td> <td> 
<ul><li>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on <font color=orangered>full-size</font> (uncropped) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> parallelism </td> <td> [ˈpærəlelɪzəm] </td> <td> 
<ul><li>Multi-GPU training exploits data <font color=orangered>parallelism</font>, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU.<span style="font-size:80%;opacity:0.8">多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次，每个GPU并行处理。</span></li><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data <font color=orangered>parallelism</font> for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> synchronous </td> <td> [ˈsɪŋkrənəs] </td> <td> 
<ul><li>Gradient computation is <font color=orangered>synchronous</font> across the GPUs, so the result is exactly the same as when training on a single GPU.<span style="font-size:80%;opacity:0.8">梯度计算在GPU之间是同步的，所以结果与在单个GPU上训练完全一样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> conceptually </td> <td> [kən'septʃʊəlɪ] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our <font color=orangered>conceptually</font> much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> speedup </td> <td> ['spi:dʌp] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a <font color=orangered>speedup</font> of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> off-the-shelf </td> <td> [ɒf ðə ʃelf] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an <font color=orangered>off-the-shelf</font> 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> NVIDIA </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>On a system equipped with four <font color=orangered>NVIDIA</font> Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.<span style="font-size:80%;opacity:0.8">在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</span></li><li>This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of <font color=orangered>NVIDIA</font> Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> titan </td> <td> [ˈtaɪtn] </td> <td> 
<ul><li>On a system equipped with four NVIDIA <font color=orangered>Titan</font> Black GPUs, training a single net took 2–3 weeks depending on the architecture.<span style="font-size:80%;opacity:0.8">在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> held-out </td> <td>  </td> <td> 
<ul><li>The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with <font color=forestgreen>held-out</font> class labels).<span style="font-size:80%;opacity:0.8">数据集包括1000个类别的图像，并分为三组：训练（130万张图像），验证（5万张图像）和测试（留有类标签的10万张图像）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> incorrectly </td> <td> [ˌɪnkə'rektlɪ] </td> <td> 
<ul><li>The former is a multi-class classification error, i.e. the proportion of <font color=orangered>incorrectly</font> classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.<span style="font-size:80%;opacity:0.8">前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> VGG </td> <td>  </td> <td> 
<ul><li>Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “<font color=forestgreen>VGG</font>” team entry to the ILSVRC-2014 competition (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>In the classification task of ILSVRC-2014 challenge (Russakovsky et al. , 2014), our “<font color=forestgreen>VGG</font>” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>Our method is denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的方法表示为“VGG”。</span></li><li>With 25.3% test error, our “<font color=forestgreen>VGG</font>” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>Our method is denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的方法被称为“VGG”。</span></li><li>Our models are denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的模型被称为“VGG”。</span></li><li>Our models are denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的模型被称为“VGG”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> Russakovsky </td> <td>  </td> <td> 
<ul><li>Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (<font color=forestgreen>Russakovsky</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>In the classification task of ILSVRC-2014 challenge (<font color=forestgreen>Russakovsky</font> et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>The localisation error is measured according to the ILSVRC criterion (<font color=forestgreen>Russakovsky</font> et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (<font color=forestgreen>Russakovsky</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> jitter </td> <td> ['dʒɪtə] </td> <td> 
<ul><li>The test image size was set as follows: Q = S for fixed S, and $Q = 0.5$($S_{min} + S_{max}$) for <font color=orangered>jittered</font> $S \in [S_{min}, S_{max}]$.<span style="font-size:80%;opacity:0.8">测试图像大小设置如下：对于固定S的Q = S，对于抖动$S \in [S_{min}, S_{max}]$，$Q = 0.5$($S_{min} + S_{max}$)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> A-LRN </td> <td>  </td> <td> 
<ul><li>First, we note that using local response normalisation (<font color=forestgreen>A-LRN</font> network) does not improve on the model A without any normalisation layers.<span style="font-size:80%;opacity:0.8">首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> saturate </td> <td> [ˈsætʃəreɪt] </td> <td> 
<ul><li>The error rate of our architecture <font color=orangered>saturates</font> when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets.<span style="font-size:80%;opacity:0.8">当深度达到19层时，我们架构的错误率饱和，但更深的模型可能有益于较大的数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> discrepancy </td> <td> [dɪsˈkrepənsi] </td> <td> 
<ul><li>Considering that a large <font color=orangered>discrepancy</font> between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: $Q = \{S − 32, S, S + 32\}$.<span style="font-size:80%;opacity:0.8">考虑到训练和测试尺度之间的巨大差异会导致性能下降，用固定S训练的模型在三个测试图像尺度上进行了评估，接近于训练一次：$Q = \{S − 32, S, S + 32\}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> mult-crop </td> <td>  </td> <td> 
<ul><li>In Table 5 we compare dense ConvNet evaluation with <font color=forestgreen>mult-crop</font> evaluation (see Sect. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> complementarity </td> <td> [ˌkɒmplɪmen'tærɪtɪ] </td> <td> 
<ul><li>We also assess the <font color=orangered>complementarity</font> of the two evaluation techniques by averaging their soft-max outputs.<span style="font-size:80%;opacity:0.8">我们还通过平均其soft-max输出来评估两种评估技术的互补性。</span></li><li>This improves the performance due to <font color=orangered>complementarity</font> of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> hypothesize </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>As noted above, we <font color=orangered>hypothesize</font> that this is due to a different treatment of convolution boundary conditions.<span style="font-size:80%;opacity:0.8">如上所述，我们假设这是由于卷积边界条件的不同处理。</span></li><li>We <font color=orangered>hypothesize</font> that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> fusion </td> <td> [ˈfju:ʒn] </td> <td> 
<ul><li>4.4 CONVNET <font color=orangered>FUSION</font><span style="font-size:80%;opacity:0.8">4.4 卷积网络融合</span></li><li>Table 6: Multiple ConvNet <font color=orangered>fusion</font> results.<span style="font-size:80%;opacity:0.8">表6：多个卷积网络融合结果</span></li><li>We also assess late <font color=orangered>fusion</font> of features, computed using two networks, which is performed by stacking their respective image descriptors.<span style="font-size:80%;opacity:0.8">我们还评估了使用两个网络计算的特征的后期融合，这是通过堆叠它们各自的图像描述符来执行的。</span></li><li>It also benefits from the <font color=orangered>fusion</font> with an object detection-assisted classification pipeline.<span style="font-size:80%;opacity:0.8">它还受益于与对象检测辅助分类流水线的融合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> ensemble </td> <td> [ɒnˈsɒmbl] </td> <td> 
<ul><li>The resulting <font color=orangered>ensemble</font> of 7 networks has 7.3% ILSVRC test error.<span style="font-size:80%;opacity:0.8">由此产生的7个网络组合具有7.3％的ILSVRC测试误差。</span></li><li>After the submission, we considered an <font color=orangered>ensemble</font> of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>In the classification task of ILSVRC-2014 challenge (Russakovsky et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an <font color=orangered>ensemble</font> of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>After the submission, we decreased the error rate to 6.8% using an <font color=orangered>ensemble</font> of 2 models.<span style="font-size:80%;opacity:0.8">提交后，我们使用2个模型的组合将错误率降低到6.8％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> Clarifai </td> <td>  </td> <td> 
<ul><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission <font color=forestgreen>Clarifai</font>, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> conventional </td> <td> [kənˈvenʃənl] </td> <td> 
<ul><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a <font color=orangered>conventional</font> ConvNet architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> ERC </td> <td>  </td> <td> 
<ul><li>This work was supported by <font color=forestgreen>ERC</font> grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> VisRec </td> <td>  </td> <td> 
<ul><li>This work was supported by ERC grant <font color=forestgreen>VisRec</font> no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> gratefully </td> <td> ['ɡreɪtfəlɪ] </td> <td> 
<ul><li>This work was supported by ERC grant VisRec no. 228180. We <font color=orangered>gratefully</font> acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>It can be seen as a special case of object detection, where a single object <font color=orangered>bounding</font> box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class.<span style="font-size:80%;opacity:0.8">它可以被看作是对象检测的一种特殊情况，其中应该为前5个类中的每一个预测单个对象边界框，而不考虑该类的实际对象数量。</span></li><li>To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the <font color=orangered>bounding</font> box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>A <font color=orangered>bounding</font> box is represented by a 4-D vector storing its center coordinates, width, and height.<span style="font-size:80%;opacity:0.8">边界框由存储其中心坐标、宽度和高度的4维矢量表示。</span></li><li>There is a choice of whether the <font color=orangered>bounding</font> box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Apart from the last <font color=orangered>bounding</font> box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted <font color=orangered>bounding</font> box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li><li>The first is used for comparing different network modifications on the validation set, and considers only the <font color=orangered>bounding</font> box prediction for the ground truth class (to factor out the classification errors).<span style="font-size:80%;opacity:0.8">第一个用于比较验证集上的不同网络修改，并且仅考虑地面真值类的边界框预测(以排除分类误差)。</span></li><li>The <font color=orangered>bounding</font> box is obtained by applying the network only to the central crop of the image.<span style="font-size:80%;opacity:0.8">通过将网络仅应用于图像的中心裁剪来获得边界框。</span></li><li>The difference is that instead of the class score map, the output of the last fully-connected layer is a set of <font color=orangered>bounding</font> box predictions.<span style="font-size:80%;opacity:0.8">不同之处在于，最后一个完全连接的层的输出是一组边界框预测，而不是类得分映射。</span></li><li>When several localisation ConvNets are used, we first take the union of their sets of <font color=orangered>bounding</font> box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>We did not use the multiple pooling offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the <font color=orangered>bounding</font> box predictions and can further improve the results.<span style="font-size:80%;opacity:0.8">我们没有使用Sermanet等人(2014)的多池补偿技术，它提高了边界框预测的空间分辨率，并可以进一步改进结果。</span></li><li>The localisation error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the <font color=orangered>bounding</font> box prediction is deemed correct if its intersection over union ratio with the ground-truth <font color=orangered>bounding</font> box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>Table 8: Localisation error for different modifications with the simplified testing protocol: the <font color=orangered>bounding</font> box is predicted from a single central image crop, and the ground-truth class is used.<span style="font-size:80%;opacity:0.8">表8：使用简化测试协议的不同修改的定位误差：从单个中心图像裁剪预测边界框，并使用实际类别。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed <font color=orangered>bounding</font> box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a <font color=orangered>bounding</font> box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>We considered two training settings: (i) computing the ConvNet features on the whole image and ignoring the provided <font color=orangered>bounding</font> box; (ii) computing the features on the whole image and on the provided <font color=orangered>bounding</font> box, and stacking them to obtain the final representation.<span style="font-size:80%;opacity:0.8">我们考虑了两个训练设置：(I)计算整个图像上的凸网特征并忽略提供的边界框；(Ii)计算整个图像和提供的边界框上的特征，并将它们堆叠以获得最终表示。</span></li><li>Our representation achieves the state of art on the VOC action classification task even without using the provided <font color=orangered>bounding</font> boxes, and the results are further improved when using both images and <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8">即使不使用提供的边界框，我们的表示也取得了VOC动作分类任务的最新水平，并且当同时使用图像和边界框时，结果得到了进一步改善。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> irrespective </td> <td> [ˌɪrɪ'spektɪv] </td> <td> 
<ul><li>It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, <font color=orangered>irrespective</font> of the actual number of objects of the class.<span style="font-size:80%;opacity:0.8">它可以被看作是对象检测的一种特殊情况，其中应该为前5个类中的每一个预测单个对象边界框，而不考虑该类的实际对象数量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> fewmodifications </td> <td>  </td> <td> 
<ul><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a <font color=forestgreen>fewmodifications</font>.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> SCR </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, <font color=forestgreen>SCR</font> (Sermanet et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (<font color=forestgreen>SCR</font>), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by <font color=forestgreen>SCR</font>.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (<font color=forestgreen>SCR</font>) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> class-specific </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is <font color=forestgreen>class-specific</font> (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> PCR </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is class-specific (per-class regression, <font color=forestgreen>PCR</font>).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (<font color=forestgreen>PCR</font>) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where <font color=forestgreen>PCR</font> was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (<font color=forestgreen>PCR</font>).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>Having determined the best localisation setting (<font color=forestgreen>PCR</font>, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> Euclidean </td> <td> [ju:ˈklidiən] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a <font color=orangered>Euclidean</font> loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> penalise </td> <td> ['pi:nəlaɪz] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which <font color=orangered>penalises</font> the deviation of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> deviation </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the <font color=orangered>deviation</font> of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> fully-fledged </td> <td> ['fʊli:fl'edʒd] </td> <td> 
<ul><li>The second, <font color=orangered>fully-fledged</font>, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a <font color=orangered>fully-fledged</font> scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the <font color=orangered>fully-fledged</font> scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li><font color=orangered>Fully-fledged</font> evaluation.<span style="font-size:80%;opacity:0.8">全面评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> scenario </td> <td> [səˈnɑ:riəʊ] </td> <td> 
<ul><li>In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a fully-fledged <font color=orangered>scenario</font> (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged <font color=orangered>scenario</font>, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> intersection </td> <td> [ˌɪntəˈsekʃn] </td> <td> 
<ul><li>The localisation error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its <font color=orangered>intersection</font> over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> class-agnostic </td> <td>  </td> <td> 
<ul><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the <font color=forestgreen>class-agnostic</font> single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> noticeably </td> <td> ['nəʊtɪsəblɪ] </td> <td> 
<ul><li>We also note that fine-tuning all layers for the localisation task leads to <font color=orangered>noticeably</font> better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> densely-computed </td> <td>  </td> <td> 
<ul><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple <font color=forestgreen>densely-computed</font> bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> Overfeat </td> <td>  </td> <td> 
<ul><li>Notably, our results are considerably better than those of the ILSVRC-2013 winner <font color=forestgreen>Overfeat</font> (Sermanet et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> envisage </td> <td> [ɪnˈvɪzɪdʒ] </td> <td> 
<ul><li>We <font color=orangered>envisage</font> that better localisation performance can be achieved if this technique is incorporated into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> extractor </td> <td> [ɪkˈstræktə(r)] </td> <td> 
<ul><li>In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature <font color=orangered>extractors</font> on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> over-fitting </td> <td>  </td> <td> 
<ul><li>In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to <font color=forestgreen>over-fitting</font>.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> Donahue </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; <font color=forestgreen>Donahue</font> et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> Razavian </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; <font color=forestgreen>Razavian</font> et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> Chatfield </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; <font color=forestgreen>Chatfield</font> et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperforming the previous best result of <font color=forestgreen>Chatfield</font> et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>Following <font color=forestgreen>Chatfield</font> et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li><li>On Caltech-256, our features outperform the state of the art (<font color=forestgreen>Chatfield</font> et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> hand-crafted </td> <td> [,hænd 'kra:ftid] </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed <font color=orangered>hand-crafted</font> representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> state-of-the-artmethods </td> <td>  </td> <td> 
<ul><li>Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the <font color=forestgreen>state-of-the-artmethods</font>.<span style="font-size:80%;opacity:0.8">遵循这一工作路线，我们研究我们的模型是否比现有技术中使用的更浅的模型具有更好的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> activations </td> <td> [,æktɪ'veɪʃən] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D <font color=orangered>activations</font> of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> penultimate </td> <td> [penˈʌltɪmət] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the <font color=orangered>penultimate</font> layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> aggregate </td> <td> [ˈægrɪgət] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are <font color=orangered>aggregated</font> across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that <font color=orangered>aggregating</font> image descriptors, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to <font color=orangered>aggregated</font> image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 154 </td> <td> descriptor </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>The resulting image <font color=orangered>descriptor</font> is L2-normalised and combined with a linear SVM classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li><li>We then perform global average pooling on the resulting feature map, which produces a 4096-D image <font color=orangered>descriptor</font>.<span style="font-size:80%;opacity:0.8">然后，我们对生成的特征映射执行全局平均池，这将生成4096-D图像描述符。</span></li><li>The <font color=orangered>descriptor</font> is then averaged with the <font color=orangered>descriptor</font> of a horizontally flipped image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li><li>Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased <font color=orangered>descriptor</font> dimensionality.<span style="font-size:80%;opacity:0.8">堆叠允许随后的分类器学习如何在一定范围内最佳地组合图像统计数据；然而，这是以增加的描述符维数为代价的。</span></li><li>We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image <font color=orangered>descriptors</font>.<span style="font-size:80%;opacity:0.8">我们还评估了使用两个网络计算的特征的后期融合，这是通过堆叠它们各自的图像描述符来执行的。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image <font color=orangered>descriptors</font>, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>Since averaging has a benefit of not inflating the <font color=orangered>descriptor</font> dimensionality, we were able to aggregated image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li><li>Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to aggregated image <font color=orangered>descriptors</font> over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li><li>We found that unlike VOC, on Caltech datasets the stacking of <font color=orangered>descriptors</font>, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li></ul>
 </td>
</tr>
<tr>
<td> 155 </td> <td> L2-normalised </td> <td>  </td> <td> 
<ul><li>The resulting image descriptor is <font color=forestgreen>L2-normalised</font> and combined with a linear SVM classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 156 </td> <td> aggregation </td> <td> [ˌæɡrɪ'ɡeɪʃn] </td> <td> 
<ul><li><font color=orangered>Aggregation</font> of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs similarly to the <font color=orangered>aggregation</font> by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 157 </td> <td> horizontally </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>The descriptor is then averaged with the descriptor of a <font color=orangered>horizontally</font> flipped image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li></ul>
 </td>
</tr>
<tr>
<td> 158 </td> <td> optimally </td> <td> ['əptəməli] </td> <td> 
<ul><li>Stacking allows a subsequent classifier to learn how to <font color=orangered>optimally</font> combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality.<span style="font-size:80%;opacity:0.8">堆叠允许随后的分类器学习如何在一定范围内最佳地组合图像统计数据；然而，这是以增加的描述符维数为代价的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 159 </td> <td> voc </td> <td>  </td> <td> 
<ul><li>Table 11: Comparison with the state of the art in image classification on <font color=forestgreen>VOC</font>-2007, <font color=forestgreen>VOC</font>-2012, Caltech-101, and Caltech-256.<span style="font-size:80%;opacity:0.8">表11：与VOC-2007、VOC-2012、CALTECH-101和CALTECH-256上图像分类的最新水平进行比较。</span></li><li>Image Classification on <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">基于VOC-2007和VOC-2012的图像分类。</span></li><li>We begin with the evaluation on the image classification task of PASCAL <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012 benchmarks (Everingham et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>The <font color=forestgreen>VOC</font> organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided).<span style="font-size:80%;opacity:0.8">VOC组织者提供了预定义的训练、验证和测试数据划分(VOC-2012的测试数据不公开；相反，提供了官方评估服务器)。</span></li><li>Notably, by examining the performance on the validation sets of <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>We hypothesize that this is due to the fact that in the <font color=forestgreen>VOC</font> dataset the objects appear over a variety of scales, so there is no particular scale-specific semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li><li>Our networks “Net-D” and “Net-E” exhibit identical performance on <font color=forestgreen>VOC</font> datasets, and their combination slightly improves the results.<span style="font-size:80%;opacity:0.8">我们的网络“Net-D”和“Net-E”在VOC数据集上表现出相同的性能，并且它们的组合稍微改善了结果。</span></li><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in <font color=forestgreen>VOC</font> datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>We found that unlike <font color=forestgreen>VOC</font>, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>On Caltech-101, our representations are competitive with the approach of He et al. (2014), which, however, performs significantly worse than our nets on <font color=forestgreen>VOC</font>-2007.<span style="font-size:80%;opacity:0.8">在CALTECH-101上，我们的陈述与He等人(2014)的方法具有竞争力，然而，与我们在VOC-2007上的网络相比，后者的表现要差得多。</span></li><li>Action Classification on <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">关于VOC-2012的行动分类。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL <font color=forestgreen>VOC</font>-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>Similarly to the <font color=forestgreen>VOC</font>-2012 object classification task, the performance is measured using the mAP.<span style="font-size:80%;opacity:0.8">与VOC-2012对象分类任务类似，使用mAP测量性能。</span></li><li>Our representation achieves the state of art on the <font color=forestgreen>VOC</font> action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes.<span style="font-size:80%;opacity:0.8">即使不使用提供的边界框，我们的表示也取得了VOC动作分类任务的最新水平，并且当同时使用图像和边界框时，结果得到了进一步改善。</span></li><li>Table 12: Comparison with the state of the art in single-image action classification on <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">表12：与VOC-2012上单个图像动作分类的最新技术进行比较。</span></li><li>v3 Adds generalisation experiments (Appendix B) on PASCAL <font color=forestgreen>VOC</font> and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li><li>Adds a comparison of the net B with a shallow net and the results on PASCAL <font color=forestgreen>VOC</font> action classification benchmark.<span style="font-size:80%;opacity:0.8">添加网络B与浅层网络的比较以及Pascal VOC动作分类基准的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 160 </td> <td> Caltech </td> <td> ['kæltek] </td> <td> 
<ul><li>Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, <font color=orangered>Caltech</font>-101, and <font color=orangered>Caltech</font>-256.<span style="font-size:80%;opacity:0.8">表11：与VOC-2007、VOC-2012、CALTECH-101和CALTECH-256上图像分类的最新水平进行比较。</span></li><li>Image Classification on <font color=orangered>Caltech</font>-101 and <font color=orangered>Caltech</font>-256.<span style="font-size:80%;opacity:0.8">基于CALTECH-101和CALTECH-256的图像分类。</span></li><li>In this section we evaluate very deep features on <font color=orangered>Caltech</font>-101 (Fei-Fei et al. , 2004) and <font color=orangered>Caltech</font>-256 (Griffin et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li><li><font color=orangered>Caltech</font>-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while <font color=orangered>Caltech</font>-256 is larger with 31K images and 257 classes.<span style="font-size:80%;opacity:0.8">Caltech-101包含标记为102个类别(101个对象类别和一个背景类别)的9K图像，而Caltech-256更大，具有31K图像和257个类别。</span></li><li>Following Chatfield et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on <font color=orangered>Caltech</font>-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li><li>On <font color=orangered>Caltech</font>-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们还生成了3个拆分，每个拆分包含每个类的60个训练图像(其余用于测试)。</span></li><li>We found that unlike VOC, on <font color=orangered>Caltech</font> datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>This can be explained by the fact that in <font color=orangered>Caltech</font> images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li><li>On <font color=orangered>Caltech</font>-101, our representations are competitive with the approach of He et al. (2014), which, however, performs significantly worse than our nets on VOC-2007.<span style="font-size:80%;opacity:0.8">在CALTECH-101上，我们的陈述与He等人(2014)的方法具有竞争力，然而，与我们在VOC-2007上的网络相比，后者的表现要差得多。</span></li><li>On <font color=orangered>Caltech</font>-256, our features outperform the state of the art (Chatfield et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li><li>v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and <font color=orangered>Caltech</font> image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 161 </td> <td> pascal </td> <td> ['pæskәl] </td> <td> 
<ul><li>We begin with the evaluation on the image classification task of <font color=orangered>PASCAL</font> VOC-2007 and VOC-2012 benchmarks (Everingham et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the <font color=orangered>PASCAL</font> VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>v3 Adds generalisation experiments (Appendix B) on <font color=orangered>PASCAL</font> VOC and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li><li>Adds a comparison of the net B with a shallow net and the results on <font color=orangered>PASCAL</font> VOC action classification benchmark.<span style="font-size:80%;opacity:0.8">添加网络B与浅层网络的比较以及Pascal VOC动作分类基准的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 162 </td> <td> Everingham </td> <td>  </td> <td> 
<ul><li>We begin with the evaluation on the image classification task of PASCAL VOC-2007 and VOC-2012 benchmarks (<font color=forestgreen>Everingham</font> et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (<font color=forestgreen>Everingham</font> et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 163 </td> <td> scale-specific </td> <td>  </td> <td> 
<ul><li>We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular <font color=forestgreen>scale-specific</font> semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such <font color=forestgreen>scale-specific</font> representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 164 </td> <td> semantics </td> <td> [sɪˈmæntɪks] </td> <td> 
<ul><li>We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific <font color=orangered>semantics</font> which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li></ul>
 </td>
</tr>
<tr>
<td> 165 </td> <td> inflate </td> <td> [ɪnˈfleɪt] </td> <td> 
<ul><li>Since averaging has a benefit of not <font color=orangered>inflating</font> the descriptor dimensionality, we were able to aggregated image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 166 </td> <td> pretrained </td> <td>  </td> <td> 
<ul><li>Our methods set the new state of the art across image representations, <font color=forestgreen>pretrained</font> on the ILSVRC dataset, outperforming the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 167 </td> <td> semantically </td> <td> [sɪ'mæntɪklɪ] </td> <td> 
<ul><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, <font color=orangered>semantically</font> close to those in VOC datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are <font color=orangered>semantically</font> different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 168 </td> <td> detection-assisted </td> <td>  </td> <td> 
<ul><li>It also benefits from the fusion with an object <font color=forestgreen>detection-assisted</font> classification pipeline.<span style="font-size:80%;opacity:0.8">它还受益于与对象检测辅助分类流水线的融合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 169 </td> <td> Fei-Fei </td> <td>  </td> <td> 
<ul><li>In this section we evaluate very deep features on Caltech-101 (<font color=forestgreen>Fei-Fei</font> et al. , 2004) and Caltech-256 (Griffin et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & <font color=forestgreen>Fei-Fei</font>, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 170 </td> <td> griffin </td> <td> [ˈgrɪfɪn] </td> <td> 
<ul><li>In this section we evaluate very deep features on Caltech-101 (Fei-Fei et al. , 2004) and Caltech-256 (<font color=orangered>Griffin</font> et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 171 </td> <td> hyper-parameter </td> <td>  </td> <td> 
<ul><li>In each split, 20% of training images were used as a validation set for <font color=forestgreen>hyper-parameter</font> selection.<span style="font-size:80%;opacity:0.8">在每次分割中，20%的训练图像被用作超参数选择的验证集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 172 </td> <td> vs </td> <td>  </td> <td> 
<ul><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object <font color=forestgreen>vs</font>. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 173 </td> <td> task-specific </td> <td>  </td> <td> 
<ul><li>Unlike other approaches, we did not incorporate any <font color=forestgreen>task-specific</font> heuristics, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 174 </td> <td> heuristic </td> <td> [hjuˈrɪstɪk] </td> <td> 
<ul><li>Unlike other approaches, we did not incorporate any task-specific <font color=orangered>heuristics</font>, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 175 </td> <td> consistently </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, <font color=orangered>consistently</font> outperforming more shallow representations.<span style="font-size:80%;opacity:0.8">自从我们的模型公开发布以来，研究界一直在积极地使用它们来完成广泛的图像识别任务，始终优于更浅的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 176 </td> <td> Girshick </td> <td>  </td> <td> 
<ul><li>For instance, <font color=forestgreen>Girshick</font> et al. (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li></ul>
 </td>
</tr>
<tr>
<td> 177 </td> <td> semantic </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in <font color=orangered>semantic</font> segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 178 </td> <td> segmentation </td> <td> [ˌsegmenˈteɪʃn] </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic <font color=orangered>segmentation</font> (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 179 </td> <td> Kiros </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (<font color=forestgreen>Kiros</font> et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 180 </td> <td> Karpathy </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; <font color=forestgreen>Karpathy</font> & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 181 </td> <td> Cimpoi </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (<font color=forestgreen>Cimpoi</font> et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 182 </td> <td> post-submission </td> <td>  </td> <td> 
<ul><li>v2 Adds <font color=forestgreen>post-submission</font> ILSVRC experiments with training set augmentation using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 183 </td> <td> Camera-ready </td> <td>  </td> <td> 
<ul><li>v6 <font color=forestgreen>Camera-ready</font> ICLR-2015 conference paper.<span style="font-size:80%;opacity:0.8">V6 复印就绪ICLR-2015会议论文。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> ILSVRC<br>(44) </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the <font color=forestgreen>ILSVRC</font>-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on <font color=forestgreen>ILSVRC</font> classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the <font color=forestgreen>ILSVRC</font> classification task in Sect. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li>For completeness, we also describe and assess our <font color=forestgreen>ILSVRC</font>-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way <font color=forestgreen>ILSVRC</font> classification and thus contains 1000 channels (one for each class).<span style="font-size:80%;opacity:0.8">一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the <font color=forestgreen>ILSVRC</font> dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the <font color=forestgreen>ILSVRC</font>-2012 (Krizhevsky et al. , 2012) and <font color=forestgreen>ILSVRC</font>-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale <font color=forestgreen>ILSVRC</font> dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a top-performing entry of the <font color=forestgreen>ILSVRC</font>-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for <font color=forestgreen>ILSVRC</font> 2012–2014 challenges).<span style="font-size:80%;opacity:0.8">在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。</span></li><li>The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in <font color=forestgreen>ILSVRC</font>, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.<span style="font-size:80%;opacity:0.8">前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</span></li><li>Certain experiments were also carried out on the test set and submitted to the official <font color=forestgreen>ILSVRC</font> server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top <font color=forestgreen>ILSVRC</font> submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>By the time of <font color=forestgreen>ILSVRC</font> submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>The resulting ensemble of 7 networks has 7.3% <font color=forestgreen>ILSVRC</font> test error.<span style="font-size:80%;opacity:0.8">由此产生的7个网络组合具有7.3％的ILSVRC测试误差。</span></li><li>In the classification task of <font color=forestgreen>ILSVRC</font>-2014 challenge (Russakovsky et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>Table 7: Comparison with the state of the art in <font color=forestgreen>ILSVRC</font> classification.<span style="font-size:80%;opacity:0.8">表7：在ILSVRC分类中与最新技术比较。</span></li><li>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the <font color=forestgreen>ILSVRC</font>-2012 and <font color=forestgreen>ILSVRC</font>-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the <font color=forestgreen>ILSVRC</font>-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>This is remarkable, considering that our best result is achieved by combining just two models —— significantly less than used in most <font color=forestgreen>ILSVRC</font> submissions.<span style="font-size:80%;opacity:0.8">这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。</span></li><li>In the main body of the paper we have considered the classification task of the <font color=forestgreen>ILSVRC</font> challenge, and performed a thorough evaluation of ConvNet architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the <font color=forestgreen>ILSVRC</font>-2013 localisation challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our <font color=forestgreen>ILSVRC</font>-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>The localisation error is measured according to the <font color=forestgreen>ILSVRC</font> criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>With 25.3% test error, our “VGG” team won the localisation challenge of <font color=forestgreen>ILSVRC</font>-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>Notably, our results are considerably better than those of the <font color=forestgreen>ILSVRC</font>-2013 winner Overfeat (Sermanet et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li><li>Table 10: Comparison with the state of the art in <font color=forestgreen>ILSVRC</font> localisation.<span style="font-size:80%;opacity:0.8">表10：与ILSVRC本地化技术的比较。</span></li><li>In the previous sections we have discussed training and evaluation of very deep ConvNets on the <font color=forestgreen>ILSVRC</font> dataset.<span style="font-size:80%;opacity:0.8">在前面的部分中，我们讨论了ILSVRC数据集上非常深的ConvNet的训练和评估。</span></li><li>In this section, we evaluate our ConvNets, pre-trained on <font color=forestgreen>ILSVRC</font>, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on <font color=forestgreen>ILSVRC</font>, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>In this evaluation, we consider two models with the best classification performance on <font color=forestgreen>ILSVRC</font>(Sect. 4) – configurations “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li><li>To utilise the ConvNets, pre-trained on <font color=forestgreen>ILSVRC</font>, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way <font color=forestgreen>ILSVRC</font> classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Aggregation of features is carried out in a similar manner to our <font color=forestgreen>ILSVRC</font> evaluation procedure (Sect. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>Results marked with * were achieved using ConvNets pre-trained on the extended <font color=forestgreen>ILSVRC</font> dataset (2000 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(2000个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the <font color=forestgreen>ILSVRC</font> dataset, outperforming the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class <font color=forestgreen>ILSVRC</font> dataset, which includes additional 1000 categories, semantically close to those in VOC datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>Results marked with * were achieved using ConvNets pre-trained on the extended <font color=forestgreen>ILSVRC</font> dataset (1512 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(1512个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Presents the experiments carried out before the <font color=forestgreen>ILSVRC</font> submission.<span style="font-size:80%;opacity:0.8">介绍在ILSVRC提交之前进行的实验。</span></li><li>v2 Adds post-submission <font color=forestgreen>ILSVRC</font> experiments with training set augmentation using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> ConvNet<br>(40) </td> <td>  </td> <td> 
<ul><li>We have made our two best-performing <font color=forestgreen>ConvNet</font> models publicly available to facilitate further research on the use of deep visual representations in computer vision.<span style="font-size:80%;opacity:0.8">我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</span></li><li>In this paper, we address another important aspect of <font color=forestgreen>ConvNet</font> architecture design —— its depth.<span style="font-size:80%;opacity:0.8">在本文中，我们解决了ConvNet架构设计的另一个重要方面——其深度。</span></li><li>As a result, we come up with significantly more accurate <font color=forestgreen>ConvNet</font> architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>In Sect. 2, we describe our <font color=forestgreen>ConvNet</font> configurations.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>To measure the improvement brought by the increased <font color=forestgreen>ConvNet</font> depth in a fair setting, all our <font color=forestgreen>ConvNet</font> layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>In this section, we first describe a generic layout of our <font color=forestgreen>ConvNet</font> configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>The <font color=forestgreen>ConvNet</font> configurations, evaluated in this paper, are outlined in Table 1, one per column.<span style="font-size:80%;opacity:0.8">本文中评估的ConvNet配置在表1中列出，每列一个。</span></li><li>Table 1: <font color=forestgreen>ConvNet</font> configurations (shown in columns).<span style="font-size:80%;opacity:0.8">表1：ConvNet配置（以列显示）。</span></li><li>Our <font color=forestgreen>ConvNet</font> configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>In this section, we describe the details of classification <font color=forestgreen>ConvNet</font> training and evaluation.<span style="font-size:80%;opacity:0.8">在本节中，我们将介绍分类ConvNet训练和评估的细节。</span></li><li>The <font color=forestgreen>ConvNet</font> training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later).<span style="font-size:80%;opacity:0.8">ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。</span></li><li>To obtain the fixed-size 224×224 <font color=forestgreen>ConvNet</font> input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Let S be the smallest side of an isotropically-rescaled training image, from which the <font color=forestgreen>ConvNet</font> input is cropped (we also refer to S as the training scale).<span style="font-size:80%;opacity:0.8">令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。</span></li><li>Given a <font color=forestgreen>ConvNet</font> configuration, we first trained the network using S = 256.<span style="font-size:80%;opacity:0.8">给定ConvNet配置，我们首先使用S=256来训练网络。</span></li><li>At test time, given a trained <font color=forestgreen>ConvNet</font> and an input image, it is classified in the following way.<span style="font-size:80%;opacity:0.8">在测试时，给出训练的ConvNet和输入图像，它按以下方式分类。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a <font color=forestgreen>ConvNet</font> to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>While more sophisticated methods of speeding up <font color=forestgreen>ConvNet</font> training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li><li>In this section, we present the image classification results achieved by the described <font color=forestgreen>ConvNet</font> architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges).<span style="font-size:80%;opacity:0.8">在本节中，我们介绍了描述的ConvNet架构（用于ILSVRC 2012-2014挑战）在ILSVRC-2012数据集上实现的图像分类结果。</span></li><li>We begin with evaluating the performance of individual <font color=forestgreen>ConvNet</font> models at a single scale with the layer configurations described in Sect. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>Table 3: <font color=forestgreen>ConvNet</font> performance at a single test scale.<span style="font-size:80%;opacity:0.8">表3：在单测试尺度的ConvNet性能</span></li><li>Second, we observe that the classification error decreases with the increased <font color=forestgreen>ConvNet</font> depth: from 11 layers in A to 19 layers in E.<span style="font-size:80%;opacity:0.8">第二，我们观察到分类误差随着ConvNet深度的增加而减小：从A中的11层到E中的19层。</span></li><li>Having evaluated the <font color=forestgreen>ConvNet</font> models at a single scale, we now assess the effect of scale jittering at test time.<span style="font-size:80%;opacity:0.8">在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。</span></li><li>Table 4: <font color=forestgreen>ConvNet</font> performance at multiple test scales.<span style="font-size:80%;opacity:0.8">表4：在多个测试尺度上的ConvNet性能</span></li><li>In Table 5 we compare dense <font color=forestgreen>ConvNet</font> evaluation with mult-crop evaluation (see Sect. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li><li>Table 5: <font color=forestgreen>ConvNet</font> evaluation techniques comparison.<span style="font-size:80%;opacity:0.8">表5：ConvNet评估技术比较。</span></li><li>Up until now, we evaluated the performance of individual <font color=forestgreen>ConvNet</font> models.<span style="font-size:80%;opacity:0.8">到目前为止，我们评估了ConvNet模型的性能。</span></li><li>Table 6: Multiple <font color=forestgreen>ConvNet</font> fusion results.<span style="font-size:80%;opacity:0.8">表6：多个卷积网络融合结果</span></li><li>Notably, we did not depart from the classical <font color=forestgreen>ConvNet</font> architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional <font color=forestgreen>ConvNet</font> architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of <font color=forestgreen>ConvNet</font> architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li><li>To perform object localisation, we use a very deep <font color=forestgreen>ConvNet</font>, where the last fully connected layer predicts the bounding box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>Apart from the last bounding box prediction layer, we use the <font color=forestgreen>ConvNet</font> architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the localisation <font color=forestgreen>ConvNet</font> to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>All <font color=forestgreen>ConvNet</font> layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>As can be seen from Table 9, application of the localisation <font color=forestgreen>ConvNet</font> to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li><li>For simplicity, pre-trained <font color=forestgreen>ConvNet</font> weights are kept fixed (no fine-tuning is performed).<span style="font-size:80%;opacity:0.8">为简单起见，预先训练的ConvNet权重保持固定(不执行微调)。</span></li><li>We considered two training settings: (i) computing the <font color=forestgreen>ConvNet</font> features on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation.<span style="font-size:80%;opacity:0.8">我们考虑了两个训练设置：(I)计算整个图像上的凸网特征并忽略提供的边界框；(Ii)计算整个图像和提供的边界框上的特征，并将它们堆叠以获得最终表示。</span></li><li>For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the <font color=forestgreen>ConvNet</font> of Krizhevsky et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> configuration<br>(29) </td> <td> [kənˌfɪgəˈreɪʃn] </td> <td> 
<ul><li>Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art <font color=orangered>configurations</font> can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li><li>In Sect. 2, we describe our ConvNet <font color=orangered>configurations</font>.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>The details of the image classification training and evaluation are then presented in Sect. 3, and the <font color=orangered>configurations</font> are compared on the ILSVRC classification task in Sect. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li>2 CONVNET <font color=orangered>CONFIGURATIONS</font><span style="font-size:80%;opacity:0.8">2. ConvNet配置</span></li><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer <font color=orangered>configurations</font> are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>In this section, we first describe a generic layout of our ConvNet <font color=orangered>configurations</font> (Sect. 2.1) and then detail the specific <font color=orangered>configurations</font> used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>In one of the <font color=orangered>configurations</font> we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>The <font color=orangered>configuration</font> of the fully connected layers is the same in all networks.<span style="font-size:80%;opacity:0.8">所有网络中全连接层的配置是相同的。</span></li><li>2.2 <font color=orangered>CONFIGURATIONS</font><span style="font-size:80%;opacity:0.8">2.2 配置</span></li><li>The ConvNet <font color=orangered>configurations</font>, evaluated in this paper, are outlined in Table 1, one per column.<span style="font-size:80%;opacity:0.8">本文中评估的ConvNet配置在表1中列出，每列一个。</span></li><li>All <font color=orangered>configurations</font> follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>Table 1: ConvNet <font color=orangered>configurations</font> (shown in columns).<span style="font-size:80%;opacity:0.8">表1：ConvNet配置（以列显示）。</span></li><li>The depth of the <font color=orangered>configurations</font> increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold).<span style="font-size:80%;opacity:0.8">随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。</span></li><li>In Table 2 we report the number of parameters for each <font color=orangered>configuration</font>.<span style="font-size:80%;opacity:0.8">在表2中，我们报告了每个配置的参数数量。</span></li><li>Our ConvNet <font color=orangered>configurations</font> are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>The incorporation of 1 × 1 conv. layers (<font color=orangered>configuration</font> C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>In the previous section we presented the details of our network <font color=orangered>configurations</font>.<span style="font-size:80%;opacity:0.8">在上一节中，我们介绍了我们的网络配置的细节。</span></li><li>To circumvent this problem, we began with training the <font color=orangered>configuration</font> A (Table 1), shallow enough to be trained with random initialisation.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li><li>Given a ConvNet <font color=orangered>configuration</font>, we first trained the network using S = 256.<span style="font-size:80%;opacity:0.8">给定ConvNet配置，我们首先使用S=256来训练网络。</span></li><li>For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same <font color=orangered>configuration</font>, pre-trained with fixed S = 384.<span style="font-size:80%;opacity:0.8">为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的S = 384进行预训练。</span></li><li>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer <font color=orangered>configurations</font> described in Sect. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>Notably, in spite of the same depth, the <font color=orangered>configuration</font> C (which contains three 1 × 1 conv. layers), performs worse than the <font color=orangered>configuration</font> D, which uses 3 × 3 conv.<span style="font-size:80%;opacity:0.8">值得注意的是，尽管深度相同，配置C（包含三个1×1卷积层）比在整个网络层中使用3×3卷积的配置D更差。</span></li><li>As before, the deepest <font color=orangered>configurations</font> (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S.<span style="font-size:80%;opacity:0.8">如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。</span></li><li>On the test set, the <font color=orangered>configuration</font> E achieves 7.3% top-5 error.<span style="font-size:80%;opacity:0.8">在测试集上，配置E实现了7.3％ top-5的错误率。</span></li><li>After the submission, we considered an ensemble of only two best-performing multi-scale models (<font color=orangered>configurations</font> D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>All ConvNet layers (except for the last one) have the <font color=orangered>configuration</font> D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>In this evaluation, we consider two models with the best classification performance on ILSVRC(Sect. 4) – <font color=orangered>configurations</font> “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> localisation<br>(25) </td> <td> [,ləukəlai'zeiʃən, -li'z-] </td> <td> 
<ul><li>These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the <font color=orangered>localisation</font> and classification tracks respectively.<span style="font-size:80%;opacity:0.8">这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。</span></li><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and <font color=orangered>localisation</font> tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>For completeness, we also describe and assess our ILSVRC-2014 object <font color=orangered>localisation</font> system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>A <font color=orangered>LOCALISATION</font><span style="font-size:80%;opacity:0.8">A 定位</span></li><li>In this section, we turn to the <font color=orangered>localisation</font> task of the challenge, which we have won in 2014 with 25.3% error.<span style="font-size:80%;opacity:0.8">在本节中，我们将转向挑战的本地化任务，我们在2014年以25.3%的错误率赢得了这项任务。</span></li><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 <font color=orangered>localisation</font> challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>A.1 <font color=orangered>LOCALISATION</font> CONVNET<span style="font-size:80%;opacity:0.8">A.1 ConvNet定位</span></li><li>To perform object <font color=orangered>localisation</font>, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>Training of <font color=orangered>localisation</font> ConvNets is similar to that of the classification ConvNets(Sect. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>We trained two <font color=orangered>localisation</font> models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the <font color=orangered>localisation</font> ConvNet to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>When several <font color=orangered>localisation</font> ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>A.2 <font color=orangered>LOCALISATION</font> EXPERIMENTS<span style="font-size:80%;opacity:0.8">A.2 定位实验</span></li><li>In this section we first determine the best-performing <font color=orangered>localisation</font> setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>The <font color=orangered>localisation</font> error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>We also note that fine-tuning all layers for the <font color=orangered>localisation</font> task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Table 8: <font color=orangered>Localisation</font> error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used.<span style="font-size:80%;opacity:0.8">表8：使用简化测试协议的不同修改的定位误差：从单个中心图像裁剪预测边界框，并使用实际类别。</span></li><li>Having determined the best <font color=orangered>localisation</font> setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>As can be seen from Table 9, application of the <font color=orangered>localisation</font> ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li><li>Table 9: <font color=orangered>Localisation</font> error<span style="font-size:80%;opacity:0.8">表9：定位错误</span></li><li>We compare our best <font color=orangered>localisation</font> result with the state of the art in Table 10.<span style="font-size:80%;opacity:0.8">我们将我们最好的本地化结果与表10中的最新技术进行了比较。</span></li><li>With 25.3% test error, our “VGG” team won the <font color=orangered>localisation</font> challenge of ILSVRC-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>We envisage that better <font color=orangered>localisation</font> performance can be achieved if this technique is incorporated into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li><li>This indicates the performance advancement brought by our very deep ConvNets – we got better results with a simpler <font color=orangered>localisation</font> method, but a more powerful representation.<span style="font-size:80%;opacity:0.8">这表明我们非常深入的ConvNets带来的性能提升-我们使用更简单的定位方法获得了更好的结果，但更强大的表示。</span></li><li>Table 10: Comparison with the state of the art in ILSVRC <font color=orangered>localisation</font>.<span style="font-size:80%;opacity:0.8">表10：与ILSVRC本地化技术的比较。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> sect<br>(24) </td> <td> [sekt] </td> <td> 
<ul><li>In <font color=orangered>Sect</font>. 2, we describe our ConvNet configurations.<span style="font-size:80%;opacity:0.8">在第2节，我们描述了我们的ConvNet配置。</span></li><li>The details of the image classification training and evaluation are then presented in <font color=orangered>Sect</font>. 3, and the configurations are compared on the ILSVRC classification task in <font color=orangered>Sect</font>. 4.<span style="font-size:80%;opacity:0.8">图像分类训练和评估的细节在第3节，并在第4节中在ILSVRC分类任务上对配置进行了比较。</span></li><li><font color=orangered>Sect</font>. 5 concludes the paper.<span style="font-size:80%;opacity:0.8">第5节总结了论文。</span></li><li>In this section, we first describe a generic layout of our ConvNet configurations (<font color=orangered>Sect</font>. 2.1) and then detail the specific configurations used in the evaluation (<font color=orangered>Sect</font>. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>Our design choices are then discussed and compared to the prior art in <font color=orangered>Sect</font>. 2.3.<span style="font-size:80%;opacity:0.8">最后，我们的设计选择将在2.3节进行讨论并与现有技术进行比较。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in <font color=orangered>Sect</font>. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>All configurations follow the generic design presented in <font color=orangered>Sect</font>. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>As will be shown in <font color=orangered>Sect</font>. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>We note that Q is not necessarily equal to the training scale S (as we will show in <font color=orangered>Sect</font>. 4, using several values of Q for each S leads to improved performance).<span style="font-size:80%;opacity:0.8">我们注意到，Q不一定等于训练尺度S（正如我们在第4节中所示，每个S使用Q的几个值会导致性能改进）。</span></li><li>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in <font color=orangered>Sect</font>. 2.2.<span style="font-size:80%;opacity:0.8">我们首先评估单个ConvNet模型在单尺度上的性能，其层结构配置如2.2节中描述。</span></li><li>layer (which has the same receptive field as explained in <font color=orangered>Sect</font>. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li><li>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see <font color=orangered>Sect</font>. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li><li>Our method is described in <font color=orangered>Sect</font>. A. 1 and evaluated in <font color=orangered>Sect</font>. A. 2.<span style="font-size:80%;opacity:0.8">我们的方法在A.1节中描述，并在A.2.节中进行评估。</span></li><li>Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (<font color=orangered>Sect</font>. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>Training of localisation ConvNets is similar to that of the classification ConvNets(<font color=orangered>Sect</font>. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (<font color=orangered>Sect</font>. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (<font color=orangered>Sect</font>. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>Similarly to the classification task (<font color=orangered>Sect</font>. 4), testing at several scales and combining the predictions of multiple networks further improves the performance.<span style="font-size:80%;opacity:0.8">类似于分类任务(Sect.4)在多个尺度上进行测试，并结合多个网络的预测进一步提高性能。</span></li><li>In this evaluation, we consider two models with the best classification performance on ILSVRC(<font color=orangered>Sect</font>. 4) – configurations “Net-D” and “Net-E” (which we made publicly available).<span style="font-size:80%;opacity:0.8">在这次评估中，我们考虑了在ILSVRC上具有最佳分类性能的两个模型(Sect.4)-配置“Net-D”和“Net-E”(我们公开提供)。</span></li><li>Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (<font color=orangered>Sect</font>. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>As was shown in <font color=orangered>Sect</font>. 4.2, evaluation over multiple scales is beneficial, so we extract features over several scales Q.<span style="font-size:80%;opacity:0.8">如4.2节所示，在多个尺度上进行评估是有益的，因此我们在几个尺度Q上提取特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> voc<br>(23) </td> <td>  </td> <td> 
<ul><li>Table 11: Comparison with the state of the art in image classification on <font color=forestgreen>VOC</font>-2007, <font color=forestgreen>VOC</font>-2012, Caltech-101, and Caltech-256.<span style="font-size:80%;opacity:0.8">表11：与VOC-2007、VOC-2012、CALTECH-101和CALTECH-256上图像分类的最新水平进行比较。</span></li><li>Image Classification on <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">基于VOC-2007和VOC-2012的图像分类。</span></li><li>We begin with the evaluation on the image classification task of PASCAL <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012 benchmarks (Everingham et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>The <font color=forestgreen>VOC</font> organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided).<span style="font-size:80%;opacity:0.8">VOC组织者提供了预定义的训练、验证和测试数据划分(VOC-2012的测试数据不公开；相反，提供了官方评估服务器)。</span></li><li>Notably, by examining the performance on the validation sets of <font color=forestgreen>VOC</font>-2007 and <font color=forestgreen>VOC</font>-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>We hypothesize that this is due to the fact that in the <font color=forestgreen>VOC</font> dataset the objects appear over a variety of scales, so there is no particular scale-specific semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li><li>Our networks “Net-D” and “Net-E” exhibit identical performance on <font color=forestgreen>VOC</font> datasets, and their combination slightly improves the results.<span style="font-size:80%;opacity:0.8">我们的网络“Net-D”和“Net-E”在VOC数据集上表现出相同的性能，并且它们的组合稍微改善了结果。</span></li><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in <font color=forestgreen>VOC</font> datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>We found that unlike <font color=forestgreen>VOC</font>, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>On Caltech-101, our representations are competitive with the approach of He et al. (2014), which, however, performs significantly worse than our nets on <font color=forestgreen>VOC</font>-2007.<span style="font-size:80%;opacity:0.8">在CALTECH-101上，我们的陈述与He等人(2014)的方法具有竞争力，然而，与我们在VOC-2007上的网络相比，后者的表现要差得多。</span></li><li>Action Classification on <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">关于VOC-2012的行动分类。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL <font color=forestgreen>VOC</font>-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>Similarly to the <font color=forestgreen>VOC</font>-2012 object classification task, the performance is measured using the mAP.<span style="font-size:80%;opacity:0.8">与VOC-2012对象分类任务类似，使用mAP测量性能。</span></li><li>Our representation achieves the state of art on the <font color=forestgreen>VOC</font> action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes.<span style="font-size:80%;opacity:0.8">即使不使用提供的边界框，我们的表示也取得了VOC动作分类任务的最新水平，并且当同时使用图像和边界框时，结果得到了进一步改善。</span></li><li>Table 12: Comparison with the state of the art in single-image action classification on <font color=forestgreen>VOC</font>-2012.<span style="font-size:80%;opacity:0.8">表12：与VOC-2012上单个图像动作分类的最新技术进行比较。</span></li><li>v3 Adds generalisation experiments (Appendix B) on PASCAL <font color=forestgreen>VOC</font> and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li><li>Adds a comparison of the net B with a shallow net and the results on PASCAL <font color=forestgreen>VOC</font> action classification benchmark.<span style="font-size:80%;opacity:0.8">添加网络B与浅层网络的比较以及Pascal VOC动作分类基准的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> bounding<br>(20) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>It can be seen as a special case of object detection, where a single object <font color=orangered>bounding</font> box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class.<span style="font-size:80%;opacity:0.8">它可以被看作是对象检测的一种特殊情况，其中应该为前5个类中的每一个预测单个对象边界框，而不考虑该类的实际对象数量。</span></li><li>To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the <font color=orangered>bounding</font> box location instead of the class scores.<span style="font-size:80%;opacity:0.8">为了进行目标定位，我们使用非常深的ConvNet，其中最后一个完全连接的层预测边界框位置，而不是类别分数。</span></li><li>A <font color=orangered>bounding</font> box is represented by a 4-D vector storing its center coordinates, width, and height.<span style="font-size:80%;opacity:0.8">边界框由存储其中心坐标、宽度和高度的4维矢量表示。</span></li><li>There is a choice of whether the <font color=orangered>bounding</font> box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Apart from the last <font color=orangered>bounding</font> box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted <font color=orangered>bounding</font> box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li><li>The first is used for comparing different network modifications on the validation set, and considers only the <font color=orangered>bounding</font> box prediction for the ground truth class (to factor out the classification errors).<span style="font-size:80%;opacity:0.8">第一个用于比较验证集上的不同网络修改，并且仅考虑地面真值类的边界框预测(以排除分类误差)。</span></li><li>The <font color=orangered>bounding</font> box is obtained by applying the network only to the central crop of the image.<span style="font-size:80%;opacity:0.8">通过将网络仅应用于图像的中心裁剪来获得边界框。</span></li><li>The difference is that instead of the class score map, the output of the last fully-connected layer is a set of <font color=orangered>bounding</font> box predictions.<span style="font-size:80%;opacity:0.8">不同之处在于，最后一个完全连接的层的输出是一组边界框预测，而不是类得分映射。</span></li><li>When several localisation ConvNets are used, we first take the union of their sets of <font color=orangered>bounding</font> box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>We did not use the multiple pooling offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the <font color=orangered>bounding</font> box predictions and can further improve the results.<span style="font-size:80%;opacity:0.8">我们没有使用Sermanet等人(2014)的多池补偿技术，它提高了边界框预测的空间分辨率，并可以进一步改进结果。</span></li><li>The localisation error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the <font color=orangered>bounding</font> box prediction is deemed correct if its intersection over union ratio with the ground-truth <font color=orangered>bounding</font> box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>Table 8: Localisation error for different modifications with the simplified testing protocol: the <font color=orangered>bounding</font> box is predicted from a single central image crop, and the ground-truth class is used.<span style="font-size:80%;opacity:0.8">表8：使用简化测试协议的不同修改的定位误差：从单个中心图像裁剪预测边界框，并使用实际类别。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed <font color=orangered>bounding</font> box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a <font color=orangered>bounding</font> box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>We considered two training settings: (i) computing the ConvNet features on the whole image and ignoring the provided <font color=orangered>bounding</font> box; (ii) computing the features on the whole image and on the provided <font color=orangered>bounding</font> box, and stacking them to obtain the final representation.<span style="font-size:80%;opacity:0.8">我们考虑了两个训练设置：(I)计算整个图像上的凸网特征并忽略提供的边界框；(Ii)计算整个图像和提供的边界框上的特征，并将它们堆叠以获得最终表示。</span></li><li>Our representation achieves the state of art on the VOC action classification task even without using the provided <font color=orangered>bounding</font> boxes, and the results are further improved when using both images and <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8">即使不使用提供的边界框，我们的表示也取得了VOC动作分类任务的最新水平，并且当同时使用图像和边界框时，结果得到了进一步改善。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> Krizhevsky<br>(17) </td> <td>  </td> <td> 
<ul><li>With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of <font color=forestgreen>Krizhevsky</font> et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); <font color=forestgreen>Krizhevsky</font> et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>All hidden layers are equipped with the rectification (ReLU (<font color=forestgreen>Krizhevsky</font> et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (<font color=forestgreen>Krizhevsky</font> et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Where applicable, the parameters for the LRN layer are those of (<font color=forestgreen>Krizhevsky</font> et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (<font color=forestgreen>Krizhevsky</font> et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (<font color=forestgreen>Krizhevsky</font> et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>The ConvNet training procedure generally follows <font color=forestgreen>Krizhevsky</font> et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later).<span style="font-size:80%;opacity:0.8">ConvNet训练过程通常遵循Krizhevsky等人（2012）（除了从多尺度训练图像中对输入裁剪图像进行采样外，如下文所述）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (<font color=forestgreen>Krizhevsky</font> et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li><li>To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (<font color=forestgreen>Krizhevsky</font> et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (<font color=forestgreen>Krizhevsky</font> et al. , 2012; Zeiler & Fergus, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (<font color=forestgreen>Krizhevsky</font> et al. , 2012), which is less efficient as it requires network re-computation for each crop.<span style="font-size:80%;opacity:0.8">由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。</span></li><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (<font color=forestgreen>Krizhevsky</font>, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (<font color=forestgreen>Krizhevsky</font> et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al. , 1989; <font color=forestgreen>Krizhevsky</font> et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>For instance, Girshick et al. (2014) achieve the state of the object detection results by replacing the ConvNet of <font color=forestgreen>Krizhevsky</font> et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li><li>Similar gains over a more shallow architecture of <font color=forestgreen>Krizhevsky</font> et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> Sermanet<br>(17) </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (<font color=forestgreen>Sermanet</font> et al. , 2014; Howard, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li><li>In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (<font color=forestgreen>Sermanet</font> et al. , 2014)).<span style="font-size:80%;opacity:0.8">尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>Then, the network is applied densely over the rescaled test image in a way similar to (<font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; <font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>For this we adopt the approach of <font color=forestgreen>Sermanet</font> et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a fewmodifications.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (<font color=forestgreen>Sermanet</font> et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>We explored both fine-tuning all layers and fine-tuning only the first two fully-connected layers, as done in (<font color=forestgreen>Sermanet</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">我们探索了微调所有层和仅微调前两个完全连接的层，如(Sermanet等人，2014年)。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of <font color=forestgreen>Sermanet</font> et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>We did not use the multiple pooling offsets technique of <font color=forestgreen>Sermanet</font> et al. (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results.<span style="font-size:80%;opacity:0.8">我们没有使用Sermanet等人(2014)的多池补偿技术，它提高了边界框预测的空间分辨率，并可以进一步改进结果。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of <font color=forestgreen>Sermanet</font> et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>We also note that fine-tuning all layers for the localisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (<font color=forestgreen>Sermanet</font> et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of <font color=forestgreen>Sermanet</font> et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (<font color=forestgreen>Sermanet</font> et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> Caltech<br>(15) </td> <td> ['kæltek] </td> <td> 
<ul><li>Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, <font color=orangered>Caltech</font>-101, and <font color=orangered>Caltech</font>-256.<span style="font-size:80%;opacity:0.8">表11：与VOC-2007、VOC-2012、CALTECH-101和CALTECH-256上图像分类的最新水平进行比较。</span></li><li>Image Classification on <font color=orangered>Caltech</font>-101 and <font color=orangered>Caltech</font>-256.<span style="font-size:80%;opacity:0.8">基于CALTECH-101和CALTECH-256的图像分类。</span></li><li>In this section we evaluate very deep features on <font color=orangered>Caltech</font>-101 (Fei-Fei et al. , 2004) and <font color=orangered>Caltech</font>-256 (Griffin et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li><li><font color=orangered>Caltech</font>-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while <font color=orangered>Caltech</font>-256 is larger with 31K images and 257 classes.<span style="font-size:80%;opacity:0.8">Caltech-101包含标记为102个类别(101个对象类别和一个背景类别)的9K图像，而Caltech-256更大，具有31K图像和257个类别。</span></li><li>Following Chatfield et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on <font color=orangered>Caltech</font>-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li><li>On <font color=orangered>Caltech</font>-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们还生成了3个拆分，每个拆分包含每个类的60个训练图像(其余用于测试)。</span></li><li>We found that unlike VOC, on <font color=orangered>Caltech</font> datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>This can be explained by the fact that in <font color=orangered>Caltech</font> images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li><li>On <font color=orangered>Caltech</font>-101, our representations are competitive with the approach of He et al. (2014), which, however, performs significantly worse than our nets on VOC-2007.<span style="font-size:80%;opacity:0.8">在CALTECH-101上，我们的陈述与He等人(2014)的方法具有竞争力，然而，与我们在VOC-2007上的网络相比，后者的表现要差得多。</span></li><li>On <font color=orangered>Caltech</font>-256, our features outperform the state of the art (Chatfield et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li><li>v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and <font color=orangered>Caltech</font> image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> ConvNets<br>(14) </td> <td>  </td> <td> 
<ul><li>With <font color=forestgreen>ConvNets</font> becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li><li>During training, the input to our <font color=forestgreen>ConvNets</font> is a fixed-size 224 × 224 RGB image.<span style="font-size:80%;opacity:0.8">在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。</span></li><li>Goodfellow et al. (2014) applied deep <font color=forestgreen>ConvNets</font> (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance.<span style="font-size:80%;opacity:0.8">Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep <font color=forestgreen>ConvNets</font>(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>As can be seen from Table 7, our very deep <font color=forestgreen>ConvNets</font> significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Training of localisation <font color=forestgreen>ConvNets</font> is similar to that of the classification <font color=forestgreen>ConvNets</font>(Sect. 3.1).<span style="font-size:80%;opacity:0.8">定位网络ConvNets的训练类似于分类ConvNets(Sect. 3.1)。</span></li><li>When several localisation <font color=forestgreen>ConvNets</font> are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union.<span style="font-size:80%;opacity:0.8">当使用几个本地化ConvNet时，我们首先获取它们的边界框预测集的并集，然后对该并集运行合并过程。</span></li><li>This indicates the performance advancement brought by our very deep <font color=forestgreen>ConvNets</font> – we got better results with a simpler localisation method, but a more powerful representation.<span style="font-size:80%;opacity:0.8">这表明我们非常深入的ConvNets带来的性能提升-我们使用更简单的定位方法获得了更好的结果，但更强大的表示。</span></li><li>In the previous sections we have discussed training and evaluation of very deep <font color=forestgreen>ConvNets</font> on the ILSVRC dataset.<span style="font-size:80%;opacity:0.8">在前面的部分中，我们讨论了ILSVRC数据集上非常深的ConvNet的训练和评估。</span></li><li>In this section, we evaluate our <font color=forestgreen>ConvNets</font>, pre-trained on ILSVRC, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li><li>To utilise the <font color=forestgreen>ConvNets</font>, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Results marked with * were achieved using <font color=forestgreen>ConvNets</font> pre-trained on the extended ILSVRC dataset (2000 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(2000个类)上预训练的ConvNets获得标有*的结果。</span></li><li>Results marked with * were achieved using <font color=forestgreen>ConvNets</font> pre-trained on the extended ILSVRC dataset (1512 classes).<span style="font-size:80%;opacity:0.8">使用在扩展ILSVRC数据集(1512个类)上预训练的ConvNets获得标有*的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> outperform<br>(13) </td> <td> [ˌaʊtpəˈfɔ:m] </td> <td> 
<ul><li>As will be shown in Sect. 4.5, our model is <font color=orangered>outperforming</font> that of Szegedy et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters <font color=orangered>outperforms</font> a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li><li>As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination <font color=orangered>outperforms</font> each of them.<span style="font-size:80%;opacity:0.8">可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。</span></li><li>As can be seen from Table 7, our very deep ConvNets significantly <font color=orangered>outperform</font> the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions.<span style="font-size:80%;opacity:0.8">从表7可以看出，我们非常深的ConvNets显著优于前一代模型，在ILSVRC-2012和ILSVRC-2013竞赛中取得了最好的结果。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially <font color=orangered>outperforms</font> the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>In terms of the single-net performance, our architecture achieves the best result (7.0% test error), <font color=orangered>outperforming</font> a single GoogLeNet by 0.9%.<span style="font-size:80%;opacity:0.8">在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。</span></li><li>In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or <font color=orangered>outperforming</font> more complex recognition pipelines built around less deep image representations.<span style="font-size:80%;opacity:0.8">在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) <font color=orangered>outperforms</font> the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was <font color=orangered>outperformed</font> by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have <font color=orangered>outperformed</font> hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, <font color=orangered>outperforming</font> the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>On Caltech-256, our features <font color=orangered>outperform</font> the state of the art (Chatfield et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li><li>Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently <font color=orangered>outperforming</font> more shallow representations.<span style="font-size:80%;opacity:0.8">自从我们的模型公开发布以来，研究界一直在积极地使用它们来完成广泛的图像识别任务，始终优于更浅的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> submission<br>(12) </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>These findings were the basis of our ImageNet Challenge 2014 <font color=orangered>submission</font>, where our team secured the first and the second places in the localisation and classification tracks respectively.<span style="font-size:80%;opacity:0.8">这些发现是我们的ImageNet Challenge 2014提交的基础，我们的团队在定位和分类过程中分别获得了第一名和第二名。</span></li><li>For instance, the best-performing <font color=orangered>submissions</font> to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>It is worth noting that after the paper <font color=orangered>submission</font> we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC <font color=orangered>submissions</font> in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>By the time of ILSVRC <font color=orangered>submission</font> we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>After the <font color=orangered>submission</font>, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>After the <font color=orangered>submission</font>, we decreased the error rate to 6.8% using an ensemble of 2 models.<span style="font-size:80%;opacity:0.8">提交后，我们使用2个模型的组合将错误率降低到6.8％。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning <font color=orangered>submission</font> Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>This is remarkable, considering that our best result is achieved by combining just two models —— significantly less than used in most ILSVRC <font color=orangered>submissions</font>.<span style="font-size:80%;opacity:0.8">这是非常显著的，考虑到我们最好的结果是仅通过组合两个模型实现的——明显少于大多数ILSVRC提交。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 <font color=orangered>submission</font>).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>Presents the experiments carried out before the ILSVRC <font color=orangered>submission</font>.<span style="font-size:80%;opacity:0.8">介绍在ILSVRC提交之前进行的实验。</span></li><li>v4 The paper is converted to ICLR-2015 <font color=orangered>submission</font> format.<span style="font-size:80%;opacity:0.8">V4将论文转换为ICLR-2015提交格式。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> receptive<br>(11) </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller <font color=orangered>receptive</font> window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>The image is passed through a stack of convolutional (conv. ) layers, where we use filters with a very small <font color=orangered>receptive</font> field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center).<span style="font-size:80%;opacity:0.8">图像通过一堆卷积（conv.）层，我们使用感受野很小的滤波器：3×3（这是捕获左/右，上/下，中心概念的最小尺寸）。</span></li><li>In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and <font color=orangered>receptive</font> fields (144M weights in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">尽管深度很大，我们的网络中权重数量并不大于具有更大卷积层宽度和感受野的较浅网络中的权重数量（144M的权重在（Sermanet等人，2014）中）。</span></li><li>Rather than using relatively large <font color=orangered>receptive</font> fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 <font color=orangered>receptive</font> fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective <font color=orangered>receptive</font> field of 5 × 5; three such layers have a 7 × 7 effective <font color=orangered>receptive</font> field.<span style="font-size:80%;opacity:0.8">很容易看到两个3×3卷积层堆叠（没有空间池化）有5×5的有效感受野；三个这样的层具有7×7的有效感受野。</span></li><li>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the <font color=orangered>receptive</font> fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network <font color=orangered>receptive</font> field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial <font color=orangered>receptive</font> fields (D is better than C).<span style="font-size:80%;opacity:0.8">这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。</span></li><li>layer (which has the same <font color=orangered>receptive</font> field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.<span style="font-size:80%;opacity:0.8">测量的浅层网络top-1错误率比网络B的top-1错误率（在中心裁剪图像上）高7％，这证实了具有小滤波器的深层网络优于具有较大滤波器的浅层网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> descriptor<br>(10) </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>The resulting image <font color=orangered>descriptor</font> is L2-normalised and combined with a linear SVM classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li><li>We then perform global average pooling on the resulting feature map, which produces a 4096-D image <font color=orangered>descriptor</font>.<span style="font-size:80%;opacity:0.8">然后，我们对生成的特征映射执行全局平均池，这将生成4096-D图像描述符。</span></li><li>The <font color=orangered>descriptor</font> is then averaged with the <font color=orangered>descriptor</font> of a horizontally flipped image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li><li>Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased <font color=orangered>descriptor</font> dimensionality.<span style="font-size:80%;opacity:0.8">堆叠允许随后的分类器学习如何在一定范围内最佳地组合图像统计数据；然而，这是以增加的描述符维数为代价的。</span></li><li>We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image <font color=orangered>descriptors</font>.<span style="font-size:80%;opacity:0.8">我们还评估了使用两个网络计算的特征的后期融合，这是通过堆叠它们各自的图像描述符来执行的。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image <font color=orangered>descriptors</font>, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>Since averaging has a benefit of not inflating the <font color=orangered>descriptor</font> dimensionality, we were able to aggregated image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li><li>Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to aggregated image <font color=orangered>descriptors</font> over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li><li>We found that unlike VOC, on Caltech datasets the stacking of <font color=orangered>descriptors</font>, computed over multiple scales, performs better than averaging or max-pooling.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> best-performing<br>(9) </td> <td>  </td> <td> 
<ul><li>We have made our two <font color=forestgreen>best-performing</font> ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.<span style="font-size:80%;opacity:0.8">我们使我们的两个性能最好的ConvNet模型可公开获得，以便进一步研究计算机视觉中深度视觉表示的使用。</span></li><li>For instance, the <font color=forestgreen>best-performing</font> submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>We have released our two <font color=forestgreen>best-performing</font> models to facilitate further research.<span style="font-size:80%;opacity:0.8">我们发布了两款表现最好的模型1，以便进一步研究。</span></li><li>After the submission, we considered an ensemble of only two <font color=forestgreen>best-performing</font> multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>For reference, our <font color=forestgreen>best-performing</font> single model achieves 7.1% error (model E, Table 5).<span style="font-size:80%;opacity:0.8">作为参考，我们表现最佳的单模型达到7.1％的误差（模型E，表5）。</span></li><li>Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the <font color=forestgreen>best-performing</font> in the classification task (Sect. 4).<span style="font-size:80%;opacity:0.8">除了最后一个边界框预测层，我们使用ConvNet体系结构D(表1)，它包含16个权重层，并且被发现在分类任务中表现最好(Sect. 4)。</span></li><li>In this section we first determine the <font color=forestgreen>best-performing</font> localisation setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our <font color=forestgreen>best-performing</font> classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>We also evaluated our <font color=forestgreen>best-performing</font> image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> fine-tune<br>(9) </td> <td> [faɪn tju:n] </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without <font color=orangered>fine-tuning</font>).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>For speed reasons, we trained multi-scale models by <font color=orangered>fine-tuning</font> all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384.<span style="font-size:80%;opacity:0.8">为了速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调，训练了多尺度模型，并用固定的S = 384进行预训练。</span></li><li>By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by <font color=orangered>fine-tuning</font> only the fully-connected layers rather than all layers).<span style="font-size:80%;opacity:0.8">在ILSVRC提交的时候，我们只训练了单规模网络，以及一个多尺度模型D（仅在全连接层进行微调而不是所有层）。</span></li><li>We explored both <font color=orangered>fine-tuning</font> all layers and <font color=orangered>fine-tuning</font> only the first two fully-connected layers, as done in (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们探索了微调所有层和仅微调前两个完全连接的层，如(Sermanet等人，2014年)。</span></li><li>We also note that <font color=orangered>fine-tuning</font> all layers for the localisation task leads to noticeably better results than <font color=orangered>fine-tuning</font> only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li><li>Having determined the best localisation setting (PCR, <font color=orangered>fine-tuning</font> of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li>For simplicity, pre-trained ConvNet weights are kept fixed (no <font color=orangered>fine-tuning</font> is performed).<span style="font-size:80%;opacity:0.8">为简单起见，预先训练的ConvNet权重保持固定(不执行微调)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> jittering<br>(9) </td> <td> [ˈdʒitərɪŋ] </td> <td> 
<ul><li>This can also be seen as training set augmentation by scale <font color=orangered>jittering</font>, where a single model is trained to recognise objects over a wide range of scales.<span style="font-size:80%;opacity:0.8">这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。</span></li><li>Finally, scale <font color=orangered>jittering</font> at training time ($S \in [256; 512]$) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time.<span style="font-size:80%;opacity:0.8">最后，训练时的尺度抖动（$S \in [256; 512]$）得到了与固定最小边（S = 256或S = 384）的图像训练相比更好的结果，即使在测试时使用单尺度。</span></li><li>This confirms that training set augmentation by scale <font color=orangered>jittering</font> is indeed helpful for capturing multi-scale image statistics.<span style="font-size:80%;opacity:0.8">这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</span></li><li>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale <font color=orangered>jittering</font> at test time.<span style="font-size:80%;opacity:0.8">在单尺度上评估ConvNet模型后，我们现在评估测试时尺度抖动的影响。</span></li><li>At the same time, scale <font color=orangered>jittering</font> at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable $ S \in [S_{min}; S_{max}] $ was evaluated over a larger range of sizes $ Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \} $.<span style="font-size:80%;opacity:0.8">同时，训练时的尺度抖动允许网络在测试时应用于更广的尺度范围，所以用变量$S \in [S_{min}; S_{max}]$训练的模型在更大的尺寸范围$ Q = \{S_{min}, 0.5(S_{min} + S_{max}), S_{max} \} $上进行评估。</span></li><li>The results, presented in Table 4, indicate that scale <font color=orangered>jittering</font> at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3).<span style="font-size:80%;opacity:0.8">表4中给出的结果表明，测试时的尺度抖动导致了更好的性能（与在单一尺度上相同模型的评估相比，如表3所示）。</span></li><li>As before, the deepest configurations (D and E) perform the best, and scale <font color=orangered>jittering</font> is better than training with a fixed smallest side S.<span style="font-size:80%;opacity:0.8">如前所述，最深的配置（D和E）执行最佳，并且尺度抖动优于使用固定最小边S的训练。</span></li><li>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale <font color=orangered>jittering</font> for our ILSVRC-2014 submission).<span style="font-size:80%;opacity:0.8">我们训练了两个定位模型，每个模型都在单个规模上：S=256和S=384(由于时间限制，我们没有在ILSVRC-2014提交中使用训练规模抖动)。</span></li><li>v2 Adds post-submission ILSVRC experiments with training set augmentation using scale <font color=orangered>jittering</font>, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> Zeiler<br>(7) </td> <td>  </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; <font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>Recently, there has been a lot of interest in such a use case (<font color=forestgreen>Zeiler</font> & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Following Chatfield et al. (2014); <font color=forestgreen>Zeiler</font> & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> Fergus<br>(7) </td> <td> ['fә:^әs] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al. , 2012; Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014)) and S = 384.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已经在现有技术中广泛使用（Krizhevsky等人，2012；Zeiler＆Fergus，2013；Sermanet等，2014））和S = 384。</span></li><li>This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & <font color=orangered>Fergus</font>, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & <font color=orangered>Fergus</font>, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Following Chatfield et al. (2014); Zeiler & <font color=orangered>Fergus</font> (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> initialise<br>(7) </td> <td> [ɪ'nɪʃəlaɪz] </td> <td> 
<ul><li>Then, when training deeper architectures, we <font color=orangered>initialised</font> the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were <font color=orangered>initialised</font> randomly).<span style="font-size:80%;opacity:0.8">然后，当训练更深的架构时，我们用网络A的层初始化前四个卷积层和最后三个全连接层（中间层被随机初始化）。</span></li><li>The biases were <font color=orangered>initialised</font> with zero.<span style="font-size:80%;opacity:0.8">偏置初始化为零。</span></li><li>It is worth noting that after the paper submission we found that it is possible to <font color=orangered>initialise</font> the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li><li>To speed-up training of the S = 384 network, it was <font color=orangered>initialised</font> with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}$.<span style="font-size:80%;opacity:0.8">为了加速S = 384网络的训练，用S = 256预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</span></li><li>Training was <font color=orangered>initialised</font> with the corresponding classification models (trained on the same scales), and the initial learning rate was set to $10^−3$.<span style="font-size:80%;opacity:0.8">使用相应的分类模型(在相同的规模上训练)初始化训练，并将初始学习率设置为$10^−3$。</span></li><li>The last fully-connected layer was <font color=orangered>initialised</font> randomly and trained from scratch.<span style="font-size:80%;opacity:0.8">最后一个完全连接的层是随机初始化的，并从头开始训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> rescale<br>(7) </td> <td> [ri:'skeɪl] </td> <td> 
<ul><li>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from <font color=orangered>rescaled</font> training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Training image <font color=orangered>rescaling</font> is explained below.<span style="font-size:80%;opacity:0.8">下面解释训练图像归一化。</span></li><li>The second approach to setting S is multi-scale training, where each training image is individually <font color=orangered>rescaled</font> by randomly sampling S from a certain range $[S_{min},S_{max}]$ (we used $S_{min} = 256$ and $S_{max} = 512$).<span style="font-size:80%;opacity:0.8">设置S的第二种方法是多尺度训练，其中每个训练图像通过从一定范围$[S_{min}，S_{max}]$（我们使用$S_{min} = 256$ 和 $S_{max} = 512$）随机采样S来单独进行归一化。</span></li><li>First, it is isotropically <font color=orangered>rescaled</font> to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale).<span style="font-size:80%;opacity:0.8">首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。</span></li><li>Then, the network is applied densely over the <font color=orangered>rescaled</font> test image in a way similar to (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>It consists of running a model over several <font color=orangered>rescaled</font> versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors.<span style="font-size:80%;opacity:0.8">它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。</span></li><li>Namely, an image is first <font color=orangered>rescaled</font> so that its smallest side equals Q, and then the network is densely applied over the image plane (which is possible when all weight layers are treated as convolutional).<span style="font-size:80%;opacity:0.8">即，首先重新缩放图像，使得其最小侧等于Q，然后在图像平面上密集地应用网络(当所有权重层都被视为卷积时，这是可能的)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> VGG<br>(7) </td> <td>  </td> <td> 
<ul><li>Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “<font color=forestgreen>VGG</font>” team entry to the ILSVRC-2014 competition (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>In the classification task of ILSVRC-2014 challenge (Russakovsky et al. , 2014), our “<font color=forestgreen>VGG</font>” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>Our method is denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的方法表示为“VGG”。</span></li><li>With 25.3% test error, our “<font color=forestgreen>VGG</font>” team won the localisation challenge of ILSVRC-2014 (Russakovsky et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li><li>Our method is denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的方法被称为“VGG”。</span></li><li>Our models are denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的模型被称为“VGG”。</span></li><li>Our models are denoted as “<font color=forestgreen>VGG</font>”.<span style="font-size:80%;opacity:0.8">我们的模型被称为“VGG”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> utilise<br>(6) </td> <td> ['ju:tɪlaɪz] </td> <td> 
<ul><li>For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014) <font color=orangered>utilised</font> smaller receptive window size and smaller stride of the first convolutional layer.<span style="font-size:80%;opacity:0.8">例如，ILSVRC-2013（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的提交使用了更小的感受窗口尺寸和更小的第一卷积层步长。</span></li><li>In one of the configurations we also <font color=orangered>utilise</font> 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>It should be noted that 1 × 1 conv. layers have recently been <font color=orangered>utilised</font> in the “Network in Network” architecture of Lin et al. (2014).<span style="font-size:80%;opacity:0.8">应该注意的是1×1卷积层最近在Lin等人(2014)的“Network in Network”架构中已经得到了使用。</span></li><li>To come up with the final prediction, we <font color=orangered>utilise</font> the greedy merging procedure of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li><li>Following that line of work, we investigate if our models lead to better performance than more shallow models <font color=orangered>utilised</font> in the state-of-the-artmethods.<span style="font-size:80%;opacity:0.8">遵循这一工作路线，我们研究我们的模型是否比现有技术中使用的更浅的模型具有更好的性能。</span></li><li>To <font color=orangered>utilise</font> the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> non-linearity<br>(6) </td> <td> ['nɒnlaɪn'ərɪtɪ] </td> <td> 
<ul><li>In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by <font color=orangered>non-linearity</font>).<span style="font-size:80%;opacity:0.8">在其中一种配置中，我们还使用了1×1卷积滤波器，可以看作输入通道的线性变换（后面是非线性）。</span></li><li>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al. , 2012)) <font color=orangered>non-linearity</font>.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with <font color=orangered>non-linearity</font> injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li><li>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the <font color=orangered>non-linearity</font> of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li><li>layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional <font color=orangered>non-linearity</font> is introduced by the rectification function.<span style="font-size:80%;opacity:0.8">即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。</span></li><li>layers throughout the network. This indicates that while the additional <font color=orangered>non-linearity</font> does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C).<span style="font-size:80%;opacity:0.8">这表明，虽然额外的非线性确实有帮助（C优于B），但也可以通过使用具有非平凡感受野（D比C好）的卷积滤波器来捕获空间上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> normalisation<br>(6) </td> <td> [,nɔ:məlai'zeiʃən] </td> <td> 
<ul><li>We note that none of our networks (except for one) contain Local Response Normalisation (LRN) <font color=orangered>normalisation</font> (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such <font color=orangered>normalisation</font> does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>First, we note that using local response <font color=orangered>normalisation</font> (A-LRN network) does not improve on the model A without any <font color=orangered>normalisation</font> layers.<span style="font-size:80%;opacity:0.8">首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。</span></li><li>We thus do not employ <font color=orangered>normalisation</font> in the deeper architectures (B–E).<span style="font-size:80%;opacity:0.8">因此，我们在较深的架构（B-E）中不采用归一化。</span></li><li>We note that none of our networks (except for one) contain Local Response <font color=orangered>Normalisation</font> (LRN) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> max-pooling<br>(5) </td> <td>  </td> <td> 
<ul><li>layers. Spatial pooling is carried out by five <font color=forestgreen>max-pooling</font> layers, which follow some of the conv. layers (not all the conv.<span style="font-size:80%;opacity:0.8">空间池化由五个最大池化层进行，这些层在一些卷积层之后（不是所有的卷积层之后都是最大池化）。</span></li><li>layers are followed by <font color=forestgreen>max-pooling</font>). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.<span style="font-size:80%;opacity:0.8">在2×2像素窗口上进行最大池化，步长为2。</span></li><li>and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each <font color=forestgreen>max-pooling</font> layer, until it reaches 512.<span style="font-size:80%;opacity:0.8">卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</span></li><li>We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or <font color=forestgreen>max-pooling</font>.<span style="font-size:80%;opacity:0.8">我们发现，与VOC不同的是，在Caltech数据集上，通过多个比例计算的描述符堆叠比平均或最大池化(max-pooling)性能更好。</span></li><li>layers are followed by max-pooling). <font color=forestgreen>Max-pooling</font> is performed over a 2 × 2 pixel window, with stride 2.<span style="font-size:80%;opacity:0.8">在2×2像素窗口上进行最大池化，步长为2。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> FC<br>(5) </td> <td>  </td> <td> 
<ul><li>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (<font color=forestgreen>FC</font>) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class).<span style="font-size:80%;opacity:0.8">一堆卷积层（在不同架构中具有不同深度）之后是三个全连接（FC）层：前两个每个都有4096个通道，第三个执行1000维ILSVRC分类，因此包含1000个通道（一个通道对应一个类别）。</span></li><li>All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 <font color=forestgreen>FC</font> layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li><li>and 3 <font color=forestgreen>FC</font> layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.<span style="font-size:80%;opacity:0.8">卷积层的宽度（通道数）相当小，从第一层中的64开始，然后在每个最大池化层之后增加2倍，直到达到512。</span></li><li>Namely, the fully-connected layers are first converted to convolutional layers (the first <font color=forestgreen>FC</font> layer to a 7 × 7 conv. layer, the last two <font color=forestgreen>FC</font> layers to 1 × 1 conv.<span style="font-size:80%;opacity:0.8">即，全连接层首先被转换成卷积层（第一FC层转换到7×7卷积层，最后两个FC层转换到1×1卷积层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> initialisation<br>(5) </td> <td> [ɪnɪʃəlaɪ'zeɪʃən] </td> <td> 
<ul><li>The <font color=orangered>initialisation</font> of the network weights is important, since bad <font color=orangered>initialisation</font> can stall learning due to the instability of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li><li>To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random <font color=orangered>initialisation</font>.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li><li>For random <font color=orangered>initialisation</font> (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random <font color=orangered>initialisation</font> procedure of Glorot & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> flip<br>(5) </td> <td> [flɪp] </td> <td> 
<ul><li>To further augment the training set, the crops underwent random horizontal <font color=orangered>flipping</font> and random RGB colour shift (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>We also augment the test set by horizontal <font color=orangered>flipping</font> of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and <font color=orangered>flipped</font> images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 <font color=orangered>flips</font>), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li><li>The descriptor is then averaged with the descriptor of a horizontally <font color=orangered>flipped</font> image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> substantially<br>(5) </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which <font color=orangered>substantially</font> increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and <font color=orangered>substantially</font> outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by <font color=orangered>substantially</font> increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with <font color=orangered>substantially</font> increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li><li>As can be seen from Table 9, application of the localisation ConvNet to the whole image <font color=orangered>substantially</font> improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth.<span style="font-size:80%;opacity:0.8">从表9可以看出，与使用中心裁剪(表8)相比，将本地化ConvNet应用于整个图像显著改善了结果，尽管使用了前5个预测的类别标签而不是真实值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> PCR<br>(5) </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is class-specific (per-class regression, <font color=forestgreen>PCR</font>).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (<font color=forestgreen>PCR</font>) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where <font color=forestgreen>PCR</font> was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (<font color=forestgreen>PCR</font>).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li><li>Having determined the best localisation setting (<font color=forestgreen>PCR</font>, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> soft-max<br>(4) </td> <td>  </td> <td> 
<ul><li>The final layer is the <font color=forestgreen>soft-max</font> layer.<span style="font-size:80%;opacity:0.8">最后一层是soft-max层。</span></li><li>We also augment the test set by horizontal flipping of the images; the <font color=forestgreen>soft-max</font> class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>We also assess the complementarity of the two evaluation techniques by averaging their <font color=forestgreen>soft-max</font> outputs.<span style="font-size:80%;opacity:0.8">我们还通过平均其soft-max输出来评估两种评估技术的互补性。</span></li><li>In this part of the experiments, we combine the outputs of several models by averaging their <font color=forestgreen>soft-max</font> class posteriors.<span style="font-size:80%;opacity:0.8">在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> Szegedy<br>(4) </td> <td>  </td> <td> 
<ul><li>GoogLeNet (<font color=forestgreen>Szegedy</font> et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>As will be shown in Sect. 4.5, our model is outperforming that of <font color=forestgreen>Szegedy</font> et al. (2014) in terms of the single-network classification accuracy.<span style="font-size:80%;opacity:0.8">正如将在第4.5节显示的那样，我们的模型在单网络分类精度方面胜过Szegedy等人（2014）。</span></li><li>At the same time, using a large set of crops, as done by <font color=forestgreen>Szegedy</font> et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net.<span style="font-size:80%;opacity:0.8">同时，如Szegedy等人（2014）所做的那样，使用大量的裁剪图像可以提高准确度，因为与全卷积网络相比，它使输入图像的采样更精细。</span></li><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by <font color=forestgreen>Szegedy</font> et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> Russakovsky<br>(4) </td> <td>  </td> <td> 
<ul><li>Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (<font color=forestgreen>Russakovsky</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">在测试集上也进行了一些实验，并将其作为ILSVRC-2014竞赛（Russakovsky等，2014）“VGG”小组的输入提交到了官方的ILSVRC服务器。</span></li><li>In the classification task of ILSVRC-2014 challenge (<font color=forestgreen>Russakovsky</font> et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>The localisation error is measured according to the ILSVRC criterion (<font color=forestgreen>Russakovsky</font> et al. , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li><li>With 25.3% test error, our “VGG” team won the localisation challenge of ILSVRC-2014 (<font color=forestgreen>Russakovsky</font> et al. , 2014).<span style="font-size:80%;opacity:0.8">以25.3%的测试误差，我们的“VGG”团队赢得了ILSVRC-2014(Russakovsky等，2014)的本地化挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> fusion<br>(4) </td> <td> [ˈfju:ʒn] </td> <td> 
<ul><li>4.4 CONVNET <font color=orangered>FUSION</font><span style="font-size:80%;opacity:0.8">4.4 卷积网络融合</span></li><li>Table 6: Multiple ConvNet <font color=orangered>fusion</font> results.<span style="font-size:80%;opacity:0.8">表6：多个卷积网络融合结果</span></li><li>We also assess late <font color=orangered>fusion</font> of features, computed using two networks, which is performed by stacking their respective image descriptors.<span style="font-size:80%;opacity:0.8">我们还评估了使用两个网络计算的特征的后期融合，这是通过堆叠它们各自的图像描述符来执行的。</span></li><li>It also benefits from the <font color=orangered>fusion</font> with an object detection-assisted classification pipeline.<span style="font-size:80%;opacity:0.8">它还受益于与对象检测辅助分类流水线的融合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> ensemble<br>(4) </td> <td> [ɒnˈsɒmbl] </td> <td> 
<ul><li>The resulting <font color=orangered>ensemble</font> of 7 networks has 7.3% ILSVRC test error.<span style="font-size:80%;opacity:0.8">由此产生的7个网络组合具有7.3％的ILSVRC测试误差。</span></li><li>After the submission, we considered an <font color=orangered>ensemble</font> of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation.<span style="font-size:80%;opacity:0.8">在提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的组合，它使用密集评估将测试误差降低到7.0％，使用密集评估和多裁剪图像评估将测试误差降低到6.8％。</span></li><li>In the classification task of ILSVRC-2014 challenge (Russakovsky et al. , 2014), our “VGG” team secured the 2nd place with 7.3% test error using an <font color=orangered>ensemble</font> of 7 models.<span style="font-size:80%;opacity:0.8">在ILSVRC-2014挑战的分类任务（Russakovsky等，2014）中，我们的“VGG”团队获得了第二名，使用7个模型的组合取得了7.3％测试误差。</span></li><li>After the submission, we decreased the error rate to 6.8% using an <font color=orangered>ensemble</font> of 2 models.<span style="font-size:80%;opacity:0.8">提交后，我们使用2个模型的组合将错误率降低到6.8％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> SCR<br>(4) </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, <font color=forestgreen>SCR</font> (Sermanet et al. , 2014)) or is class-specific (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (<font color=forestgreen>SCR</font>), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by <font color=forestgreen>SCR</font>.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li><li>All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (<font color=forestgreen>SCR</font>) or per-class regression (PCR).<span style="font-size:80%;opacity:0.8">所有ConvNet层(最后一层除外)都使用配置D(表1)，而最后一层执行单类回归(SCR)或逐类回归(PCR)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> fully-fledged<br>(4) </td> <td> ['fʊli:fl'edʒd] </td> <td> 
<ul><li>The second, <font color=orangered>fully-fledged</font>, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. 3.2).<span style="font-size:80%;opacity:0.8">第二个全面的测试程序基于定位网络ConvNet对整个图像的密集应用，类似于分类任务(3.2节)。</span></li><li>In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a <font color=orangered>fully-fledged</font> scenario (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the <font color=orangered>fully-fledged</font> scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li><li><font color=orangered>Fully-fledged</font> evaluation.<span style="font-size:80%;opacity:0.8">全面评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> Chatfield<br>(4) </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; <font color=forestgreen>Chatfield</font> et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li><li>Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperforming the previous best result of <font color=forestgreen>Chatfield</font> et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li><li>Following <font color=forestgreen>Chatfield</font> et al. (2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into training and test data, so that each split contains 30 training images per class, and up to 50 test images per class.<span style="font-size:80%;opacity:0.8">遵循Chatfield等人(2014)；Zeiler&Fergus(2013)；He等人(2014)，在Caltech-101上，我们将3个随机分割生成到训练和测试数据中，因此每个分割包含每个类30个训练图像，每个类最多包含50个测试图像。</span></li><li>On Caltech-256, our features outperform the state of the art (<font color=forestgreen>Chatfield</font> et al. , 2014) by a large margin (8.6%).<span style="font-size:80%;opacity:0.8">在Caltech-256上，我们的功能远远超过最先进的技术(Chatfield等人，2014)(8.6%)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> pascal<br>(4) </td> <td> ['pæskәl] </td> <td> 
<ul><li>We begin with the evaluation on the image classification task of <font color=orangered>PASCAL</font> VOC-2007 and VOC-2012 benchmarks (Everingham et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the <font color=orangered>PASCAL</font> VOC-2012 action classification task (Everingham et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li><li>v3 Adds generalisation experiments (Appendix B) on <font color=orangered>PASCAL</font> VOC and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li><li>Adds a comparison of the net B with a shallow net and the results on <font color=orangered>PASCAL</font> VOC action classification benchmark.<span style="font-size:80%;opacity:0.8">添加网络B与浅层网络的比较以及Pascal VOC动作分类基准的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> ICLR<br>(3) </td> <td>  </td> <td> 
<ul><li>Source: 3rd International Conference on Learning Representations, <font color=forestgreen>ICLR</font> 2015 - Conference Track Proceedings<span style="font-size:80%;opacity:0.8"></span></li><li>v4 The paper is converted to <font color=forestgreen>ICLR</font>-2015 submission format.<span style="font-size:80%;opacity:0.8">V4将论文转换为ICLR-2015提交格式。</span></li><li>v6 Camera-ready <font color=forestgreen>ICLR</font>-2015 conference paper.<span style="font-size:80%;opacity:0.8">V6 复印就绪ICLR-2015会议论文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> generalise<br>(3) </td> <td> ['dʒenərəlaɪz] </td> <td> 
<ul><li>We also show that our representations <font color=orangered>generalise</font> well to other datasets, where they achieve state-of-the-art results.<span style="font-size:80%;opacity:0.8">我们还表明，我们的表示对于其他数据集泛化的很好，在其它数据集上取得了最好的结果。</span></li><li>In the appendix, we also show that our models <font color=orangered>generalise</font> well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations.<span style="font-size:80%;opacity:0.8">在附录中，我们还显示了我们的模型很好地泛化到各种各样的任务和数据集上，可以匹敌或超越更复杂的识别流程，其构建围绕不深的图像表示。</span></li><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, <font color=orangered>generalise</font> well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> densely<br>(3) </td> <td> [denslɪ] </td> <td> 
<ul><li>Another line of improvements dealt with training and testing the networks <font color=orangered>densely</font> over the whole image and over multiple scales (Sermanet et al. , 2014; Howard, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li><li>Then, the network is applied <font color=orangered>densely</font> over the rescaled test image in a way similar to (Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">然后，网络以类似于（Sermanet等人，2014）的方式密集地应用于归一化的测试图像上。</span></li><li>Namely, an image is first rescaled so that its smallest side equals Q, and then the network is <font color=orangered>densely</font> applied over the image plane (which is possible when all weight layers are treated as convolutional).<span style="font-size:80%;opacity:0.8">即，首先重新缩放图像，使得其最小侧等于Q，然后在图像平面上密集地应用网络(当所有权重层都被视为卷积时，这是可能的)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> applicable<br>(3) </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also <font color=orangered>applicable</font> to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>Where <font color=orangered>applicable</font>, the parameters for the LRN layer are those of (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li><li>For random initialisation (where <font color=orangered>applicable</font>), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ variance.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> generalisation<br>(3) </td> <td> [ˌdʒenərəlaɪ'zeɪʃən] </td> <td> 
<ul><li>For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the <font color=orangered>generalisation</font> of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li><li>B <font color=orangered>GENERALISATION</font> OF VERY DEEP FEATURES<span style="font-size:80%;opacity:0.8">B 非常深层特征的概括</span></li><li>v3 Adds <font color=orangered>generalisation</font> experiments (Appendix B) on PASCAL VOC and Caltech image classification datasets.<span style="font-size:80%;opacity:0.8">V3增加了对Pascal VOC和Caltech图像分类数据集的泛化实验(附录B)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> revision<br>(3) </td> <td> [rɪˈvɪʒn] </td> <td> 
<ul><li>Finally, Appendix C contains the list of major paper <font color=orangered>revisions</font>.<span style="font-size:80%;opacity:0.8">最后，附录C包含了主要的论文修订列表。</span></li><li>C PAPER <font color=orangered>REVISIONS</font><span style="font-size:80%;opacity:0.8">C 论文修订</span></li><li>Here we present the list of major paper <font color=orangered>revisions</font>, outlining the substantial changes for the convenience of the reader.<span style="font-size:80%;opacity:0.8">为了方便读者，我们在这里列出了主要的论文修订列表，概述了重要的变化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> CONVNET<br>(3) </td> <td>  </td> <td> 
<ul><li>2 <font color=forestgreen>CONVNET</font> CONFIGURATIONS<span style="font-size:80%;opacity:0.8">2. ConvNet配置</span></li><li>4.4 <font color=forestgreen>CONVNET</font> FUSION<span style="font-size:80%;opacity:0.8">4.4 卷积网络融合</span></li><li>A.1 LOCALISATION <font color=forestgreen>CONVNET</font><span style="font-size:80%;opacity:0.8">A.1 ConvNet定位</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> fixed-size<br>(3) </td> <td>  </td> <td> 
<ul><li>During training, the input to our ConvNets is a <font color=forestgreen>fixed-size</font> 224 × 224 RGB image.<span style="font-size:80%;opacity:0.8">在训练期间，我们的ConvNet的输入是固定大小的224×224 RGB图像。</span></li><li>To obtain the <font color=forestgreen>fixed-size</font> 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li><li>Finally, to obtain a <font color=forestgreen>fixed-size</font> vector of class scores for the image, the class score map is spatially averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> rectification<br>(3) </td> <td> [ˌrektɪfɪ'keɪʃn] </td> <td> 
<ul><li>All hidden layers are equipped with the <font color=orangered>rectification</font> (ReLU (Krizhevsky et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>First, we incorporate three non-linear <font color=orangered>rectification</font> layers instead of a single one, which makes the decision function more discriminative.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li><li>layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the <font color=orangered>rectification</font> function.<span style="font-size:80%;opacity:0.8">即使在我们的案例下，1×1卷积基本上是在相同维度空间上的线性投影（输入和输出通道的数量相同），由修正函数引入附加的非线性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> incorporate<br>(3) </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>First, we <font color=orangered>incorporate</font> three non-linear rectification layers instead of a single one, which makes the decision function more discriminative.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li><li>We envisage that better localisation performance can be achieved if this technique is <font color=orangered>incorporated</font> into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li><li>Unlike other approaches, we did not <font color=orangered>incorporate</font> any task-specific heuristics, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> regularisation<br>(3) </td> <td> [,reɡjulərai'zeiʃən] </td> <td> 
<ul><li>This can be seen as imposing a <font color=orangered>regularisation</font> on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li><li>The training was regularised by weight decay (the L2 penalty multiplier set to $5 \times 10^{−4}$) and dropout <font color=orangered>regularisation</font> for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit <font color=orangered>regularisation</font> imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> GoogLeNet<br>(3) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>GoogLeNet</font> (Szegedy et al. , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li><li>Our result is also competitive with respect to the classification task winner (<font color=forestgreen>GoogLeNet</font> with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li><li>In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single <font color=forestgreen>GoogLeNet</font> by 0.9%.<span style="font-size:80%;opacity:0.8">在单网络性能方面，我们的架构取得了最好节果（7.0％测试误差），超过单个GoogLeNet 0.9％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> LeCun<br>(3) </td> <td>  </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (<font color=forestgreen>LeCun</font> et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li><li>Notably, we did not depart from the classical ConvNet architecture of <font color=forestgreen>LeCun</font> et al. (1989), but improved it by substantially increasing the depth.<span style="font-size:80%;opacity:0.8">值得注意的是，我们并没有偏离LeCun（1989）等人经典的ConvNet架构，但通过大幅增加深度改善了它。</span></li><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (<font color=forestgreen>LeCun</font> et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> augmentation<br>(3) </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>This can also be seen as training set <font color=orangered>augmentation</font> by scale jittering, where a single model is trained to recognise objects over a wide range of scales.<span style="font-size:80%;opacity:0.8">这也可以看作是通过尺度抖动进行训练集增强，其中单个模型被训练在一定尺度范围内识别对象。</span></li><li>This confirms that training set <font color=orangered>augmentation</font> by scale jittering is indeed helpful for capturing multi-scale image statistics.<span style="font-size:80%;opacity:0.8">这证实了通过尺度抖动进行的训练集增强确实有助于捕获多尺度图像统计。</span></li><li>v2 Adds post-submission ILSVRC experiments with training set <font color=orangered>augmentation</font> using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> posterior<br>(3) </td> <td> [pɒˈstɪəriə(r)] </td> <td> 
<ul><li>We also augment the test set by horizontal flipping of the images; the soft-max class <font color=orangered>posteriors</font> of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li><li>It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class <font color=orangered>posteriors</font>.<span style="font-size:80%;opacity:0.8">它包括在一张测试图像的几个归一化版本上运行模型（对应于不同的Q值），然后对所得到的类别后验进行平均。</span></li><li>In this part of the experiments, we combine the outputs of several models by averaging their soft-max class <font color=orangered>posteriors</font>.<span style="font-size:80%;opacity:0.8">在这部分实验中，我们通过对soft-max类别后验进行平均，结合了几种模型的输出。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> aggregate<br>(3) </td> <td> [ˈægrɪgət] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are <font color=orangered>aggregated</font> across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that <font color=orangered>aggregating</font> image descriptors, computed at multiple scales, by averaging performs similarly to the aggregation by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li><li>Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to <font color=orangered>aggregated</font> image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> thorough<br>(2) </td> <td> [ˈθʌrə] </td> <td> 
<ul><li>Our main contribution is a <font color=orangered>thorough</font> evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li><li>In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a <font color=orangered>thorough</font> evaluation of ConvNet architectures of different depth.<span style="font-size:80%;opacity:0.8">在论文的主体部分，我们考虑了ILSVRC挑战的分类任务，并对不同深度的ConvNet架构进行了深入的评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> SVM<br>(2) </td> <td>  </td> <td> 
<ul><li>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear <font color=forestgreen>SVM</font> without fine-tuning).<span style="font-size:80%;opacity:0.8">因此，我们提出了更为精确的ConvNet架构，不仅可以在ILSVRC分类和定位任务上取得的最佳的准确性，而且还适用于其它的图像识别数据集，它们可以获得优异的性能，即使使用相对简单流程的一部分（例如，通过线性SVM分类深度特征而不进行微调）。</span></li><li>The resulting image descriptor is L2-normalised and combined with a linear <font color=forestgreen>SVM</font> classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> Ciresan<br>(2) </td> <td>  </td> <td> 
<ul><li>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by <font color=forestgreen>Ciresan</font> et al. (2011); Krizhevsky et al. (2012).<span style="font-size:80%;opacity:0.8">为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都使用相同的规则，灵感来自Ciresan等（2011）；Krizhevsky等人（2012年）。</span></li><li>Small-size convolution filters have been previously used by <font color=forestgreen>Ciresan</font> et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> generic<br>(2) </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>In this section, we first describe a <font color=orangered>generic</font> layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2).<span style="font-size:80%;opacity:0.8">在本节中，我们首先描述我们的ConvNet配置的通用设计（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。</span></li><li>All configurations follow the <font color=orangered>generic</font> design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv.<span style="font-size:80%;opacity:0.8">所有配置都遵循2.1节提出的通用设计，并且仅是深度不同：从网络A中的11个加权层（8个卷积层和3个FC层）到网络E中的19个加权层（16个卷积层和3个FC层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> ReLU<br>(2) </td> <td>  </td> <td> 
<ul><li>All hidden layers are equipped with the rectification (<font color=forestgreen>ReLU</font> (Krizhevsky et al. , 2012)) non-linearity.<span style="font-size:80%;opacity:0.8">所有隐藏层都配备了修正（ReLU（Krizhevsky等，2012））非线性。</span></li><li>The <font color=forestgreen>ReLU</font> activation function is not shown for brevity.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> LRN<br>(2) </td> <td>  </td> <td> 
<ul><li>We note that none of our networks (except for one) contain Local Response Normalisation (<font color=forestgreen>LRN</font>) normalisation (Krizhevsky et al. , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.<span style="font-size:80%;opacity:0.8">我们注意到，我们的网络（除了一个）都不包含局部响应规范化（LRN）（Krizhevsky等，2012）：将在第4节看到，这种规范化并不能提高在ILSVRC数据集上的性能，但增加了内存消耗和计算时间。</span></li><li>Where applicable, the parameters for the <font color=forestgreen>LRN</font> layer are those of (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">在应用的地方，LRN层的参数是（Krizhevsky等，2012）的参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> bold<br>(2) </td> <td> [bəʊld] </td> <td> 
<ul><li>The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in <font color=orangered>bold</font>).<span style="font-size:80%;opacity:0.8">随着更多的层被添加，配置的深度从左（A）增加到右（E）（添加的层以粗体显示）。</span></li><li>Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in <font color=orangered>bold</font> in Table 4).<span style="font-size:80%;opacity:0.8">我们在验证集上的最佳单网络性能为24.8％/7.5％ top-1/top-5的错误率（在表4中用粗体突出显示）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> brevity<br>(2) </td> <td> [ˈbrevəti] </td> <td> 
<ul><li>The ReLU activation function is not shown for <font color=orangered>brevity</font>.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li><li>In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for <font color=orangered>brevity</font>.<span style="font-size:80%;opacity:0.8">在这些实验中，最小图像侧被设置为S=384；S=256的结果表现出相同的行为，为了简洁起见不显示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> top-performing<br>(2) </td> <td>  </td> <td> 
<ul><li>Our ConvNet configurations are quite different from the ones used in the <font color=forestgreen>top-performing</font> entries of the ILSVRC-2012 (Krizhevsky et al. , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">我们的ConvNet配置与ILSVRC-2012（Krizhevsky等，2012）和ILSVRC-2013比赛（Zeiler＆Fergus，2013；Sermanet等，2014）表现最佳的参赛提交中使用的ConvNet配置有很大不同。</span></li><li>GoogLeNet (Szegedy et al. , 2014), a <font color=forestgreen>top-performing</font> entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions).<span style="font-size:80%;opacity:0.8">GooLeNet（Szegedy等，2014），ILSVRC-2014分类任务的表现最好的项目，是独立于我们工作之外的开发的，但是类似的是它是基于非常深的ConvNets（22个权重层）和小卷积滤波器（除了3×3，它们也使用了1×1和5×5卷积）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> convolved<br>(2) </td> <td> [kənˈvɔlvd] </td> <td> 
<ul><li>Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al. , 2012), or 7 × 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet et al. , 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are <font color=orangered>convolved</font> with the input at every pixel (with stride 1).<span style="font-size:80%;opacity:0.8">不是在第一卷积层中使用相对较大的感受野（例如，在（Krizhevsky等人，2012）中的11×11，步长为4，或在（Zeiler＆Fergus，2013；Sermanet等，2014）中的7×7，步长为2），我们在整个网络使用非常小的3×3感受野，与输入的每个像素（步长为1）进行卷积。</span></li><li>Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the <font color=orangered>convolved</font> feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> momentum<br>(2) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with <font color=orangered>momentum</font>.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li><li>The batch size was set to 256, <font color=orangered>momentum</font> to 0.9.<span style="font-size:80%;opacity:0.8">批量大小设为256，动量为0.9。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> epoch<br>(2) </td> <td> [ˈi:pɒk] </td> <td> 
<ul><li>In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 <font color=orangered>epochs</font>).<span style="font-size:80%;opacity:0.8">学习率总共降低3次，学习在37万次迭代后停止（74个epochs）。</span></li><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less <font color=orangered>epochs</font> to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> augment<br>(2) </td> <td> [ɔ:gˈment] </td> <td> 
<ul><li>To further <font color=orangered>augment</font> the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al. , 2012).<span style="font-size:80%;opacity:0.8">为了进一步增强训练集，裁剪图像经过了随机水平翻转和随机RGB颜色偏移（Krizhevsky等，2012）。</span></li><li>We also <font color=orangered>augment</font> the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.<span style="font-size:80%;opacity:0.8">我们还通过水平翻转图像来增强测试集；将原始图像和翻转图像的soft-max类后验进行平均，以获得图像的最终分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> uncropped<br>(2) </td> <td> [ʌn'krɒpt] </td> <td> 
<ul><li>layers). The resulting fully-convolutional net is then applied to the whole (<font color=orangered>uncropped</font>) image.<span style="font-size:80%;opacity:0.8">然后将所得到的全卷积网络应用于整个（未裁剪）图像上。</span></li><li>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (<font color=orangered>uncropped</font>) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> vector<br>(2) </td> <td> [ˈvektə(r)] </td> <td> 
<ul><li>Finally, to obtain a fixed-size <font color=orangered>vector</font> of class scores for the image, the class score map is spatially averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li><li>A bounding box is represented by a 4-D <font color=orangered>vector</font> storing its center coordinates, width, and height.<span style="font-size:80%;opacity:0.8">边界框由存储其中心坐标、宽度和高度的4维矢量表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> spatially<br>(2) </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>Finally, to obtain a fixed-size vector of class scores for the image, the class score map is <font color=orangered>spatially</font> averaged (sum-pooled).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li><li>To come up with the final prediction, we utilise the greedy merging procedure of Sermanet et al. (2014), which first merges <font color=orangered>spatially</font> close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet.<span style="font-size:80%;opacity:0.8">为了得到最终的预测，我们利用Sermanet等人(2014)的贪婪合并过程，它首先合并空间上接近的预测(通过平均它们的坐标)，然后基于从分类ConvNet获得的类别得分对它们进行评级。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> complementary<br>(2) </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>Also, multi-crop evaluation is <font color=orangered>complementary</font> to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured.<span style="font-size:80%;opacity:0.8">此外，由于不同的卷积边界条件，多裁剪图像评估是密集评估的补充：当将ConvNet应用于裁剪图像时，卷积特征图用零填充，而在密集评估的情况下，相同裁剪图像的填充自然会来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获了更多的上下文。</span></li><li>As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed <font color=orangered>complementary</font>, as their combination outperforms each of them.<span style="font-size:80%;opacity:0.8">可以看出，使用多裁剪图像表现比密集评估略好，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> parallelism<br>(2) </td> <td> [ˈpærəlelɪzəm] </td> <td> 
<ul><li>Multi-GPU training exploits data <font color=orangered>parallelism</font>, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU.<span style="font-size:80%;opacity:0.8">多GPU训练利用数据并行性，通过将每批训练图像分成几个GPU批次，每个GPU并行处理。</span></li><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data <font color=orangered>parallelism</font> for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> NVIDIA<br>(2) </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>On a system equipped with four <font color=orangered>NVIDIA</font> Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.<span style="font-size:80%;opacity:0.8">在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</span></li><li>This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of <font color=orangered>NVIDIA</font> Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> complementarity<br>(2) </td> <td> [ˌkɒmplɪmen'tærɪtɪ] </td> <td> 
<ul><li>We also assess the <font color=orangered>complementarity</font> of the two evaluation techniques by averaging their soft-max outputs.<span style="font-size:80%;opacity:0.8">我们还通过平均其soft-max输出来评估两种评估技术的互补性。</span></li><li>This improves the performance due to <font color=orangered>complementarity</font> of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al. , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al. , 2014).<span style="font-size:80%;opacity:0.8">由于模型的互补性，这提高了性能，并且在了2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013；Sermanet等，2014）ILSVRC的顶级提交中使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> hypothesize<br>(2) </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>As noted above, we <font color=orangered>hypothesize</font> that this is due to a different treatment of convolution boundary conditions.<span style="font-size:80%;opacity:0.8">如上所述，我们假设这是由于卷积边界条件的不同处理。</span></li><li>We <font color=orangered>hypothesize</font> that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> scenario<br>(2) </td> <td> [səˈnɑ:riəʊ] </td> <td> 
<ul><li>In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a fully-fledged <font color=orangered>scenario</font> (the second protocol).<span style="font-size:80%;opacity:0.8">在本节中，我们首先确定性能最佳的本地化设置(使用第一个测试协议)，然后在完全成熟的场景(第二个协议)中对其进行评估。</span></li><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged <font color=orangered>scenario</font>, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> aggregation<br>(2) </td> <td> [ˌæɡrɪ'ɡeɪʃn] </td> <td> 
<ul><li><font color=orangered>Aggregation</font> of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2).<span style="font-size:80%;opacity:0.8">特征的聚合是以与我们的ILSVRC评估程序类似的方式进行的(Sect.3.2)。</span></li><li>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs similarly to the <font color=orangered>aggregation</font> by stacking.<span style="font-size:80%;opacity:0.8">值得注意的是，通过检查VOC-2007和VOC-2012验证集的性能，我们发现通过平均来聚合在多个比例下计算的图像描述符的性能类似于通过堆叠进行聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> Everingham<br>(2) </td> <td>  </td> <td> 
<ul><li>We begin with the evaluation on the image classification task of PASCAL VOC-2007 and VOC-2012 benchmarks (<font color=forestgreen>Everingham</font> et al. , 2015).<span style="font-size:80%;opacity:0.8">我们首先评估Pascal VOC-2007和VOC-2012基准的图像分类任务(Everingham等，2015)。</span></li><li>We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (<font color=forestgreen>Everingham</font> et al. , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action.<span style="font-size:80%;opacity:0.8">我们还在Pascal VOC-2012动作分类任务(Everingham等人，2015)上评估了我们的最佳性能图像表示(Net-D和Net-E特征的叠加)，该任务包括从单个图像预测动作类，给定执行者的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> scale-specific<br>(2) </td> <td>  </td> <td> 
<ul><li>We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular <font color=forestgreen>scale-specific</font> semantics which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such <font color=forestgreen>scale-specific</font> representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> semantically<br>(2) </td> <td> [sɪ'mæntɪklɪ] </td> <td> 
<ul><li>It should be noted that the method of Wei et al. (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, <font color=orangered>semantically</font> close to those in VOC datasets.<span style="font-size:80%;opacity:0.8">应该注意的是Wei等人(2014)的方法在VOC-2012上实现了1%的mAP改善，在扩展的2000类ILSVRC数据集上进行了预训练，该数据集包括另外1000个类别，在语义上接近于VOC数据集中的类别。</span></li><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are <font color=orangered>semantically</font> different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> Fei-Fei<br>(2) </td> <td>  </td> <td> 
<ul><li>In this section we evaluate very deep features on Caltech-101 (<font color=forestgreen>Fei-Fei</font> et al. , 2004) and Caltech-256 (Griffin et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & <font color=forestgreen>Fei-Fei</font>, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> Simonyan<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Simonyan</font>, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Karen<br>(1) </td> <td> ['ka:rən] </td> <td> 
<ul><li>Simonyan, <font color=orangered>Karen</font> (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> Zisserman<br>(1) </td> <td>  </td> <td> 
<ul><li>Simonyan, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); <font color=forestgreen>Zisserman</font>, Andrew<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> andrew<br>(1) </td> <td> [ˈændru:] </td> <td> 
<ul><li>Simonyan, Karen (Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom); Zisserman, <font color=orangered>Andrew</font><span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> prior-art<br>(1) </td> <td>  </td> <td> 
<ul><li>Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the <font color=forestgreen>prior-art</font> configurations can be achieved by pushing the depth to 16–19 weight layers.<span style="font-size:80%;opacity:0.8">我们的主要贡献是使用非常小的（3×3）卷积滤波器架构对网络深度的增加进行了全面评估，这表明通过将深度推到16-19加权层可以实现对现有技术配置的显著改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> commodity<br>(1) </td> <td> [kəˈmɒdəti] </td> <td> 
<ul><li>With ConvNets becoming more of a <font color=orangered>commodity</font> in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy.<span style="font-size:80%;opacity:0.8">随着ConvNets在计算机视觉领域越来越商品化，为了达到更好的准确性，已经进行了许多尝试来改进Krizhevsky等人（2012）最初的架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> Howard<br>(1) </td> <td> [ˈhauəd] </td> <td> 
<ul><li>Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al. , 2014; <font color=orangered>Howard</font>, 2014).<span style="font-size:80%;opacity:0.8">另一条改进措施在整个图像和多个尺度上对网络进行密集地训练和测试（Sermanet等，2014；Howard，2014）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> completeness<br>(1) </td> <td> [kəm'pli:tnəs] </td> <td> 
<ul><li>For <font color=orangered>completeness</font>, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<span style="font-size:80%;opacity:0.8">为了完整起见，我们还将在附录A中描述和评估我们的ILSVRC-2014目标定位系统，并在附录B中讨论了非常深的特征在其它数据集上的泛化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> activation<br>(1) </td> <td> [ˌæktɪ'veɪʃn] </td> <td> 
<ul><li>The ReLU <font color=orangered>activation</font> function is not shown for brevity.<span style="font-size:80%;opacity:0.8">为了简洁起见，不显示ReLU激活功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> discriminative<br>(1) </td> <td> [dɪs'krɪmɪnətɪv] </td> <td> 
<ul><li>First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more <font color=orangered>discriminative</font>.<span style="font-size:80%;opacity:0.8">首先，我们结合了三个非线性修正层，而不是单一的，这使得决策函数更具判别性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> parametrised<br>(1) </td> <td>  </td> <td> 
<ul><li>Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is <font color=forestgreen>parametrised</font> by $3(3^2C^2)=27C^2$ weights; at the same time, a single 7 × 7 conv. layer would require $7^2C^2=49C^2$ parameters, i.e. 81% more.<span style="font-size:80%;opacity:0.8">其次，我们减少参数的数量：假设三层3×3卷积堆叠的输入和输出有C个通道，堆叠卷积层的参数为$3(3^2C^2)=27C^2$个权重；同时，单个7×7卷积层将需要$7^2C^2=49C^2$个参数，即参数多81％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> decomposition<br>(1) </td> <td> [ˌdi:kɒmpə'zɪʃn] </td> <td> 
<ul><li>This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a <font color=orangered>decomposition</font> through the 3 × 3 filters (with non-linearity injected in between).<span style="font-size:80%;opacity:0.8">这可以看作是对7×7卷积滤波器进行正则化，迫使它们通过3×3滤波器（在它们之间注入非线性）进行分解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> incorporation<br>(1) </td> <td> [ɪnˌkɔ:pə'reɪʃn] </td> <td> 
<ul><li>The <font color=orangered>incorporation</font> of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv.<span style="font-size:80%;opacity:0.8">结合1×1卷积层（配置C，表1）是增加决策函数非线性而不影响卷积层感受野的一种方式。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> Small-size<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Small-size</font> convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset.<span style="font-size:80%;opacity:0.8">Ciresan等人（2011）以前使用小尺寸的卷积滤波器，但是他们的网络深度远远低于我们的网络，他们并没有在大规模的ILSVRC数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> Goodfellow<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Goodfellow</font> et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance.<span style="font-size:80%;opacity:0.8">Goodfellow等人（2014）在街道号识别任务中采用深层ConvNets（11个权重层），显示出增加的深度导致了更好的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> topology<br>(1) </td> <td> [tə'pɒlədʒɪ] </td> <td> 
<ul><li>Their network <font color=orangered>topology</font> is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation.<span style="font-size:80%;opacity:0.8">然而，它们的网络拓扑结构比我们的更复杂，并且在第一层中特征图的空间分辨率被更积极地减少，以减少计算量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> optimise<br>(1) </td> <td> ['ɒptɪmaɪz] </td> <td> 
<ul><li>Namely, the training is carried out by <font color=orangered>optimising</font> the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> multinomial<br>(1) </td> <td> [ˌmʌltɪ'nəʊmɪəl] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the <font color=orangered>multinomial</font> logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> descent<br>(1) </td> <td> [dɪˈsent] </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient <font color=orangered>descent</font> (based on back-propagation (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> back-propagation<br>(1) </td> <td>  </td> <td> 
<ul><li>Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on <font color=forestgreen>back-propagation</font> (LeCun et al. , 1989)) with momentum.<span style="font-size:80%;opacity:0.8">也就是说，通过使用具有动量的小批量梯度下降（基于反向传播（LeCun等人，1989））优化多项式逻辑回归目标函数来进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> regularise<br>(1) </td> <td> ['regjʊləraɪz] </td> <td> 
<ul><li>The training was <font color=orangered>regularised</font> by weight decay (the L2 penalty multiplier set to $5 \times 10^{−4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> multiplier<br>(1) </td> <td> [ˈmʌltɪplaɪə(r)] </td> <td> 
<ul><li>The training was regularised by weight decay (the L2 penalty <font color=orangered>multiplier</font> set to $5 \times 10^{−4}$) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).<span style="font-size:80%;opacity:0.8">训练通过权重衰减（L2惩罚乘子设定为$5\times 10^{−4}$）进行正则化，前两个全连接层执行丢弃正则化（丢弃率设定为0.5）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> conjecture<br>(1) </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> pre-initialisation<br>(1) </td> <td>  </td> <td> 
<ul><li>We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al. , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) <font color=forestgreen>pre-initialisation</font> of certain layers.<span style="font-size:80%;opacity:0.8">我们推测，尽管与（Krizhevsky等，2012）相比我们的网络参数更多，网络的深度更大，但网络需要更小的epoch就可以收敛，这是由于（a）由更大的深度和更小的卷积滤波器尺寸引起的隐式正则化，（b）某些层的预初始化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> stall<br>(1) </td> <td> [stɔ:l] </td> <td> 
<ul><li>The initialisation of the network weights is important, since bad initialisation can <font color=orangered>stall</font> learning due to the instability of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> instability<br>(1) </td> <td> [ˌɪnstəˈbɪləti] </td> <td> 
<ul><li>The initialisation of the network weights is important, since bad initialisation can stall learning due to the <font color=orangered>instability</font> of gradient in deep nets.<span style="font-size:80%;opacity:0.8">网络权重的初始化是重要的，因为由于深度网络中梯度的不稳定，不好的初始化可能会阻碍学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> circumvent<br>(1) </td> <td> [ˌsɜ:kəmˈvent] </td> <td> 
<ul><li>To <font color=orangered>circumvent</font> this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation.<span style="font-size:80%;opacity:0.8">为了规避这个问题，我们开始训练配置A（表1），足够浅以随机初始化进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> pre-initialised<br>(1) </td> <td>  </td> <td> 
<ul><li>We did not decrease the learning rate for the <font color=forestgreen>pre-initialised</font> layers, allowing them to change during learning.<span style="font-size:80%;opacity:0.8">我们没有减少预初始化层的学习率，允许他们在学习过程中改变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> variance<br>(1) </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and $10^{−2}$ <font color=orangered>variance</font>.<span style="font-size:80%;opacity:0.8">对于随机初始化（如果应用），我们从均值为0和方差为$10^{−2}$的正态分布中采样权重。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> Glorot<br>(1) </td> <td>  </td> <td> 
<ul><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of <font color=forestgreen>Glorot</font> & Bengio (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> Bengio<br>(1) </td> <td>  </td> <td> 
<ul><li>It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & <font color=forestgreen>Bengio</font> (2010).<span style="font-size:80%;opacity:0.8">值得注意的是，在提交论文之后，我们发现可以通过使用Glorot & Bengio（2010）的随机初始化程序来初始化权重而不进行预训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> SGD<br>(1) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per <font color=orangered>SGD</font> iteration).<span style="font-size:80%;opacity:0.8">为了获得固定大小的224×224 ConvNet输入图像，它们从归一化的训练图像中被随机裁剪（每个图像每次SGD迭代进行一次裁剪）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> isotropically-rescaled<br>(1) </td> <td>  </td> <td> 
<ul><li>Let S be the smallest side of an <font color=forestgreen>isotropically-rescaled</font> training image, from which the ConvNet input is cropped (we also refer to S as the training scale).<span style="font-size:80%;opacity:0.8">令S是等轴归一化的训练图像的最小边，ConvNet输入从S中裁剪（我们也将S称为训练尺度）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> whole-image<br>(1) </td> <td>  </td> <td> 
<ul><li>While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture <font color=forestgreen>whole-image</font> statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.<span style="font-size:80%;opacity:0.8">虽然裁剪尺寸固定为224×224，但原则上S可以是不小于224的任何值：对于S=224，裁剪图像将捕获整个图像的统计数据，完全扩展训练图像的最小边；对于S»224，裁剪图像将对应于图像的一小部分，包含小对象或对象的一部分。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> speed-up<br>(1) </td> <td> [s'pi:d'ʌp] </td> <td> 
<ul><li>To <font color=orangered>speed-up</font> training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of $10^{−3}$.<span style="font-size:80%;opacity:0.8">为了加速S = 384网络的训练，用S = 256预训练的权重来进行初始化，我们使用较小的初始学习率$10^{−3}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> isotropically<br>(1) </td> <td>  </td> <td> 
<ul><li>First, it is <font color=forestgreen>isotropically</font> rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale).<span style="font-size:80%;opacity:0.8">首先，将其等轴地归一化到预定义的最小图像边，表示为Q（我们也将其称为测试尺度）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> sum-pooled<br>(1) </td> <td>  </td> <td> 
<ul><li>Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (<font color=forestgreen>sum-pooled</font>).<span style="font-size:80%;opacity:0.8">最后，为了获得图像的类别分数的固定大小的向量，类得分图在空间上平均（和池化）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> re-computation<br>(1) </td> <td>  </td> <td> 
<ul><li>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al. , 2012), which is less efficient as it requires network <font color=forestgreen>re-computation</font> for each crop.<span style="font-size:80%;opacity:0.8">由于全卷积网络被应用在整个图像上，所以不需要在测试时对采样多个裁剪图像（Krizhevsky等，2012），因为它需要网络重新计算每个裁剪图像，这样效率较低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> comparable<br>(1) </td> <td> [ˈkɒmpərəbl] </td> <td> 
<ul><li>While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is <font color=orangered>comparable</font> to 144 crops over 4 scales used by Szegedy et al. (2014).<span style="font-size:80%;opacity:0.8">虽然我们认为在实践中，多裁剪图像的计算时间增加并不足以证明准确性的潜在收益，但作为参考，我们还在每个尺度使用50个裁剪图像（5×5规则网格，2次翻转）评估了我们的网络，在3个尺度上总共150个裁剪图像，与Szegedy等人(2014)在4个尺度上使用的144个裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> Caffe<br>(1) </td> <td>  </td> <td> 
<ul><li>Our implementation is derived from the publicly available C++ <font color=forestgreen>Caffe</font> toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> full-size<br>(1) </td> <td> [ˈfulˈsaiz] </td> <td> 
<ul><li>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on <font color=orangered>full-size</font> (uncropped) images at multiple scales (as described above).<span style="font-size:80%;opacity:0.8">我们的实现来源于公开的C++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含了一些重大的修改，使我们能够对安装在单个系统中的多个GPU进行训练和评估，也能训练和评估在多个尺度上（如上所述）的全尺寸（未裁剪）图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> synchronous<br>(1) </td> <td> [ˈsɪŋkrənəs] </td> <td> 
<ul><li>Gradient computation is <font color=orangered>synchronous</font> across the GPUs, so the result is exactly the same as when training on a single GPU.<span style="font-size:80%;opacity:0.8">梯度计算在GPU之间是同步的，所以结果与在单个GPU上训练完全一样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> conceptually<br>(1) </td> <td> [kən'septʃʊəlɪ] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our <font color=orangered>conceptually</font> much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> speedup<br>(1) </td> <td> ['spi:dʌp] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a <font color=orangered>speedup</font> of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> off-the-shelf<br>(1) </td> <td> [ɒf ðə ʃelf] </td> <td> 
<ul><li>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an <font color=orangered>off-the-shelf</font> 4-GPU system, as compared to using a single GPU.<span style="font-size:80%;opacity:0.8">最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们对网络的不同层之间采用模型和数据并行，我们发现我们概念上更简单的方案与使用单个GPU相比，在现有的4-GPU系统上已经提供了3.75倍的加速。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> titan<br>(1) </td> <td> [ˈtaɪtn] </td> <td> 
<ul><li>On a system equipped with four NVIDIA <font color=orangered>Titan</font> Black GPUs, training a single net took 2–3 weeks depending on the architecture.<span style="font-size:80%;opacity:0.8">在配备四个NVIDIA Titan Black GPU的系统上，根据架构训练单个网络需要2-3周时间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> held-out<br>(1) </td> <td>  </td> <td> 
<ul><li>The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with <font color=forestgreen>held-out</font> class labels).<span style="font-size:80%;opacity:0.8">数据集包括1000个类别的图像，并分为三组：训练（130万张图像），验证（5万张图像）和测试（留有类标签的10万张图像）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> incorrectly<br>(1) </td> <td> [ˌɪnkə'rektlɪ] </td> <td> 
<ul><li>The former is a multi-class classification error, i.e. the proportion of <font color=orangered>incorrectly</font> classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.<span style="font-size:80%;opacity:0.8">前者是多类分类误差，即不正确分类图像的比例；后者是ILSVRC中使用的主要评估标准，并且计算为图像真实类别在前5个预测类别之外的图像比例。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> jitter<br>(1) </td> <td> ['dʒɪtə] </td> <td> 
<ul><li>The test image size was set as follows: Q = S for fixed S, and $Q = 0.5$($S_{min} + S_{max}$) for <font color=orangered>jittered</font> $S \in [S_{min}, S_{max}]$.<span style="font-size:80%;opacity:0.8">测试图像大小设置如下：对于固定S的Q = S，对于抖动$S \in [S_{min}, S_{max}]$，$Q = 0.5$($S_{min} + S_{max}$)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> A-LRN<br>(1) </td> <td>  </td> <td> 
<ul><li>First, we note that using local response normalisation (<font color=forestgreen>A-LRN</font> network) does not improve on the model A without any normalisation layers.<span style="font-size:80%;opacity:0.8">首先，我们注意到，使用局部响应归一化（A-LRN网络）在没有任何归一化层的情况下，对模型A没有改善。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> saturate<br>(1) </td> <td> [ˈsætʃəreɪt] </td> <td> 
<ul><li>The error rate of our architecture <font color=orangered>saturates</font> when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets.<span style="font-size:80%;opacity:0.8">当深度达到19层时，我们架构的错误率饱和，但更深的模型可能有益于较大的数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> discrepancy<br>(1) </td> <td> [dɪsˈkrepənsi] </td> <td> 
<ul><li>Considering that a large <font color=orangered>discrepancy</font> between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: $Q = \{S − 32, S, S + 32\}$.<span style="font-size:80%;opacity:0.8">考虑到训练和测试尺度之间的巨大差异会导致性能下降，用固定S训练的模型在三个测试图像尺度上进行了评估，接近于训练一次：$Q = \{S − 32, S, S + 32\}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> mult-crop<br>(1) </td> <td>  </td> <td> 
<ul><li>In Table 5 we compare dense ConvNet evaluation with <font color=forestgreen>mult-crop</font> evaluation (see Sect. 3.2 for details).<span style="font-size:80%;opacity:0.8">在表5中，我们将稠密ConvNet评估与多裁剪图像评估进行比较（细节参见第3.2节）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> Clarifai<br>(1) </td> <td>  </td> <td> 
<ul><li>Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission <font color=forestgreen>Clarifai</font>, which achieved 11.2% with outside training data and 11.7% without it.<span style="font-size:80%;opacity:0.8">我们的结果对于分类任务获胜者（GoogLeNet具有6.7％的错误率）也具有竞争力，并且大大优于ILSVRC-2013获胜者Clarifai的提交，其使用外部训练数据取得了11.2％的错误率，没有外部数据则为11.7％。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> conventional<br>(1) </td> <td> [kənˈvenʃənl] </td> <td> 
<ul><li>It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a <font color=orangered>conventional</font> ConvNet architecture (LeCun et al. , 1989; Krizhevsky et al. , 2012) with substantially increased depth.<span style="font-size:80%;opacity:0.8">已经证明，表示深度有利于分类精度，并且深度大大增加的传统ConvNet架构（LeCun等，1989；Krizhevsky等，2012）可以实现ImageNet挑战数据集上的最佳性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> ERC<br>(1) </td> <td>  </td> <td> 
<ul><li>This work was supported by <font color=forestgreen>ERC</font> grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> VisRec<br>(1) </td> <td>  </td> <td> 
<ul><li>This work was supported by ERC grant <font color=forestgreen>VisRec</font> no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> gratefully<br>(1) </td> <td> ['ɡreɪtfəlɪ] </td> <td> 
<ul><li>This work was supported by ERC grant VisRec no. 228180. We <font color=orangered>gratefully</font> acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.<span style="font-size:80%;opacity:0.8">这项工作得到ERC授权的VisRec编号228180的支持.我们非常感谢NVIDIA公司捐赠GPU为此研究使用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> irrespective<br>(1) </td> <td> [ˌɪrɪ'spektɪv] </td> <td> 
<ul><li>It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, <font color=orangered>irrespective</font> of the actual number of objects of the class.<span style="font-size:80%;opacity:0.8">它可以被看作是对象检测的一种特殊情况，其中应该为前5个类中的每一个预测单个对象边界框，而不考虑该类的实际对象数量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> fewmodifications<br>(1) </td> <td>  </td> <td> 
<ul><li>For this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation challenge, with a <font color=forestgreen>fewmodifications</font>.<span style="font-size:80%;opacity:0.8">为此，我们采用Sermanet等人(2014)的方法，仅作了几处修改。Sermanet等人是ILSVRC-2013本地化挑战的获胜者。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> class-specific<br>(1) </td> <td>  </td> <td> 
<ul><li>There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al. , 2014)) or is <font color=forestgreen>class-specific</font> (per-class regression, PCR).<span style="font-size:80%;opacity:0.8">可以选择边界框预测是跨所有类别共享(单个类别回归，SCR(Sermanet et al.，2014))或是特定类别(逐个类别回归，PCR)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> Euclidean<br>(1) </td> <td> [ju:ˈklidiən] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a <font color=orangered>Euclidean</font> loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> penalise<br>(1) </td> <td> ['pi:nəlaɪz] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which <font color=orangered>penalises</font> the deviation of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> deviation<br>(1) </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the <font color=orangered>deviation</font> of the predicted bounding box parameters from the ground-truth.<span style="font-size:80%;opacity:0.8">主要的区别是我们用欧几里得损失代替逻辑回归目标，这惩罚了预测的边界框参数与实际值的偏差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> intersection<br>(1) </td> <td> [ˌɪntəˈsekʃn] </td> <td> 
<ul><li>The localisation error is measured according to the ILSVRC criterion (Russakovsky et al. , 2014), i.e. the bounding box prediction is deemed correct if its <font color=orangered>intersection</font> over union ratio with the ground-truth bounding box is above 0.5.<span style="font-size:80%;opacity:0.8">根据ILSVRC标准测量定位误差(Russakovsky等人，2014)，即如果边界框预测与实际边界框的相交超过并比大于0.5，则认为其是正确的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> class-agnostic<br>(1) </td> <td>  </td> <td> 
<ul><li>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the <font color=forestgreen>class-agnostic</font> single-class regression (SCR), which differs from the findings of Sermanet et al. (2014), where PCR was outperformed by SCR.<span style="font-size:80%;opacity:0.8">设置比较。从表8可以看出，逐类回归(PCR)优于类不可知的单类回归(SCR)，这与Sermanet等人(2014)的发现不同，后者的PCR表现优于SCR。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> noticeably<br>(1) </td> <td> ['nəʊtɪsəblɪ] </td> <td> 
<ul><li>We also note that fine-tuning all layers for the localisation task leads to <font color=orangered>noticeably</font> better results than fine-tuning only the fully-connected layers (as done in (Sermanet et al. , 2014)).<span style="font-size:80%;opacity:0.8">我们还注意到，为本地化任务微调所有层比仅微调完全连接的层(如(Sermanet et al.，2014)中所做的)会导致明显更好的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> densely-computed<br>(1) </td> <td>  </td> <td> 
<ul><li>Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple <font color=forestgreen>densely-computed</font> bounding box predictions are merged using the method of Sermanet et al. (2014).<span style="font-size:80%;opacity:0.8">在确定了最佳定位设置(PCR，所有层的微调)之后，我们现在将其应用于完全成熟的场景中，其中使用我们性能最佳的分类系统预测的top-5个类别标签(Sect. 4.5)，并且使用Sermanet等人(2014年)的方法合并多个密集计算的边界框预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> Overfeat<br>(1) </td> <td>  </td> <td> 
<ul><li>Notably, our results are considerably better than those of the ILSVRC-2013 winner <font color=forestgreen>Overfeat</font> (Sermanet et al. , 2014), even though we used less scales and did not employ their resolution enhancement technique.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的结果比ILSVRC-2013获奖者Overfeat(Sermanet等人，2014)的结果要好得多，尽管我们使用了更少的比例并且没有使用他们的分辨率增强技术。</span></li></ul>
 </td>
</tr>
<tr>
<td> 154 </td> <td> envisage<br>(1) </td> <td> [ɪnˈvɪzɪdʒ] </td> <td> 
<ul><li>We <font color=orangered>envisage</font> that better localisation performance can be achieved if this technique is incorporated into our method.<span style="font-size:80%;opacity:0.8">我们设想，如果将这种技术结合到我们的方法中，可以获得更好的定位性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 155 </td> <td> extractor<br>(1) </td> <td> [ɪkˈstræktə(r)] </td> <td> 
<ul><li>In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature <font color=orangered>extractors</font> on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 156 </td> <td> over-fitting<br>(1) </td> <td>  </td> <td> 
<ul><li>In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature extractors on other, smaller, datasets, where training large models from scratch is not feasible due to <font color=forestgreen>over-fitting</font>.<span style="font-size:80%;opacity:0.8">在本节中，我们将在ILSVRC上预训练的ConvNets评估为其他较小数据集上的特征提取器，其中由于过度拟合，从头训练大型模型是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 157 </td> <td> Donahue<br>(1) </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; <font color=forestgreen>Donahue</font> et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 158 </td> <td> Razavian<br>(1) </td> <td>  </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; <font color=forestgreen>Razavian</font> et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 159 </td> <td> hand-crafted<br>(1) </td> <td> [,hænd 'kra:ftid] </td> <td> 
<ul><li>Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue et al. , 2013; Razavian et al. , 2014; Chatfield et al. , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed <font color=orangered>hand-crafted</font> representations by a large margin.<span style="font-size:80%;opacity:0.8">最近，人们对这样一个用例非常感兴趣(Zeiler&Fergus，2013；Donahue等人，2013；Razvian等人，2014；Chatfield等人，2014)，因为事实证明，在ILSVRC上学习的深层图像表示很好地推广到其他数据集，在这些数据集中，它们的表现远远超过手工表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 160 </td> <td> state-of-the-artmethods<br>(1) </td> <td>  </td> <td> 
<ul><li>Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the <font color=forestgreen>state-of-the-artmethods</font>.<span style="font-size:80%;opacity:0.8">遵循这一工作路线，我们研究我们的模型是否比现有技术中使用的更浅的模型具有更好的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 161 </td> <td> activations<br>(1) </td> <td> [,æktɪ'veɪʃən] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D <font color=orangered>activations</font> of the penultimate layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 162 </td> <td> penultimate<br>(1) </td> <td> [penˈʌltɪmət] </td> <td> 
<ul><li>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the <font color=orangered>penultimate</font> layer as image features, which are aggregated across multiple locations and scales.<span style="font-size:80%;opacity:0.8">为了利用在ILSVRC上预先训练的ConvNets对其他数据集进行图像分类，我们删除了最后一个完全连接的层(它执行1000种ILSVRC分类)，并使用倒数第二层的4096-D激活作为图像特征，这些图像特征在多个位置和规模上聚合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 163 </td> <td> L2-normalised<br>(1) </td> <td>  </td> <td> 
<ul><li>The resulting image descriptor is <font color=forestgreen>L2-normalised</font> and combined with a linear SVM classifier, trained on the target dataset.<span style="font-size:80%;opacity:0.8">得到的图像描述符是L2归一化的，并与线性SVM分类器结合，在目标数据集上训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 164 </td> <td> horizontally<br>(1) </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>The descriptor is then averaged with the descriptor of a <font color=orangered>horizontally</font> flipped image.<span style="font-size:80%;opacity:0.8">然后将描述符与水平翻转图像的描述符进行平均。</span></li></ul>
 </td>
</tr>
<tr>
<td> 165 </td> <td> optimally<br>(1) </td> <td> ['əptəməli] </td> <td> 
<ul><li>Stacking allows a subsequent classifier to learn how to <font color=orangered>optimally</font> combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality.<span style="font-size:80%;opacity:0.8">堆叠允许随后的分类器学习如何在一定范围内最佳地组合图像统计数据；然而，这是以增加的描述符维数为代价的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 166 </td> <td> semantics<br>(1) </td> <td> [sɪˈmæntɪks] </td> <td> 
<ul><li>We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, so there is no particular scale-specific <font color=orangered>semantics</font> which a classifier could exploit.<span style="font-size:80%;opacity:0.8">我们假设这是由于在VOC数据集中对象出现在各种尺度上的事实，因此没有分类器可以利用的特定尺度语义。</span></li></ul>
 </td>
</tr>
<tr>
<td> 167 </td> <td> inflate<br>(1) </td> <td> [ɪnˈfleɪt] </td> <td> 
<ul><li>Since averaging has a benefit of not <font color=orangered>inflating</font> the descriptor dimensionality, we were able to aggregated image descriptors over a wide range of scales: $Q \in \{256, 384, 512, 640, 768\}$.<span style="font-size:80%;opacity:0.8">由于平均具有不膨胀描述符维度的优点，我们能够在广泛的范围内聚合图像描述符：$Q \in \{256，384，512，640，768\}$中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 168 </td> <td> pretrained<br>(1) </td> <td>  </td> <td> 
<ul><li>Our methods set the new state of the art across image representations, <font color=forestgreen>pretrained</font> on the ILSVRC dataset, outperforming the previous best result of Chatfield et al. (2014) by more than 6%.<span style="font-size:80%;opacity:0.8">我们的方法在图像表示上设置了新的技术状态，在ILSVRC数据集上进行了预训练，性能优于Chatfield等人(2014)之前的最佳结果有超过6%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 169 </td> <td> detection-assisted<br>(1) </td> <td>  </td> <td> 
<ul><li>It also benefits from the fusion with an object <font color=forestgreen>detection-assisted</font> classification pipeline.<span style="font-size:80%;opacity:0.8">它还受益于与对象检测辅助分类流水线的融合。</span></li></ul>
 </td>
</tr>
<tr>
<td> 170 </td> <td> griffin<br>(1) </td> <td> [ˈgrɪfɪn] </td> <td> 
<ul><li>In this section we evaluate very deep features on Caltech-101 (Fei-Fei et al. , 2004) and Caltech-256 (<font color=orangered>Griffin</font> et al. , 2007) image classification benchmarks.<span style="font-size:80%;opacity:0.8">在本节中，我们评估了Caltech-101(Fei-Fei等人，2004)和Caltech-256(Griffin等人，2007)图像分类基准的非常深入的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 171 </td> <td> hyper-parameter<br>(1) </td> <td>  </td> <td> 
<ul><li>In each split, 20% of training images were used as a validation set for <font color=forestgreen>hyper-parameter</font> selection.<span style="font-size:80%;opacity:0.8">在每次分割中，20%的训练图像被用作超参数选择的验证集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 172 </td> <td> vs<br>(1) </td> <td>  </td> <td> 
<ul><li>This can be explained by the fact that in Caltech images objects typically occupy the whole image, so multi-scale image features are semantically different (capturing the whole object <font color=forestgreen>vs</font>. object parts), and stacking allows a classifier to exploit such scale-specific representations.<span style="font-size:80%;opacity:0.8">这可以通过以下事实来解释：在Caltech图像中，对象通常占据整个图像，因此多尺度图像特征在语义上是不同的(捕获整个对象与对象部分)，并且堆叠允许分类器利用这种比例特定的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 173 </td> <td> task-specific<br>(1) </td> <td>  </td> <td> 
<ul><li>Unlike other approaches, we did not incorporate any <font color=forestgreen>task-specific</font> heuristics, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 174 </td> <td> heuristic<br>(1) </td> <td> [hjuˈrɪstɪk] </td> <td> 
<ul><li>Unlike other approaches, we did not incorporate any task-specific <font color=orangered>heuristics</font>, but relied on the representation power of very deep convolutional features.<span style="font-size:80%;opacity:0.8">与其他方法不同，我们没有包含任何特定于任务的启发式方法，而是依赖于非常深的卷积特征的表示能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 175 </td> <td> consistently<br>(1) </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, <font color=orangered>consistently</font> outperforming more shallow representations.<span style="font-size:80%;opacity:0.8">自从我们的模型公开发布以来，研究界一直在积极地使用它们来完成广泛的图像识别任务，始终优于更浅的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 176 </td> <td> Girshick<br>(1) </td> <td>  </td> <td> 
<ul><li>For instance, <font color=forestgreen>Girshick</font> et al. (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer model.<span style="font-size:80%;opacity:0.8">例如，Girshick等人(2014)通过使用我们的16层模型替换Krizhevsky等人(2012)的ConvNet来实现对象检测结果的状态。</span></li></ul>
 </td>
</tr>
<tr>
<td> 177 </td> <td> semantic<br>(1) </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in <font color=orangered>semantic</font> segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 178 </td> <td> segmentation<br>(1) </td> <td> [ˌsegmenˈteɪʃn] </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic <font color=orangered>segmentation</font> (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 179 </td> <td> Kiros<br>(1) </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (<font color=forestgreen>Kiros</font> et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 180 </td> <td> Karpathy<br>(1) </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; <font color=forestgreen>Karpathy</font> & Fei-Fei, 2014), texture and material recognition (Cimpoi et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 181 </td> <td> Cimpoi<br>(1) </td> <td>  </td> <td> 
<ul><li>Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been observed in semantic segmentation (Long et al. , 2014), image caption generation (Kiros et al. , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (<font color=forestgreen>Cimpoi</font> et al. , 2014; Bell et al. , 2014).<span style="font-size:80%;opacity:0.8">Krizhevsky等人(2012)在更浅的架构，已经在语义分割(Long等人，2014)、图像字幕生成(Kiros等人，2014；Karpathy&Fei-Fei，2014)、纹理和材料识别(Cimpoi等人，2014；Bell等人，2014)中观察到能获得类似的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 182 </td> <td> post-submission<br>(1) </td> <td>  </td> <td> 
<ul><li>v2 Adds <font color=forestgreen>post-submission</font> ILSVRC experiments with training set augmentation using scale jittering, which improves the performance.<span style="font-size:80%;opacity:0.8">V2增加了提交后的ILSVRC实验，使用比例抖动增强训练集，从而提高了性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 183 </td> <td> Camera-ready<br>(1) </td> <td>  </td> <td> 
<ul><li>v6 <font color=forestgreen>Camera-ready</font> ICLR-2015 conference paper.<span style="font-size:80%;opacity:0.8">V6 复印就绪ICLR-2015会议论文。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>