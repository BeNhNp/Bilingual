<html>
<head>
<meta charset="utf-8">
<title> CRNN - An End-to-End Trainable Neural Network </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> Baoguang </td> <td>  </td> <td> 
<ul><li>Authors: Shi, <font color=forestgreen>Baoguang</font> (School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> Huazhong </td> <td>  </td> <td> 
<ul><li>Authors: Shi, Baoguang (School of Electronic Information and Communications, <font color=forestgreen>Huazhong</font> University of Science and Technology, Wuhan; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> Wuhan </td> <td> ['wu:'hɑ:n] </td> <td> 
<ul><li>Authors: Shi, Baoguang (School of Electronic Information and Communications, Huazhong University of Science and Technology, <font color=orangered>Wuhan</font>; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> long-standing </td> <td> [ˈlɔŋstædiŋ] </td> <td> 
<ul><li>Image-based sequence recognition has been a <font color=orangered>long-standing</font> research topic in computer vision.<span style="font-size:80%;opacity:0.8">基于图像的序列识别一直是计算机视觉中长期存在的研究课题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> extraction </td> <td> [ɪkˈstrækʃn] </td> <td> 
<ul><li>A novel neural network architecture, which integrates feature <font color=orangered>extraction</font>, sequence modeling and transcription into a unified framework, is proposed.<span style="font-size:80%;opacity:0.8">提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。</span></li><li>2.1. Feature Sequence <font color=orangered>Extraction</font><span style="font-size:80%;opacity:0.8">2.1. 特征序列提取</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> transcription </td> <td> [trænˈskrɪpʃn] </td> <td> 
<ul><li>A novel neural network architecture, which integrates feature extraction, sequence modeling and <font color=orangered>transcription</font> into a unified framework, is proposed.<span style="font-size:80%;opacity:0.8">提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。</span></li><li>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a <font color=orangered>transcription</font> layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>2) recurrent layers, which predict a label distribution for each frame; 3) <font color=orangered>transcription</font> layer, which translates the per-frame predictions into the final label sequence.<span style="font-size:80%;opacity:0.8">2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</span></li><li>The <font color=orangered>transcription</font> layer at the top of CRNN is adopted to translate the per-frame predictions by the recurrent layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>2.3. <font color=orangered>Transcription</font><span style="font-size:80%;opacity:0.8">2.3. 转录</span></li><li><font color=orangered>Transcription</font> is the process of converting the per-frame predictions made by RNN into a label sequence.<span style="font-size:80%;opacity:0.8">转录是将RNN所做的每帧预测转换成标签序列的过程。</span></li><li>Mathematically, <font color=orangered>transcription</font> is to find the label sequence with the highest probability conditioned on the per-frame predictions.<span style="font-size:80%;opacity:0.8">数学上，转录是根据每帧预测找到具有最高概率的标签序列。</span></li><li>In practice, there exists two modes of <font color=orangered>transcription</font>, namely the lexicon-free and lexicon-based transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In practice, there exists two modes of transcription, namely the lexicon-free and lexicon-based <font color=orangered>transcriptions</font>.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>2.3.2 Lexicon-free <font color=orangered>transcription</font><span style="font-size:80%;opacity:0.8">2.3.2 无字典转录</span></li><li>2.3.3 Lexicon-based <font color=orangered>transcription</font><span style="font-size:80%;opacity:0.8">2.3.3 基于词典的转录</span></li><li>1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free <font color=orangered>transcription</font>, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>In particular, in the <font color=orangered>transcription</font> layer, error differentials are back-propagated with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the <font color=orangered>transcription</font> layer (in C++) and the BK-tree data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li><li>Larger \delta results in more candidates, thus more accurate lexicon-based <font color=orangered>transcription</font>.<span style="font-size:80%;opacity:0.8">更大的\delta导致更多的候选目标，从而基于词典的转录更准确。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> trainable </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end <font color=orangered>trainable</font>, in contrast to most of the existing algorithms whose components are separately trained and tuned.<span style="font-size:80%;opacity:0.8">与以前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。</span></li><li>Being robust, rich and <font color=orangered>trainable</font>, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [25, 12].<span style="font-size:80%;opacity:0.8">鲁棒的，丰富的和可训练的深度卷积特征已被广泛应用于各种视觉识别任务[25,12]。</span></li><li>Attributes for comparison include: 1) being end-to-end <font color=orangered>trainable</font> (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end <font color=orangered>trainable</font> model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li>E2E Train: This column is to show whether a certain text reading model is end-to-end <font color=orangered>trainable</font>, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training.<span style="font-size:80%;opacity:0.8">E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。</span></li><li>An End-to-End <font color=orangered>Trainable</font> Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition<span style="font-size:80%;opacity:0.8">基于图像序列识别的端到端可训练神经网络及其在场景文本识别中的应用</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> arbitrary </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>(2) It naturally handles sequences in <font color=orangered>arbitrary</font> lengths, involving no character segmentation or horizontal scale normalization.<span style="font-size:80%;opacity:0.8">（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。</span></li><li>Thirdly, RNN is able to operate on sequences of <font color=orangered>arbitrary</font> lengths, traversing from starts to ends.<span style="font-size:80%;opacity:0.8">第三，RNN能够从头到尾对任意长度的序列进行操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> segmentation </td> <td> [ˌsegmenˈteɪʃn] </td> <td> 
<ul><li>(2) It naturally handles sequences in arbitrary lengths, involving no character <font color=orangered>segmentation</font> or horizontal scale normalization.<span style="font-size:80%;opacity:0.8">（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。</span></li><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/<font color=orangered>segmentation</font>, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> predefined </td> <td> [pri:dɪ'faɪnd] </td> <td> 
<ul><li>(3) It is not confined to any <font color=orangered>predefined</font> lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> lexicon </td> <td> [ˈleksɪkən] </td> <td> 
<ul><li>(3) It is not confined to any predefined <font color=orangered>lexicon</font> and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>A <font color=orangered>lexicon</font> is a set of label sequences that prediction is constraint to, e.g. a spell checking dictionary.<span style="font-size:80%;opacity:0.8">词典是一组标签序列，预测受拼写检查字典约束。</span></li><li>In lexicon-free mode, predictions are made without any <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在无词典模式中，预测时没有任何词典。</span></li><li>In lexicon-based mode, each test sample is associated with a <font color=orangered>lexicon</font> {\cal D}.<span style="font-size:80%;opacity:0.8">在基于字典的模式中，每个测试采样与词典{\cal D}相关联。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the <font color=orangered>lexicon</font> that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the <font color=orangered>lexicon</font>, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large <font color=orangered>lexicons</font>, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the <font color=orangered>lexicon</font> that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the <font color=orangered>lexicon</font>, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>1 for all sequences in the <font color=orangered>lexicon</font> and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>The search time complexity of BK-tree is $O(\log|{\cal D}|)$, where $|{\cal D}|$ is the <font color=orangered>lexicon</font> size.<span style="font-size:80%;opacity:0.8">BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。</span></li><li>Therefore this scheme readily extends to very large <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">因此，这个方案很容易扩展到非常大的词典。</span></li><li>In our approach, a BK-tree is constructed offline for a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在我们的方法中，一个词典离线构造一个BK树。</span></li><li>Each test image is associated with a 50-words <font color=orangered>lexicon</font> which is defined by Wang et al. [34].<span style="font-size:80%;opacity:0.8">每张测试图像与由Wang等人[34]定义的50词的词典相关联。</span></li><li>A full <font color=orangered>lexicon</font> is built by combining all the per-image lexicons.<span style="font-size:80%;opacity:0.8">通过组合所有的每张图像词汇构建完整的词典。</span></li><li>A full lexicon is built by combining all the per-image <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">通过组合所有的每张图像词汇构建完整的词典。</span></li><li>In addition, we use a 50k words <font color=orangered>lexicon</font> consisting of the words in the Hunspell spell-checking dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li><li>Each image has been associated to a 50-words <font color=orangered>lexicon</font> and a 1k-words <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">每张图像关联一个50词的词典和一个1000词的词典。</span></li><li>Each word image has a 50 words <font color=orangered>lexicon</font> defined by Wang et al. [34].<span style="font-size:80%;opacity:0.8">每张单词图像都有一个由Wang等人[34]定义的50个词的词典。</span></li><li>The average testing time is 0.16s/sample, as measured on IC03 without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">平均测试时间为0.16s/样本，在IC03上测得的，没有词典。</span></li><li>The approximate <font color=orangered>lexicon</font> search is applied to the 50k <font color=orangered>lexicon</font> of IC03, with the parameter δ set to 3.<span style="font-size:80%;opacity:0.8">近似词典搜索应用于IC03的50k词典，参数δ设置为3。</span></li><li>In the second row, “50”, “1k”, “50k” and “Full” denote the <font color=orangered>lexicon</font> used, and “None” denotes recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在第二行，“50”，“1k”，“50k”和“Full”表示使用的字典，“None”表示识别没有字典。</span></li><li>In the constrained <font color=orangered>lexicon</font> cases, our method consistently outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained <font color=orangered>lexicon</font> cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Note that the blanks in the “none” columns of Table 2 denote that such approaches are unable to be applied to recognition without <font color=orangered>lexicon</font> or did not report the recognition accuracies in the unconstrained cases.<span style="font-size:80%;opacity:0.8">注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。</span></li><li>The best persformance is reported by [22] in the unconstrained <font color=orangered>lexicon</font> cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a <font color=orangered>lexicon</font> as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li><li>In this sense, our results in the unconstrained <font color=orangered>lexicon</font> case are still promising.<span style="font-size:80%;opacity:0.8">在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</span></li><li>Red bars: <font color=orangered>lexicon</font> search time per sample.<span style="font-size:80%;opacity:0.8">红条：每个样本的词典搜索时间。</span></li><li>Tested on the IC03 dataset with the 50k <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在IC03数据集上使用50k词典进行的测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> lexicon-free </td> <td>  </td> <td> 
<ul><li>(3) It is not confined to any predefined lexicon and achieves remarkable performances in both <font color=forestgreen>lexicon-free</font> and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>In practice, there exists two modes of transcription, namely the <font color=forestgreen>lexicon-free</font> and lexicon-based transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In <font color=forestgreen>lexicon-free</font> mode, predictions are made without any lexicon.<span style="font-size:80%;opacity:0.8">在无词典模式中，预测时没有任何词典。</span></li><li>1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via <font color=forestgreen>lexicon-free</font> transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in <font color=forestgreen>lexicon-free</font> mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li><li>*[22] is not <font color=forestgreen>lexicon-free</font> in the strict sense, as its outputs are constrained to a 90k dictionary.<span style="font-size:80%;opacity:0.8">*[22]严格意义上讲不是无字典的，因为它的输出限制在90K的字典。</span></li><li>2.3.2 <font color=forestgreen>Lexicon-free</font> transcription<span style="font-size:80%;opacity:0.8">2.3.2 无字典转录</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> lexicon-based </td> <td>  </td> <td> 
<ul><li>(3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and <font color=forestgreen>lexicon-based</font> scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>In practice, there exists two modes of transcription, namely the lexicon-free and <font color=forestgreen>lexicon-based</font> transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In <font color=forestgreen>lexicon-based</font> mode, predictions are made by choosing the label sequence that has the highest probability.<span style="font-size:80%;opacity:0.8">在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。</span></li><li>In <font color=forestgreen>lexicon-based</font> mode, each test sample is associated with a lexicon {\cal D}.<span style="font-size:80%;opacity:0.8">在基于字典的模式中，每个测试采样与词典{\cal D}相关联。</span></li><li>Larger \delta results in more candidates, thus more accurate <font color=forestgreen>lexicon-based</font> transcription.<span style="font-size:80%;opacity:0.8">更大的\delta导致更多的候选目标，从而基于词典的转录更准确。</span></li><li>2.3.3 <font color=forestgreen>Lexicon-based</font> transcription<span style="font-size:80%;opacity:0.8">2.3.3 基于词典的转录</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> real-world </td> <td>  </td> <td> 
<ul><li>(4) It generates an effective yet much smaller model, which is more practical for <font color=forestgreen>real-world</font> application scenarios.<span style="font-size:80%;opacity:0.8">（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。</span></li><li>Our network is trained on the synthetic data once, and tested on all other <font color=forestgreen>real-world</font> test datasets without any fine-tuning on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and <font color=forestgreen>real-world</font> data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and <font color=forestgreen>real-world</font> data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li><li>To further speed up CRNN and make it more practical in <font color=forestgreen>real-world</font> applications is another direction that is worthy of exploration in the future.<span style="font-size:80%;opacity:0.8">进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</span></li><li>It contains 200 samples, some of which are shown in Fig. 5. b; 3) “<font color=forestgreen>Real-World</font>”, which contains 200 images of score fragments taken from music books with a phone camera.<span style="font-size:80%;opacity:0.8">它包含200个样本，其中一些如图5.b所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的200张图像。</span></li><li>(c) <font color=forestgreen>Real-world</font> score images taken with a mobile phone camera.<span style="font-size:80%;opacity:0.8">(c)用手机相机拍摄的现实世界的乐谱图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> scenario </td> <td> [səˈnɑ:riəʊ] </td> <td> 
<ul><li>(4) It generates an effective yet much smaller model, which is more practical for real-world application <font color=orangered>scenarios</font>.<span style="font-size:80%;opacity:0.8">（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> IIIT-5K </td> <td>  </td> <td> 
<ul><li>The experiments on standard benchmarks, including the <font color=forestgreen>IIIT-5K</font>, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts.<span style="font-size:80%;opacity:0.8">在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有技术的更有优势。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> ICDAR </td> <td>  </td> <td> 
<ul><li>The experiments on standard benchmarks, including the IIIT-5K, Street View Text and <font color=forestgreen>ICDAR</font> datasets, demonstrate the superiority of the proposed algorithm over the prior arts.<span style="font-size:80%;opacity:0.8">在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有技术的更有优势。</span></li><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely <font color=forestgreen>ICDAR</font> 2003 (IC03), <font color=forestgreen>ICDAR</font> 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> generality </td> <td> [ˌdʒenəˈræləti] </td> <td> 
<ul><li>Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the <font color=orangered>generality</font> of it.<span style="font-size:80%;opacity:0.8">此外，提出的算法在基于图像的音乐配乐识别任务中表现良好，这显然证实了它的泛化性。</span></li><li>To further demonstrate the <font color=orangered>generality</font> of CRNN, we verify the proposed algorithm on a music score recognition task in Sec. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li><li>The results have shown the <font color=orangered>generality</font> of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li><li>In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the <font color=orangered>generality</font> of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> revival </td> <td> [rɪˈvaɪvl] </td> <td> 
<ul><li>Recently, the community has seen a strong <font color=orangered>revival</font> of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (DCNN), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> specifically </td> <td> [spəˈsɪfɪkli] </td> <td> 
<ul><li>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, <font color=orangered>specifically</font> Deep Convolutional Neural Networks (DCNN), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li><li>The main contribution of this paper is a novel neural network model, whose network architecture is <font color=orangered>specifically</font> designed for recognizing sequence-like objects in images.<span style="font-size:80%;opacity:0.8">本文的主要贡献是一种新颖的神经网络模型，其网络架构设计专门用于识别图像中的类序列对象。</span></li><li>The candidates ${\cal N}_{\delta}(\mathbf{l}’)$ can be found efficiently with the BK-tree data structure[9], which is a metric tree <font color=orangered>specifically</font> adapted to discrete metric spaces.<span style="font-size:80%;opacity:0.8">可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。</span></li><li><font color=orangered>Specifically</font>, each feature vector of a feature sequence is generated from left to right on the feature maps by column.<span style="font-size:80%;opacity:0.8">具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。</span></li><li><font color=orangered>Specifically</font>, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> DCNN </td> <td>  </td> <td> 
<ul><li>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (<font color=forestgreen>DCNN</font>), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li><li>Consequently, the most popular deep models like <font color=forestgreen>DCNN</font> [25, 26] cannot be directly applied to sequence prediction, since <font color=forestgreen>DCNN</font> models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li><li>For example, the algorithms in [35, 8] firstly detect individual characters and then recognize these detected characters with <font color=forestgreen>DCNN</font> models, which are trained using labeled character images.<span style="font-size:80%;opacity:0.8">例如，[35,8]中的算法首先检测单个字符，然后用DCNN模型识别这些检测到的字符，并使用标注的字符图像进行训练。</span></li><li>In summary, current systems based on <font color=forestgreen>DCNN</font> can not be directly used for image-based sequence recognition.<span style="font-size:80%;opacity:0.8">总之，目前基于DCNN的系统不能直接用于基于图像的序列识别。</span></li><li>The proposed neural network model is named as Convolutional Recurrent Neural Network (CRNN), since it is a combination of <font color=forestgreen>DCNN</font> and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>2) It has the same property of <font color=forestgreen>DCNN</font> on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li><li>5) It achieves better or highly competitive performance on scene texts (word recognition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard <font color=forestgreen>DCNN</font> model, consuming less storage space.<span style="font-size:80%;opacity:0.8">5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现[23,8]。6）它比标准DCNN模型包含的参数要少得多，占用更少的存储空间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> drastically </td> <td> ['drɑ:stɪklɪ] </td> <td> 
<ul><li>Another unique property of sequence-like objects is that their lengths may vary <font color=orangered>drastically</font>.<span style="font-size:80%;opacity:0.8">类序列对象的另一个独特之处在于它们的长度可能会有很大变化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> congratulations </td> <td> [kənˌgrætjʊ'leɪʃənz] </td> <td> 
<ul><li>For instance, English words can either consist of 2 characters such as “OK” or 15 characters such as “<font color=orangered>congratulations</font>”.<span style="font-size:80%;opacity:0.8">例如，英文单词可以由2个字符组成，如“OK”，或由15个字符组成，如“congratulations”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> incapable </td> <td> [ɪnˈkeɪpəbl] </td> <td> 
<ul><li>Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are <font color=orangered>incapable</font> of producing a variable-length label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> variable-length </td> <td> ['veərɪəbll'eŋθ] </td> <td> 
<ul><li>Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a <font color=orangered>variable-length</font> label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> generalized </td> <td> [ˈdʒenrəlaɪzd] </td> <td> 
<ul><li>It turns out a large trained model with a huge number of classes, which is difficult to be <font color=orangered>generalized</font> to other types of sequence-like objects, such as Chinese texts, musical scores, etc., because the numbers of basic combinations of such kind of sequences can be greater than 1 million.<span style="font-size:80%;opacity:0.8">结果是一个大的训练模型中有很多类，这很难泛化到其它类型的类序列对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于100万。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> geometrical </td> <td> [ˌdʒi:ə'metrɪkl] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of <font color=orangered>geometrical</font> or image features from handwritten texts, while Su and Lu [33] convert word images into sequential HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> handwritten </td> <td> [ˌhændˈrɪtn] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from <font color=orangered>handwritten</font> texts, while Su and Lu [33] convert word images into sequential HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> sequential </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into <font color=orangered>sequential</font> HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li><li>Such component is used to extract a <font color=orangered>sequential</font> feature representation from an input image.<span style="font-size:80%;opacity:0.8">这样的组件用于从输入图像中提取序列特征表示。</span></li><li>In CRNN, we convey deep features into <font color=orangered>sequential</font> representations in order to be invariant to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> hog </td> <td> [hɒg] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into sequential <font color=orangered>HOG</font> features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> conventional </td> <td> [kənˈvenʃənl] </td> <td> 
<ul><li>Several <font color=orangered>conventional</font> scene text recognition methods that are not based on neural networks also brought insightful ideas and novel representations into this field.<span style="font-size:80%;opacity:0.8">一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。</span></li><li>For sequence-like objects, CRNN possesses several distinctive advantages over <font color=orangered>conventional</font> neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>Compared with the <font color=orangered>conventional</font> momentum [31] method, ADADELTA requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>In the 3rd and the 4th max-pooling layers, we adopt 1 × 2 sized rectangular pooling windows instead of the <font color=orangered>conventional</font> squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li><li>Moreover, as CRNN abandons fully connected layers used in <font color=orangered>conventional</font> neural networks, it results in a much more compact and efficient model.<span style="font-size:80%;opacity:0.8">此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。</span></li><li>The experiments on the scene text recognition benchmarks demonstrate that CRNN achieves superior or highly competitive performance, compared with <font color=orangered>conventional</font> methods as well as other CNN and RNN based algorithms.<span style="font-size:80%;opacity:0.8">在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> insightful </td> <td> [ˈɪnsaɪtfʊl] </td> <td> 
<ul><li>Several conventional scene text recognition methods that are not based on neural networks also brought <font color=orangered>insightful</font> ideas and novel representations into this field.<span style="font-size:80%;opacity:0.8">一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> Almazan </td> <td>  </td> <td> 
<ul><li>For example, <font color=forestgreen>Almazan</font> et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> Rodriguez-Serrano </td> <td>  </td> <td> 
<ul><li>For example, Almazan et al. [5] and <font color=forestgreen>Rodriguez-Serrano</font> et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> vectorial </td> <td> [vek'tɒrɪəl] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common <font color=orangered>vectorial</font> subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> subspace </td> <td> ['sʌbspeɪs] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial <font color=orangered>subspace</font>, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> retrieval </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a <font color=orangered>retrieval</font> problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> Gordo </td> <td>  </td> <td> 
<ul><li>Yao et al. [36] and <font color=forestgreen>Gordo</font> et al. [14] used mid-level features for scene text recognition.<span style="font-size:80%;opacity:0.8">Yao等人[36]和Gordo等人[14]使用中层特征进行场景文本识别。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> outperform </td> <td> [ˌaʊtpəˈfɔ:m] </td> <td> 
<ul><li>Though achieved promising performance on standard benchmarks, these methods are generally <font color=orangered>outperformed</font> by previous algorithms based on neural networks [8, 22], as well as the approach proposed in this paper.<span style="font-size:80%;opacity:0.8">虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法[8,22]以及本文提出的方法通常都优于这些方法。</span></li><li>In the constrained lexicon cases, our method consistently <font color=orangered>outperforms</font> most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li><li>The CRNN <font color=orangered>outperforms</font> the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>In addition, CRNN significantly <font color=orangered>outperforms</font> other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> CRNN </td> <td>  </td> <td> 
<ul><li>The proposed neural network model is named as Convolutional Recurrent Neural Network (<font color=forestgreen>CRNN</font>), since it is a combination of DCNN and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>For sequence-like objects, <font color=forestgreen>CRNN</font> possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>The network architecture of <font color=forestgreen>CRNN</font>, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a transcription layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>At the bottom of <font color=forestgreen>CRNN</font>, the convolutional layers automatically extract a feature sequence from each input image.<span style="font-size:80%;opacity:0.8">在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。</span></li><li>The transcription layer at the top of <font color=forestgreen>CRNN</font> is adopted to translate the per-frame predictions by the recurrent layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>Though <font color=forestgreen>CRNN</font> is composed of different kinds of network architectures (eg. CNN and RNN), it can be jointly trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li><li>In <font color=forestgreen>CRNN</font> model, the component of convolutional layers is constructed by taking the convolutional and max-pooling layers from a standard CNN model (fully-connected layers are removed).<span style="font-size:80%;opacity:0.8">在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。</span></li><li>In <font color=forestgreen>CRNN</font>, we convey deep features into sequential representations in order to be invariant to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li><li>To evaluate the effectiveness of the proposed <font color=forestgreen>CRNN</font> model, we conducted experiments on standard benchmarks for scene text recognition and musical score recognition, which are both challenging vision tasks.<span style="font-size:80%;opacity:0.8">为了评估提出的CRNN模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。</span></li><li>The datasets and setting for training and testing are given in Sec. 3.1, the detailed settings of <font color=forestgreen>CRNN</font> for scene text images is provided in Sec. 3.2, and the results with the comprehensive comparisons are reported in Sec. 3.3.<span style="font-size:80%;opacity:0.8">数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。</span></li><li>To further demonstrate the generality of <font color=forestgreen>CRNN</font>, we verify the proposed algorithm on a music score recognition task in Sec. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li><li>Even though the <font color=forestgreen>CRNN</font> model is purely trained with synthetic text data, it works well on real images from standard text recognition benchmarks.<span style="font-size:80%;opacity:0.8">即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</span></li><li>All the recognition accuracies on the above four public datasets, obtained by the proposed <font color=forestgreen>CRNN</font> model and the recent state-of-the-arts techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.<span style="font-size:80%;opacity:0.8">提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</span></li><li>Unlike [22], <font color=forestgreen>CRNN</font> is not limited to recognize a word in a known dictionary, and able to handle random strings (e.g. telephone numbers), sentences or other scripts like Chinese words.<span style="font-size:80%;opacity:0.8">与[22]不同，CRNN不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。</span></li><li>Therefore, the results of <font color=forestgreen>CRNN</font> are competitive on all the testing datasets.<span style="font-size:80%;opacity:0.8"> 因此，CRNN的结果在所有测试数据集上都具有竞争力。</span></li><li>As can be observed from Table 3, only the models based on deep neural networks including [22, 21] as well as <font color=forestgreen>CRNN</font> have this property.<span style="font-size:80%;opacity:0.8">从表3可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。</span></li><li>As the input and output labels of <font color=forestgreen>CRNN</font> can be a sequence, character-level annotations are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li><li>In <font color=forestgreen>CRNN</font>, all layers have weight-sharing connections, and the fully-connected layers are not needed.<span style="font-size:80%;opacity:0.8">在CRNN中，所有的层有权重共享连接，不需要全连接层。</span></li><li>Consequently, the number of parameters of <font color=forestgreen>CRNN</font> is much less than the models learned on the variants of CNN [22, 21], resulting in a much smaller model compared with [22, 21].<span style="font-size:80%;opacity:0.8">因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。</span></li><li>Table 3 clearly shows the differences among different approaches in details, and fully demonstrates the advantages of <font color=forestgreen>CRNN</font> over other competing methods.<span style="font-size:80%;opacity:0.8">表3详细列出了不同方法之间的差异，充分展示了CRNN与其它竞争方法的优势。</span></li><li>We cast the OMR as a sequence recognition problem, and predict a sequence of musical notes directly from the image with <font color=forestgreen>CRNN</font>.<span style="font-size:80%;opacity:0.8">我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。</span></li><li>To prepare the training data needed by <font color=forestgreen>CRNN</font>, we collect 2650 images from [2].<span style="font-size:80%;opacity:0.8">为了准备CRNN所需的训练数据，我们从[2]中收集了2650张图像。</span></li><li>Since we have limited training data, we use a simplified <font color=forestgreen>CRNN</font> configuration in order to reduce model capacity.<span style="font-size:80%;opacity:0.8">由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。</span></li><li>The <font color=forestgreen>CRNN</font> outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The <font color=forestgreen>CRNN</font>, on the other hand, uses convolutional features that are highly robust to noises and distortions.<span style="font-size:80%;opacity:0.8">另一方面，CRNN使用对噪声和扭曲具有鲁棒性的卷积特征。</span></li><li>Besides, recurrent layers in <font color=forestgreen>CRNN</font> can utilize contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li><li>Comparison of pitch recognition accuracies, among <font color=forestgreen>CRNN</font> and two commercial OMR systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>The results have shown the generality of <font color=forestgreen>CRNN</font>, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li><li>In this paper, we have presented a novel neural network architecture, called Convolutional Recurrent Neural Network (<font color=forestgreen>CRNN</font>), which integrates the advantages of both Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).<span style="font-size:80%;opacity:0.8">在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。</span></li><li><font color=forestgreen>CRNN</font> is able to take input images of varying dimensions and produces predictions with different lengths.<span style="font-size:80%;opacity:0.8">CRNN能够获取不同尺寸的输入图像，并产生不同长度的预测。</span></li><li>Moreover, as <font color=forestgreen>CRNN</font> abandons fully connected layers used in conventional neural networks, it results in a much more compact and efficient model.<span style="font-size:80%;opacity:0.8">此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。</span></li><li>All these properties make <font color=forestgreen>CRNN</font> an excellent approach for image-based sequence recognition.<span style="font-size:80%;opacity:0.8">所有这些属性使得CRNN成为一种基于图像序列识别的极好方法。</span></li><li>The experiments on the scene text recognition benchmarks demonstrate that <font color=forestgreen>CRNN</font> achieves superior or highly competitive performance, compared with conventional methods as well as other CNN and RNN based algorithms.<span style="font-size:80%;opacity:0.8">在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。</span></li><li>In addition, <font color=forestgreen>CRNN</font> significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of <font color=forestgreen>CRNN</font>.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li><li>Actually, <font color=forestgreen>CRNN</font> is a general framework, thus it can be applied to other domains and problems (such as Chinese character recognition), which involve sequence prediction in images.<span style="font-size:80%;opacity:0.8">实际上，CRNN是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。</span></li><li>To further speed up <font color=forestgreen>CRNN</font> and make it more practical in real-world applications is another direction that is worthy of exploration in the future.<span style="font-size:80%;opacity:0.8">进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> annotation </td> <td> [ˌænə'teɪʃn] </td> <td> 
<ul><li>For sequence-like objects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed <font color=orangered>annotations</font> (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level <font color=orangered>annotations</font> for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li><li>CharGT-Free: This column is to indicate whether the character-level <font color=orangered>annotations</font> are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li><li>As the input and output labels of CRNN can be a sequence, character-level <font color=orangered>annotations</font> are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li><li>It directly runs on coarse level labels (e.g. words), requiring no detailed <font color=orangered>annotations</font> for each individual element (e.g. characters) in the training phase.<span style="font-size:80%;opacity:0.8">它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> informative </td> <td> [ɪnˈfɔ:mətɪv] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning <font color=orangered>informative</font> representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> hand-craft </td> <td> ['hæn(d)krɑːft] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither <font color=orangered>hand-craft</font> features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> binarization </td> <td>  </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including <font color=forestgreen>binarization</font>/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li><li>The main reason is that they rely on robust <font color=forestgreen>binarization</font> to detect staff lines and notes, but the <font color=forestgreen>binarization</font> step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> localization </td> <td> [ˌləʊkəlaɪ'zeɪʃn] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component <font color=orangered>localization</font>, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> unconstrained </td> <td> [ˌʌnkən'streɪnd] </td> <td> 
<ul><li>3) It has the same property of RNN, being able to produce a sequence of labels; 4) It is <font color=orangered>unconstrained</font> to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases;<span style="font-size:80%;opacity:0.8">3）具有与RNN相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；</span></li><li>In the <font color=orangered>unconstrained</font> lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Note that the blanks in the “none” columns of Table 2 denote that such approaches are unable to be applied to recognition without lexicon or did not report the recognition accuracies in the <font color=orangered>unconstrained</font> cases.<span style="font-size:80%;opacity:0.8">注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。</span></li><li>The best persformance is reported by [22] in the <font color=orangered>unconstrained</font> lexicon cases, benefiting from its large dictionary, however, it is not a model strictly <font color=orangered>unconstrained</font> to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li><li>In this sense, our results in the <font color=orangered>unconstrained</font> lexicon case are still promising.<span style="font-size:80%;opacity:0.8">在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</span></li><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, CharGT-Free, <font color=orangered>Unconstrained</font>, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (<font color=orangered>Unconstrained</font>); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=orangered>Unconstrained</font>: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling out-of-dictionary words or random sequences.<span style="font-size:80%;opacity:0.8">Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> recurrent </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the <font color=orangered>recurrent</font> layers, and a transcription layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>2) <font color=orangered>recurrent</font> layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence.<span style="font-size:80%;opacity:0.8">2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</span></li><li>On top of the convolutional network, a <font color=orangered>recurrent</font> network is built for making prediction for each frame of the feature sequence, outputted by the convolutional layers.<span style="font-size:80%;opacity:0.8">在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。</span></li><li>The transcription layer at the top of CRNN is adopted to translate the per-frame predictions by the <font color=orangered>recurrent</font> layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>Then a sequence of feature vectors is extracted from the feature maps produced by the component of convolutional layers, which is the input for the <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。</span></li><li>A deep bidirectional Recurrent Neural Network is built on the top of the convolutional layers, as the <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>The <font color=orangered>recurrent</font> layers predict a label distribution $y_t$ for each frame $x_t$ in the feature sequence $x = x_1,…,x_T$.<span style="font-size:80%;opacity:0.8">循环层预测特征序列$x = x_1,…,x_T$中每一帧$x_t$的标签分布$y_t$。</span></li><li>The advantages of the <font color=orangered>recurrent</font> layers are three-fold.<span style="font-size:80%;opacity:0.8">循环层的优点是三重的。</span></li><li>Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the <font color=orangered>recurrent</font> layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li><li>In <font color=orangered>recurrent</font> layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the <font color=orangered>recurrent</font> layers, the sequence of propagated differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li><li>In practice, we create a custom network layer, called “Map-to-Sequence”, as the bridge between convolutional layers and <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</span></li><li>where $\mathbf{y}_{i}$ is the sequence produced by the <font color=orangered>recurrent</font> and convolutional layers from $I_{i}$.<span style="font-size:80%;opacity:0.8">$\mathbf{y}_{i}$是循环层和卷积层从I{i}$生成的序列。</span></li><li>In the <font color=orangered>recurrent</font> layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li><li>The network not only has deep convolutional layers, but also has <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">网络不仅有深度卷积层，而且还有循环层。</span></li><li>Besides, <font color=orangered>recurrent</font> layers in CRNN can utilize contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li><li><font color=orangered>Recurrent</font> neural networks (RNN) models, another important branch of the deep neural networks family, were mainly designed for handling sequences.<span style="font-size:80%;opacity:0.8">循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要是设计来处理序列。</span></li><li>The proposed neural network model is named as Convolutional <font color=orangered>Recurrent</font> Neural Network (CRNN), since it is a combination of DCNN and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>A deep bidirectional <font color=orangered>Recurrent</font> Neural Network is built on the top of the convolutional layers, as the recurrent layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>In this paper, we have presented a novel neural network architecture, called Convolutional <font color=orangered>Recurrent</font> Neural Network (CRNN), which integrates the advantages of both Convolutional Neural Networks (CNN) and <font color=orangered>Recurrent</font> Neural Networks (RNN).<span style="font-size:80%;opacity:0.8">在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> outputted </td> <td> ['aʊt.pʊt] </td> <td> 
<ul><li>On top of the convolutional network, a recurrent network is built for making prediction for each frame of the feature sequence, <font color=orangered>outputted</font> by the convolutional layers.<span style="font-size:80%;opacity:0.8">在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> eg </td> <td>  </td> <td> 
<ul><li>Though CRNN is composed of different kinds of network architectures (<font color=forestgreen>eg</font>. CNN and RNN), it can be jointly trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> jointly </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>Though CRNN is composed of different kinds of network architectures (eg. CNN and RNN), it can be <font color=orangered>jointly</font> trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li><li>Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to <font color=orangered>jointly</font> train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> max-pooling </td> <td>  </td> <td> 
<ul><li>In CRNN model, the component of convolutional layers is constructed by taking the convolutional and <font color=forestgreen>max-pooling</font> layers from a standard CNN model (fully-connected layers are removed).<span style="font-size:80%;opacity:0.8">在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。</span></li><li>As the layers of convolution, <font color=forestgreen>max-pooling</font>, and element-wise activation function operate on local regions, they are translation invariant.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li><li>In the 3rd and the 4th <font color=forestgreen>max-pooling</font> layers, we adopt 1 × 2 sized rectangular pooling windows instead of the conventional squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> vector </td> <td> [ˈvektə(r)] </td> <td> 
<ul><li>Then a sequence of feature <font color=orangered>vectors</font> is extracted from the feature maps produced by the component of convolutional layers, which is the input for the recurrent layers.<span style="font-size:80%;opacity:0.8">然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。</span></li><li>Specifically, each feature <font color=orangered>vector</font> of a feature sequence is generated from left to right on the feature maps by column.<span style="font-size:80%;opacity:0.8">具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。</span></li><li>This means the i-th feature <font color=orangered>vector</font> is the concatenation of the i-th columns of all the maps.<span style="font-size:80%;opacity:0.8">这意味着第i个特征向量是所有特征图第i列的连接。</span></li><li>As illustrated in Fig. 2, each <font color=orangered>vector</font> in the feature sequence is associated with a receptive field, and can be considered as the image descriptor for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li><li>Each <font color=orangered>vector</font> in the extracted feature sequence is associated with a receptive field on the input image, and can be considered as the feature <font color=orangered>vector</font> of that field.<span style="font-size:80%;opacity:0.8">提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> concatenation </td> <td> [kənˌkætəˈneɪʃn] </td> <td> 
<ul><li>This means the i-th feature vector is the <font color=orangered>concatenation</font> of the i-th columns of all the maps.<span style="font-size:80%;opacity:0.8">这意味着第i个特征向量是所有特征图第i列的连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> activation </td> <td> [ˌæktɪ'veɪʃn] </td> <td> 
<ul><li>As the layers of convolution, max-pooling, and element-wise <font color=orangered>activation</font> function operate on local regions, they are translation invariant.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> invariant </td> <td> [ɪnˈveəriənt] </td> <td> 
<ul><li>As the layers of convolution, max-pooling, and element-wise activation function operate on local regions, they are translation <font color=orangered>invariant</font>.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li><li>In CRNN, we convey deep features into sequential representations in order to be <font color=orangered>invariant</font> to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> receptive </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>Therefore, each column of the feature maps corresponds to a rectangle region of the original image (termed the <font color=orangered>receptive</font> field), and such rectangle regions are in the same order to their corresponding columns on the feature maps from left to right.<span style="font-size:80%;opacity:0.8">因此，特征图的每列对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。</span></li><li>As illustrated in Fig. 2, each vector in the feature sequence is associated with a <font color=orangered>receptive</font> field, and can be considered as the image descriptor for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li><li>The <font color=orangered>receptive</font> field.<span style="font-size:80%;opacity:0.8">感受野。</span></li><li>Each vector in the extracted feature sequence is associated with a <font color=orangered>receptive</font> field on the input image, and can be considered as the feature vector of that field.<span style="font-size:80%;opacity:0.8">提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</span></li><li>On top of that, the rectangular pooling windows yield rectangular <font color=orangered>receptive</font> fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.<span style="font-size:80%;opacity:0.8">最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如i和l。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> descriptor </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>As illustrated in Fig. 2, each vector in the feature sequence is associated with a receptive field, and can be considered as the image <font color=orangered>descriptor</font> for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> holistic </td> <td> [həʊˈlɪstɪk] </td> <td> 
<ul><li>However, these approaches usually extract <font color=orangered>holistic</font> representation of the whole image by CNN, then the local deep features are collected for recognizing each component of a sequence-like object.<span style="font-size:80%;opacity:0.8">然而，这些方法通常通过CNN提取整个图像的整体表示，然后收集局部深度特征来识别类序列对象的每个分量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> bidirectional </td> <td> [ˌbaɪdəˈrekʃənl] </td> <td> 
<ul><li>A deep <font color=orangered>bidirectional</font> Recurrent Neural Network is built on the top of the convolutional layers, as the recurrent layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>(b) The structure of deep <font color=orangered>bidirectional</font> LSTM we use in our paper.<span style="font-size:80%;opacity:0.8">（b）我们论文中使用的深度双向LSTM结构。</span></li><li>Combining a forward (left to right) and a backward (right to left) LSTMs results in a <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">合并前向（从左到右）和后向（从右到左）LSTM的结果到双向LSTM中。</span></li><li>Stacking multiple <font color=orangered>bidirectional</font> LSTM results in a deep <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">在深度双向LSTM中堆叠多个双向LSTM结果。</span></li><li>Therefore, we follow [17] and combine two LSTMs, one forward and one backward, into a <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">因此，我们遵循[17]，将两个LSTM，一个向前和一个向后组合到一个双向LSTM中。</span></li><li>Furthermore, multiple <font color=orangered>bidirectional</font> LSTMs can be stacked, resulting in a deep <font color=orangered>bidirectional</font> LSTM as illustrated in Fig. 3. b.<span style="font-size:80%;opacity:0.8">此外，可以堆叠多个双向LSTM，得到如图3.b所示的深双向LSTM。</span></li><li>Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer <font color=orangered>bidirectional</font> LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> capability </td> <td> [ˌkeɪpəˈbɪləti] </td> <td> 
<ul><li>Firstly, RNN has a strong <font color=orangered>capability</font> of capturing contextual information within a sequence.<span style="font-size:80%;opacity:0.8">首先，RNN具有很强的捕获序列内上下文信息的能力。</span></li><li>But it provides a new scheme for OMR, and has shown promising <font color=orangered>capabilities</font> in pitch recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> contextual </td> <td> [kənˈtekstʃuəl] </td> <td> 
<ul><li>Firstly, RNN has a strong capability of capturing <font color=orangered>contextual</font> information within a sequence.<span style="font-size:80%;opacity:0.8">首先，RNN具有很强的捕获序列内上下文信息的能力。</span></li><li>Using <font color=orangered>contextual</font> cues for image-based sequence recognition is more stable and helpful than treating each symbol independently.<span style="font-size:80%;opacity:0.8">对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。</span></li><li>Besides, recurrent layers in CRNN can utilize <font color=orangered>contextual</font> information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> cue </td> <td> [kju:] </td> <td> 
<ul><li>Using contextual <font color=orangered>cues</font> for image-based sequence recognition is more stable and helpful than treating each symbol independently.<span style="font-size:80%;opacity:0.8">对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> il </td> <td>  </td> <td> 
<ul><li>Besides, some ambiguous characters are easier to distinguish when observing their contexts, e.g. it is easier to recognize “<font color=forestgreen>il</font>” by contrasting the character heights than by recognizing each of them separately.<span style="font-size:80%;opacity:0.8">此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> Secondly </td> <td> [ˈsekəndli] </td> <td> 
<ul><li><font color=orangered>Secondly</font>, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> back-propagates </td> <td>  </td> <td> 
<ul><li>Secondly, RNN can <font color=forestgreen>back-propagates</font> error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> differential </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>Secondly, RNN can back-propagates error <font color=orangered>differentials</font> to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li><li>In recurrent layers, error <font color=orangered>differentials</font> are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the recurrent layers, the sequence of propagated <font color=orangered>differentials</font> are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li><li>In particular, in the transcription layer, error <font color=orangered>differentials</font> are back-propagated with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li><li>In the recurrent layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error <font color=orangered>differentials</font>.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> traverse </td> <td> [trəˈvɜ:s] </td> <td> 
<ul><li>Thirdly, RNN is able to operate on sequences of arbitrary lengths, <font color=orangered>traversing</font> from starts to ends.<span style="font-size:80%;opacity:0.8">第三，RNN能够从头到尾对任意长度的序列进行操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> utilize </td> <td> [ˈju:təlaɪz] </td> <td> 
<ul><li>In this way, past contexts $\lbrace x_{t\prime} \rbrace _{t \prime < t}$ are captured and <font color=orangered>utilized</font> for prediction.<span style="font-size:80%;opacity:0.8">以这种方式，过去的上下文${\lbrace x_{t\prime} \rbrace _{t \prime < t}$被捕获并用于预测。</span></li><li>Besides, recurrent layers in CRNN can <font color=orangered>utilize</font> contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> Long-Short </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Long-Short</font> Term Memory 18, 11 is a type of RNN unit that is specially designed to address this problem.<span style="font-size:80%;opacity:0.8">长短时记忆[18,11]（LSTM）是一种专门设计用于解决这个问题的RNN单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> multiplicative </td> <td> ['mʌltɪplɪkeɪtɪv] </td> <td> 
<ul><li>An LSTM (illustrated in Fig. 3) consists of a memory cell and three <font color=orangered>multiplicative</font> gates, namely the input, output and forget gates.<span style="font-size:80%;opacity:0.8">LSTM（图3所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> Conceptually </td> <td> [kən'septʃʊəlɪ] </td> <td> 
<ul><li><font color=orangered>Conceptually</font>, the memory cell stores the past contexts, and the input and output gates allow the cell to store contexts for a long period of time.<span style="font-size:80%;opacity:0.8">在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> long-range </td> <td> [lɒŋ reɪndʒ] </td> <td> 
<ul><li>The special design of LSTM allows it to capture <font color=orangered>long-range</font> dependencies, which often occur in image-based sequences.<span style="font-size:80%;opacity:0.8">LSTM的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> directional </td> <td> [dəˈrekʃənl] </td> <td> 
<ul><li>LSTM is <font color=orangered>directional</font>, it only uses past contexts.<span style="font-size:80%;opacity:0.8">LSTM是定向的，它只使用过去的上下文。</span></li><li>Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single <font color=orangered>directional</font> LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> complementary </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>However, in image-based sequences, contexts from both directions are useful and <font color=orangered>complementary</font> to each other.<span style="font-size:80%;opacity:0.8">然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> propagate </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>In recurrent layers, error differentials are <font color=orangered>propagated</font> in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the recurrent layers, the sequence of <font color=orangered>propagated</font> differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> BPTT </td> <td>  </td> <td> 
<ul><li>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (<font color=forestgreen>BPTT</font>).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>In the recurrent layers, the Back-Propagation Through Time (<font color=forestgreen>BPTT</font>) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> concatenate </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>At the bottom of the recurrent layers, the sequence of propagated differentials are <font color=orangered>concatenated</font> into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> invert </td> <td> [ɪnˈvɜ:t] </td> <td> 
<ul><li>At the bottom of the recurrent layers, the sequence of propagated differentials are concatenated into maps, <font color=orangered>inverting</font> the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> Map-to-Sequence </td> <td>  </td> <td> 
<ul><li>In practice, we create a custom network layer, called “<font color=forestgreen>Map-to-Sequence</font>”, as the bridge between convolutional layers and recurrent layers.<span style="font-size:80%;opacity:0.8">实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> Mathematically </td> <td> [ˌmæθə'mætɪklɪ] </td> <td> 
<ul><li><font color=orangered>Mathematically</font>, transcription is to find the label sequence with the highest probability conditioned on the per-frame predictions.<span style="font-size:80%;opacity:0.8">数学上，转录是根据每帧预测找到具有最高概率的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> conditional </td> <td> [kənˈdɪʃənl] </td> <td> 
<ul><li>We adopt the <font color=orangered>conditional</font> probability defined in the Connectionist Temporal Classification (CTC) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li><li>The formulation of the <font color=orangered>conditional</font> probability is briefly described as follows: The input is a sequence $y = y_1,…,y_T$ where T is the sequence length.<span style="font-size:80%;opacity:0.8">条件概率的公式简要描述如下：输入是序列$y = y_1,…,y_T$，其中T是序列长度。</span></li><li>Then, the <font color=orangered>conditional</font> probability is defined as the sum of probabilities of all $\boldsymbol{\pi}$ that are mapped by ${\cal B}$ onto $\mathbf{l}$:<span style="font-size:80%;opacity:0.8">然后，条件概率被定义为由${\cal B}$映射到$\mathbf{l}$上的所有$\boldsymbol{\pi}$的概率之和：</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest <font color=orangered>conditional</font> probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>The objective is to minimize the negative log-likelihood of <font color=orangered>conditional</font> probability of ground truth:<span style="font-size:80%;opacity:0.8">目标是最小化真实条件概率的负对数似然：</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> Connectionist </td> <td> [kə'nekʃənɪst] </td> <td> 
<ul><li>We adopt the conditional probability defined in the <font color=orangered>Connectionist</font> Temporal Classification (CTC) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> ctc </td> <td>  </td> <td> 
<ul><li>We adopt the conditional probability defined in the Connectionist Temporal Classification (<font color=forestgreen>CTC</font>) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> log-likelihood </td> <td>  </td> <td> 
<ul><li>Consequently, when we use the negative <font color=forestgreen>log-likelihood</font> of this probability as the objective to train the network, we only need images and their corresponding label sequences, avoiding the labor of labeling positions of individual characters.<span style="font-size:80%;opacity:0.8">因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。</span></li><li>The objective is to minimize the negative <font color=forestgreen>log-likelihood</font> of conditional probability of ground truth:<span style="font-size:80%;opacity:0.8">目标是最小化真实条件概率的负对数似然：</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> sequence-to-sequence </td> <td>  </td> <td> 
<ul><li>A <font color=forestgreen>sequence-to-sequence</font> mapping function ${\cal B}$ is defined on sequence $\boldsymbol{\pi}\in{\cal L}’^{T}$, where T is the length.<span style="font-size:80%;opacity:0.8">序列到序列的映射函数${\cal B}$定义在序列$\boldsymbol{\pi}\in{\cal L}’^{T}$上，其中T是长度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> -hh-e-l-ll-oo- </td> <td>  </td> <td> 
<ul><li>For example, B maps “–hh-e-l-ll-oo–” (’-’ represents ’blank’) onto “hello”.<span style="font-size:80%;opacity:0.8">例如，${\cal B}$将“–hh-e-l-ll-oo–”（-表示blank）映射到“hello”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> stampt </td> <td>  </td> <td> 
<ul><li>where the probability of $\boldsymbol{\pi}$ is defined as $p(\boldsymbol{\pi}|\mathbf{y})=\prod{t=1}^{T}y{\pi{t}}^{t},y{\pi{t}}^{t}$ is the probability of having label $\pi{t}$ at time <font color=forestgreen>stampt</font>.<span style="font-size:80%;opacity:0.8">$\boldsymbol{\pi}$的概率定义为$p(\boldsymbol{\pi}|\mathbf{y})=\prod{t=1}^{T}y{\pi{t}}^{t}，y{\pi{t}}^{t}$是时刻t时有标签$\pi{t}$的概率。</span></li><li>The sequence $\mathbf{l}^{*}$ is approximately found by $\mathbf{l}^{*}\approx{\cal B}(\arg\max{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$, i.e. taking the most probable label $\pi{t}$ at each time <font color=forestgreen>stampt</font>, and map the resulted sequence onto $\mathbf{l}^{*}$.<span style="font-size:80%;opacity:0.8">序列$\mathbf{l}^{*}$通过$\mathbf{l}^{*}\approx{\cal B}(\arg\max{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$近似发现，即在每个时间戳t采用最大概率的标签$\pi{t}$，并将结果序列映射到$\mathbf{l}^{*}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> eq </td> <td>  </td> <td> 
<ul><li>Directly computing <font color=forestgreen>Eq</font>.1 would be computationally infeasible due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li><li>However, <font color=forestgreen>Eq</font>.1 can be efficiently computed using the forward-backward algorithm described in [15].<span style="font-size:80%;opacity:0.8">然而，使用[15]中描述的前向算法可以有效计算方程1。</span></li><li>In this mode, the sequence $\mathbf{l}^{*}$ that has the highest probability as defined in <font color=forestgreen>Eq</font>.1 is taken as the prediction.<span style="font-size:80%;opacity:0.8">在这种模式下，将具有方程1中定义的最高概率的序列$\mathbf{l}^{*}$作为预测。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in <font color=forestgreen>Eq</font>.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, to test the impact of parameter \delta, we experiment different values of \delta in <font color=forestgreen>Eq</font>.2.<span style="font-size:80%;opacity:0.8">另外，为了测试参数\delta的影响，我们在方程2中实验了\delta的不同值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> computationally </td> <td>  </td> <td> 
<ul><li>Directly computing Eq.1 would be <font color=forestgreen>computationally</font> infeasible due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> infeasible </td> <td> [ɪn'fi:zəbl] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally <font color=orangered>infeasible</font> due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> exponentially </td> <td> [ˌekspə'nenʃəlɪ] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally infeasible due to the <font color=orangered>exponentially</font> large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> summation </td> <td> [sʌˈmeɪʃn] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally infeasible due to the exponentially large number of <font color=orangered>summation</font> items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> forward-backward </td> <td>  </td> <td> 
<ul><li>However, Eq.1 can be efficiently computed using the <font color=forestgreen>forward-backward</font> algorithm described in [15].<span style="font-size:80%;opacity:0.8">然而，使用[15]中描述的前向算法可以有效计算方程1。</span></li><li>In particular, in the transcription layer, error differentials are back-propagated with the <font color=forestgreen>forward-backward</font> algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> tractable </td> <td> [ˈtræktəbl] </td> <td> 
<ul><li>Since there exists no <font color=orangered>tractable</font> algorithm to precisely find the solution, we use the strategy adopted in [15].<span style="font-size:80%;opacity:0.8">由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> k-words </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Each image has been associated to a 50-words lexicon and a 1k-words lexicon.<span style="font-size:80%;opacity:0.8">每张图像关联一个50词的词典和一个1000词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> Hunspell </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words <font color=forestgreen>Hunspell</font> spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, we use a 50k words lexicon consisting of the words in the <font color=forestgreen>Hunspell</font> spell-checking dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> spell-checking </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell <font color=forestgreen>spell-checking</font> dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, we use a 50k words lexicon consisting of the words in the Hunspell <font color=forestgreen>spell-checking</font> dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> time-consuming </td> <td> [taɪm kən'sju:mɪŋ] </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very <font color=orangered>time-consuming</font> to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> exhaustive </td> <td> [ɪgˈzɔ:stɪv] </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an <font color=orangered>exhaustive</font> search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> nearest-neighbor </td> <td> ['nɪərɪstn'eɪbɔ:] </td> <td> 
<ul><li>This indicates that we can limit our search to the <font color=orangered>nearest-neighbor</font> candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> maximal </td> <td> [ˈmæksɪml] </td> <td> 
<ul><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the <font color=orangered>maximal</font> edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> transcribe </td> <td> [trænˈskraɪb] </td> <td> 
<ul><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence <font color=orangered>transcribed</font> from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> BK-tree </td> <td>  </td> <td> 
<ul><li>The candidates ${\cal N}_{\delta}(\mathbf{l}’)$ can be found efficiently with the <font color=forestgreen>BK-tree</font> data structure[9], which is a metric tree specifically adapted to discrete metric spaces.<span style="font-size:80%;opacity:0.8">可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。</span></li><li>The search time complexity of <font color=forestgreen>BK-tree</font> is $O(\log|{\cal D}|)$, where $|{\cal D}|$ is the lexicon size.<span style="font-size:80%;opacity:0.8">BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。</span></li><li>In our approach, a <font color=forestgreen>BK-tree</font> is constructed offline for a lexicon.<span style="font-size:80%;opacity:0.8">在我们的方法中，一个词典离线构造一个BK树。</span></li><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the <font color=forestgreen>BK-tree</font> data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li><li>On the other hand, the computational cost grows with larger \delta, due to longer <font color=forestgreen>BK-tree</font> search time, as well as larger number of candidate sequences for testing.<span style="font-size:80%;opacity:0.8">另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着\delta的增大而增加。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> stochastic </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>The network is trained with <font color=orangered>stochastic</font> gradient descent (SGD).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> descent </td> <td> [dɪˈsent] </td> <td> 
<ul><li>The network is trained with stochastic gradient <font color=orangered>descent</font> (SGD).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>The network is trained with stochastic gradient descent (<font color=orangered>SGD</font>).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> back-propagation </td> <td>  </td> <td> 
<ul><li>Gradients are calculated by the <font color=forestgreen>back-propagation</font> algorithm.<span style="font-size:80%;opacity:0.8">梯度由反向传播算法计算。</span></li><li>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. <font color=forestgreen>Back-Propagation</font> Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>In the recurrent layers, the <font color=forestgreen>Back-Propagation</font> Through Time (BPTT) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> back-propagated </td> <td>  </td> <td> 
<ul><li>In particular, in the transcription layer, error differentials are <font color=forestgreen>back-propagated</font> with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> ADADELTA </td> <td>  </td> <td> 
<ul><li>For optimization, we use the <font color=forestgreen>ADADELTA</font> [37] to automatically calculate per-dimension learning rates.<span style="font-size:80%;opacity:0.8">为了优化，我们使用ADADELTA[37]自动计算每维的学习率。</span></li><li>Compared with the conventional momentum [31] method, <font color=forestgreen>ADADELTA</font> requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>More importantly, we find that optimization using <font color=forestgreen>ADADELTA</font> converges faster than the momentum method.<span style="font-size:80%;opacity:0.8">更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</span></li><li>Networks are trained with <font color=forestgreen>ADADELTA</font>, setting the parameter ρ to 0.9.<span style="font-size:80%;opacity:0.8">网络用ADADELTA训练，将参数ρ设置为0.9。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>Compared with the conventional <font color=orangered>momentum</font> [31] method, ADADELTA requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>More importantly, we find that optimization using ADADELTA converges faster than the <font color=orangered>momentum</font> method.<span style="font-size:80%;opacity:0.8">更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> sec </td> <td> [sek] </td> <td> 
<ul><li>The datasets and setting for training and testing are given in <font color=orangered>Sec</font>. 3.1, the detailed settings of CRNN for scene text images is provided in <font color=orangered>Sec</font>. 3.2, and the results with the comprehensive comparisons are reported in <font color=orangered>Sec</font>. 3.3.<span style="font-size:80%;opacity:0.8">数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。</span></li><li>To further demonstrate the generality of CRNN, we verify the proposed algorithm on a music score recognition task in <font color=orangered>Sec</font>. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> synthetic </td> <td> [sɪnˈθetɪk] </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the <font color=orangered>synthetic</font> dataset (Synth) released by Jaderberg et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li><li>Such images are generated by a <font color=orangered>synthetic</font> text engine and are highly realistic.<span style="font-size:80%;opacity:0.8">这样的图像由合成文本引擎生成并且是非常现实的。</span></li><li>Our network is trained on the <font color=orangered>synthetic</font> data once, and tested on all other real-world test datasets without any fine-tuning on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li><li>Even though the CRNN model is purely trained with <font color=orangered>synthetic</font> text data, it works well on real images from standard text recognition benchmarks.<span style="font-size:80%;opacity:0.8">即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</span></li><li>Our method uses only <font color=orangered>synthetic</font> text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> synth </td> <td> [sɪnθ] </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the synthetic dataset (<font color=orangered>Synth</font>) released by Jaderberg et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> Jaderberg </td> <td>  </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the synthetic dataset (Synth) released by <font color=forestgreen>Jaderberg</font> et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> fine-tune </td> <td> [faɪn tju:n] </td> <td> 
<ul><li>Our network is trained on the synthetic data once, and tested on all other real-world test datasets without any <font color=orangered>fine-tuning</font> on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> IC </td> <td> [ai'si:] </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li>IC03 [27] test dataset contains 251 scene images with labeled text bounding boxes.<span style="font-size:80%;opacity:0.8">IC03[27]测试数据集包含251个具有标记文本边界框的场景图像。</span></li><li>IC13 [24] test dataset inherits most of its data from IC03.<span style="font-size:80%;opacity:0.8">IC13[24]测试数据集继承了IC03中的大部分数据。</span></li><li>The average testing time is 0.16s/sample, as measured on IC03 without a lexicon.<span style="font-size:80%;opacity:0.8">平均测试时间为0.16s/样本，在IC03上测得的，没有词典。</span></li><li>The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter δ set to 3.<span style="font-size:80%;opacity:0.8">近似词典搜索应用于IC03的50k词典，参数δ设置为3。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Tested on the IC03 dataset with the 50k lexicon.<span style="font-size:80%;opacity:0.8">在IC03数据集上使用50k词典进行的测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> IIIT </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), <font color=forestgreen>IIIT</font> 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> k-word </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> IIIT5k </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (<font color=forestgreen>IIIT5k</font>), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li><font color=forestgreen>IIIT5k</font> [28] contains 3,000 cropped word test images collected from the Internet.<span style="font-size:80%;opacity:0.8">IIIT5k[28]包含从互联网收集的3000张裁剪的词测试图像。</span></li><li>Specifically, we obtain superior performance on <font color=forestgreen>IIIT5k</font>, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> SVT </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (<font color=forestgreen>SVT</font>).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li><font color=forestgreen>SVT</font> [34] test dataset consists of 249 street view images collected from Google Street View.<span style="font-size:80%;opacity:0.8">SVT[34]测试数据集由从Google街景视图收集的249张街景图像组成。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and <font color=forestgreen>SVT</font> compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained lexicon cases, our method achieves the best performance on <font color=forestgreen>SVT</font>, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>IC03 [27] test dataset contains 251 scene images with labeled text <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8">IC03[27]测试数据集包含251个具有标记文本边界框的场景图像。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth <font color=orangered>bounding</font> boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> non-alphanumeric </td> <td>  </td> <td> 
<ul><li>Following Wang et al. [34], we ignore images that either contain <font color=forestgreen>non-alphanumeric</font> characters or have less than three characters, and get a test set with 860 cropped text images.<span style="font-size:80%;opacity:0.8">王等人[34]，我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有860个裁剪的文本图像的测试集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> configuration </td> <td> [kənˌfɪgəˈreɪʃn] </td> <td> 
<ul><li>The network <font color=orangered>configuration</font> we use in our experiments is summarized in Table 1.<span style="font-size:80%;opacity:0.8">在实验中我们使用的网络配置总结在表1中。</span></li><li>Network <font color=orangered>configuration</font> summary.<span style="font-size:80%;opacity:0.8">网络配置总结。</span></li><li>Since we have limited training data, we use a simplified CRNN <font color=orangered>configuration</font> in order to reduce model capacity.<span style="font-size:80%;opacity:0.8">由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。</span></li><li>Different from the <font color=orangered>configuration</font> specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> VGG-VeryDeep </td> <td>  </td> <td> 
<ul><li>The architecture of the convolutional layers is based on the <font color=forestgreen>VGG-VeryDeep</font> architectures [32].<span style="font-size:80%;opacity:0.8">卷积层的架构是基于VGG-VeryDeep的架构[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> tweak </td> <td> [twi:k] </td> <td> 
<ul><li>A <font color=orangered>tweak</font> is made in order to make it suitable for recognizing English texts.<span style="font-size:80%;opacity:0.8">为了使其适用于识别英文文本，对其进行了调整。</span></li><li>This <font color=orangered>tweak</font> yields feature maps with larger width, hence longer feature sequence.<span style="font-size:80%;opacity:0.8">这种调整产生宽度较大的特征图，因此具有更长的特征序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> rectangular </td> <td> [rek'tæŋɡjələ(r)] </td> <td> 
<ul><li>In the 3rd and the 4th max-pooling layers, we adopt 1 × 2 sized <font color=orangered>rectangular</font> pooling windows instead of the conventional squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li><li>On top of that, the <font color=orangered>rectangular</font> pooling windows yield <font color=orangered>rectangular</font> receptive fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.<span style="font-size:80%;opacity:0.8">最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如i和l。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> CUDA </td> <td>  </td> <td> 
<ul><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/<font color=forestgreen>CUDA</font>), the transcription layer (in C++) and the BK-tree data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> GHz </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 <font color=forestgreen>GHz</font> Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> Xeon </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) <font color=forestgreen>Xeon</font>(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> GB </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> ram </td> <td> [ræm] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB <font color=orangered>RAM</font> and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li><li>Our model has 8.3 million parameters, taking only 33MB <font color=orangered>RAM</font> (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.<span style="font-size:80%;opacity:0.8">我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> NVIDIA </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an <font color=orangered>NVIDIA</font>(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> Tesla </td> <td> ['teslә] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) <font color=orangered>Tesla</font>(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> TM </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(<font color=forestgreen>TM</font>) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> convergence </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>The training process takes about 50 hours to reach <font color=orangered>convergence</font>.<span style="font-size:80%;opacity:0.8">训练过程大约需要50个小时才能达到收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> proportionally </td> <td> [prə'pɔ:ʃənlɪ] </td> <td> 
<ul><li>Widths are <font color=orangered>proportionally</font> scaled with heights, but at least 100 pixels.<span style="font-size:80%;opacity:0.8">宽度与高度成比例地缩放，但至少为100像素。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> comparative </td> <td> [kəmˈpærətɪv] </td> <td> 
<ul><li>3.3. <font color=orangered>Comparative</font> Evaluation<span style="font-size:80%;opacity:0.8">3.3. 比较评估</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> state-of-the-arts </td> <td>  </td> <td> 
<ul><li>All the recognition accuracies on the above four public datasets, obtained by the proposed CRNN model and the recent <font color=forestgreen>state-of-the-arts</font> techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.<span style="font-size:80%;opacity:0.8">提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</span></li><li>In the constrained lexicon cases, our method consistently outperforms most <font color=forestgreen>state-of-the-arts</font> approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> consistently </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>In the constrained lexicon cases, our method <font color=orangered>consistently</font> outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> PhotoOCR </td> <td>  </td> <td> 
<ul><li>Our method uses only synthetic text with word level labels as the training data, very different to <font color=forestgreen>PhotoOCR</font> [8] which used 7.9 millions of real word images with character-level annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> character-level </td> <td>  </td> <td> 
<ul><li>Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with <font color=forestgreen>character-level</font> annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li><li>CharGT-Free: This column is to indicate whether the <font color=forestgreen>character-level</font> annotations are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li><li>As the input and output labels of CRNN can be a sequence, <font color=forestgreen>character-level</font> annotations are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> persformance </td> <td>  </td> <td> 
<ul><li>The best <font color=forestgreen>persformance</font> is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> strictly </td> <td> [ˈstrɪktli] </td> <td> 
<ul><li>The best persformance is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model <font color=orangered>strictly</font> unconstrained to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> E2E </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named <font color=forestgreen>E2E</font> Train, Conv Ftrs, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (<font color=forestgreen>E2E</font> Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=forestgreen>E2E</font> Train: This column is to show whether a certain text reading model is end-to-end trainable, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training.<span style="font-size:80%;opacity:0.8">E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> Ftrs </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv <font color=forestgreen>Ftrs</font>, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv <font color=forestgreen>Ftrs</font>); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li>Conv <font color=forestgreen>Ftrs</font>: This column is to indicate whether an approach uses the convolutional features learned from training images directly or handcraft features as the basic representations.<span style="font-size:80%;opacity:0.8">Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> CharGT-Free </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, <font color=forestgreen>CharGT-Free</font>, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (<font color=forestgreen>CharGT-Free</font>); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=forestgreen>CharGT-Free</font>: This column is to indicate whether the character-level annotations are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> hand-crafted </td> <td> [,hænd 'kra:ftid] </td> <td> 
<ul><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using <font color=orangered>hand-crafted</font> ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> handcraft </td> <td> [ˈhændkrɑ:ft] </td> <td> 
<ul><li>Conv Ftrs: This column is to indicate whether an approach uses the convolutional features learned from training images directly or <font color=orangered>handcraft</font> features as the basic representations.<span style="font-size:80%;opacity:0.8">Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> out-of-dictionary </td> <td>  </td> <td> 
<ul><li>Unconstrained: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling <font color=forestgreen>out-of-dictionary</font> words or random sequences.<span style="font-size:80%;opacity:0.8">Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> incremental </td> <td> [ˌɪŋkrə'mentl] </td> <td> 
<ul><li>Notice that though the recent models learned by label embedding [5, 14] and <font color=orangered>incremental</font> learning [22] achieved highly competitive performance, they are constrained to a specific dictionary.<span style="font-size:80%;opacity:0.8">注意尽管最近通过标签嵌入[5, 14]和增强学习[22]学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> weight-sharing </td> <td>  </td> <td> 
<ul><li>In CRNN, all layers have <font color=forestgreen>weight-sharing</font> connections, and the fully-connected layers are not needed.<span style="font-size:80%;opacity:0.8">在CRNN中，所有的层有权重共享连接，不需要全连接层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> variant </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Consequently, the number of parameters of CRNN is much less than the models learned on the <font color=orangered>variants</font> of CNN [22, 21], resulting in a much smaller model compared with [22, 21].<span style="font-size:80%;opacity:0.8">因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> mb </td> <td>  </td> <td> 
<ul><li>Our model has 8.3 million parameters, taking only 33MB RAM (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.<span style="font-size:80%;opacity:0.8">我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</span></li></ul>
 </td>
</tr>
<tr>
<td> 154 </td> <td> computational </td> <td> [ˌkɒmpjuˈteɪʃənl] </td> <td> 
<ul><li>On the other hand, the <font color=orangered>computational</font> cost grows with larger \delta, due to longer BK-tree search time, as well as larger number of candidate sequences for testing.<span style="font-size:80%;opacity:0.8">另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着\delta的增大而增加。</span></li></ul>
 </td>
</tr>
<tr>
<td> 155 </td> <td> tradeoff </td> <td> ['treɪdˌɔ:f] </td> <td> 
<ul><li>In practice, we choose \delta=3 as a <font color=orangered>tradeoff</font> between accuracy and speed.<span style="font-size:80%;opacity:0.8">实际上，我们选择\delta=3作为精度和速度之间的折衷。</span></li></ul>
 </td>
</tr>
<tr>
<td> 156 </td> <td> OMR </td> <td>  </td> <td> 
<ul><li>Recognizing musical scores in images is known as the Optical Music Recognition (<font color=forestgreen>OMR</font>) problem.<span style="font-size:80%;opacity:0.8">识别图像中的乐谱被称为光学音乐识别（OMR）问题。</span></li><li>We cast the <font color=forestgreen>OMR</font> as a sequence recognition problem, and predict a sequence of musical notes directly from the image with CRNN.<span style="font-size:80%;opacity:0.8">我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。</span></li><li>For comparison, we evaluate two commercial <font color=forestgreen>OMR</font> engines, namely the Capella Scan [3] and the PhotoScore [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>Comparison of pitch recognition accuracies, among CRNN and two commercial <font color=forestgreen>OMR</font> systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>But it provides a new scheme for <font color=forestgreen>OMR</font>, and has shown promising capabilities in pitch recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li><li>In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (<font color=forestgreen>OMR</font>), which verifies the generality of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 157 </td> <td> binirization </td> <td>  </td> <td> 
<ul><li>Previous methods often requires image preprocessing (mostly <font color=forestgreen>binirization</font>), staff lines detection and individual notes recognition [29].<span style="font-size:80%;opacity:0.8">以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别[29]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 158 </td> <td> pitch </td> <td> [pɪtʃ] </td> <td> 
<ul><li>For simplicity, we recognize <font color=orangered>pitches</font> only, ignore all chords and assume the same major scales (C major) for all scores.<span style="font-size:80%;opacity:0.8">为了简单起见，我们仅认识音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C大调）。</span></li><li>To the best of our knowledge, there exists no public datasets for evaluating algorithms on <font color=orangered>pitch</font> recognition.<span style="font-size:80%;opacity:0.8">据我们所知，没有用于评估音调识别算法的公共数据集。</span></li><li>Two measures are used for evaluating the recognition performance: 1) fragment accuracy, i.e. the percentage of score fragments correctly recognized; 2) average edit distance, i.e. the average edit distance between predicted <font color=orangered>pitch</font> sequences and the ground truths.<span style="font-size:80%;opacity:0.8">使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。</span></li><li>Comparison of <font color=orangered>pitch</font> recognition accuracies, among CRNN and two commercial OMR systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>But it provides a new scheme for OMR, and has shown promising capabilities in <font color=orangered>pitch</font> recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 159 </td> <td> ezpitches </td> <td>  </td> <td> 
<ul><li>We manually label the ground truth label sequences (sequences of not <font color=forestgreen>ezpitches</font>) for all the images.<span style="font-size:80%;opacity:0.8">我们手动标记所有图像的真实标签序列（不是的音调序列）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 160 </td> <td> augment </td> <td> [ɔ:gˈment] </td> <td> 
<ul><li>The collected images are <font color=orangered>augmented</font> to 265k training samples by being rotated, scaled and corrupted with noise, and by replacing their backgrounds with natural images.<span style="font-size:80%;opacity:0.8">收集到的图像通过旋转，缩放和用噪声损坏增强到了265k个训练样本，并用自然图像替换它们的背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 161 </td> <td> synthesize </td> <td> [ˈsɪnθəsaɪz] </td> <td> 
<ul><li>a; 2) “<font color=orangered>Synthesized</font>”, which is created from “Clean”, using the augmentation strategy mentioned above.<span style="font-size:80%;opacity:0.8">实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。</span></li><li>Figure 5. (a) Clean musical scores images collected from [2] (b) <font color=orangered>Synthesized</font> musical score images.<span style="font-size:80%;opacity:0.8">图5。(a)从[2]中收集的干净的乐谱图像。(b)合成的乐谱图像。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on <font color=orangered>synthesized</font> and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on <font color=orangered>synthesized</font> and real-world data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 162 </td> <td> augmentation </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>a; 2) “Synthesized”, which is created from “Clean”, using the <font color=orangered>augmentation</font> strategy mentioned above.<span style="font-size:80%;opacity:0.8">实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。</span></li></ul>
 </td>
</tr>
<tr>
<td> 163 </td> <td> tab </td> <td> [tæb] </td> <td> 
<ul><li>Different from the configuration specified in <font color=orangered>Tab</font>. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li><li><font color=orangered>Tab</font>.<span style="font-size:80%;opacity:0.8">表4总结了结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 164 </td> <td> Capella </td> <td> [kəˈpelə] </td> <td> 
<ul><li>For comparison, we evaluate two commercial OMR engines, namely the <font color=orangered>Capella</font> Scan [3] and the PhotoScore [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The <font color=orangered>Capella</font> Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>Compared with <font color=orangered>Capella</font> Scan and PhotoScore, our CRNN-based system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 165 </td> <td> PhotoScore </td> <td>  </td> <td> 
<ul><li>For comparison, we evaluate two commercial OMR engines, namely the Capella Scan [3] and the <font color=forestgreen>PhotoScore</font> [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and <font color=forestgreen>PhotoScore</font> systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>Compared with Capella Scan and <font color=forestgreen>PhotoScore</font>, our CRNN-based system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 166 </td> <td> clutter </td> <td> [ˈklʌtə(r)] </td> <td> 
<ul><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and <font color=orangered>cluttered</font> background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 167 </td> <td> minimal </td> <td> [ˈmɪnɪməl] </td> <td> 
<ul><li>The results have shown the generality of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring <font color=orangered>minimal</font> domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li></ul>
 </td>
</tr>
<tr>
<td> 168 </td> <td> CRNN-based </td> <td>  </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our <font color=forestgreen>CRNN-based</font> system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 169 </td> <td> preliminary </td> <td> [prɪˈlɪmɪnəri] </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our CRNN-based system is still <font color=orangered>preliminary</font> and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 170 </td> <td> functionality </td> <td> [ˌfʌŋkʃəˈnæləti] </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our CRNN-based system is still preliminary and misses many <font color=orangered>functionalities</font>.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> CRNN<br>(37) </td> <td>  </td> <td> 
<ul><li>The proposed neural network model is named as Convolutional Recurrent Neural Network (<font color=forestgreen>CRNN</font>), since it is a combination of DCNN and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>For sequence-like objects, <font color=forestgreen>CRNN</font> possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>The network architecture of <font color=forestgreen>CRNN</font>, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a transcription layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>At the bottom of <font color=forestgreen>CRNN</font>, the convolutional layers automatically extract a feature sequence from each input image.<span style="font-size:80%;opacity:0.8">在CRNN的底部，卷积层自动从每个输入图像中提取特征序列。</span></li><li>The transcription layer at the top of <font color=forestgreen>CRNN</font> is adopted to translate the per-frame predictions by the recurrent layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>Though <font color=forestgreen>CRNN</font> is composed of different kinds of network architectures (eg. CNN and RNN), it can be jointly trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li><li>In <font color=forestgreen>CRNN</font> model, the component of convolutional layers is constructed by taking the convolutional and max-pooling layers from a standard CNN model (fully-connected layers are removed).<span style="font-size:80%;opacity:0.8">在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。</span></li><li>In <font color=forestgreen>CRNN</font>, we convey deep features into sequential representations in order to be invariant to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li><li>To evaluate the effectiveness of the proposed <font color=forestgreen>CRNN</font> model, we conducted experiments on standard benchmarks for scene text recognition and musical score recognition, which are both challenging vision tasks.<span style="font-size:80%;opacity:0.8">为了评估提出的CRNN模型的有效性，我们在场景文本识别和乐谱识别的标准基准数据集上进行了实验，这些都是具有挑战性的视觉任务。</span></li><li>The datasets and setting for training and testing are given in Sec. 3.1, the detailed settings of <font color=forestgreen>CRNN</font> for scene text images is provided in Sec. 3.2, and the results with the comprehensive comparisons are reported in Sec. 3.3.<span style="font-size:80%;opacity:0.8">数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。</span></li><li>To further demonstrate the generality of <font color=forestgreen>CRNN</font>, we verify the proposed algorithm on a music score recognition task in Sec. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li><li>Even though the <font color=forestgreen>CRNN</font> model is purely trained with synthetic text data, it works well on real images from standard text recognition benchmarks.<span style="font-size:80%;opacity:0.8">即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</span></li><li>All the recognition accuracies on the above four public datasets, obtained by the proposed <font color=forestgreen>CRNN</font> model and the recent state-of-the-arts techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.<span style="font-size:80%;opacity:0.8">提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</span></li><li>Unlike [22], <font color=forestgreen>CRNN</font> is not limited to recognize a word in a known dictionary, and able to handle random strings (e.g. telephone numbers), sentences or other scripts like Chinese words.<span style="font-size:80%;opacity:0.8">与[22]不同，CRNN不限于识别已知字典中的单词，并且能够处理随机字符串（例如电话号码），句子或其他诸如中文单词的脚本。</span></li><li>Therefore, the results of <font color=forestgreen>CRNN</font> are competitive on all the testing datasets.<span style="font-size:80%;opacity:0.8"> 因此，CRNN的结果在所有测试数据集上都具有竞争力。</span></li><li>As can be observed from Table 3, only the models based on deep neural networks including [22, 21] as well as <font color=forestgreen>CRNN</font> have this property.<span style="font-size:80%;opacity:0.8">从表3可以看出，只有基于深度神经网络的模型，包括[22,21]以及CRNN具有这种性质。</span></li><li>As the input and output labels of <font color=forestgreen>CRNN</font> can be a sequence, character-level annotations are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li><li>In <font color=forestgreen>CRNN</font>, all layers have weight-sharing connections, and the fully-connected layers are not needed.<span style="font-size:80%;opacity:0.8">在CRNN中，所有的层有权重共享连接，不需要全连接层。</span></li><li>Consequently, the number of parameters of <font color=forestgreen>CRNN</font> is much less than the models learned on the variants of CNN [22, 21], resulting in a much smaller model compared with [22, 21].<span style="font-size:80%;opacity:0.8">因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。</span></li><li>Table 3 clearly shows the differences among different approaches in details, and fully demonstrates the advantages of <font color=forestgreen>CRNN</font> over other competing methods.<span style="font-size:80%;opacity:0.8">表3详细列出了不同方法之间的差异，充分展示了CRNN与其它竞争方法的优势。</span></li><li>We cast the OMR as a sequence recognition problem, and predict a sequence of musical notes directly from the image with <font color=forestgreen>CRNN</font>.<span style="font-size:80%;opacity:0.8">我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。</span></li><li>To prepare the training data needed by <font color=forestgreen>CRNN</font>, we collect 2650 images from [2].<span style="font-size:80%;opacity:0.8">为了准备CRNN所需的训练数据，我们从[2]中收集了2650张图像。</span></li><li>Since we have limited training data, we use a simplified <font color=forestgreen>CRNN</font> configuration in order to reduce model capacity.<span style="font-size:80%;opacity:0.8">由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。</span></li><li>The <font color=forestgreen>CRNN</font> outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The <font color=forestgreen>CRNN</font>, on the other hand, uses convolutional features that are highly robust to noises and distortions.<span style="font-size:80%;opacity:0.8">另一方面，CRNN使用对噪声和扭曲具有鲁棒性的卷积特征。</span></li><li>Besides, recurrent layers in <font color=forestgreen>CRNN</font> can utilize contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li><li>Comparison of pitch recognition accuracies, among <font color=forestgreen>CRNN</font> and two commercial OMR systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>The results have shown the generality of <font color=forestgreen>CRNN</font>, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li><li>In this paper, we have presented a novel neural network architecture, called Convolutional Recurrent Neural Network (<font color=forestgreen>CRNN</font>), which integrates the advantages of both Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN).<span style="font-size:80%;opacity:0.8">在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。</span></li><li><font color=forestgreen>CRNN</font> is able to take input images of varying dimensions and produces predictions with different lengths.<span style="font-size:80%;opacity:0.8">CRNN能够获取不同尺寸的输入图像，并产生不同长度的预测。</span></li><li>Moreover, as <font color=forestgreen>CRNN</font> abandons fully connected layers used in conventional neural networks, it results in a much more compact and efficient model.<span style="font-size:80%;opacity:0.8">此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。</span></li><li>All these properties make <font color=forestgreen>CRNN</font> an excellent approach for image-based sequence recognition.<span style="font-size:80%;opacity:0.8">所有这些属性使得CRNN成为一种基于图像序列识别的极好方法。</span></li><li>The experiments on the scene text recognition benchmarks demonstrate that <font color=forestgreen>CRNN</font> achieves superior or highly competitive performance, compared with conventional methods as well as other CNN and RNN based algorithms.<span style="font-size:80%;opacity:0.8">在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。</span></li><li>In addition, <font color=forestgreen>CRNN</font> significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of <font color=forestgreen>CRNN</font>.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li><li>Actually, <font color=forestgreen>CRNN</font> is a general framework, thus it can be applied to other domains and problems (such as Chinese character recognition), which involve sequence prediction in images.<span style="font-size:80%;opacity:0.8">实际上，CRNN是一个通用框架，因此可以应用于其它的涉及图像序列预测的领域和问题（如汉字识别）。</span></li><li>To further speed up <font color=forestgreen>CRNN</font> and make it more practical in real-world applications is another direction that is worthy of exploration in the future.<span style="font-size:80%;opacity:0.8">进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> lexicon<br>(32) </td> <td> [ˈleksɪkən] </td> <td> 
<ul><li>(3) It is not confined to any predefined <font color=orangered>lexicon</font> and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>A <font color=orangered>lexicon</font> is a set of label sequences that prediction is constraint to, e.g. a spell checking dictionary.<span style="font-size:80%;opacity:0.8">词典是一组标签序列，预测受拼写检查字典约束。</span></li><li>In lexicon-free mode, predictions are made without any <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在无词典模式中，预测时没有任何词典。</span></li><li>In lexicon-based mode, each test sample is associated with a <font color=orangered>lexicon</font> {\cal D}.<span style="font-size:80%;opacity:0.8">在基于字典的模式中，每个测试采样与词典{\cal D}相关联。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the <font color=orangered>lexicon</font> that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the <font color=orangered>lexicon</font>, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large <font color=orangered>lexicons</font>, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the <font color=orangered>lexicon</font> that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the <font color=orangered>lexicon</font>, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>1 for all sequences in the <font color=orangered>lexicon</font> and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>The search time complexity of BK-tree is $O(\log|{\cal D}|)$, where $|{\cal D}|$ is the <font color=orangered>lexicon</font> size.<span style="font-size:80%;opacity:0.8">BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。</span></li><li>Therefore this scheme readily extends to very large <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">因此，这个方案很容易扩展到非常大的词典。</span></li><li>In our approach, a BK-tree is constructed offline for a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在我们的方法中，一个词典离线构造一个BK树。</span></li><li>Each test image is associated with a 50-words <font color=orangered>lexicon</font> which is defined by Wang et al. [34].<span style="font-size:80%;opacity:0.8">每张测试图像与由Wang等人[34]定义的50词的词典相关联。</span></li><li>A full <font color=orangered>lexicon</font> is built by combining all the per-image lexicons.<span style="font-size:80%;opacity:0.8">通过组合所有的每张图像词汇构建完整的词典。</span></li><li>A full lexicon is built by combining all the per-image <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">通过组合所有的每张图像词汇构建完整的词典。</span></li><li>In addition, we use a 50k words <font color=orangered>lexicon</font> consisting of the words in the Hunspell spell-checking dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li><li>Each image has been associated to a 50-words <font color=orangered>lexicon</font> and a 1k-words <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">每张图像关联一个50词的词典和一个1000词的词典。</span></li><li>Each word image has a 50 words <font color=orangered>lexicon</font> defined by Wang et al. [34].<span style="font-size:80%;opacity:0.8">每张单词图像都有一个由Wang等人[34]定义的50个词的词典。</span></li><li>The average testing time is 0.16s/sample, as measured on IC03 without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">平均测试时间为0.16s/样本，在IC03上测得的，没有词典。</span></li><li>The approximate <font color=orangered>lexicon</font> search is applied to the 50k <font color=orangered>lexicon</font> of IC03, with the parameter δ set to 3.<span style="font-size:80%;opacity:0.8">近似词典搜索应用于IC03的50k词典，参数δ设置为3。</span></li><li>In the second row, “50”, “1k”, “50k” and “Full” denote the <font color=orangered>lexicon</font> used, and “None” denotes recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在第二行，“50”，“1k”，“50k”和“Full”表示使用的字典，“None”表示识别没有字典。</span></li><li>In the constrained <font color=orangered>lexicon</font> cases, our method consistently outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained <font color=orangered>lexicon</font> cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Note that the blanks in the “none” columns of Table 2 denote that such approaches are unable to be applied to recognition without <font color=orangered>lexicon</font> or did not report the recognition accuracies in the unconstrained cases.<span style="font-size:80%;opacity:0.8">注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。</span></li><li>The best persformance is reported by [22] in the unconstrained <font color=orangered>lexicon</font> cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a <font color=orangered>lexicon</font> as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li><li>In this sense, our results in the unconstrained <font color=orangered>lexicon</font> case are still promising.<span style="font-size:80%;opacity:0.8">在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</span></li><li>Red bars: <font color=orangered>lexicon</font> search time per sample.<span style="font-size:80%;opacity:0.8">红条：每个样本的词典搜索时间。</span></li><li>Tested on the IC03 dataset with the 50k <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在IC03数据集上使用50k词典进行的测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> recurrent<br>(21) </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the <font color=orangered>recurrent</font> layers, and a transcription layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>2) <font color=orangered>recurrent</font> layers, which predict a label distribution for each frame; 3) transcription layer, which translates the per-frame predictions into the final label sequence.<span style="font-size:80%;opacity:0.8">2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</span></li><li>On top of the convolutional network, a <font color=orangered>recurrent</font> network is built for making prediction for each frame of the feature sequence, outputted by the convolutional layers.<span style="font-size:80%;opacity:0.8">在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。</span></li><li>The transcription layer at the top of CRNN is adopted to translate the per-frame predictions by the <font color=orangered>recurrent</font> layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>Then a sequence of feature vectors is extracted from the feature maps produced by the component of convolutional layers, which is the input for the <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。</span></li><li>A deep bidirectional Recurrent Neural Network is built on the top of the convolutional layers, as the <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>The <font color=orangered>recurrent</font> layers predict a label distribution $y_t$ for each frame $x_t$ in the feature sequence $x = x_1,…,x_T$.<span style="font-size:80%;opacity:0.8">循环层预测特征序列$x = x_1,…,x_T$中每一帧$x_t$的标签分布$y_t$。</span></li><li>The advantages of the <font color=orangered>recurrent</font> layers are three-fold.<span style="font-size:80%;opacity:0.8">循环层的优点是三重的。</span></li><li>Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the <font color=orangered>recurrent</font> layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li><li>In <font color=orangered>recurrent</font> layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the <font color=orangered>recurrent</font> layers, the sequence of propagated differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li><li>In practice, we create a custom network layer, called “Map-to-Sequence”, as the bridge between convolutional layers and <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</span></li><li>where $\mathbf{y}_{i}$ is the sequence produced by the <font color=orangered>recurrent</font> and convolutional layers from $I_{i}$.<span style="font-size:80%;opacity:0.8">$\mathbf{y}_{i}$是循环层和卷积层从I{i}$生成的序列。</span></li><li>In the <font color=orangered>recurrent</font> layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li><li>The network not only has deep convolutional layers, but also has <font color=orangered>recurrent</font> layers.<span style="font-size:80%;opacity:0.8">网络不仅有深度卷积层，而且还有循环层。</span></li><li>Besides, <font color=orangered>recurrent</font> layers in CRNN can utilize contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li><li><font color=orangered>Recurrent</font> neural networks (RNN) models, another important branch of the deep neural networks family, were mainly designed for handling sequences.<span style="font-size:80%;opacity:0.8">循环神经网络（RNN）模型是深度神经网络家族中的另一个重要分支，主要是设计来处理序列。</span></li><li>The proposed neural network model is named as Convolutional <font color=orangered>Recurrent</font> Neural Network (CRNN), since it is a combination of DCNN and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>A deep bidirectional <font color=orangered>Recurrent</font> Neural Network is built on the top of the convolutional layers, as the recurrent layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>In this paper, we have presented a novel neural network architecture, called Convolutional <font color=orangered>Recurrent</font> Neural Network (CRNN), which integrates the advantages of both Convolutional Neural Networks (CNN) and <font color=orangered>Recurrent</font> Neural Networks (RNN).<span style="font-size:80%;opacity:0.8">在本文中，我们提出了一种新颖的神经网络架构，称为卷积循环神经网络（CRNN），其集成了卷积神经网络（CNN）和循环神经网络（RNN）的优点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> transcription<br>(15) </td> <td> [trænˈskrɪpʃn] </td> <td> 
<ul><li>A novel neural network architecture, which integrates feature extraction, sequence modeling and <font color=orangered>transcription</font> into a unified framework, is proposed.<span style="font-size:80%;opacity:0.8">提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。</span></li><li>The network architecture of CRNN, as shown in Fig. 1, consists of three components, including the convolutional layers, the recurrent layers, and a <font color=orangered>transcription</font> layer, from bottom to top.<span style="font-size:80%;opacity:0.8">如图1所示，CRNN的网络架构由三部分组成，包括卷积层，循环层和转录层，从底向上。</span></li><li>2) recurrent layers, which predict a label distribution for each frame; 3) <font color=orangered>transcription</font> layer, which translates the per-frame predictions into the final label sequence.<span style="font-size:80%;opacity:0.8">2) 循环层，预测每一帧的标签分布；3) 转录层，将每一帧的预测变为最终的标签序列。</span></li><li>The <font color=orangered>transcription</font> layer at the top of CRNN is adopted to translate the per-frame predictions by the recurrent layers into a label sequence.<span style="font-size:80%;opacity:0.8">采用CRNN顶部的转录层将循环层的每帧预测转化为标签序列。</span></li><li>2.3. <font color=orangered>Transcription</font><span style="font-size:80%;opacity:0.8">2.3. 转录</span></li><li><font color=orangered>Transcription</font> is the process of converting the per-frame predictions made by RNN into a label sequence.<span style="font-size:80%;opacity:0.8">转录是将RNN所做的每帧预测转换成标签序列的过程。</span></li><li>Mathematically, <font color=orangered>transcription</font> is to find the label sequence with the highest probability conditioned on the per-frame predictions.<span style="font-size:80%;opacity:0.8">数学上，转录是根据每帧预测找到具有最高概率的标签序列。</span></li><li>In practice, there exists two modes of <font color=orangered>transcription</font>, namely the lexicon-free and lexicon-based transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In practice, there exists two modes of transcription, namely the lexicon-free and lexicon-based <font color=orangered>transcriptions</font>.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>2.3.2 Lexicon-free <font color=orangered>transcription</font><span style="font-size:80%;opacity:0.8">2.3.2 无字典转录</span></li><li>2.3.3 Lexicon-based <font color=orangered>transcription</font><span style="font-size:80%;opacity:0.8">2.3.3 基于词典的转录</span></li><li>1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via lexicon-free <font color=orangered>transcription</font>, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>In particular, in the <font color=orangered>transcription</font> layer, error differentials are back-propagated with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the <font color=orangered>transcription</font> layer (in C++) and the BK-tree data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li><li>Larger \delta results in more candidates, thus more accurate lexicon-based <font color=orangered>transcription</font>.<span style="font-size:80%;opacity:0.8">更大的\delta导致更多的候选目标，从而基于词典的转录更准确。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> IC<br>(11) </td> <td> [ai'si:] </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li>IC03 [27] test dataset contains 251 scene images with labeled text bounding boxes.<span style="font-size:80%;opacity:0.8">IC03[27]测试数据集包含251个具有标记文本边界框的场景图像。</span></li><li>IC13 [24] test dataset inherits most of its data from IC03.<span style="font-size:80%;opacity:0.8">IC13[24]测试数据集继承了IC03中的大部分数据。</span></li><li>The average testing time is 0.16s/sample, as measured on IC03 without a lexicon.<span style="font-size:80%;opacity:0.8">平均测试时间为0.16s/样本，在IC03上测得的，没有词典。</span></li><li>The approximate lexicon search is applied to the 50k lexicon of IC03, with the parameter δ set to 3.<span style="font-size:80%;opacity:0.8">近似词典搜索应用于IC03的50k词典，参数δ设置为3。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Tested on the IC03 dataset with the 50k lexicon.<span style="font-size:80%;opacity:0.8">在IC03数据集上使用50k词典进行的测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> unconstrained<br>(9) </td> <td> [ˌʌnkən'streɪnd] </td> <td> 
<ul><li>3) It has the same property of RNN, being able to produce a sequence of labels; 4) It is <font color=orangered>unconstrained</font> to the lengths of sequence-like objects, requiring only height normalization in both training and testing phases;<span style="font-size:80%;opacity:0.8">3）具有与RNN相同的性质，能够产生一系列标签；4）对类序列对象的长度无约束，只需要在训练阶段和测试阶段对高度进行归一化；</span></li><li>In the <font color=orangered>unconstrained</font> lexicon cases, our method achieves the best performance on SVT, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li><li>Note that the blanks in the “none” columns of Table 2 denote that such approaches are unable to be applied to recognition without lexicon or did not report the recognition accuracies in the <font color=orangered>unconstrained</font> cases.<span style="font-size:80%;opacity:0.8">注意，表2的“none”列中的空白表示这种方法不能应用于没有词典的识别，或者在无约束的情况下不能报告识别精度。</span></li><li>The best persformance is reported by [22] in the <font color=orangered>unconstrained</font> lexicon cases, benefiting from its large dictionary, however, it is not a model strictly <font color=orangered>unconstrained</font> to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li><li>In this sense, our results in the <font color=orangered>unconstrained</font> lexicon case are still promising.<span style="font-size:80%;opacity:0.8">在这个意义上，我们在无限制词典表中的结果仍然是有前途的。</span></li><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, CharGT-Free, <font color=orangered>Unconstrained</font>, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (<font color=orangered>Unconstrained</font>); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=orangered>Unconstrained</font>: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling out-of-dictionary words or random sequences.<span style="font-size:80%;opacity:0.8">Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> bidirectional<br>(9) </td> <td> [ˌbaɪdəˈrekʃənl] </td> <td> 
<ul><li>A deep <font color=orangered>bidirectional</font> Recurrent Neural Network is built on the top of the convolutional layers, as the recurrent layers.<span style="font-size:80%;opacity:0.8">一个深度双向循环神经网络是建立在卷积层的顶部，作为循环层。</span></li><li>(b) The structure of deep <font color=orangered>bidirectional</font> LSTM we use in our paper.<span style="font-size:80%;opacity:0.8">（b）我们论文中使用的深度双向LSTM结构。</span></li><li>Combining a forward (left to right) and a backward (right to left) LSTMs results in a <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">合并前向（从左到右）和后向（从右到左）LSTM的结果到双向LSTM中。</span></li><li>Stacking multiple <font color=orangered>bidirectional</font> LSTM results in a deep <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">在深度双向LSTM中堆叠多个双向LSTM结果。</span></li><li>Therefore, we follow [17] and combine two LSTMs, one forward and one backward, into a <font color=orangered>bidirectional</font> LSTM.<span style="font-size:80%;opacity:0.8">因此，我们遵循[17]，将两个LSTM，一个向前和一个向后组合到一个双向LSTM中。</span></li><li>Furthermore, multiple <font color=orangered>bidirectional</font> LSTMs can be stacked, resulting in a deep <font color=orangered>bidirectional</font> LSTM as illustrated in Fig. 3. b.<span style="font-size:80%;opacity:0.8">此外，可以堆叠多个双向LSTM，得到如图3.b所示的深双向LSTM。</span></li><li>Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer <font color=orangered>bidirectional</font> LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> DCNN<br>(8) </td> <td>  </td> <td> 
<ul><li>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (<font color=forestgreen>DCNN</font>), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li><li>Consequently, the most popular deep models like <font color=forestgreen>DCNN</font> [25, 26] cannot be directly applied to sequence prediction, since <font color=forestgreen>DCNN</font> models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a variable-length label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li><li>For example, the algorithms in [35, 8] firstly detect individual characters and then recognize these detected characters with <font color=forestgreen>DCNN</font> models, which are trained using labeled character images.<span style="font-size:80%;opacity:0.8">例如，[35,8]中的算法首先检测单个字符，然后用DCNN模型识别这些检测到的字符，并使用标注的字符图像进行训练。</span></li><li>In summary, current systems based on <font color=forestgreen>DCNN</font> can not be directly used for image-based sequence recognition.<span style="font-size:80%;opacity:0.8">总之，目前基于DCNN的系统不能直接用于基于图像的序列识别。</span></li><li>The proposed neural network model is named as Convolutional Recurrent Neural Network (CRNN), since it is a combination of <font color=forestgreen>DCNN</font> and RNN.<span style="font-size:80%;opacity:0.8">所提出的神经网络模型被称为卷积循环神经网络（CRNN），因为它是DCNN和RNN的组合。</span></li><li>2) It has the same property of <font color=forestgreen>DCNN</font> on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li><li>5) It achieves better or highly competitive performance on scene texts (word recognition) than the prior arts [23, 8]; 6) It contains much less parameters than a standard <font color=forestgreen>DCNN</font> model, consuming less storage space.<span style="font-size:80%;opacity:0.8">5）与现有技术相比，它在场景文本（字识别）上获得更好或更具竞争力的表现[23,8]。6）它比标准DCNN模型包含的参数要少得多，占用更少的存储空间。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> lexicon-free<br>(7) </td> <td>  </td> <td> 
<ul><li>(3) It is not confined to any predefined lexicon and achieves remarkable performances in both <font color=forestgreen>lexicon-free</font> and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>In practice, there exists two modes of transcription, namely the <font color=forestgreen>lexicon-free</font> and lexicon-based transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In <font color=forestgreen>lexicon-free</font> mode, predictions are made without any lexicon.<span style="font-size:80%;opacity:0.8">在无词典模式中，预测时没有任何词典。</span></li><li>1 for all sequences in the lexicon and choose the one with the highest probability. To solve this problem, we observe that the label sequences predicted via <font color=forestgreen>lexicon-free</font> transcription, described in 2.3.2, are often close to the ground-truth under the edit distance metric.<span style="font-size:80%;opacity:0.8">然而，对于大型词典，例如5万个词的Hunspell拼写检查词典[1]，对词典进行详尽的搜索是非常耗时的，即对词典中的所有序列计算方程1，并选择概率最高的一个。为了解决这个问题，我们观察到，2.3.2中描述的通过无词典转录预测的标签序列通常在编辑距离度量下接近于实际结果。</span></li><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in <font color=forestgreen>lexicon-free</font> mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li><li>*[22] is not <font color=forestgreen>lexicon-free</font> in the strict sense, as its outputs are constrained to a 90k dictionary.<span style="font-size:80%;opacity:0.8">*[22]严格意义上讲不是无字典的，因为它的输出限制在90K的字典。</span></li><li>2.3.2 <font color=forestgreen>Lexicon-free</font> transcription<span style="font-size:80%;opacity:0.8">2.3.2 无字典转录</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> real-world<br>(7) </td> <td>  </td> <td> 
<ul><li>(4) It generates an effective yet much smaller model, which is more practical for <font color=forestgreen>real-world</font> application scenarios.<span style="font-size:80%;opacity:0.8">（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。</span></li><li>Our network is trained on the synthetic data once, and tested on all other <font color=forestgreen>real-world</font> test datasets without any fine-tuning on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and <font color=forestgreen>real-world</font> data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and <font color=forestgreen>real-world</font> data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li><li>To further speed up CRNN and make it more practical in <font color=forestgreen>real-world</font> applications is another direction that is worthy of exploration in the future.<span style="font-size:80%;opacity:0.8">进一步加快CRNN，使其在现实应用中更加实用，是未来值得探索的另一个方向。</span></li><li>It contains 200 samples, some of which are shown in Fig. 5. b; 3) “<font color=forestgreen>Real-World</font>”, which contains 200 images of score fragments taken from music books with a phone camera.<span style="font-size:80%;opacity:0.8">它包含200个样本，其中一些如图5.b所示；3）“现实世界”，其中包含用手机相机拍摄的音乐书籍中的200张图像。</span></li><li>(c) <font color=forestgreen>Real-world</font> score images taken with a mobile phone camera.<span style="font-size:80%;opacity:0.8">(c)用手机相机拍摄的现实世界的乐谱图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> trainable<br>(6) </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end <font color=orangered>trainable</font>, in contrast to most of the existing algorithms whose components are separately trained and tuned.<span style="font-size:80%;opacity:0.8">与以前的场景文本识别系统相比，所提出的架构具有四个不同的特性：（1）与大多数现有的组件需要单独训练和协调的算法相比，它是端对端训练的。</span></li><li>Being robust, rich and <font color=orangered>trainable</font>, deep convolutional features have been widely adopted for different kinds of visual recognition tasks [25, 12].<span style="font-size:80%;opacity:0.8">鲁棒的，丰富的和可训练的深度卷积特征已被广泛应用于各种视觉识别任务[25,12]。</span></li><li>Attributes for comparison include: 1) being end-to-end <font color=orangered>trainable</font> (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end <font color=orangered>trainable</font> model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li>E2E Train: This column is to show whether a certain text reading model is end-to-end <font color=orangered>trainable</font>, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training.<span style="font-size:80%;opacity:0.8">E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。</span></li><li>An End-to-End <font color=orangered>Trainable</font> Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition<span style="font-size:80%;opacity:0.8">基于图像序列识别的端到端可训练神经网络及其在场景文本识别中的应用</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> lexicon-based<br>(6) </td> <td>  </td> <td> 
<ul><li>(3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and <font color=forestgreen>lexicon-based</font> scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li><li>In practice, there exists two modes of transcription, namely the lexicon-free and <font color=forestgreen>lexicon-based</font> transcriptions.<span style="font-size:80%;opacity:0.8">在实践中，存在两种转录模式，即无词典转录和基于词典的转录。</span></li><li>In <font color=forestgreen>lexicon-based</font> mode, predictions are made by choosing the label sequence that has the highest probability.<span style="font-size:80%;opacity:0.8">在基于词典的模式中，通过选择具有最高概率的标签序列进行预测。</span></li><li>In <font color=forestgreen>lexicon-based</font> mode, each test sample is associated with a lexicon {\cal D}.<span style="font-size:80%;opacity:0.8">在基于字典的模式中，每个测试采样与词典{\cal D}相关联。</span></li><li>Larger \delta results in more candidates, thus more accurate <font color=forestgreen>lexicon-based</font> transcription.<span style="font-size:80%;opacity:0.8">更大的\delta导致更多的候选目标，从而基于词典的转录更准确。</span></li><li>2.3.3 <font color=forestgreen>Lexicon-based</font> transcription<span style="font-size:80%;opacity:0.8">2.3.3 基于词典的转录</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> conventional<br>(6) </td> <td> [kənˈvenʃənl] </td> <td> 
<ul><li>Several <font color=orangered>conventional</font> scene text recognition methods that are not based on neural networks also brought insightful ideas and novel representations into this field.<span style="font-size:80%;opacity:0.8">一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。</span></li><li>For sequence-like objects, CRNN possesses several distinctive advantages over <font color=orangered>conventional</font> neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed annotations (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>Compared with the <font color=orangered>conventional</font> momentum [31] method, ADADELTA requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>In the 3rd and the 4th max-pooling layers, we adopt 1 × 2 sized rectangular pooling windows instead of the <font color=orangered>conventional</font> squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li><li>Moreover, as CRNN abandons fully connected layers used in <font color=orangered>conventional</font> neural networks, it results in a much more compact and efficient model.<span style="font-size:80%;opacity:0.8">此外，由于CRNN放弃了传统神经网络中使用的全连接层，因此得到了更加紧凑和高效的模型。</span></li><li>The experiments on the scene text recognition benchmarks demonstrate that CRNN achieves superior or highly competitive performance, compared with <font color=orangered>conventional</font> methods as well as other CNN and RNN based algorithms.<span style="font-size:80%;opacity:0.8">在场景文本识别基准数据集上的实验表明，与传统方法以及其它基于CNN和RNN的算法相比，CRNN实现了优异或极具竞争力的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> vector<br>(6) </td> <td> [ˈvektə(r)] </td> <td> 
<ul><li>Then a sequence of feature <font color=orangered>vectors</font> is extracted from the feature maps produced by the component of convolutional layers, which is the input for the recurrent layers.<span style="font-size:80%;opacity:0.8">然后从卷积层组件产生的特征图中提取特征向量序列，这些特征向量序列作为循环层的输入。</span></li><li>Specifically, each feature <font color=orangered>vector</font> of a feature sequence is generated from left to right on the feature maps by column.<span style="font-size:80%;opacity:0.8">具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。</span></li><li>This means the i-th feature <font color=orangered>vector</font> is the concatenation of the i-th columns of all the maps.<span style="font-size:80%;opacity:0.8">这意味着第i个特征向量是所有特征图第i列的连接。</span></li><li>As illustrated in Fig. 2, each <font color=orangered>vector</font> in the feature sequence is associated with a receptive field, and can be considered as the image descriptor for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li><li>Each <font color=orangered>vector</font> in the extracted feature sequence is associated with a receptive field on the input image, and can be considered as the feature <font color=orangered>vector</font> of that field.<span style="font-size:80%;opacity:0.8">提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> OMR<br>(6) </td> <td>  </td> <td> 
<ul><li>Recognizing musical scores in images is known as the Optical Music Recognition (<font color=forestgreen>OMR</font>) problem.<span style="font-size:80%;opacity:0.8">识别图像中的乐谱被称为光学音乐识别（OMR）问题。</span></li><li>We cast the <font color=forestgreen>OMR</font> as a sequence recognition problem, and predict a sequence of musical notes directly from the image with CRNN.<span style="font-size:80%;opacity:0.8">我们将OMR作为序列识别问题，直接用CRNN从图像中预测音符的序列。</span></li><li>For comparison, we evaluate two commercial <font color=forestgreen>OMR</font> engines, namely the Capella Scan [3] and the PhotoScore [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>Comparison of pitch recognition accuracies, among CRNN and two commercial <font color=forestgreen>OMR</font> systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>But it provides a new scheme for <font color=forestgreen>OMR</font>, and has shown promising capabilities in pitch recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li><li>In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (<font color=forestgreen>OMR</font>), which verifies the generality of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> specifically<br>(5) </td> <td> [spəˈsɪfɪkli] </td> <td> 
<ul><li>Recently, the community has seen a strong revival of neural networks, which is mainly stimulated by the great success of deep neural network models, <font color=orangered>specifically</font> Deep Convolutional Neural Networks (DCNN), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li><li>The main contribution of this paper is a novel neural network model, whose network architecture is <font color=orangered>specifically</font> designed for recognizing sequence-like objects in images.<span style="font-size:80%;opacity:0.8">本文的主要贡献是一种新颖的神经网络模型，其网络架构设计专门用于识别图像中的类序列对象。</span></li><li>The candidates ${\cal N}_{\delta}(\mathbf{l}’)$ can be found efficiently with the BK-tree data structure[9], which is a metric tree <font color=orangered>specifically</font> adapted to discrete metric spaces.<span style="font-size:80%;opacity:0.8">可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。</span></li><li><font color=orangered>Specifically</font>, each feature vector of a feature sequence is generated from left to right on the feature maps by column.<span style="font-size:80%;opacity:0.8">具体地，特征序列的每一个特征向量在特征图上按列从左到右生成。</span></li><li><font color=orangered>Specifically</font>, we obtain superior performance on IIIT5k, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> annotation<br>(5) </td> <td> [ˌænə'teɪʃn] </td> <td> 
<ul><li>For sequence-like objects, CRNN possesses several distinctive advantages over conventional neural network models: 1) It can be directly learned from sequence labels (for instance, words), requiring no detailed <font color=orangered>annotations</font> (for instance, characters);<span style="font-size:80%;opacity:0.8">对于类序列对象，CRNN与传统神经网络模型相比具有一些独特的优点：1）可以直接从序列标签（例如单词）学习，不需要详细的标注（例如字符）；</span></li><li>Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level <font color=orangered>annotations</font> for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li><li>CharGT-Free: This column is to indicate whether the character-level <font color=orangered>annotations</font> are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li><li>As the input and output labels of CRNN can be a sequence, character-level <font color=orangered>annotations</font> are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li><li>It directly runs on coarse level labels (e.g. words), requiring no detailed <font color=orangered>annotations</font> for each individual element (e.g. characters) in the training phase.<span style="font-size:80%;opacity:0.8">它直接在粗粒度的标签（例如单词）上运行，在训练阶段不需要详细标注每一个单独的元素（例如字符）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> receptive<br>(5) </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>Therefore, each column of the feature maps corresponds to a rectangle region of the original image (termed the <font color=orangered>receptive</font> field), and such rectangle regions are in the same order to their corresponding columns on the feature maps from left to right.<span style="font-size:80%;opacity:0.8">因此，特征图的每列对应于原始图像的一个矩形区域（称为感受野），并且这些矩形区域与特征图上从左到右的相应列具有相同的顺序。</span></li><li>As illustrated in Fig. 2, each vector in the feature sequence is associated with a <font color=orangered>receptive</font> field, and can be considered as the image descriptor for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li><li>The <font color=orangered>receptive</font> field.<span style="font-size:80%;opacity:0.8">感受野。</span></li><li>Each vector in the extracted feature sequence is associated with a <font color=orangered>receptive</font> field on the input image, and can be considered as the feature vector of that field.<span style="font-size:80%;opacity:0.8">提取的特征序列中的每一个向量关联输入图像的一个感受野，可认为是该区域的特征向量。</span></li><li>On top of that, the rectangular pooling windows yield rectangular <font color=orangered>receptive</font> fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.<span style="font-size:80%;opacity:0.8">最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如i和l。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> differential<br>(5) </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>Secondly, RNN can back-propagates error <font color=orangered>differentials</font> to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li><li>In recurrent layers, error <font color=orangered>differentials</font> are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the recurrent layers, the sequence of propagated <font color=orangered>differentials</font> are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li><li>In particular, in the transcription layer, error <font color=orangered>differentials</font> are back-propagated with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li><li>In the recurrent layers, the Back-Propagation Through Time (BPTT) is applied to calculate the error <font color=orangered>differentials</font>.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> conditional<br>(5) </td> <td> [kənˈdɪʃənl] </td> <td> 
<ul><li>We adopt the <font color=orangered>conditional</font> probability defined in the Connectionist Temporal Classification (CTC) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li><li>The formulation of the <font color=orangered>conditional</font> probability is briefly described as follows: The input is a sequence $y = y_1,…,y_T$ where T is the sequence length.<span style="font-size:80%;opacity:0.8">条件概率的公式简要描述如下：输入是序列$y = y_1,…,y_T$，其中T是序列长度。</span></li><li>Then, the <font color=orangered>conditional</font> probability is defined as the sum of probabilities of all $\boldsymbol{\pi}$ that are mapped by ${\cal B}$ onto $\mathbf{l}$:<span style="font-size:80%;opacity:0.8">然后，条件概率被定义为由${\cal B}$映射到$\mathbf{l}$上的所有$\boldsymbol{\pi}$的概率之和：</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest <font color=orangered>conditional</font> probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>The objective is to minimize the negative log-likelihood of <font color=orangered>conditional</font> probability of ground truth:<span style="font-size:80%;opacity:0.8">目标是最小化真实条件概率的负对数似然：</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> eq<br>(5) </td> <td>  </td> <td> 
<ul><li>Directly computing <font color=forestgreen>Eq</font>.1 would be computationally infeasible due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li><li>However, <font color=forestgreen>Eq</font>.1 can be efficiently computed using the forward-backward algorithm described in [15].<span style="font-size:80%;opacity:0.8">然而，使用[15]中描述的前向算法可以有效计算方程1。</span></li><li>In this mode, the sequence $\mathbf{l}^{*}$ that has the highest probability as defined in <font color=forestgreen>Eq</font>.1 is taken as the prediction.<span style="font-size:80%;opacity:0.8">在这种模式下，将具有方程1中定义的最高概率的序列$\mathbf{l}^{*}$作为预测。</span></li><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in <font color=forestgreen>Eq</font>.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, to test the impact of parameter \delta, we experiment different values of \delta in <font color=forestgreen>Eq</font>.2.<span style="font-size:80%;opacity:0.8">另外，为了测试参数\delta的影响，我们在方程2中实验了\delta的不同值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> BK-tree<br>(5) </td> <td>  </td> <td> 
<ul><li>The candidates ${\cal N}_{\delta}(\mathbf{l}’)$ can be found efficiently with the <font color=forestgreen>BK-tree</font> data structure[9], which is a metric tree specifically adapted to discrete metric spaces.<span style="font-size:80%;opacity:0.8">可以使用BK树数据结构[9]有效地找到候选目标${\cal N}_{\delta}(\mathbf{l}’)$，这是一种专门适用于离散度量空间的度量树。</span></li><li>The search time complexity of <font color=forestgreen>BK-tree</font> is $O(\log|{\cal D}|)$, where $|{\cal D}|$ is the lexicon size.<span style="font-size:80%;opacity:0.8">BK树的搜索时间复杂度为$O(\log|{\cal D}|)$，其中$|{\cal D}|$是词典大小。</span></li><li>In our approach, a <font color=forestgreen>BK-tree</font> is constructed offline for a lexicon.<span style="font-size:80%;opacity:0.8">在我们的方法中，一个词典离线构造一个BK树。</span></li><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/CUDA), the transcription layer (in C++) and the <font color=forestgreen>BK-tree</font> data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li><li>On the other hand, the computational cost grows with larger \delta, due to longer <font color=forestgreen>BK-tree</font> search time, as well as larger number of candidate sequences for testing.<span style="font-size:80%;opacity:0.8">另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着\delta的增大而增加。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> synthetic<br>(5) </td> <td> [sɪnˈθetɪk] </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the <font color=orangered>synthetic</font> dataset (Synth) released by Jaderberg et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li><li>Such images are generated by a <font color=orangered>synthetic</font> text engine and are highly realistic.<span style="font-size:80%;opacity:0.8">这样的图像由合成文本引擎生成并且是非常现实的。</span></li><li>Our network is trained on the <font color=orangered>synthetic</font> data once, and tested on all other real-world test datasets without any fine-tuning on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li><li>Even though the CRNN model is purely trained with <font color=orangered>synthetic</font> text data, it works well on real images from standard text recognition benchmarks.<span style="font-size:80%;opacity:0.8">即使CRNN模型是在纯合成文本数据上训练，但它在标准文本识别基准数据集的真实图像上工作良好。</span></li><li>Our method uses only <font color=orangered>synthetic</font> text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with character-level annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> pitch<br>(5) </td> <td> [pɪtʃ] </td> <td> 
<ul><li>For simplicity, we recognize <font color=orangered>pitches</font> only, ignore all chords and assume the same major scales (C major) for all scores.<span style="font-size:80%;opacity:0.8">为了简单起见，我们仅认识音调，忽略所有和弦，并假定所有乐谱具有相同的大调音阶（C大调）。</span></li><li>To the best of our knowledge, there exists no public datasets for evaluating algorithms on <font color=orangered>pitch</font> recognition.<span style="font-size:80%;opacity:0.8">据我们所知，没有用于评估音调识别算法的公共数据集。</span></li><li>Two measures are used for evaluating the recognition performance: 1) fragment accuracy, i.e. the percentage of score fragments correctly recognized; 2) average edit distance, i.e. the average edit distance between predicted <font color=orangered>pitch</font> sequences and the ground truths.<span style="font-size:80%;opacity:0.8">使用两种方法来评估识别性能：1）片段准确度，即正确识别的乐谱片段的百分比；2）平均编辑距离，即预测音调序列与真实值之间的平均编辑距离。</span></li><li>Comparison of <font color=orangered>pitch</font> recognition accuracies, among CRNN and two commercial OMR systems, on the three datasets we have collected.<span style="font-size:80%;opacity:0.8">在我们收集的数据集上，CRNN和两个商业OMR系统对音调识别准确率的对比。</span></li><li>But it provides a new scheme for OMR, and has shown promising capabilities in <font color=orangered>pitch</font> recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> generality<br>(4) </td> <td> [ˌdʒenəˈræləti] </td> <td> 
<ul><li>Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the <font color=orangered>generality</font> of it.<span style="font-size:80%;opacity:0.8">此外，提出的算法在基于图像的音乐配乐识别任务中表现良好，这显然证实了它的泛化性。</span></li><li>To further demonstrate the <font color=orangered>generality</font> of CRNN, we verify the proposed algorithm on a music score recognition task in Sec. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li><li>The results have shown the <font color=orangered>generality</font> of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring minimal domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li><li>In addition, CRNN significantly outperforms other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the <font color=orangered>generality</font> of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> outperform<br>(4) </td> <td> [ˌaʊtpəˈfɔ:m] </td> <td> 
<ul><li>Though achieved promising performance on standard benchmarks, these methods are generally <font color=orangered>outperformed</font> by previous algorithms based on neural networks [8, 22], as well as the approach proposed in this paper.<span style="font-size:80%;opacity:0.8">虽然在标准基准数据集上取得了有效的性能，但是前面的基于神经网络的算法[8,22]以及本文提出的方法通常都优于这些方法。</span></li><li>In the constrained lexicon cases, our method consistently <font color=orangered>outperforms</font> most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li><li>The CRNN <font color=orangered>outperforms</font> the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>In addition, CRNN significantly <font color=orangered>outperforms</font> other competitors on a benchmark for Optical Music Recognition (OMR), which verifies the generality of CRNN.<span style="font-size:80%;opacity:0.8">此外，CRNN在光学音乐识别（OMR）的基准数据集上显著优于其它的竞争者，这验证了CRNN的泛化性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> ADADELTA<br>(4) </td> <td>  </td> <td> 
<ul><li>For optimization, we use the <font color=forestgreen>ADADELTA</font> [37] to automatically calculate per-dimension learning rates.<span style="font-size:80%;opacity:0.8">为了优化，我们使用ADADELTA[37]自动计算每维的学习率。</span></li><li>Compared with the conventional momentum [31] method, <font color=forestgreen>ADADELTA</font> requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>More importantly, we find that optimization using <font color=forestgreen>ADADELTA</font> converges faster than the momentum method.<span style="font-size:80%;opacity:0.8">更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</span></li><li>Networks are trained with <font color=forestgreen>ADADELTA</font>, setting the parameter ρ to 0.9.<span style="font-size:80%;opacity:0.8">网络用ADADELTA训练，将参数ρ设置为0.9。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> sec<br>(4) </td> <td> [sek] </td> <td> 
<ul><li>The datasets and setting for training and testing are given in <font color=orangered>Sec</font>. 3.1, the detailed settings of CRNN for scene text images is provided in <font color=orangered>Sec</font>. 3.2, and the results with the comprehensive comparisons are reported in <font color=orangered>Sec</font>. 3.3.<span style="font-size:80%;opacity:0.8">数据集和训练测试的设置见3.1小节，场景文本图像中CRNN的详细设置见3.2小节，综合比较的结果在3.3小节报告。</span></li><li>To further demonstrate the generality of CRNN, we verify the proposed algorithm on a music score recognition task in <font color=orangered>Sec</font>. 3.4.<span style="font-size:80%;opacity:0.8">为了进一步证明CRNN的泛化性，在3.4小节我们在乐谱识别任务上验证了提出的算法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> SVT<br>(4) </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (<font color=forestgreen>SVT</font>).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li><font color=forestgreen>SVT</font> [34] test dataset consists of 249 street view images collected from Google Street View.<span style="font-size:80%;opacity:0.8">SVT[34]测试数据集由从Google街景视图收集的249张街景图像组成。</span></li><li>Specifically, we obtain superior performance on IIIT5k, and <font color=forestgreen>SVT</font> compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li><li>In the unconstrained lexicon cases, our method achieves the best performance on <font color=forestgreen>SVT</font>, yet, is still behind some approaches [8, 22] on IC03 and IC13.<span style="font-size:80%;opacity:0.8">在无约束词典的情况下，我们的方法在SVT上仍取得了最佳性能，但在IC03和IC13上仍然落后于一些方法[8,22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> configuration<br>(4) </td> <td> [kənˌfɪgəˈreɪʃn] </td> <td> 
<ul><li>The network <font color=orangered>configuration</font> we use in our experiments is summarized in Table 1.<span style="font-size:80%;opacity:0.8">在实验中我们使用的网络配置总结在表1中。</span></li><li>Network <font color=orangered>configuration</font> summary.<span style="font-size:80%;opacity:0.8">网络配置总结。</span></li><li>Since we have limited training data, we use a simplified CRNN <font color=orangered>configuration</font> in order to reduce model capacity.<span style="font-size:80%;opacity:0.8">由于我们的训练数据有限，因此我们使用简化的CRNN配置来减少模型容量。</span></li><li>Different from the <font color=orangered>configuration</font> specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> synthesize<br>(4) </td> <td> [ˈsɪnθəsaɪz] </td> <td> 
<ul><li>a; 2) “<font color=orangered>Synthesized</font>”, which is created from “Clean”, using the augmentation strategy mentioned above.<span style="font-size:80%;opacity:0.8">实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。</span></li><li>Figure 5. (a) Clean musical scores images collected from [2] (b) <font color=orangered>Synthesized</font> musical score images.<span style="font-size:80%;opacity:0.8">图5。(a)从[2]中收集的干净的乐谱图像。(b)合成的乐谱图像。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on <font color=orangered>synthesized</font> and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on <font color=orangered>synthesized</font> and real-world data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> ICDAR<br>(3) </td> <td>  </td> <td> 
<ul><li>The experiments on standard benchmarks, including the IIIT-5K, Street View Text and <font color=forestgreen>ICDAR</font> datasets, demonstrate the superiority of the proposed algorithm over the prior arts.<span style="font-size:80%;opacity:0.8">在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有技术的更有优势。</span></li><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely <font color=forestgreen>ICDAR</font> 2003 (IC03), <font color=forestgreen>ICDAR</font> 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> sequential<br>(3) </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into <font color=orangered>sequential</font> HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li><li>Such component is used to extract a <font color=orangered>sequential</font> feature representation from an input image.<span style="font-size:80%;opacity:0.8">这样的组件用于从输入图像中提取序列特征表示。</span></li><li>In CRNN, we convey deep features into <font color=orangered>sequential</font> representations in order to be invariant to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> binarization<br>(3) </td> <td>  </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including <font color=forestgreen>binarization</font>/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li><li>The main reason is that they rely on robust <font color=forestgreen>binarization</font> to detect staff lines and notes, but the <font color=forestgreen>binarization</font> step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and cluttered background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> max-pooling<br>(3) </td> <td>  </td> <td> 
<ul><li>In CRNN model, the component of convolutional layers is constructed by taking the convolutional and <font color=forestgreen>max-pooling</font> layers from a standard CNN model (fully-connected layers are removed).<span style="font-size:80%;opacity:0.8">在CRNN模型中，通过采用标准CNN模型（去除全连接层）中的卷积层和最大池化层来构造卷积层的组件。</span></li><li>As the layers of convolution, <font color=forestgreen>max-pooling</font>, and element-wise activation function operate on local regions, they are translation invariant.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li><li>In the 3rd and the 4th <font color=forestgreen>max-pooling</font> layers, we adopt 1 × 2 sized rectangular pooling windows instead of the conventional squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> contextual<br>(3) </td> <td> [kənˈtekstʃuəl] </td> <td> 
<ul><li>Firstly, RNN has a strong capability of capturing <font color=orangered>contextual</font> information within a sequence.<span style="font-size:80%;opacity:0.8">首先，RNN具有很强的捕获序列内上下文信息的能力。</span></li><li>Using <font color=orangered>contextual</font> cues for image-based sequence recognition is more stable and helpful than treating each symbol independently.<span style="font-size:80%;opacity:0.8">对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。</span></li><li>Besides, recurrent layers in CRNN can utilize <font color=orangered>contextual</font> information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> back-propagation<br>(3) </td> <td>  </td> <td> 
<ul><li>Gradients are calculated by the <font color=forestgreen>back-propagation</font> algorithm.<span style="font-size:80%;opacity:0.8">梯度由反向传播算法计算。</span></li><li>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. <font color=forestgreen>Back-Propagation</font> Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>In the recurrent layers, the <font color=forestgreen>Back-Propagation</font> Through Time (BPTT) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> IIIT5k<br>(3) </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (<font color=forestgreen>IIIT5k</font>), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li><li><font color=forestgreen>IIIT5k</font> [28] contains 3,000 cropped word test images collected from the Internet.<span style="font-size:80%;opacity:0.8">IIIT5k[28]包含从互联网收集的3000张裁剪的词测试图像。</span></li><li>Specifically, we obtain superior performance on <font color=forestgreen>IIIT5k</font>, and SVT compared to [22], only achieved lower performance on IC03 with the “Full” lexicon.<span style="font-size:80%;opacity:0.8">具体来说，与[22]相比，我们在IIIT5k和SVT上获得了卓越的性能，仅在IC03上通过“Full”词典实现了较低性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> rectangular<br>(3) </td> <td> [rek'tæŋɡjələ(r)] </td> <td> 
<ul><li>In the 3rd and the 4th max-pooling layers, we adopt 1 × 2 sized <font color=orangered>rectangular</font> pooling windows instead of the conventional squared ones.<span style="font-size:80%;opacity:0.8">在第3和第4个最大池化层中，我们采用1×2大小的矩形池化窗口而不是传统的平方形。</span></li><li>On top of that, the <font color=orangered>rectangular</font> pooling windows yield <font color=orangered>rectangular</font> receptive fields (illustrated in Fig. 2), which are beneficial for recognizing some characters that have narrow shapes, such as ’i’ and ’l’.<span style="font-size:80%;opacity:0.8">最重要的是，矩形池窗口产生矩形感受野（如图2所示），这有助于识别一些具有窄形状的字符，例如i和l。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> character-level<br>(3) </td> <td>  </td> <td> 
<ul><li>Our method uses only synthetic text with word level labels as the training data, very different to PhotoOCR [8] which used 7.9 millions of real word images with <font color=forestgreen>character-level</font> annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li><li>CharGT-Free: This column is to indicate whether the <font color=forestgreen>character-level</font> annotations are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li><li>As the input and output labels of CRNN can be a sequence, <font color=forestgreen>character-level</font> annotations are not necessary.<span style="font-size:80%;opacity:0.8">由于CRNN的输入和输出标签是序列，因此字符级标注是不必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> E2E<br>(3) </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named <font color=forestgreen>E2E</font> Train, Conv Ftrs, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (<font color=forestgreen>E2E</font> Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=forestgreen>E2E</font> Train: This column is to show whether a certain text reading model is end-to-end trainable, without any preprocess or through several separated steps, which indicates such approaches are elegant and clean for training.<span style="font-size:80%;opacity:0.8">E2E Train：这一列是为了显示某种文字阅读模型是否可以进行端到端的训练，无需任何预处理或经过几个分离的步骤，这表明这种方法对于训练是优雅且干净的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> Ftrs<br>(3) </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv <font color=forestgreen>Ftrs</font>, CharGT-Free, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv <font color=forestgreen>Ftrs</font>); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li>Conv <font color=forestgreen>Ftrs</font>: This column is to indicate whether an approach uses the convolutional features learned from training images directly or handcraft features as the basic representations.<span style="font-size:80%;opacity:0.8">Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> CharGT-Free<br>(3) </td> <td>  </td> <td> 
<ul><li>For further understanding the advantages of the proposed algorithm over other text recognition approaches, we provide a comprehensive comparison on several properties named E2E Train, Conv Ftrs, <font color=forestgreen>CharGT-Free</font>, Unconstrained, and Model Size, as summarized in Table 3.<span style="font-size:80%;opacity:0.8">为了进一步了解与其它文本识别方法相比，所提出算法的优点，我们提供了在一些特性上的综合比较，这些特性名称为E2E Train，Conv Ftrs，CharGT-Free，Unconstrained和Model Size，如表3所示。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (<font color=forestgreen>CharGT-Free</font>); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li><li><font color=forestgreen>CharGT-Free</font>: This column is to indicate whether the character-level annotations are essential for training the model.<span style="font-size:80%;opacity:0.8">CharGT-Free：这一列用来表明字符级标注对于训练模型是否是必要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> Capella<br>(3) </td> <td> [kəˈpelə] </td> <td> 
<ul><li>For comparison, we evaluate two commercial OMR engines, namely the <font color=orangered>Capella</font> Scan [3] and the PhotoScore [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The <font color=orangered>Capella</font> Scan and PhotoScore systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>Compared with <font color=orangered>Capella</font> Scan and PhotoScore, our CRNN-based system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> PhotoScore<br>(3) </td> <td>  </td> <td> 
<ul><li>For comparison, we evaluate two commercial OMR engines, namely the Capella Scan [3] and the <font color=forestgreen>PhotoScore</font> [4].<span style="font-size:80%;opacity:0.8">为了比较，我们评估了两种商用OMR引擎，即Capella Scan[3]和PhotoScore[4]。</span></li><li>The CRNN outperforms the two commercial systems by a large margin. The Capella Scan and <font color=forestgreen>PhotoScore</font> systems perform reasonably well on the Clean dataset, but their performances drop significantly on synthesized and real-world data.<span style="font-size:80%;opacity:0.8">Capella Scan和PhotoScore系统在干净的数据集上表现相当不错，但是它们的性能在合成和现实世界数据方面显著下降。</span></li><li>Compared with Capella Scan and <font color=forestgreen>PhotoScore</font>, our CRNN-based system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> extraction<br>(2) </td> <td> [ɪkˈstrækʃn] </td> <td> 
<ul><li>A novel neural network architecture, which integrates feature <font color=orangered>extraction</font>, sequence modeling and transcription into a unified framework, is proposed.<span style="font-size:80%;opacity:0.8">提出了一种将特征提取，序列建模和转录整合到统一框架中的新型神经网络架构。</span></li><li>2.1. Feature Sequence <font color=orangered>Extraction</font><span style="font-size:80%;opacity:0.8">2.1. 特征序列提取</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> arbitrary<br>(2) </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>(2) It naturally handles sequences in <font color=orangered>arbitrary</font> lengths, involving no character segmentation or horizontal scale normalization.<span style="font-size:80%;opacity:0.8">（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。</span></li><li>Thirdly, RNN is able to operate on sequences of <font color=orangered>arbitrary</font> lengths, traversing from starts to ends.<span style="font-size:80%;opacity:0.8">第三，RNN能够从头到尾对任意长度的序列进行操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> segmentation<br>(2) </td> <td> [ˌsegmenˈteɪʃn] </td> <td> 
<ul><li>(2) It naturally handles sequences in arbitrary lengths, involving no character <font color=orangered>segmentation</font> or horizontal scale normalization.<span style="font-size:80%;opacity:0.8">（2）它自然地处理任意长度的序列，不涉及字符分割或水平尺度归一化。</span></li><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/<font color=orangered>segmentation</font>, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> jointly<br>(2) </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>Though CRNN is composed of different kinds of network architectures (eg. CNN and RNN), it can be <font color=orangered>jointly</font> trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li><li>Secondly, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to <font color=orangered>jointly</font> train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> invariant<br>(2) </td> <td> [ɪnˈveəriənt] </td> <td> 
<ul><li>As the layers of convolution, max-pooling, and element-wise activation function operate on local regions, they are translation <font color=orangered>invariant</font>.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li><li>In CRNN, we convey deep features into sequential representations in order to be <font color=orangered>invariant</font> to the length variation of sequence-like objects.<span style="font-size:80%;opacity:0.8">在CRNN中，我们将深度特征传递到序列表示中，以便对类序列对象的长度变化保持不变。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> capability<br>(2) </td> <td> [ˌkeɪpəˈbɪləti] </td> <td> 
<ul><li>Firstly, RNN has a strong <font color=orangered>capability</font> of capturing contextual information within a sequence.<span style="font-size:80%;opacity:0.8">首先，RNN具有很强的捕获序列内上下文信息的能力。</span></li><li>But it provides a new scheme for OMR, and has shown promising <font color=orangered>capabilities</font> in pitch recognition.<span style="font-size:80%;opacity:0.8">但它为OMR提供了一个新的方案，并且在音高识别方面表现出有前途的能力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> utilize<br>(2) </td> <td> [ˈju:təlaɪz] </td> <td> 
<ul><li>In this way, past contexts $\lbrace x_{t\prime} \rbrace _{t \prime &lt; t}$ are captured and <font color=orangered>utilized</font> for prediction.<span style="font-size:80%;opacity:0.8">以这种方式，过去的上下文${\lbrace x_{t\prime} \rbrace _{t \prime &lt; t}$被捕获并用于预测。</span></li><li>Besides, recurrent layers in CRNN can <font color=orangered>utilize</font> contextual information in the score.<span style="font-size:80%;opacity:0.8">此外，CRNN中的循环层可以利用乐谱中的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> directional<br>(2) </td> <td> [dəˈrekʃənl] </td> <td> 
<ul><li>LSTM is <font color=orangered>directional</font>, it only uses past contexts.<span style="font-size:80%;opacity:0.8">LSTM是定向的，它只使用过去的上下文。</span></li><li>Different from the configuration specified in Tab. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single <font color=orangered>directional</font> LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> propagate<br>(2) </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>In recurrent layers, error differentials are <font color=orangered>propagated</font> in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (BPTT).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>At the bottom of the recurrent layers, the sequence of <font color=orangered>propagated</font> differentials are concatenated into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> BPTT<br>(2) </td> <td>  </td> <td> 
<ul><li>In recurrent layers, error differentials are propagated in the opposite directions of the arrows shown in Fig. 3. b, i.e. Back-Propagation Through Time (<font color=forestgreen>BPTT</font>).<span style="font-size:80%;opacity:0.8">在循环层中，误差在图3.b所示箭头的相反方向传播，即反向传播时间（BPTT）。</span></li><li>In the recurrent layers, the Back-Propagation Through Time (<font color=forestgreen>BPTT</font>) is applied to calculate the error differentials.<span style="font-size:80%;opacity:0.8">在循环层中，应用随时间反向传播（BPTT）来计算误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> log-likelihood<br>(2) </td> <td>  </td> <td> 
<ul><li>Consequently, when we use the negative <font color=forestgreen>log-likelihood</font> of this probability as the objective to train the network, we only need images and their corresponding label sequences, avoiding the labor of labeling positions of individual characters.<span style="font-size:80%;opacity:0.8">因此，当我们使用这种概率的负对数似然作为训练网络的目标函数时，我们只需要图像及其相应的标签序列，避免了标注单个字符位置的劳动。</span></li><li>The objective is to minimize the negative <font color=forestgreen>log-likelihood</font> of conditional probability of ground truth:<span style="font-size:80%;opacity:0.8">目标是最小化真实条件概率的负对数似然：</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> stampt<br>(2) </td> <td>  </td> <td> 
<ul><li>where the probability of $\boldsymbol{\pi}$ is defined as $p(\boldsymbol{\pi}|\mathbf{y})=\prod{t=1}^{T}y{\pi{t}}^{t},y{\pi{t}}^{t}$ is the probability of having label $\pi{t}$ at time <font color=forestgreen>stampt</font>.<span style="font-size:80%;opacity:0.8">$\boldsymbol{\pi}$的概率定义为$p(\boldsymbol{\pi}|\mathbf{y})=\prod{t=1}^{T}y{\pi{t}}^{t}，y{\pi{t}}^{t}$是时刻t时有标签$\pi{t}$的概率。</span></li><li>The sequence $\mathbf{l}^{*}$ is approximately found by $\mathbf{l}^{*}\approx{\cal B}(\arg\max{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$, i.e. taking the most probable label $\pi{t}$ at each time <font color=forestgreen>stampt</font>, and map the resulted sequence onto $\mathbf{l}^{*}$.<span style="font-size:80%;opacity:0.8">序列$\mathbf{l}^{*}$通过$\mathbf{l}^{*}\approx{\cal B}(\arg\max{\boldsymbol{\pi}}p(\boldsymbol{\pi}|\mathbf{y}))$近似发现，即在每个时间戳t采用最大概率的标签$\pi{t}$，并将结果序列映射到$\mathbf{l}^{*}$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> forward-backward<br>(2) </td> <td>  </td> <td> 
<ul><li>However, Eq.1 can be efficiently computed using the <font color=forestgreen>forward-backward</font> algorithm described in [15].<span style="font-size:80%;opacity:0.8">然而，使用[15]中描述的前向算法可以有效计算方程1。</span></li><li>In particular, in the transcription layer, error differentials are back-propagated with the <font color=forestgreen>forward-backward</font> algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> k-words<br>(2) </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>Each image has been associated to a 50-words lexicon and a 1k-words lexicon.<span style="font-size:80%;opacity:0.8">每张图像关联一个50词的词典和一个1000词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> Hunspell<br>(2) </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words <font color=forestgreen>Hunspell</font> spell-checking dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, we use a 50k words lexicon consisting of the words in the <font color=forestgreen>Hunspell</font> spell-checking dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> spell-checking<br>(2) </td> <td>  </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell <font color=forestgreen>spell-checking</font> dictionary [1], it would be very time-consuming to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li><li>In addition, we use a 50k words lexicon consisting of the words in the Hunspell <font color=forestgreen>spell-checking</font> dictionary [1].<span style="font-size:80%;opacity:0.8">此外，我们使用由Hunspell拼写检查字典[1]中的单词组成的5万个词的词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> momentum<br>(2) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>Compared with the conventional <font color=orangered>momentum</font> [31] method, ADADELTA requires no manual setting of a learning rate.<span style="font-size:80%;opacity:0.8">与传统的动量[31]方法相比，ADADELTA不需要手动设置学习率。</span></li><li>More importantly, we find that optimization using ADADELTA converges faster than the <font color=orangered>momentum</font> method.<span style="font-size:80%;opacity:0.8">更重要的是，我们发现使用ADADELTA的优化收敛速度比动量方法快。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> bounding<br>(2) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>IC03 [27] test dataset contains 251 scene images with labeled text <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8">IC03[27]测试数据集包含251个具有标记文本边界框的场景图像。</span></li><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using hand-crafted ones (Conv Ftrs); 3) requiring no ground truth <font color=orangered>bounding</font> boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> tweak<br>(2) </td> <td> [twi:k] </td> <td> 
<ul><li>A <font color=orangered>tweak</font> is made in order to make it suitable for recognizing English texts.<span style="font-size:80%;opacity:0.8">为了使其适用于识别英文文本，对其进行了调整。</span></li><li>This <font color=orangered>tweak</font> yields feature maps with larger width, hence longer feature sequence.<span style="font-size:80%;opacity:0.8">这种调整产生宽度较大的特征图，因此具有更长的特征序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> ram<br>(2) </td> <td> [ræm] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB <font color=orangered>RAM</font> and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li><li>Our model has 8.3 million parameters, taking only 33MB <font color=orangered>RAM</font> (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.<span style="font-size:80%;opacity:0.8">我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> state-of-the-arts<br>(2) </td> <td>  </td> <td> 
<ul><li>All the recognition accuracies on the above four public datasets, obtained by the proposed CRNN model and the recent <font color=forestgreen>state-of-the-arts</font> techniques including the approaches based on deep models [23, 22, 21], are shown in Table 2.<span style="font-size:80%;opacity:0.8">提出的CRNN模型在上述四个公共数据集上获得的所有识别精度以及最近的最新技术，包括基于深度模型[23,22,21]的方法如表2所示。</span></li><li>In the constrained lexicon cases, our method consistently outperforms most <font color=forestgreen>state-of-the-arts</font> approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> tab<br>(2) </td> <td> [tæb] </td> <td> 
<ul><li>Different from the configuration specified in <font color=orangered>Tab</font>. 1, the 4th and 6th convolution layers are removed, and the 2-layer bidirectional LSTM is replaced by a 2-layer single directional LSTM.<span style="font-size:80%;opacity:0.8">与表1中指定的配置不同，我们移除了第4和第6卷积层，将2层双向LSTM替换为2层单向LSTM。</span></li><li><font color=orangered>Tab</font>.<span style="font-size:80%;opacity:0.8">表4总结了结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> Baoguang<br>(1) </td> <td>  </td> <td> 
<ul><li>Authors: Shi, <font color=forestgreen>Baoguang</font> (School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> Huazhong<br>(1) </td> <td>  </td> <td> 
<ul><li>Authors: Shi, Baoguang (School of Electronic Information and Communications, <font color=forestgreen>Huazhong</font> University of Science and Technology, Wuhan; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> Wuhan<br>(1) </td> <td> ['wu:'hɑ:n] </td> <td> 
<ul><li>Authors: Shi, Baoguang (School of Electronic Information and Communications, Huazhong University of Science and Technology, <font color=orangered>Wuhan</font>; 430074, China); Bai, Xiang; Yao, Cong<span style="font-size:80%;opacity:0.8"></span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> long-standing<br>(1) </td> <td> [ˈlɔŋstædiŋ] </td> <td> 
<ul><li>Image-based sequence recognition has been a <font color=orangered>long-standing</font> research topic in computer vision.<span style="font-size:80%;opacity:0.8">基于图像的序列识别一直是计算机视觉中长期存在的研究课题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> predefined<br>(1) </td> <td> [pri:dɪ'faɪnd] </td> <td> 
<ul><li>(3) It is not confined to any <font color=orangered>predefined</font> lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks.<span style="font-size:80%;opacity:0.8">（3）它不仅限于任何预定义的词汇，并且在无词典和基于词典的场景文本识别任务中都取得了显著的表现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> scenario<br>(1) </td> <td> [səˈnɑ:riəʊ] </td> <td> 
<ul><li>(4) It generates an effective yet much smaller model, which is more practical for real-world application <font color=orangered>scenarios</font>.<span style="font-size:80%;opacity:0.8">（4）它产生了一个有效而小得多的模型，这对于现实世界的应用场景更为实用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> IIIT-5K<br>(1) </td> <td>  </td> <td> 
<ul><li>The experiments on standard benchmarks, including the <font color=forestgreen>IIIT-5K</font>, Street View Text and ICDAR datasets, demonstrate the superiority of the proposed algorithm over the prior arts.<span style="font-size:80%;opacity:0.8">在包括IIIT-5K，Street View Text和ICDAR数据集在内的标准基准数据集上的实验证明了提出的算法比现有技术的更有优势。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> revival<br>(1) </td> <td> [rɪˈvaɪvl] </td> <td> 
<ul><li>Recently, the community has seen a strong <font color=orangered>revival</font> of neural networks, which is mainly stimulated by the great success of deep neural network models, specifically Deep Convolutional Neural Networks (DCNN), in various vision tasks.<span style="font-size:80%;opacity:0.8">最近，社区已经看到神经网络的强大复兴，这主要受到深度神经网络模型，特别是深度卷积神经网络（DCNN）在各种视觉任务中的巨大成功的推动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> drastically<br>(1) </td> <td> ['drɑ:stɪklɪ] </td> <td> 
<ul><li>Another unique property of sequence-like objects is that their lengths may vary <font color=orangered>drastically</font>.<span style="font-size:80%;opacity:0.8">类序列对象的另一个独特之处在于它们的长度可能会有很大变化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> congratulations<br>(1) </td> <td> [kənˌgrætjʊ'leɪʃənz] </td> <td> 
<ul><li>For instance, English words can either consist of 2 characters such as “OK” or 15 characters such as “<font color=orangered>congratulations</font>”.<span style="font-size:80%;opacity:0.8">例如，英文单词可以由2个字符组成，如“OK”，或由15个字符组成，如“congratulations”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> incapable<br>(1) </td> <td> [ɪnˈkeɪpəbl] </td> <td> 
<ul><li>Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are <font color=orangered>incapable</font> of producing a variable-length label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> variable-length<br>(1) </td> <td> ['veərɪəbll'eŋθ] </td> <td> 
<ul><li>Consequently, the most popular deep models like DCNN [25, 26] cannot be directly applied to sequence prediction, since DCNN models often operate on inputs and outputs with fixed dimensions, and thus are incapable of producing a <font color=orangered>variable-length</font> label sequence.<span style="font-size:80%;opacity:0.8">因此，最流行的深度模型像DCNN[25,26]不能直接应用于序列预测，因为DCNN模型通常对具有固定维度的输入和输出进行操作，因此不能产生可变长度的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> generalized<br>(1) </td> <td> [ˈdʒenrəlaɪzd] </td> <td> 
<ul><li>It turns out a large trained model with a huge number of classes, which is difficult to be <font color=orangered>generalized</font> to other types of sequence-like objects, such as Chinese texts, musical scores, etc., because the numbers of basic combinations of such kind of sequences can be greater than 1 million.<span style="font-size:80%;opacity:0.8">结果是一个大的训练模型中有很多类，这很难泛化到其它类型的类序列对象，如中文文本，音乐配乐等，因为这种序列的基本组合数目可能大于100万。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> geometrical<br>(1) </td> <td> [ˌdʒi:ə'metrɪkl] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of <font color=orangered>geometrical</font> or image features from handwritten texts, while Su and Lu [33] convert word images into sequential HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> handwritten<br>(1) </td> <td> [ˌhændˈrɪtn] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from <font color=orangered>handwritten</font> texts, while Su and Lu [33] convert word images into sequential HOG features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> hog<br>(1) </td> <td> [hɒg] </td> <td> 
<ul><li>For example, Graves et al. [16] extract a set of geometrical or image features from handwritten texts, while Su and Lu [33] convert word images into sequential <font color=orangered>HOG</font> features.<span style="font-size:80%;opacity:0.8">例如，Graves等[16]从手写文本中提取一系列几何或图像特征，而Su和Lu[33]将字符图像转换为序列HOG特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> insightful<br>(1) </td> <td> [ˈɪnsaɪtfʊl] </td> <td> 
<ul><li>Several conventional scene text recognition methods that are not based on neural networks also brought <font color=orangered>insightful</font> ideas and novel representations into this field.<span style="font-size:80%;opacity:0.8">一些不是基于神经网络的传统场景文本识别方法也为这一领域带来了有见地的想法和新颖的表现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> Almazan<br>(1) </td> <td>  </td> <td> 
<ul><li>For example, <font color=forestgreen>Almazan</font> et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Rodriguez-Serrano<br>(1) </td> <td>  </td> <td> 
<ul><li>For example, Almazan et al. [5] and <font color=forestgreen>Rodriguez-Serrano</font> et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> vectorial<br>(1) </td> <td> [vek'tɒrɪəl] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common <font color=orangered>vectorial</font> subspace, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> subspace<br>(1) </td> <td> ['sʌbspeɪs] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial <font color=orangered>subspace</font>, and word recognition is converted into a retrieval problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> retrieval<br>(1) </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>For example, Almazan et al. [5] and Rodriguez-Serrano et al. [30] proposed to embed word images and text strings in a common vectorial subspace, and word recognition is converted into a <font color=orangered>retrieval</font> problem.<span style="font-size:80%;opacity:0.8">例如，Almazan等人[5]和Rodriguez-Serrano等人[30]提出将单词图像和文本字符串嵌入到公共向量子空间中，并将词识别转换为检索问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> Gordo<br>(1) </td> <td>  </td> <td> 
<ul><li>Yao et al. [36] and <font color=forestgreen>Gordo</font> et al. [14] used mid-level features for scene text recognition.<span style="font-size:80%;opacity:0.8">Yao等人[36]和Gordo等人[14]使用中层特征进行场景文本识别。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> informative<br>(1) </td> <td> [ɪnˈfɔ:mətɪv] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning <font color=orangered>informative</font> representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> hand-craft<br>(1) </td> <td> ['hæn(d)krɑːft] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither <font color=orangered>hand-craft</font> features nor preprocessing steps, including binarization/segmentation, component localization, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> localization<br>(1) </td> <td> [ˌləʊkəlaɪ'zeɪʃn] </td> <td> 
<ul><li>2) It has the same property of DCNN on learning informative representations directly from image data, requiring neither hand-craft features nor preprocessing steps, including binarization/segmentation, component <font color=orangered>localization</font>, etc.;<span style="font-size:80%;opacity:0.8">2）直接从图像数据学习信息表示时具有与DCNN相同的性质，既不需要手工特征也不需要预处理步骤，包括二值化/分割，组件定位等；</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> outputted<br>(1) </td> <td> ['aʊt.pʊt] </td> <td> 
<ul><li>On top of the convolutional network, a recurrent network is built for making prediction for each frame of the feature sequence, <font color=orangered>outputted</font> by the convolutional layers.<span style="font-size:80%;opacity:0.8">在卷积网络之上，构建了一个循环网络，用于对卷积层输出的特征序列的每一帧进行预测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> eg<br>(1) </td> <td>  </td> <td> 
<ul><li>Though CRNN is composed of different kinds of network architectures (<font color=forestgreen>eg</font>. CNN and RNN), it can be jointly trained with one loss function.<span style="font-size:80%;opacity:0.8">虽然CRNN由不同类型的网络架构（如CNN和RNN）组成，但可以通过一个损失函数进行联合训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> concatenation<br>(1) </td> <td> [kənˌkætəˈneɪʃn] </td> <td> 
<ul><li>This means the i-th feature vector is the <font color=orangered>concatenation</font> of the i-th columns of all the maps.<span style="font-size:80%;opacity:0.8">这意味着第i个特征向量是所有特征图第i列的连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> activation<br>(1) </td> <td> [ˌæktɪ'veɪʃn] </td> <td> 
<ul><li>As the layers of convolution, max-pooling, and element-wise <font color=orangered>activation</font> function operate on local regions, they are translation invariant.<span style="font-size:80%;opacity:0.8">由于卷积层，最大池化层和元素激活函数在局部区域上执行，因此它们是平移不变的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> descriptor<br>(1) </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>As illustrated in Fig. 2, each vector in the feature sequence is associated with a receptive field, and can be considered as the image <font color=orangered>descriptor</font> for that region.<span style="font-size:80%;opacity:0.8">如图2所示，特征序列中的每个向量关联一个感受野，并且可以被认为是该区域的图像描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> holistic<br>(1) </td> <td> [həʊˈlɪstɪk] </td> <td> 
<ul><li>However, these approaches usually extract <font color=orangered>holistic</font> representation of the whole image by CNN, then the local deep features are collected for recognizing each component of a sequence-like object.<span style="font-size:80%;opacity:0.8">然而，这些方法通常通过CNN提取整个图像的整体表示，然后收集局部深度特征来识别类序列对象的每个分量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> cue<br>(1) </td> <td> [kju:] </td> <td> 
<ul><li>Using contextual <font color=orangered>cues</font> for image-based sequence recognition is more stable and helpful than treating each symbol independently.<span style="font-size:80%;opacity:0.8">对于基于图像的序列识别使用上下文提示比独立处理每个符号更稳定且更有帮助。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> il<br>(1) </td> <td>  </td> <td> 
<ul><li>Besides, some ambiguous characters are easier to distinguish when observing their contexts, e.g. it is easier to recognize “<font color=forestgreen>il</font>” by contrasting the character heights than by recognizing each of them separately.<span style="font-size:80%;opacity:0.8">此外，一些模糊的字符在观察其上下文时更容易区分，例如，通过对比字符高度更容易识别“il”而不是分别识别它们中的每一个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> Secondly<br>(1) </td> <td> [ˈsekəndli] </td> <td> 
<ul><li><font color=orangered>Secondly</font>, RNN can back-propagates error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> back-propagates<br>(1) </td> <td>  </td> <td> 
<ul><li>Secondly, RNN can <font color=forestgreen>back-propagates</font> error differentials to its input, i.e. the convolutional layer, allowing us to jointly train the recurrent layers and the convolutional layers in a unified network.<span style="font-size:80%;opacity:0.8">其次，RNN可以将误差差值反向传播到其输入，即卷积层，从而允许我们在统一的网络中共同训练循环层和卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> traverse<br>(1) </td> <td> [trəˈvɜ:s] </td> <td> 
<ul><li>Thirdly, RNN is able to operate on sequences of arbitrary lengths, <font color=orangered>traversing</font> from starts to ends.<span style="font-size:80%;opacity:0.8">第三，RNN能够从头到尾对任意长度的序列进行操作。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> Long-Short<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Long-Short</font> Term Memory 18, 11 is a type of RNN unit that is specially designed to address this problem.<span style="font-size:80%;opacity:0.8">长短时记忆[18,11]（LSTM）是一种专门设计用于解决这个问题的RNN单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> multiplicative<br>(1) </td> <td> ['mʌltɪplɪkeɪtɪv] </td> <td> 
<ul><li>An LSTM (illustrated in Fig. 3) consists of a memory cell and three <font color=orangered>multiplicative</font> gates, namely the input, output and forget gates.<span style="font-size:80%;opacity:0.8">LSTM（图3所示）由一个存储单元和三个多重门组成，即输入，输出和遗忘门。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> Conceptually<br>(1) </td> <td> [kən'septʃʊəlɪ] </td> <td> 
<ul><li><font color=orangered>Conceptually</font>, the memory cell stores the past contexts, and the input and output gates allow the cell to store contexts for a long period of time.<span style="font-size:80%;opacity:0.8">在概念上，存储单元存储过去的上下文，并且输入和输出门允许单元长时间地存储上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> long-range<br>(1) </td> <td> [lɒŋ reɪndʒ] </td> <td> 
<ul><li>The special design of LSTM allows it to capture <font color=orangered>long-range</font> dependencies, which often occur in image-based sequences.<span style="font-size:80%;opacity:0.8">LSTM的特殊设计允许它捕获长距离依赖，这经常发生在基于图像的序列中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> complementary<br>(1) </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>However, in image-based sequences, contexts from both directions are useful and <font color=orangered>complementary</font> to each other.<span style="font-size:80%;opacity:0.8">然而，在基于图像的序列中，两个方向的上下文是相互有用且互补的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> concatenate<br>(1) </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>At the bottom of the recurrent layers, the sequence of propagated differentials are <font color=orangered>concatenated</font> into maps, inverting the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> invert<br>(1) </td> <td> [ɪnˈvɜ:t] </td> <td> 
<ul><li>At the bottom of the recurrent layers, the sequence of propagated differentials are concatenated into maps, <font color=orangered>inverting</font> the operation of converting feature maps into feature sequences, and fed back to the convolutional layers.<span style="font-size:80%;opacity:0.8">在循环层的底部，传播差异的序列被连接成映射，将特征映射转换为特征序列的操作进行反转并反馈到卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> Map-to-Sequence<br>(1) </td> <td>  </td> <td> 
<ul><li>In practice, we create a custom network layer, called “<font color=forestgreen>Map-to-Sequence</font>”, as the bridge between convolutional layers and recurrent layers.<span style="font-size:80%;opacity:0.8">实际上，我们创建一个称为“Map-to-Sequence”的自定义网络层，作为卷积层和循环层之间的桥梁。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> Mathematically<br>(1) </td> <td> [ˌmæθə'mætɪklɪ] </td> <td> 
<ul><li><font color=orangered>Mathematically</font>, transcription is to find the label sequence with the highest probability conditioned on the per-frame predictions.<span style="font-size:80%;opacity:0.8">数学上，转录是根据每帧预测找到具有最高概率的标签序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> Connectionist<br>(1) </td> <td> [kə'nekʃənɪst] </td> <td> 
<ul><li>We adopt the conditional probability defined in the <font color=orangered>Connectionist</font> Temporal Classification (CTC) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> ctc<br>(1) </td> <td>  </td> <td> 
<ul><li>We adopt the conditional probability defined in the Connectionist Temporal Classification (<font color=forestgreen>CTC</font>) layer proposed by Graves et al. [15].<span style="font-size:80%;opacity:0.8">我们采用Graves等人[15]提出的联接时间分类（CTC）层中定义的条件概率。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> sequence-to-sequence<br>(1) </td> <td>  </td> <td> 
<ul><li>A <font color=forestgreen>sequence-to-sequence</font> mapping function ${\cal B}$ is defined on sequence $\boldsymbol{\pi}\in{\cal L}’^{T}$, where T is the length.<span style="font-size:80%;opacity:0.8">序列到序列的映射函数${\cal B}$定义在序列$\boldsymbol{\pi}\in{\cal L}’^{T}$上，其中T是长度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> -hh-e-l-ll-oo-<br>(1) </td> <td>  </td> <td> 
<ul><li>For example, B maps “–hh-e-l-ll-oo–” (’-’ represents ’blank’) onto “hello”.<span style="font-size:80%;opacity:0.8">例如，${\cal B}$将“–hh-e-l-ll-oo–”（-表示blank）映射到“hello”。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> computationally<br>(1) </td> <td>  </td> <td> 
<ul><li>Directly computing Eq.1 would be <font color=forestgreen>computationally</font> infeasible due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> infeasible<br>(1) </td> <td> [ɪn'fi:zəbl] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally <font color=orangered>infeasible</font> due to the exponentially large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> exponentially<br>(1) </td> <td> [ˌekspə'nenʃəlɪ] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally infeasible due to the <font color=orangered>exponentially</font> large number of summation items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> summation<br>(1) </td> <td> [sʌˈmeɪʃn] </td> <td> 
<ul><li>Directly computing Eq.1 would be computationally infeasible due to the exponentially large number of <font color=orangered>summation</font> items.<span style="font-size:80%;opacity:0.8">由于存在指数级数量的求和项，直接计算方程1在计算上是不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> tractable<br>(1) </td> <td> [ˈtræktəbl] </td> <td> 
<ul><li>Since there exists no <font color=orangered>tractable</font> algorithm to precisely find the solution, we use the strategy adopted in [15].<span style="font-size:80%;opacity:0.8">由于不存在用于精确找到解的可行方法，我们采用[15]中的策略。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> time-consuming<br>(1) </td> <td> [taɪm kən'sju:mɪŋ] </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very <font color=orangered>time-consuming</font> to perform an exhaustive search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> exhaustive<br>(1) </td> <td> [ɪgˈzɔ:stɪv] </td> <td> 
<ul><li>Basically, the label sequence is recognized by choosing the sequence in the lexicon that has highest conditional probability defined in Eq.1, i.e. $\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$. However, for large lexicons, e.g. the 50k-words Hunspell spell-checking dictionary [1], it would be very time-consuming to perform an <font color=orangered>exhaustive</font> search over the lexicon, i.e. to compute Equation.<span style="font-size:80%;opacity:0.8">基本上，通过选择词典中具有方程1中定义的最高条件概率的序列来识别标签序列，即$\mathbf{l}^{*}=\arg\max_{\mathbf{l}\in{\cal D}}p(\mathbf{l}|\mathbf{y})$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> nearest-neighbor<br>(1) </td> <td> ['nɪərɪstn'eɪbɔ:] </td> <td> 
<ul><li>This indicates that we can limit our search to the <font color=orangered>nearest-neighbor</font> candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> maximal<br>(1) </td> <td> [ˈmæksɪml] </td> <td> 
<ul><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the <font color=orangered>maximal</font> edit distance and $\mathbf{l}’$ is the sequence transcribed from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> transcribe<br>(1) </td> <td> [trænˈskraɪb] </td> <td> 
<ul><li>This indicates that we can limit our search to the nearest-neighbor candidates ${\cal N}_{\delta}(\mathbf{l}’)$, where $\delta$ is the maximal edit distance and $\mathbf{l}’$ is the sequence <font color=orangered>transcribed</font> from $\mathbf{y}$ in lexicon-free mode:<span style="font-size:80%;opacity:0.8">这表示我们可以将搜索限制在最近邻候选目标${\cal N}_{\delta}(\mathbf{l}’)$，其中$\delta$是最大编辑距离，$\mathbf{l}’$是在无词典模式下从$\mathbf{y}$转录的序列：</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> stochastic<br>(1) </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>The network is trained with <font color=orangered>stochastic</font> gradient descent (SGD).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> descent<br>(1) </td> <td> [dɪˈsent] </td> <td> 
<ul><li>The network is trained with stochastic gradient <font color=orangered>descent</font> (SGD).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> SGD<br>(1) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>The network is trained with stochastic gradient descent (<font color=orangered>SGD</font>).<span style="font-size:80%;opacity:0.8">网络使用随机梯度下降（SGD）进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> back-propagated<br>(1) </td> <td>  </td> <td> 
<ul><li>In particular, in the transcription layer, error differentials are <font color=forestgreen>back-propagated</font> with the forward-backward algorithm, as described in [15].<span style="font-size:80%;opacity:0.8">特别地，在转录层中，如[15]所述，误差使用前向算法进行反向传播。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> synth<br>(1) </td> <td> [sɪnθ] </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the synthetic dataset (<font color=orangered>Synth</font>) released by Jaderberg et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> Jaderberg<br>(1) </td> <td>  </td> <td> 
<ul><li>For all the experiments for scene text recognition, we use the synthetic dataset (Synth) released by <font color=forestgreen>Jaderberg</font> et al. [20] as the training data.<span style="font-size:80%;opacity:0.8">对于场景文本识别的所有实验，我们使用Jaderberg等人[20]发布的合成数据集（Synth）作为训练数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> fine-tune<br>(1) </td> <td> [faɪn tju:n] </td> <td> 
<ul><li>Our network is trained on the synthetic data once, and tested on all other real-world test datasets without any <font color=orangered>fine-tuning</font> on their training data.<span style="font-size:80%;opacity:0.8">我们的网络在合成数据上进行了一次训练，并在所有其它现实世界的测试数据集上进行了测试，而没有在其训练数据上进行任何微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> IIIT<br>(1) </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), <font color=forestgreen>IIIT</font> 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> k-word<br>(1) </td> <td>  </td> <td> 
<ul><li>Four popular benchmarks for scene text recognition are used for performance evaluation, namely ICDAR 2003 (IC03), ICDAR 2013 (IC13), IIIT 5k-word (IIIT5k), and Street View Text (SVT).<span style="font-size:80%;opacity:0.8">有四个流行的基准数据集用于场景文本识别的性能评估，即ICDAR 2003（IC03），ICDAR 2013（IC13），IIIT 5k-word（IIIT5k）和Street View Text (SVT)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> non-alphanumeric<br>(1) </td> <td>  </td> <td> 
<ul><li>Following Wang et al. [34], we ignore images that either contain <font color=forestgreen>non-alphanumeric</font> characters or have less than three characters, and get a test set with 860 cropped text images.<span style="font-size:80%;opacity:0.8">王等人[34]，我们忽略包含非字母数字字符或少于三个字符的图像，并获得具有860个裁剪的文本图像的测试集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> VGG-VeryDeep<br>(1) </td> <td>  </td> <td> 
<ul><li>The architecture of the convolutional layers is based on the <font color=forestgreen>VGG-VeryDeep</font> architectures [32].<span style="font-size:80%;opacity:0.8">卷积层的架构是基于VGG-VeryDeep的架构[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> CUDA<br>(1) </td> <td>  </td> <td> 
<ul><li>We implement the network within the Torch7 [10] framework, with custom implementations for the LSTM units (in Torch7/<font color=forestgreen>CUDA</font>), the transcription layer (in C++) and the BK-tree data structure (in C++).<span style="font-size:80%;opacity:0.8">我们在Torch7[10]框架内实现了网络，使用定制实现的LSTM单元（Torch7/CUDA），转录层（C++）和BK树数据结构（C++）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> GHz<br>(1) </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 <font color=forestgreen>GHz</font> Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> Xeon<br>(1) </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) <font color=forestgreen>Xeon</font>(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> GB<br>(1) </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> NVIDIA<br>(1) </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an <font color=orangered>NVIDIA</font>(R) Tesla(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> Tesla<br>(1) </td> <td> ['teslә] </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) <font color=orangered>Tesla</font>(TM) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> TM<br>(1) </td> <td>  </td> <td> 
<ul><li>Experiments are carried out on a workstation with a 2.50 GHz Intel(R) Xeon(R) E5-2609 CPU, 64GB RAM and an NVIDIA(R) Tesla(<font color=forestgreen>TM</font>) K40 GPU.<span style="font-size:80%;opacity:0.8">实验在具有2.50 GHz Intel（R）Xeon E5-2609 CPU，64GB RAM和NVIDIA（R）Tesla(TM) K40 GPU的工作站上进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> convergence<br>(1) </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>The training process takes about 50 hours to reach <font color=orangered>convergence</font>.<span style="font-size:80%;opacity:0.8">训练过程大约需要50个小时才能达到收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> proportionally<br>(1) </td> <td> [prə'pɔ:ʃənlɪ] </td> <td> 
<ul><li>Widths are <font color=orangered>proportionally</font> scaled with heights, but at least 100 pixels.<span style="font-size:80%;opacity:0.8">宽度与高度成比例地缩放，但至少为100像素。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> comparative<br>(1) </td> <td> [kəmˈpærətɪv] </td> <td> 
<ul><li>3.3. <font color=orangered>Comparative</font> Evaluation<span style="font-size:80%;opacity:0.8">3.3. 比较评估</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> consistently<br>(1) </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>In the constrained lexicon cases, our method <font color=orangered>consistently</font> outperforms most state-of-the-arts approaches, and in average beats the best text reader proposed in [22].<span style="font-size:80%;opacity:0.8">在有约束词典的情况中，我们的方法始终优于大多数最新的方法，并且平均打败了[22]中提出的最佳文本阅读器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> PhotoOCR<br>(1) </td> <td>  </td> <td> 
<ul><li>Our method uses only synthetic text with word level labels as the training data, very different to <font color=forestgreen>PhotoOCR</font> [8] which used 7.9 millions of real word images with character-level annotations for training.<span style="font-size:80%;opacity:0.8">我们的方法只使用具有单词级标签的合成文本作为训练数据，与PhotoOCR[8]非常不同，后者使用790万个具有字符级标注的真实单词图像进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> persformance<br>(1) </td> <td>  </td> <td> 
<ul><li>The best <font color=forestgreen>persformance</font> is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model strictly unconstrained to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> strictly<br>(1) </td> <td> [ˈstrɪktli] </td> <td> 
<ul><li>The best persformance is reported by [22] in the unconstrained lexicon cases, benefiting from its large dictionary, however, it is not a model <font color=orangered>strictly</font> unconstrained to a lexicon as mentioned before.<span style="font-size:80%;opacity:0.8">[22]中报告的最佳性能是在无约束词典的情况下，受益于它的大字典，然而，它不是前面提到的严格的无约束词典模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> hand-crafted<br>(1) </td> <td> [,hænd 'kra:ftid] </td> <td> 
<ul><li>Attributes for comparison include: 1) being end-to-end trainable (E2E Train); 2) using convolutional features that are directly learned from images rather than using <font color=orangered>hand-crafted</font> ones (Conv Ftrs); 3) requiring no ground truth bounding boxes for characters during training (CharGT-Free); 4) not confined to a pre-defined dictionary (Unconstrained); 5) the model size (if an end-to-end trainable model is used), measured by the number of model parameters (Model Size, M stands for millions).<span style="font-size:80%;opacity:0.8">比较的属性包括：1)端到端训练(E2E Train)；2)从图像中直接学习卷积特征而不是使用手动设计的特征(Conv Ftrs)；3)训练期间不需要字符的实际边界框(CharGT-Free)；4)不受限于预定义字典(Unconstrained)；5)模型大小（如果使用端到端模型），通过模型参数数量来衡量(Model Size, M表示百万)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 154 </td> <td> handcraft<br>(1) </td> <td> [ˈhændkrɑ:ft] </td> <td> 
<ul><li>Conv Ftrs: This column is to indicate whether an approach uses the convolutional features learned from training images directly or <font color=orangered>handcraft</font> features as the basic representations.<span style="font-size:80%;opacity:0.8">Conv Ftrs：这一列用来表明一个方法是否使用从训练图像直接学习到的卷积特征或手动特征作为基本的表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 155 </td> <td> out-of-dictionary<br>(1) </td> <td>  </td> <td> 
<ul><li>Unconstrained: This column is to indicate whether the trained model is constrained to a specific dictionary, unable to handling <font color=forestgreen>out-of-dictionary</font> words or random sequences.<span style="font-size:80%;opacity:0.8">Unconstrained：这一列用来表明训练模型是否受限于一个特定的字典，是否不能处理字典之外的单词或随机序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 156 </td> <td> incremental<br>(1) </td> <td> [ˌɪŋkrə'mentl] </td> <td> 
<ul><li>Notice that though the recent models learned by label embedding [5, 14] and <font color=orangered>incremental</font> learning [22] achieved highly competitive performance, they are constrained to a specific dictionary.<span style="font-size:80%;opacity:0.8">注意尽管最近通过标签嵌入[5, 14]和增强学习[22]学习到的模型取得了非常有竞争力的性能，但它们受限于一个特定的字典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 157 </td> <td> weight-sharing<br>(1) </td> <td>  </td> <td> 
<ul><li>In CRNN, all layers have <font color=forestgreen>weight-sharing</font> connections, and the fully-connected layers are not needed.<span style="font-size:80%;opacity:0.8">在CRNN中，所有的层有权重共享连接，不需要全连接层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 158 </td> <td> variant<br>(1) </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Consequently, the number of parameters of CRNN is much less than the models learned on the <font color=orangered>variants</font> of CNN [22, 21], resulting in a much smaller model compared with [22, 21].<span style="font-size:80%;opacity:0.8">因此，CRNN的参数数量远小于CNN变体[22,21]所得到的模型，导致与[22,21]相比，模型要小得多。</span></li></ul>
 </td>
</tr>
<tr>
<td> 159 </td> <td> mb<br>(1) </td> <td>  </td> <td> 
<ul><li>Our model has 8.3 million parameters, taking only 33MB RAM (using 4-bytes single-precision float for each parameter), thus it can be easily ported to mobile devices.<span style="font-size:80%;opacity:0.8">我们的模型有830万个参数，只有33MB RAM（每个参数使用4字节单精度浮点数），因此可以轻松地移植到移动设备上。</span></li></ul>
 </td>
</tr>
<tr>
<td> 160 </td> <td> computational<br>(1) </td> <td> [ˌkɒmpjuˈteɪʃənl] </td> <td> 
<ul><li>On the other hand, the <font color=orangered>computational</font> cost grows with larger \delta, due to longer BK-tree search time, as well as larger number of candidate sequences for testing.<span style="font-size:80%;opacity:0.8">另一方面，由于更长的BK树搜索时间，以及更大数量的候选序列用于测试，计算成本随着\delta的增大而增加。</span></li></ul>
 </td>
</tr>
<tr>
<td> 161 </td> <td> tradeoff<br>(1) </td> <td> ['treɪdˌɔ:f] </td> <td> 
<ul><li>In practice, we choose \delta=3 as a <font color=orangered>tradeoff</font> between accuracy and speed.<span style="font-size:80%;opacity:0.8">实际上，我们选择\delta=3作为精度和速度之间的折衷。</span></li></ul>
 </td>
</tr>
<tr>
<td> 162 </td> <td> binirization<br>(1) </td> <td>  </td> <td> 
<ul><li>Previous methods often requires image preprocessing (mostly <font color=forestgreen>binirization</font>), staff lines detection and individual notes recognition [29].<span style="font-size:80%;opacity:0.8">以前的方法通常需要图像预处理（主要是二值化），五线谱检测和单个音符识别[29]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 163 </td> <td> ezpitches<br>(1) </td> <td>  </td> <td> 
<ul><li>We manually label the ground truth label sequences (sequences of not <font color=forestgreen>ezpitches</font>) for all the images.<span style="font-size:80%;opacity:0.8">我们手动标记所有图像的真实标签序列（不是的音调序列）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 164 </td> <td> augment<br>(1) </td> <td> [ɔ:gˈment] </td> <td> 
<ul><li>The collected images are <font color=orangered>augmented</font> to 265k training samples by being rotated, scaled and corrupted with noise, and by replacing their backgrounds with natural images.<span style="font-size:80%;opacity:0.8">收集到的图像通过旋转，缩放和用噪声损坏增强到了265k个训练样本，并用自然图像替换它们的背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 165 </td> <td> augmentation<br>(1) </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>a; 2) “Synthesized”, which is created from “Clean”, using the <font color=orangered>augmentation</font> strategy mentioned above.<span style="font-size:80%;opacity:0.8">实例如图5.a所示；2）“合成的”，使用“纯净的”创建的，使用了上述的增强策略。</span></li></ul>
 </td>
</tr>
<tr>
<td> 166 </td> <td> clutter<br>(1) </td> <td> [ˈklʌtə(r)] </td> <td> 
<ul><li>The main reason is that they rely on robust binarization to detect staff lines and notes, but the binarization step often fails on synthesized and real-world data due to bad lighting condition, noise corruption and <font color=orangered>cluttered</font> background.<span style="font-size:80%;opacity:0.8">主要原因是它们依赖于强大的二值化来检五线谱和音符，但是由于光线不良，噪音破坏和杂乱的背景，二值化步骤经常会在合成数据和现实数据上失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 167 </td> <td> minimal<br>(1) </td> <td> [ˈmɪnɪməl] </td> <td> 
<ul><li>The results have shown the generality of CRNN, in that it can be readily applied to other image-based sequence recognition problems, requiring <font color=orangered>minimal</font> domain knowledge.<span style="font-size:80%;opacity:0.8">结果显示了CRNN的泛化性，因为它可以很容易地应用于其它的基于图像的序列识别问题，需要极少的领域知识。</span></li></ul>
 </td>
</tr>
<tr>
<td> 168 </td> <td> CRNN-based<br>(1) </td> <td>  </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our <font color=forestgreen>CRNN-based</font> system is still preliminary and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 169 </td> <td> preliminary<br>(1) </td> <td> [prɪˈlɪmɪnəri] </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our CRNN-based system is still <font color=orangered>preliminary</font> and misses many functionalities.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 170 </td> <td> functionality<br>(1) </td> <td> [ˌfʌŋkʃəˈnæləti] </td> <td> 
<ul><li>Compared with Capella Scan and PhotoScore, our CRNN-based system is still preliminary and misses many <font color=orangered>functionalities</font>.<span style="font-size:80%;opacity:0.8">与Capella Scan和PhotoScore相比，我们的基于CRNN的系统仍然是初步的，并且缺少许多功能。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>