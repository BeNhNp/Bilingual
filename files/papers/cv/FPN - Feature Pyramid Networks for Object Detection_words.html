<html>
<head>
<meta charset="utf-8">
<title> FPN - Feature Pyramid Networks for Object Detection </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> pyramidal </td> <td> ['pɪrəmɪdl] </td> <td> 
<ul><li>In this paper, we exploit the inherent multi-scale, <font color=orangered>pyramidal</font> hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost.<span style="font-size:80%;opacity:0.8"> 在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。</span></li><li>(c) An alternative is to reuse the <font color=orangered>pyramidal</font> feature hierarchy computed by a ConvNet as if it were a featurized image pyramid.<span style="font-size:80%;opacity:0.8"> （c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li>A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, <font color=orangered>pyramidal</font> shape.<span style="font-size:80%;opacity:0.8"> 深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s <font color=orangered>pyramidal</font> feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8"> 单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>The goal of this paper is to naturally leverage the <font color=orangered>pyramidal</font> shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8"> 本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>Although these methods adopt architectures with <font color=orangered>pyramidal</font> shapes, they are unlike featurized image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2.<span style="font-size:80%;opacity:0.8"> 尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。</span></li><li>In fact, for the <font color=orangered>pyramidal</font> architecture in Fig. 2 (top), image pyramids are still needed to recognize objects across multiple scales [28].<span style="font-size:80%;opacity:0.8"> 事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</span></li><li>Our goal is to leverage a ConvNet’s <font color=orangered>pyramidal</font> feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8"> 我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>This architecture simulates the effect of reusing the <font color=orangered>pyramidal</font> feature hierarchy (Fig. 1(b)).<span style="font-size:80%;opacity:0.8"> 该架构模拟了重用金字塔特征层次结构的效果（图1（b））。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> lateral </td> <td> [ˈlætərəl] </td> <td> 
<ul><li>A top-down architecture with <font color=orangered>lateral</font> connections is developed for building high-level semantic feature maps at all scales.<span style="font-size:80%;opacity:0.8"> 开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and <font color=orangered>lateral</font> connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8"> 为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>There are recent methods exploiting <font color=orangered>lateral</font>/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and <font color=orangered>lateral</font> connections, as introduced in the following.<span style="font-size:80%;opacity:0.8"> 如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>Top-down pathway and <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径和横向连接。</span></li><li>These features are then enhanced with features from the bottom-up pathway via <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8"> 这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。</span></li><li>Each <font color=orangered>lateral</font> connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway.<span style="font-size:80%;opacity:0.8"> 每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>A building block illustrating the <font color=orangered>lateral</font> connection and the top-down pathway, merged by addition.<span style="font-size:80%;opacity:0.8"> 构建模块说明了横向连接和自顶向下路径，通过加法合并。</span></li><li>The columns “<font color=orangered>lateral</font>” and “top-down” denote the presence of lateral and top-down connections, respectively.<span style="font-size:80%;opacity:0.8"> 列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。</span></li><li>The columns “lateral” and “top-down” denote the presence of <font color=orangered>lateral</font> and top-down connections, respectively.<span style="font-size:80%;opacity:0.8"> 列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。</span></li><li>With this modification, the 1×1 <font color=orangered>lateral</font> connections followed by 3×3 convolutions are attached to the bottom-up pyramid.<span style="font-size:80%;opacity:0.8"> 通过这种修改，将1×1横向连接和后面的3×3卷积添加到自下而上的金字塔中。</span></li><li>How important are <font color=orangered>lateral</font> connections? Table 1(e) shows the ablation results of a top-down feature pyramid without the 1×1 lateral connections.<span style="font-size:80%;opacity:0.8"> 横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>How important are lateral connections? Table 1(e) shows the ablation results of a top-down feature pyramid without the 1×1 <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8"> 横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>More precise locations of features can be directly passed from the finer levels of the bottom-up maps via the <font color=orangered>lateral</font> connections to the top-down maps.<span style="font-size:80%;opacity:0.8"> 更精确的特征位置可以通过横向连接直接从自下而上映射的更精细层级传递到自上而下的映射。</span></li><li>Table 2(d) and (e) show that removing top-down connections or removing <font color=orangered>lateral</font> connections leads to inferior results, similar to what we have observed in the above sub-section for RPN.<span style="font-size:80%;opacity:0.8"> 表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> semantic </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>A top-down architecture with lateral connections is developed for building high-level <font color=orangered>semantic</font> feature maps at all scales.<span style="font-size:80%;opacity:0.8"> 开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。</span></li><li>Aside from being capable of representing higher-level <font color=orangered>semantics</font>, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8"> 除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large <font color=orangered>semantic</font> gaps caused by different depths.<span style="font-size:80%;opacity:0.8"> 这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。</span></li><li>The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong <font color=orangered>semantics</font> at all scales.<span style="font-size:80%;opacity:0.8"> 本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>The result is a feature pyramid that has rich <font color=orangered>semantics</font> at all levels and is built quickly from a single input image scale.<span style="font-size:80%;opacity:0.8"> 其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。</span></li><li>FCN [24] sums partial scores for each category over multiple scales to compute <font color=orangered>semantic</font> segmentations.<span style="font-size:80%;opacity:0.8"> FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。</span></li><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and <font color=orangered>semantic</font> levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has <font color=orangered>semantics</font> from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8"> 我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level <font color=orangered>semantics</font> throughout.<span style="font-size:80%;opacity:0.8"> 我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>The bottom-up feature map is of lower-level <font color=orangered>semantics</font>, but its activations are more accurately localized as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8"> 自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li><li>The good performance of sharing parameters indicates that all levels of our pyramid share similar <font color=orangered>semantic</font> levels.<span style="font-size:80%;opacity:0.8"> 共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single higher-level feature map is not enough because there is a trade-off between coarser resolutions and stronger <font color=orangered>semantics</font>.<span style="font-size:80%;opacity:0.8"> 表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li><li>We conjecture that this is because there are large <font color=orangered>semantic</font> gaps between different levels on the bottom-up pyramid (Fig. 1(b)), especially for very deep ResNets.<span style="font-size:80%;opacity:0.8"> 我们推测这是因为自下而上的金字塔（图1（b））的不同层次之间存在较大的语义差距，尤其是对于非常深的ResNets。</span></li><li>This top-down pyramid has strong <font color=orangered>semantic</font> features and fine resolutions.<span style="font-size:80%;opacity:0.8"> 这个自顶向下的金字塔具有强大的语义特征和良好的分辨率。</span></li><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly <font color=orangered>semantic</font> feature maps of $P_2$ (i.e., the finest level in our pyramids).<span style="font-size:80%;opacity:0.8"> 金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> FPN </td> <td> [!≈ ef pi: en] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (<font color=orangered>FPN</font>), shows significant improvement as a generic feature extractor in several applications.<span style="font-size:80%;opacity:0.8"> 这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>Using <font color=orangered>FPN</font> in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners.<span style="font-size:80%;opacity:0.8"> 在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。</span></li><li>(d) Our proposed Feature Pyramid Network (<font color=orangered>FPN</font>) is fast like (b) and (c), but more accurate.<span style="font-size:80%;opacity:0.8"> （d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。</span></li><li>We evaluate our method, called a Feature Pyramid Network (<font color=orangered>FPN</font>), in various systems for detection and segmentation [11, 29, 27].<span style="font-size:80%;opacity:0.8"> 我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。</span></li><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on <font color=orangered>FPN</font> and a basic Faster R-CNN detector [29], surpassing all existing heavily-engineered single-model entries of competition winners.<span style="font-size:80%;opacity:0.8"> 没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li><li>In ablation experiments, we find that for bounding box proposals, <font color=orangered>FPN</font> significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8"> 在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>As a result, <font color=orangered>FPNs</font> are able to achieve higher accuracy than all existing state-of-the-art methods.<span style="font-size:80%;opacity:0.8"> 因此，FPN能够比所有现有的最先进方法获得更高的准确度。</span></li><li>We also generalize <font color=orangered>FPNs</font> to instance segmentation proposals in Sec.6.<span style="font-size:80%;opacity:0.8"> 在第6节中我们还将FPN泛化到实例细分提议。</span></li><li>We adapt RPN by replacing the single-scale feature map with our <font color=orangered>FPN</font>.<span style="font-size:80%;opacity:0.8"> 我们通过用我们的FPN替换单尺度特征映射来适应RPN。</span></li><li>With the above adaptations, RPN can be naturally trained and tested with our <font color=orangered>FPN</font>, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8"> 通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>To use it with our <font color=orangered>FPN</font>, we need to assign RoIs of different scales to the pyramid levels.<span style="font-size:80%;opacity:0.8"> 要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</span></li><li>Training RPN with <font color=orangered>FPN</font> on 8 GPUs takes about 8 hours on COCO.<span style="font-size:80%;opacity:0.8"> 使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</span></li><li>Placing <font color=orangered>FPN</font> in RPN improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale RPN baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8"> 将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>As a results, <font color=orangered>FPN</font> has an $AR^1k$ score 10 points higher than Table 1(e).<span style="font-size:80%;opacity:0.8"> 因此，FPN的$AR^1k$的得分比表1（e）高10个点。</span></li><li>Next we investigate <font color=orangered>FPN</font> for region-based (non-sliding window) detectors.<span style="font-size:80%;opacity:0.8"> 接下来我们研究基于区域（非滑动窗口）检测器的FPN。</span></li><li>Training Fast R-CNN with <font color=orangered>FPN</font> takes about 10 hours on the COCO dataset.<span style="font-size:80%;opacity:0.8"> 使用FPN在COCO数据集上训练Fast R-CNN需要约10小时。</span></li><li>To better investigate <font color=orangered>FPN</font>’s effects on the region-based detector alone, we conduct ablations of Fast R-CNN on a fixed set of proposals.<span style="font-size:80%;opacity:0.8"> 为了更好地调查FPN对仅基于区域的检测器的影响，我们在一组固定的提议上进行Fast R-CNN的消融。</span></li><li>We choose to freeze the proposals as computed by RPN on <font color=orangered>FPN</font> (Table 1(c)), because it has good performance on small objects that are to be recognized by the detector.<span style="font-size:80%;opacity:0.8"> 我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。</span></li><li>Table 2(c) shows the results of our <font color=orangered>FPN</font> in Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 表2（c）显示了Fast R-CNN中我们的FPN结果。</span></li><li>Under controlled settings, our <font color=orangered>FPN</font> (Table 3(c)) is better than this strong baseline by 2.3 points AP and 3.8 points AP@0.5.<span style="font-size:80%;opacity:0.8"> 在受控的环境下，我们的FPN（表3（c））比这个强劲的基线要好2.3个点的AP和3.8个点的AP@0.5。</span></li><li>More object detection results using Faster R-CNN and our <font color=orangered>FPNs</font>, evaluated on minival.<span style="font-size:80%;opacity:0.8"> 使用Faster R-CNN和我们的FPN在minival上的更多目标检测结果。</span></li><li>Our method introduces small extra cost by the extra layers in the <font color=orangered>FPN</font>, but has a lighter weight head.<span style="font-size:80%;opacity:0.8"> 我们的方法通过FPN中的额外层引入了较小的额外成本，但具有更轻的头部。</span></li><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to <font color=orangered>FPNs</font> and should boost accuracy further.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li><li>Recently, <font color=orangered>FPN</font> has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。</span></li><li>In this section we use <font color=orangered>FPNs</font> to generate segmentation proposals, following the DeepMask/SharpMask framework [27, 28].<span style="font-size:80%;opacity:0.8"> 在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li>It is easy to adapt <font color=orangered>FPN</font> to generate mask proposals.<span style="font-size:80%;opacity:0.8"> 改编FPN生成掩码提议很容易。</span></li><li><font color=orangered>FPN</font> for object segment proposals.<span style="font-size:80%;opacity:0.8"> 目标分割提议的FPN。</span></li><li>Our baseline <font color=orangered>FPN</font> model with a single 5×5 MLP achieves an AR of 43.4.<span style="font-size:80%;opacity:0.8"> 我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。</span></li><li>DeepMask, SharpMask, and <font color=orangered>FPN</font> use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8"> DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li>Our approach, based on <font color=orangered>FPNs</font>, is substantially faster (our models run at 6 to 7 FPS).<span style="font-size:80%;opacity:0.8"> 我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> generic </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a <font color=orangered>generic</font> feature extractor in several applications.<span style="font-size:80%;opacity:0.8"> 这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>Our method is a <font color=orangered>generic</font> solution for building feature pyramids inside deep ConvNets.<span style="font-size:80%;opacity:0.8"> 我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。</span></li><li>Our method is a <font color=orangered>generic</font> pyramid representation and can be used in applications other than object detection.<span style="font-size:80%;opacity:0.8"> 我们的方法是一种通用金字塔表示，可用于除目标检测之外的其他应用。</span></li><li>These results demonstrate that our model is a <font color=orangered>generic</font> feature extractor and can replace image pyramids for other multi-scale detection problems.<span style="font-size:80%;opacity:0.8"> 这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> extractor </td> <td> [ɪkˈstræktə(r)] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature <font color=orangered>extractor</font> in several applications.<span style="font-size:80%;opacity:0.8"> 这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>These results demonstrate that our model is a generic feature <font color=orangered>extractor</font> and can replace image pyramids for other multi-scale detection problems.<span style="font-size:80%;opacity:0.8"> 这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> surpass </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, <font color=orangered>surpassing</font> all existing single-model entries including those from the COCO 2016 challenge winners.<span style="font-size:80%;opacity:0.8"> 在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。</span></li><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], <font color=orangered>surpassing</font> all existing heavily-engineered single-model entries of competition winners.<span style="font-size:80%;opacity:0.8"> 没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li><li>Table 4 compares our method with the single-model results of the COCO competition winners, including the 2016 winner G-RMI and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has <font color=orangered>surpassed</font> these strong, heavily engineered competitors.<span style="font-size:80%;opacity:0.8"> 表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> FPS </td> <td> ['efp'i:'es] </td> <td> 
<ul><li>In addition, our method can run at 6 <font color=orangered>FPS</font> on a GPU and thus is a practical and accurate solution to multi-scale object detection.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。</span></li><li>Our approach, based on FPNs, is substantially faster (our models run at 6 to 7 <font color=orangered>FPS</font>).<span style="font-size:80%;opacity:0.8"> 我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> featurize </td> <td> ['fi:tʃәraiz] </td> <td> 
<ul><li>Feature pyramids built upon image pyramids (for short we call these <font color=orangered>featurized</font> image pyramids) form the basis of a standard solution [1] (Fig. 1(a)).<span style="font-size:80%;opacity:0.8"> 建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。</span></li><li>(c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a <font color=orangered>featurized</font> image pyramid.<span style="font-size:80%;opacity:0.8"> （c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li><font color=orangered>Featurized</font> image pyramids were heavily used in the era of hand-engineered features [5, 25].<span style="font-size:80%;opacity:0.8"> 特征化图像金字塔在手工设计的时代被大量使用[5，25]。</span></li><li>All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on <font color=orangered>featurized</font> image pyramids (e.g., [16, 35]).<span style="font-size:80%;opacity:0.8"> 在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。</span></li><li>For these reasons, Fast and Faster R-CNN [11, 29] opt to not use <font color=orangered>featurized</font> image pyramids under default settings.<span style="font-size:80%;opacity:0.8"> 出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a <font color=orangered>featurized</font> image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8"> 单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>In other words, we show how to create in-network feature pyramids that can be used to replace <font color=orangered>featurized</font> image pyramids without sacrificing representational power, speed, or memory.<span style="font-size:80%;opacity:0.8"> 换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li><li>Our model echoes a <font color=orangered>featurized</font> image pyramid, which has not been explored in these works.<span style="font-size:80%;opacity:0.8"> 我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</span></li><li>There has also been significant interest in computing <font color=orangered>featurized</font> image pyramids quickly.<span style="font-size:80%;opacity:0.8"> 这对快速计算特征化图像金字塔也很有意义。</span></li><li>Although these methods adopt architectures with pyramidal shapes, they are unlike <font color=orangered>featurized</font> image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2.<span style="font-size:80%;opacity:0.8"> 尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。</span></li><li>Because all levels of the pyramid use shared classifiers/regressors as in a traditional <font color=orangered>featurized</font> image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps.<span style="font-size:80%;opacity:0.8"> 由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为d）。</span></li><li>This advantage is analogous to that of using a <font color=orangered>featurized</font> image pyramid, where a common head classifier can be applied to features computed at any image scale.<span style="font-size:80%;opacity:0.8"> 这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> scale-invariant </td> <td> [!≈ skeɪl ɪnˈveəriənt] </td> <td> 
<ul><li>These pyramids are <font color=orangered>scale-invariant</font> in the sense that an object’s scale change is offset by shifting its level in the pyramid.<span style="font-size:80%;opacity:0.8"> 这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> ConvNet </td> <td>  </td> <td> 
<ul><li>(c) An alternative is to reuse the pyramidal feature hierarchy computed by a <font color=forestgreen>ConvNet</font> as if it were a featurized image pyramid.<span style="font-size:80%;opacity:0.8"> （c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li>For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (<font color=forestgreen>ConvNets</font>) [19, 20].<span style="font-size:80%;opacity:0.8"> 对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。</span></li><li>Aside from being capable of representing higher-level semantics, <font color=forestgreen>ConvNets</font> are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8"> 除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>A deep <font color=forestgreen>ConvNet</font> computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape.<span style="font-size:80%;opacity:0.8"> 深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a <font color=forestgreen>ConvNet</font>’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8"> 单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>The goal of this paper is to naturally leverage the pyramidal shape of a <font color=forestgreen>ConvNet</font>’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8"> 本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>Before HOG and SIFT, early work on face detection with <font color=forestgreen>ConvNets</font> [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8"> 在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li><li>Deep <font color=forestgreen>ConvNet</font> object detectors.<span style="font-size:80%;opacity:0.8"> Deep ConvNet目标检测器。</span></li><li>With the development of modern deep <font color=forestgreen>ConvNets</font> [19], object detectors like OverFeat [34] and R-CNN [12] showed dramatic improvements in accuracy.<span style="font-size:80%;opacity:0.8"> 随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。</span></li><li>OverFeat adopted a strategy similar to early neural network face detectors by applying a <font color=forestgreen>ConvNet</font> as a sliding window detector on an image pyramid.<span style="font-size:80%;opacity:0.8"> OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。</span></li><li>R-CNN adopted a region proposal-based strategy [37] in which each proposal was scale-normalized before classifying with a <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8"> R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。</span></li><li>A number of recent approaches improve detection and segmentation by using different layers in a <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8"> 一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。</span></li><li>Our goal is to leverage a <font color=forestgreen>ConvNet</font>’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8"> 我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>The bottom-up pathway is the feed-forward computation of the backbone <font color=forestgreen>ConvNet</font>, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8"> 自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>Our method is a generic solution for building feature pyramids inside deep <font color=forestgreen>ConvNets</font>.<span style="font-size:80%;opacity:0.8"> 我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。</span></li><li>We have presented a clean and simple framework for building feature pyramids inside <font color=forestgreen>ConvNets</font>.<span style="font-size:80%;opacity:0.8"> 我们提出了一个干净而简单的框架，用于在ConvNets内部构建特征金字塔。</span></li><li>Finally, our study suggests that despite the strong representational power of deep <font color=forestgreen>ConvNets</font> and their implicit robustness to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8"> 最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> semantically </td> <td> [sɪ'mæntɪklɪ] </td> <td> 
<ul><li>In this figure, feature maps are indicate by blue outlines and thicker outlines denote <font color=orangered>semantically</font> stronger features.<span style="font-size:80%;opacity:0.8"> 在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</span></li><li>The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are <font color=orangered>semantically</font> strong, including the high-resolution levels.<span style="font-size:80%;opacity:0.8"> 对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, <font color=orangered>semantically</font> strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8"> 为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, <font color=orangered>semantically</font> weak features via a top-down pathway and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8"> 为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but <font color=orangered>semantically</font> stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> hand-engineered </td> <td> [!≈ hænd 'endʒɪn'ɪərd] </td> <td> 
<ul><li>Featurized image pyramids were heavily used in the era of <font color=orangered>hand-engineered</font> features [5, 25].<span style="font-size:80%;opacity:0.8"> 特征化图像金字塔在手工设计的时代被大量使用[5，25]。</span></li><li><font color=orangered>Hand-engineered</font> features and early neural networks.<span style="font-size:80%;opacity:0.8"> 手工设计特征和早期神经网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> DPM </td> <td> [!≈ di: pi: em] </td> <td> 
<ul><li>They were so critical that object detectors like <font color=orangered>DPM</font> [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave).<span style="font-size:80%;opacity:0.8"> 它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> e.g. </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (<font color=orangered>e.g.</font>, 10 scales per octave).<span style="font-size:80%;opacity:0.8"> 它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li><li>All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (<font color=orangered>e.g.</font>, [16, 35]).<span style="font-size:80%;opacity:0.8"> 在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。</span></li><li>Inference time increases considerably (<font color=orangered>e.g.</font>, by four times [11]), making this approach impractical for real applications.<span style="font-size:80%;opacity:0.8"> 推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。</span></li><li>But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (<font color=orangered>e.g.</font>, conv4_3 of VGG nets [36]) and then by adding several new layers.<span style="font-size:80%;opacity:0.8"> 但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。</span></li><li>On the contrary, our method leverages the architecture as a feature pyramid where predictions (<font color=orangered>e.g.</font>, object detections) are independently made on each level (Fig. 2 bottom).<span style="font-size:80%;opacity:0.8"> 相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。</span></li><li>Top: a top-down architecture with skip connections, where predictions are made on the finest level (<font color=orangered>e.g.</font>, [28]).<span style="font-size:80%;opacity:0.8"> 顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。</span></li><li>This process is independent of the backbone convolutional architectures (<font color=orangered>e.g.</font>, [19, 36, 16]), and in this paper we present results using ResNets [16].<span style="font-size:80%;opacity:0.8"> 这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。</span></li><li>We have experimented with more sophisticated blocks (<font color=orangered>e.g.</font>, using multi-layer residual blocks [16] as the connections) and observed marginally better results.<span style="font-size:80%;opacity:0.8"> 我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li><li>Existing mask proposal methods [27, 28, 4] are based on densely sampled image pyramids (<font color=orangered>e.g.</font>, scaled by 2^{\lbrace −2:0.5:1 \rbrace} in [27, 28]), making them computationally expensive.<span style="font-size:80%;opacity:0.8"> 现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为2^{\lbrace −2:0.5:1 \rbrace}），使得它们是计算昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> octave </td> <td> [ˈɒktɪv] </td> <td> 
<ul><li>They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per <font color=orangered>octave</font>).<span style="font-size:80%;opacity:0.8"> 它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li><li>Additionally, motivated by the use of 2 scales per <font color=orangered>octave</font> in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8"> 此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>Additionally, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half <font color=orangered>octaves</font>.<span style="font-size:80%;opacity:0.8"> 此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>Half <font color=orangered>octaves</font> are handled by an MLP on 7x7 windows ($7 \approx 5 \sqrt 2$), not shown here.<span style="font-size:80%;opacity:0.8"> 半个组由MLP在7x7窗口（ $7 \ approx 5 \ sqrt 2 $）处理，此处未展示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> higher-level </td> <td> [!≈ ˈhaɪə(r) ˈlevl] </td> <td> 
<ul><li>Aside from being capable of representing <font color=orangered>higher-level</font> semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8"> 除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single <font color=orangered>higher-level</font> feature map is not enough because there is a trade-off between coarser resolutions and stronger semantics.<span style="font-size:80%;opacity:0.8"> 表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> variance </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>Aside from being capable of representing higher-level semantics, ConvNets are also more robust to <font color=orangered>variance</font> in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8"> 除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale <font color=orangered>variance</font>.<span style="font-size:80%;opacity:0.8"> RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> robustness </td> <td> [rəʊ'bʌstnəs] </td> <td> 
<ul><li>But even with this <font color=orangered>robustness</font>, pyramids are still needed to get the most accurate results.<span style="font-size:80%;opacity:0.8"> 但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。</span></li><li>Our pyramid representation greatly improves RPN’s <font color=orangered>robustness</font> to object scale variation.<span style="font-size:80%;opacity:0.8"> 我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</span></li><li>RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its <font color=orangered>robustness</font> to scale variance.<span style="font-size:80%;opacity:0.8"> RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li><li>Finally, our study suggests that despite the strong representational power of deep ConvNets and their implicit <font color=orangered>robustness</font> to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8"> 最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> featurizing </td> <td>  </td> <td> 
<ul><li>The principle advantage of <font color=forestgreen>featurizing</font> each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.<span style="font-size:80%;opacity:0.8"> 对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</span></li><li>Nevertheless, <font color=forestgreen>featurizing</font> each level of an image pyramid has obvious limitations.<span style="font-size:80%;opacity:0.8"> 尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> impractical </td> <td> [ɪmˈpræktɪkl] </td> <td> 
<ul><li>Inference time increases considerably (e.g., by four times [11]), making this approach <font color=orangered>impractical</font> for real applications.<span style="font-size:80%;opacity:0.8"> 推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> infeasible </td> <td> [ɪn'fi:zəbl] </td> <td> 
<ul><li>Moreover, training deep networks end-to-end on an image pyramid is <font color=orangered>infeasible</font> in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an inconsistency between train/test-time inference.<span style="font-size:80%;opacity:0.8"> 此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> inconsistency </td> <td> [ˌɪnkən'sɪstənsɪ] </td> <td> 
<ul><li>Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an <font color=orangered>inconsistency</font> between train/test-time inference.<span style="font-size:80%;opacity:0.8"> 此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> subsampling </td> <td>  </td> <td> 
<ul><li>A deep ConvNet computes a feature hierarchy layer by layer, and with <font color=forestgreen>subsampling</font> layers the feature hierarchy has an inherent multi-scale, pyramidal shape.<span style="font-size:80%;opacity:0.8"> 深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> in-network </td> <td> [!≈ ɪn ˈnetwɜ:k] </td> <td> 
<ul><li>This <font color=orangered>in-network</font> feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths.<span style="font-size:80%;opacity:0.8"> 这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。</span></li><li>In other words, we show how to create <font color=orangered>in-network</font> feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory.<span style="font-size:80%;opacity:0.8"> 换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> representational </td> <td> [ˌreprɪzenˈteɪʃnl] </td> <td> 
<ul><li>The high-resolution maps have low-level features that harm their <font color=orangered>representational</font> capacity for object recognition.<span style="font-size:80%;opacity:0.8"> 高分辨率映射具有损害其目标识别表示能力的低级特征。</span></li><li>In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing <font color=orangered>representational</font> power, speed, or memory.<span style="font-size:80%;opacity:0.8"> 换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li><li>Finally, our study suggests that despite the strong <font color=orangered>representational</font> power of deep ConvNets and their implicit robustness to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8"> 最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> Ideally </td> <td> [aɪ'di:əlɪ] </td> <td> 
<ul><li><font color=orangered>Ideally</font>, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost.<span style="font-size:80%;opacity:0.8"> 理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> SSD-style </td> <td>  </td> <td> 
<ul><li>Ideally, the <font color=forestgreen>SSD-style</font> pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost.<span style="font-size:80%;opacity:0.8"> 理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> forego </td> <td> [fɔ:ˈɡəu] </td> <td> 
<ul><li>But to avoid using low-level features SSD <font color=orangered>foregoes</font> reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4_3 of VGG nets [36]) and then by adding several new layers.<span style="font-size:80%;opacity:0.8"> 但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> higher-resolution </td> <td> [!≈ ˈhaɪə(r) ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>Thus it misses the opportunity to reuse the <font color=orangered>higher-resolution</font> maps of the feature hierarchy.<span style="font-size:80%;opacity:0.8"> 因此它错过了重用特征层级的更高分辨率映射的机会。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> leverage </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>The goal of this paper is to naturally <font color=orangered>leverage</font> the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8"> 本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>On the contrary, our method <font color=orangered>leverages</font> the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. 2 bottom).<span style="font-size:80%;opacity:0.8"> 相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。</span></li><li>Bottom: our model that has a similar structure but <font color=orangered>leverages</font> it as a feature pyramid, with predictions made independently at all levels.<span style="font-size:80%;opacity:0.8"> 底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</span></li><li>Our goal is to <font color=orangered>leverage</font> a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8"> 我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> pathway </td> <td> [ˈpɑ:θweɪ] </td> <td> 
<ul><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down <font color=orangered>pathway</font> and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8"> 为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>The construction of our pyramid involves a bottom-up <font color=orangered>pathway</font>, a top-down pathway, and lateral connections, as introduced in the following.<span style="font-size:80%;opacity:0.8"> 如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>The construction of our pyramid involves a bottom-up pathway, a top-down <font color=orangered>pathway</font>, and lateral connections, as introduced in the following.<span style="font-size:80%;opacity:0.8"> 如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>Bottom-up <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8"> 自下而上的路径。</span></li><li>The bottom-up <font color=orangered>pathway</font> is the feed-forward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8"> 自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>Top-down <font color=orangered>pathway</font> and lateral connections.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径和横向连接。</span></li><li>The top-down <font color=orangered>pathway</font> hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li><li>These features are then enhanced with features from the bottom-up <font color=orangered>pathway</font> via lateral connections.<span style="font-size:80%;opacity:0.8"> 这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。</span></li><li>Each lateral connection merges feature maps of the same spatial size from the bottom-up <font color=orangered>pathway</font> and the top-down pathway.<span style="font-size:80%;opacity:0.8"> 每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8"> 每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>A building block illustrating the lateral connection and the top-down <font color=orangered>pathway</font>, merged by addition.<span style="font-size:80%;opacity:0.8"> 构建模块说明了横向连接和自顶向下路径，通过加法合并。</span></li><li>How important is top-down enrichment? Table 1(d) shows the results of our feature pyramid without the top-down <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8"> 自上而下的改进有多重要？表1（d）显示了没有自上而下路径的特征金字塔的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> heavily-engineered </td> <td> [!≈ ˈhevɪli 'endʒɪn'ɪərd] </td> <td> 
<ul><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], surpassing all existing <font color=orangered>heavily-engineered</font> single-model entries of competition winners.<span style="font-size:80%;opacity:0.8"> 没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> ablation </td> <td> [əˈbleɪʃn] </td> <td> 
<ul><li>In <font color=orangered>ablation</font> experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8"> 在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We train using the union of 80k train images and a 35k subset of val images (trainval35k [2]), and report <font color=orangered>ablations</font> on a 5k subset of val images (minival).<span style="font-size:80%;opacity:0.8"> 我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>5.1.1 <font color=orangered>Ablation</font> Experiments<span style="font-size:80%;opacity:0.8"> 5.1.1 消融实验</span></li><li>How important are lateral connections? Table 1(e) shows the <font color=orangered>ablation</font> results of a top-down feature pyramid without the 1×1 lateral connections.<span style="font-size:80%;opacity:0.8"> 横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>To better investigate FPN’s effects on the region-based detector alone, we conduct <font color=orangered>ablations</font> of Fast R-CNN on a fixed set of proposals.<span style="font-size:80%;opacity:0.8"> 为了更好地调查FPN对仅基于区域的检测器的影响，我们在一组固定的提议上进行Fast R-CNN的消融。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>In ablation experiments, we find that for <font color=orangered>bounding</font> box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8"> 在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>In the following we adopt our method in RPN [29] for <font color=orangered>bounding</font> box proposal generation and in Fast R-CNN [11] for object detection.<span style="font-size:80%;opacity:0.8"> 在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。</span></li><li>In the original RPN design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and <font color=orangered>bounding</font> box regression.<span style="font-size:80%;opacity:0.8"> 在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>The object/non-object criterion and <font color=orangered>bounding</font> box regression target are defined with respect to a set of reference boxes called anchors [29].<span style="font-size:80%;opacity:0.8"> 目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。</span></li><li>We assign training labels to the anchors based on their Intersection-over-Union (IoU) ratios with ground-truth <font color=orangered>bounding</font> boxes as in [29].<span style="font-size:80%;opacity:0.8"> 如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and <font color=orangered>bounding</font> box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8"> 我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li><li>So unlike [16], we simply adopt RoI pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected (fc) layers (each followed by ReLU) before the final classification and <font color=orangered>bounding</font> box regression layers.<span style="font-size:80%;opacity:0.8"> 因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（fc）层（每层后都接ReLU层）。</span></li><li><font color=orangered>Bounding</font> box proposal results using RPN [29], evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8"> 使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> COCO-style </td> <td> [!≈ 'kəʊkəʊ staɪl] </td> <td> 
<ul><li>In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the <font color=orangered>COCO-style</font> Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8"> 在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We evaluate the <font color=orangered>COCO-style</font> Average Recall (AR) and AR on small, medium, and large objects ($AR_s$, $AR_m$, and $AR_l$) following the definitions in [21].<span style="font-size:80%;opacity:0.8"> 根据[21]中的定义，我们评估了COCO类型的平均召回率（AR）和在小型，中型和大型目标($AR_s$, $AR_m$, and $AR_lv)上的AR。</span></li><li>We evaluate object detection by the <font color=orangered>COCO-style</font> Average Precision (AP) and PASCAL-style AP (at a single IoU threshold of 0.5).<span style="font-size:80%;opacity:0.8"> 我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> PASCAL-style </td> <td> [!≈ 'pæskәl staɪl] </td> <td> 
<ul><li>In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and <font color=orangered>PASCAL-style</font> AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8"> 在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We evaluate object detection by the COCO-style Average Precision (AP) and <font color=orangered>PASCAL-style</font> AP (at a single IoU threshold of 0.5).<span style="font-size:80%;opacity:0.8"> 我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> consistently </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>In addition, our pyramid structure can be trained end-to-end with all scales and is used <font color=orangered>consistently</font> at train/test time, which would be memory-infeasible using image pyramids.<span style="font-size:80%;opacity:0.8"> 另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> memory-infeasible </td> <td> [!≈ ˈmeməri ɪn'fi:zəbl] </td> <td> 
<ul><li>In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be <font color=orangered>memory-infeasible</font> using image pyramids.<span style="font-size:80%;opacity:0.8"> 另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> SIFT </td> <td> [sɪft] </td> <td> 
<ul><li><font color=orangered>SIFT</font> features [25] were originally extracted at scale-space extrema and used for feature point matching.<span style="font-size:80%;opacity:0.8"> SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li><li>HOG features [5], and later <font color=orangered>SIFT</font> features as well, were computed densely over entire image pyramids.<span style="font-size:80%;opacity:0.8"> HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。</span></li><li>These HOG and <font color=orangered>SIFT</font> pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more.<span style="font-size:80%;opacity:0.8"> 这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。</span></li><li>Before HOG and <font color=orangered>SIFT</font>, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8"> 在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> scale-space </td> <td> [!≈ skeɪl speɪs] </td> <td> 
<ul><li>SIFT features [25] were originally extracted at <font color=orangered>scale-space</font> extrema and used for feature point matching.<span style="font-size:80%;opacity:0.8"> SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> extrema </td> <td> [ɪks'tri:mə] </td> <td> 
<ul><li>SIFT features [25] were originally extracted at scale-space <font color=orangered>extrema</font> and used for feature point matching.<span style="font-size:80%;opacity:0.8"> SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> HOG </td> <td> [hɒg] </td> <td> 
<ul><li><font color=orangered>HOG</font> features [5], and later SIFT features as well, were computed densely over entire image pyramids.<span style="font-size:80%;opacity:0.8"> HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。</span></li><li>These <font color=orangered>HOG</font> and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more.<span style="font-size:80%;opacity:0.8"> 这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。</span></li><li>Before <font color=orangered>HOG</font> and SIFT, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8"> 在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> sparsely </td> <td> [spɑ:slɪ] </td> <td> 
<ul><li>Dollar et al. [6] demonstrated fast pyramid computation by first computing a <font color=orangered>sparsely</font> sampled (in scale) pyramid and then interpolating missing levels.<span style="font-size:80%;opacity:0.8"> Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> interpolate </td> <td> [ɪnˈtɜ:pəleɪt] </td> <td> 
<ul><li>Dollar et al. [6] demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then <font color=orangered>interpolating</font> missing levels.<span style="font-size:80%;opacity:0.8"> Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> OverFeat </td> <td>  </td> <td> 
<ul><li>With the development of modern deep ConvNets [19], object detectors like <font color=forestgreen>OverFeat</font> [34] and R-CNN [12] showed dramatic improvements in accuracy.<span style="font-size:80%;opacity:0.8"> 随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。</span></li><li><font color=forestgreen>OverFeat</font> adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid.<span style="font-size:80%;opacity:0.8"> OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> scale-normalized </td> <td> [!≈ skeɪl 'nɔ:məlaɪzd] </td> <td> 
<ul><li>R-CNN adopted a region proposal-based strategy [37] in which each proposal was <font color=orangered>scale-normalized</font> before classifying with a ConvNet.<span style="font-size:80%;opacity:0.8"> R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> SPPnet </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SPPnet</font> [15] demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale.<span style="font-size:80%;opacity:0.8"> SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> trade-off </td> <td> [ˈtreɪdˌɔ:f, -ˌɔf] </td> <td> 
<ul><li>Recent and more accurate detection methods like Fast R-CNN [11] and Faster R-CNN [29] advocate using features computed from a single scale, because it offers a good <font color=orangered>trade-off</font> between accuracy and speed.<span style="font-size:80%;opacity:0.8"> 最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single higher-level feature map is not enough because there is a <font color=orangered>trade-off</font> between coarser resolutions and stronger semantics.<span style="font-size:80%;opacity:0.8"> 表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> FCN </td> <td> [!≈ ef si: en] </td> <td> 
<ul><li><font color=orangered>FCN</font> [24] sums partial scores for each category over multiple scales to compute semantic segmentations.<span style="font-size:80%;opacity:0.8"> FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。</span></li><li>Ghiasi et al. [8] present a Laplacian pyramid presentation for <font color=orangered>FCNs</font> to progressively refine segmentation.<span style="font-size:80%;opacity:0.8"> Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> hypercolumn </td> <td> [haɪpə'kɒləm] </td> <td> 
<ul><li><font color=orangered>Hypercolumns</font> [13] uses a similar method for object instance segmentation.<span style="font-size:80%;opacity:0.8"> Hypercolumns[13]使用类似的方法进行目标实例分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> HyperNet </td> <td>  </td> <td> 
<ul><li>Several other approaches (<font color=forestgreen>HyperNet</font> [18], ParseNet [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8"> 在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> ParseNet </td> <td>  </td> <td> 
<ul><li>Several other approaches (HyperNet [18], <font color=forestgreen>ParseNet</font> [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8"> 在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> ION </td> <td> [ˈaɪən] </td> <td> 
<ul><li>Several other approaches (HyperNet [18], ParseNet [23], and <font color=orangered>ION</font> [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8"> 在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> concatenate </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>Several other approaches (HyperNet [18], ParseNet [23], and ION [2]) <font color=orangered>concatenate</font> features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8"> 在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> MS-CNN </td> <td>  </td> <td> 
<ul><li>SSD [22] and <font color=forestgreen>MS-CNN</font> [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.<span style="font-size:80%;opacity:0.8"> SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> U-Net </td> <td> [!≈ ju: net] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including <font color=orangered>U-Net</font> [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> SharpMask </td> <td>  </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and <font color=forestgreen>SharpMask</font> [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>In this section we use FPNs to generate segmentation proposals, following the DeepMask/<font color=forestgreen>SharpMask</font> framework [27, 28].<span style="font-size:80%;opacity:0.8"> 在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li>DeepMask/<font color=forestgreen>SharpMask</font> were trained on image crops for predicting instance segments and object/non-object scores.<span style="font-size:80%;opacity:0.8"> DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。</span></li><li>DeepMask, <font color=forestgreen>SharpMask</font>, and FPN use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8"> DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li>DeepMask and <font color=forestgreen>SharpMask</font> performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ variants).<span style="font-size:80%;opacity:0.8"> DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> Recombinator </td> <td> [riːkəm'bɪnətə] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, <font color=orangered>Recombinator</font> networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> Hourglass </td> <td> [ˈaʊəglɑ:s] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked <font color=orangered>Hourglass</font> networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> keypoint </td> <td> [ki:'pɔɪnt] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for <font color=orangered>keypoint</font> estimation.<span style="font-size:80%;opacity:0.8"> 最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>Recently, FPN has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and <font color=orangered>keypoint</font> estimation.<span style="font-size:80%;opacity:0.8"> 最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> Ghiasi </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Ghiasi</font> et al. [8] present a Laplacian pyramid presentation for FCNs to progressively refine segmentation.<span style="font-size:80%;opacity:0.8"> Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> Laplacian </td> <td> [lɑ:'plɑ:siәn] </td> <td> 
<ul><li>Ghiasi et al. [8] present a <font color=orangered>Laplacian</font> pyramid presentation for FCNs to progressively refine segmentation.<span style="font-size:80%;opacity:0.8"> Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> progressively </td> <td> [prəˈgresɪvli] </td> <td> 
<ul><li>Ghiasi et al. [8] present a Laplacian pyramid presentation for FCNs to <font color=orangered>progressively</font> refine segmentation.<span style="font-size:80%;opacity:0.8"> Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> general-purpose </td> <td> ['dʒenrəl 'pɜ:pəs] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is <font color=orangered>general-purpose</font> and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8"> 由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> proposer </td> <td> [prəˈpəʊzə(r)] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window <font color=orangered>proposers</font> (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8"> 由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> RPN </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window proposers (Region Proposal Network, <font color=orangered>RPN</font> for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8"> 由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li><li>In the following we adopt our method in <font color=orangered>RPN</font> [29] for bounding box proposal generation and in Fast R-CNN [11] for object detection.<span style="font-size:80%;opacity:0.8"> 在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。</span></li><li>4.1. Feature Pyramid Networks for <font color=orangered>RPN</font><span style="font-size:80%;opacity:0.8"> 4.1. RPN的特征金字塔网络</span></li><li><font color=orangered>RPN</font> [29] is a sliding-window class-agnostic object detector.<span style="font-size:80%;opacity:0.8"> RPN[29]是一个滑动窗口类不可知的目标检测器。</span></li><li>In the original <font color=orangered>RPN</font> design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression.<span style="font-size:80%;opacity:0.8"> 在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>We adapt <font color=orangered>RPN</font> by replacing the single-scale feature map with our FPN.<span style="font-size:80%;opacity:0.8"> 我们通过用我们的FPN替换单尺度特征映射来适应RPN。</span></li><li>With the above adaptations, <font color=orangered>RPN</font> can be naturally trained and tested with our FPN, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8"> 通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>5.1. Region Proposal with <font color=orangered>RPN</font><span style="font-size:80%;opacity:0.8"> 5.1. 区域提议与RPN</span></li><li>For all <font color=orangered>RPN</font> experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored.<span style="font-size:80%;opacity:0.8"> 对于所有的RPN实验（包括基准数据集），我们都包含了图像外部的锚盒来进行训练，这不同于[29]中的忽略这些锚盒。</span></li><li>Training <font color=orangered>RPN</font> with FPN on 8 GPUs takes about 8 hours on COCO.<span style="font-size:80%;opacity:0.8"> 使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</span></li><li>Bounding box proposal results using <font color=orangered>RPN</font> [29], evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8"> 使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li><li>For fair comparisons with original <font color=orangered>RPNs</font>[29], we run two baselines (Table 1(a, b)) using the single-scale map of $C_4$ (the same as [16]) or $C_5$, both using the same hyper-parameters as ours, including using 5 scale anchors of $\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$.<span style="font-size:80%;opacity:0.8"> 为了与原始RPNs[29]进行公平比较，我们使用$C_4$(与[16]相同)或$C_5$的单尺度映射运行了两个基线（表1（a，b）），都使用与我们相同的超参数，包括使用5种尺度锚点$\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$。</span></li><li>Placing FPN in <font color=orangered>RPN</font> improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale RPN baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8"> 将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>Placing FPN in RPN improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale <font color=orangered>RPN</font> baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8"> 将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>Our pyramid representation greatly improves <font color=orangered>RPN</font>’s robustness to object scale variation.<span style="font-size:80%;opacity:0.8"> 我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</span></li><li>The results in Table 1(d) are just on par with the <font color=orangered>RPN</font> baseline and lag far behind ours.<span style="font-size:80%;opacity:0.8"> 表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li><li><font color=orangered>RPN</font> is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale variance.<span style="font-size:80%;opacity:0.8"> RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li><li>We choose to freeze the proposals as computed by <font color=orangered>RPN</font> on FPN (Table 1(c)), because it has good performance on small objects that are to be recognized by the detector.<span style="font-size:80%;opacity:0.8"> 我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。</span></li><li>For simplicity we do not share features between Fast R-CNN and <font color=orangered>RPN</font>, except when specified.<span style="font-size:80%;opacity:0.8"> 为了简单起见，我们不在Fast R-CNN和RPN之间共享特征，除非指定。</span></li><li>Object detection results using Fast R-CNN [11] on a fixed set of proposals (<font color=orangered>RPN</font>, ${P_k}$, Table 1(c)), evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8"> 使用Fast R-CNN[11]在一组固定提议（RPN，${P_k}$，表1（c））上的目标检测结果，在COCO的minival数据集上进行评估。</span></li><li>Table 2(d) and (e) show that removing top-down connections or removing lateral connections leads to inferior results, similar to what we have observed in the above sub-section for <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8"> 表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li><li>Despite the good accuracy of this variant, it is based on the <font color=orangered>RPN</font> proposals of ${P_k}$ and has thus already benefited from the pyramid representation.<span style="font-size:80%;opacity:0.8"> 尽管这个变体具有很好的准确性，但它是基于${P_k}$的RPN提议的，因此已经从金字塔表示中受益。</span></li><li>But in a Faster R-CNN system [29], the <font color=orangered>RPN</font> and Fast R-CNN must use the same network backbone in order to make feature sharing possible.<span style="font-size:80%;opacity:0.8"> 但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用相同的骨干网络来实现特征共享。</span></li><li>Table 3 shows the comparisons between our method and two baselines, all using consistent backbone architectures for <font color=orangered>RPN</font> and Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。</span></li><li>The backbone network for <font color=orangered>RPN</font> are consistent with Fast R-CNN.<span style="font-size:80%;opacity:0.8"> RPN与Fast R-CNN的骨干网络是一致的。</span></li><li>In the above, for simplicity we do not share the features between <font color=orangered>RPN</font> and Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 在上面，为了简单起见，我们不共享RPN和Fast R-CNN之间的特征。</span></li><li>The two MLPs play a similar role as anchors in <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8"> 这两个MLP在RPN中扮演着类似于锚点的角色。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> Sec.6. </td> <td>  </td> <td> 
<ul><li>We also generalize FPNs to instance segmentation proposals in <font color=forestgreen>Sec.6.</font><span style="font-size:80%;opacity:0.8"> 在第6节中我们还将FPN泛化到实例细分提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> arbitrary </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>Our method takes a single-scale image of an <font color=orangered>arbitrary</font> size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion.<span style="font-size:80%;opacity:0.8"> 我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> proportionally </td> <td> [prə'pɔ:ʃənlɪ] </td> <td> 
<ul><li>Our method takes a single-scale image of an arbitrary size as input, and outputs <font color=orangered>proportionally</font> sized feature maps at multiple levels, in a fully convolutional fashion.<span style="font-size:80%;opacity:0.8"> 我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> backbone </td> <td> [ˈbækbəʊn] </td> <td> 
<ul><li>This process is independent of the <font color=orangered>backbone</font> convolutional architectures (e.g., [19, 36, 16]), and in this paper we present results using ResNets [16].<span style="font-size:80%;opacity:0.8"> 这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。</span></li><li>The bottom-up pathway is the feed-forward computation of the <font color=orangered>backbone</font> ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8"> 自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>As is common practice [12], all network <font color=orangered>backbones</font> are pre-trained on the ImageNet1k classification set [33] and then fine-tuned on the detection dataset.<span style="font-size:80%;opacity:0.8"> 正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。</span></li><li>But in a Faster R-CNN system [29], the RPN and Fast R-CNN must use the same network <font color=orangered>backbone</font> in order to make feature sharing possible.<span style="font-size:80%;opacity:0.8"> 但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用相同的骨干网络来实现特征共享。</span></li><li>Table 3 shows the comparisons between our method and two baselines, all using consistent <font color=orangered>backbone</font> architectures for RPN and Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。</span></li><li>The <font color=orangered>backbone</font> network for RPN are consistent with Fast R-CNN.<span style="font-size:80%;opacity:0.8"> RPN与Fast R-CNN的骨干网络是一致的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> feed-forward </td> <td> ['fi:df'ɔ:wəd] </td> <td> 
<ul><li>The bottom-up pathway is the <font color=orangered>feed-forward</font> computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8"> 自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> residual </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>Specifically, for ResNets [16] we use the feature activations output by each stage’s last <font color=orangered>residual</font> block.<span style="font-size:80%;opacity:0.8"> 具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。</span></li><li>We denote the output of these last <font color=orangered>residual</font> blocks as $\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$ for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image.<span style="font-size:80%;opacity:0.8"> 对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2, C_3, C_4, C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。</span></li><li>We have experimented with more sophisticated blocks (e.g., using multi-layer <font color=orangered>residual</font> blocks [16] as the connections) and observed marginally better results.<span style="font-size:80%;opacity:0.8"> 我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> hallucinate </td> <td> [həˈlu:sɪneɪt] </td> <td> 
<ul><li>The top-down pathway <font color=orangered>hallucinates</font> higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> spatially </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>The top-down pathway hallucinates higher resolution features by upsampling <font color=orangered>spatially</font> coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8"> 自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> lower-level </td> <td> [!≈ ˈləʊə(r) ˈlevl] </td> <td> 
<ul><li>The bottom-up feature map is of <font color=orangered>lower-level</font> semantics, but its activations are more accurately localized as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8"> 自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> localized </td> <td> [ˈləʊkəlaɪzd] </td> <td> 
<ul><li>The bottom-up feature map is of lower-level semantics, but its activations are more accurately <font color=orangered>localized</font> as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8"> 自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> subsampled </td> <td>  </td> <td> 
<ul><li>The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was <font color=forestgreen>subsampled</font> fewer times.<span style="font-size:80%;opacity:0.8"> 自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> coarser-resolution </td> <td> [!≈ kɔ:sə ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>With a <font color=orangered>coarser-resolution</font> feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity).<span style="font-size:80%;opacity:0.8"> 使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> upsampled </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>upsampled</font> map is then merged with the corresponding bottom-up map (which undergoes a 1×1 convolutional layer to reduce channel dimensions) by element-wise addition.<span style="font-size:80%;opacity:0.8"> 然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。</span></li><li>But we argue that the locations of these features are not precise, because these maps have been downsampled and <font color=forestgreen>upsampled</font> several times.<span style="font-size:80%;opacity:0.8"> 但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> iterated </td> <td> [ˈɪtəˌreɪtid] </td> <td> 
<ul><li>This process is <font color=orangered>iterated</font> until the finest resolution map is generated.<span style="font-size:80%;opacity:0.8"> 迭代这个过程，直到生成最佳分辨率映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> append </td> <td> [əˈpend] </td> <td> 
<ul><li>Finally, we <font color=orangered>append</font> a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling.<span style="font-size:80%;opacity:0.8"> 最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> alias </td> <td> [ˈeɪliəs] </td> <td> 
<ul><li>Finally, we append a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the <font color=orangered>aliasing</font> effect of upsampling.<span style="font-size:80%;opacity:0.8"> 最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> regressor </td> <td> [rɪ'gresə(r)] </td> <td> 
<ul><li>Because all levels of the pyramid use shared classifiers/<font color=orangered>regressors</font> as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps.<span style="font-size:80%;opacity:0.8"> 由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为d）。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box <font color=orangered>regressors</font>) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8"> 我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> non-linearity </td> <td> ['nɒnlaɪn'ərɪtɪ] </td> <td> 
<ul><li>There are no <font color=orangered>non-linearities</font> in these extra layers, which we have empirically found to have minor impacts.<span style="font-size:80%;opacity:0.8"> 在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> empirically </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li>There are no non-linearities in these extra layers, which we have <font color=orangered>empirically</font> found to have minor impacts.<span style="font-size:80%;opacity:0.8"> 在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> minor </td> <td> [ˈmaɪnə(r)] </td> <td> 
<ul><li>There are no non-linearities in these extra layers, which we have empirically found to have <font color=orangered>minor</font> impacts.<span style="font-size:80%;opacity:0.8"> 在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> marginally </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>We have experimented with more sophisticated blocks (e.g., using multi-layer residual blocks [16] as the connections) and observed <font color=orangered>marginally</font> better results.<span style="font-size:80%;opacity:0.8"> 我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li><li>Its result (33.4 AP) is <font color=orangered>marginally</font> worse than that of using all pyramid levels (33.9 AP, Table 2(c)).<span style="font-size:80%;opacity:0.8"> 其结果（33.4 AP）略低于使用所有金字塔等级（33.9 AP，表2（c））的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> minimal </td> <td> [ˈmɪnɪməl] </td> <td> 
<ul><li>To demonstrate the simplicity and effectiveness of our method, we make <font color=orangered>minimal</font> modifications to the original systems of [29, 11] when adapting them to our feature pyramid.<span style="font-size:80%;opacity:0.8"> 为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> class-agnostic </td> <td> [!≈ klɑ:s ægˈnɒstɪk] </td> <td> 
<ul><li>RPN [29] is a sliding-window <font color=orangered>class-agnostic</font> object detector.<span style="font-size:80%;opacity:0.8"> RPN[29]是一个滑动窗口类不可知的目标检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> subnetwork </td> <td>  </td> <td> 
<ul><li>In the original RPN design, a small <font color=forestgreen>subnetwork</font> is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression.<span style="font-size:80%;opacity:0.8"> 在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>In [16], a ResNet’s conv5 layers (a 9-layer deep <font color=forestgreen>subnetwork</font>) are adopted as the head on top of the conv4 features, but our method has already harnessed conv5 to construct the feature pyramid.<span style="font-size:80%;opacity:0.8"> 在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> Intersection-over-Union </td> <td> [!≈ ˌɪntəˈsekʃn ˈəʊvə(r) ˈju:niən] </td> <td> 
<ul><li>We assign training labels to the anchors based on their <font color=orangered>Intersection-over-Union</font> (IoU) ratios with ground-truth bounding boxes as in [29].<span style="font-size:80%;opacity:0.8"> 如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> analogous </td> <td> [əˈnæləgəs] </td> <td> 
<ul><li>This advantage is <font color=orangered>analogous</font> to that of using a featurized image pyramid, where a common head classifier can be applied to features computed at any image scale.<span style="font-size:80%;opacity:0.8"> 这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</span></li><li><font color=orangered>Analogous</font> to the ResNet-based Faster R-CNN system [16] that uses $C_4$ as the single-scale feature map, we set $k_0$ to 4. Intuitively, Eqn.<span style="font-size:80%;opacity:0.8"> 类似于基于ResNet的Faster R-CNN系统[16]使用$C_4$作为单尺度特征映射，我们将$k_0$设置为4。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> adaptation </td> <td> [ˌædæpˈteɪʃn] </td> <td> 
<ul><li>With the above <font color=orangered>adaptations</font>, RPN can be naturally trained and tested with our FPN, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8"> 通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>Based on these <font color=orangered>adaptations</font>, we can train and test Fast R-CNN on top of the feature pyramid.<span style="font-size:80%;opacity:0.8"> 基于这些改编，我们可以在特征金字塔之上训练和测试Fast R-CNN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> roi </td> <td> [rwɑ:] </td> <td> 
<ul><li>Fast R-CNN [11] is a region-based object detector in which Region-of-Interest (<font color=orangered>RoI</font>) pooling is used to extract features.<span style="font-size:80%;opacity:0.8"> Fast R-CNN[11]是一个基于区域的目标检测器，利用感兴趣区域（RoI）池化来提取特征。</span></li><li>To use it with our FPN, we need to assign <font color=orangered>RoIs</font> of different scales to the pyramid levels.<span style="font-size:80%;opacity:0.8"> 要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</span></li><li>Formally, we assign an <font color=orangered>RoI</font> of width $w$ and height $h$ (on the input image to the network) to the level $P_k$ of our feature pyramid by:<span style="font-size:80%;opacity:0.8"> 在形式上，我们通过以下公式将宽度为$w$和高度为$h$（在网络上的输入图像上）的RoI分配到特征金字塔的级别$P_k$上：</span></li><li>Here 224 is the canonical ImageNet pre-training size, and $k_0$ is the target level on which an <font color=orangered>RoI</font> with $w\times h=224^2$ should be mapped into.<span style="font-size:80%;opacity:0.8"> 这里224是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。</span></li><li>(1) means that if the <font color=orangered>RoI</font>’s scale becomes smaller (say, 1/2 of 224), it should be mapped into a finer-resolution level (say, $k=3$).<span style="font-size:80%;opacity:0.8"> 直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如k=3）。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all <font color=orangered>RoIs</font> of all levels.<span style="font-size:80%;opacity:0.8"> 我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li><li>So unlike [16], we simply adopt <font color=orangered>RoI</font> pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected (fc) layers (each followed by ReLU) before the final classification and bounding box regression layers.<span style="font-size:80%;opacity:0.8"> 因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（fc）层（每层后都接ReLU层）。</span></li><li>Each mini-batch involves 2 image per GPU and 512 <font color=orangered>RoIs</font> per image.<span style="font-size:80%;opacity:0.8"> 每个小批量数据包括每个GPU2张图像和每张图像上512个RoI。</span></li><li>We use 2000 <font color=orangered>RoIs</font> per image for training and 1000 for testing.<span style="font-size:80%;opacity:0.8"> 我们每张图像使用2000个RoIs进行训练，1000个RoI进行测试。</span></li><li>As a ResNet-based Fast R-CNN baseline, following [16], we adopt <font color=orangered>RoI</font> pooling with an output size of 14×14 and attach all conv5 layers as the hidden layers of the head.<span style="font-size:80%;opacity:0.8"> 作为基于ResNet的Fast R-CNN基线，遵循[16]，我们采用输出尺寸为14×14的RoI池化，并将所有conv5层作为头部的隐藏层。</span></li><li>We argue that this is because <font color=orangered>RoI</font> pooling is a warping-like operation, which is less sensitive to the region’s scales.<span style="font-size:80%;opacity:0.8"> 我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。</span></li><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 <font color=orangered>RoIs</font> per image which accelerate convergence, in contrast to 64 RoIs in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8"> 我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 <font color=orangered>RoIs</font> in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8"> 我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> canonical </td> <td> [kəˈnɒnɪkl] </td> <td> 
<ul><li>Here 224 is the <font color=orangered>canonical</font> ImageNet pre-training size, and $k_0$ is the target level on which an RoI with $w\times h=224^2$ should be mapped into.<span style="font-size:80%;opacity:0.8"> 这里224是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。</span></li><li>Both the corresponding image region size (light orange) and <font color=orangered>canonical</font> object size (dark orange) are shown.<span style="font-size:80%;opacity:0.8"> 显示了相应的图像区域大小（浅橙色）和典型目标大小（深橙色）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> finer-resolution </td> <td> [!≈ 'faɪnə ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>(1) means that if the RoI’s scale becomes smaller (say, 1/2 of 224), it should be mapped into a <font color=orangered>finer-resolution</font> level (say, $k=3$).<span style="font-size:80%;opacity:0.8"> 直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如k=3）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> predictor </td> <td> [prɪˈdɪktə(r)] </td> <td> 
<ul><li>We attach <font color=orangered>predictor</font> heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8"> 我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> class-specific </td> <td> [!≈ klɑ:s spəˈsɪfɪk] </td> <td> 
<ul><li>We attach predictor heads (in Fast R-CNN the heads are <font color=orangered>class-specific</font> classifiers and bounding box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8"> 我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> harness </td> <td> [ˈhɑ:nɪs] </td> <td> 
<ul><li>In [16], a ResNet’s conv5 layers (a 9-layer deep subnetwork) are adopted as the head on top of the conv4 features, but our method has already <font color=orangered>harnessed</font> conv5 to construct the feature pyramid.<span style="font-size:80%;opacity:0.8"> 在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> MLP </td> <td> [!≈ em el pi:] </td> <td> 
<ul><li>Note that compared to the standard conv5 head, our 2-fc <font color=orangered>MLP</font> head is lighter weight and faster.<span style="font-size:80%;opacity:0.8"> 请注意，与标准的conv5头部相比，我们的2-fc MLP头部更轻更快。</span></li><li>Table 2(b) is a baseline exploiting an <font color=orangered>MLP</font> head with 2 hidden fc layers, similar to the head in our architecture.<span style="font-size:80%;opacity:0.8"> 表2（b）是利用MLP头部的基线，其具有2个隐藏的fc层，类似于我们的架构中的头部。</span></li><li>On top of each level of the feature pyramid, we apply a small 5×5 <font color=orangered>MLP</font> to predict 14×14 masks and object scores in a fully convolutional fashion, see Fig. 4.<span style="font-size:80%;opacity:0.8"> 在特征金字塔的每个层级上，我们应用一个小的5×5MLP以全卷积方式预测14×14掩码和目标分数，参见图4。</span></li><li>Additionally, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second <font color=orangered>MLP</font> of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8"> 此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>The two <font color=orangered>MLPs</font> play a similar role as anchors in RPN.<span style="font-size:80%;opacity:0.8"> 这两个MLP在RPN中扮演着类似于锚点的角色。</span></li><li>We apply a small <font color=orangered>MLP</font> on 5x5 windows to generate dense object segments with output dimension of 14x14.<span style="font-size:80%;opacity:0.8"> 我们在5x5窗口上应用一个小的MLP来生成输出尺寸为14x14的密集目标块。</span></li><li>Half octaves are handled by an <font color=orangered>MLP</font> on 7x7 windows ($7 \approx 5 \sqrt 2$), not shown here.<span style="font-size:80%;opacity:0.8"> 半个组由MLP在7x7窗口（ $7 \ approx 5 \ sqrt 2 $）处理，此处未展示。</span></li><li>Our baseline FPN model with a single 5×5 <font color=orangered>MLP</font> achieves an AR of 43.4.<span style="font-size:80%;opacity:0.8"> 我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。</span></li><li>Switching to a slightly larger 7×7 <font color=orangered>MLP</font> leaves accuracy largely unchanged.<span style="font-size:80%;opacity:0.8"> 切换到稍大的7×7MLP，精度基本保持不变。</span></li><li>Using both <font color=orangered>MLPs</font> together increases accuracy to 45.7 AR.<span style="font-size:80%;opacity:0.8"> 同时使用两个MLP将精度提高到了45.7的AR。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> trainval35k </td> <td>  </td> <td> 
<ul><li>We train using the union of 80k train images and a 35k subset of val images (<font color=forestgreen>trainval35k</font> [2]), and report ablations on a 5k subset of val images (minival).<span style="font-size:80%;opacity:0.8"> 我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>All models are trained on <font color=forestgreen>trainval35k</font>.<span style="font-size:80%;opacity:0.8"> 所有模型都是通过trainval35k训练的。</span></li><li>Models are trained on the <font color=forestgreen>trainval35k</font> set.<span style="font-size:80%;opacity:0.8"> 模型在trainval35k数据集上训练。</span></li><li>Models are trained on the <font color=forestgreen>trainval35k</font> set and use ResNet-50. ^†Provided by authors of [16].<span style="font-size:80%;opacity:0.8"> 模型在trainval35k数据集上训练并使用ResNet-50。^†由[16]的作者提供。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> minival </td> <td>  </td> <td> 
<ul><li>We train using the union of 80k train images and a 35k subset of val images (trainval35k [2]), and report ablations on a 5k subset of val images (<font color=forestgreen>minival</font>).<span style="font-size:80%;opacity:0.8"> 我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>Bounding box proposal results using RPN [29], evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8"> 使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li><li>Object detection results using Fast R-CNN [11] on a fixed set of proposals (RPN, ${P_k}$, Table 1(c)), evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8"> 使用Fast R-CNN[11]在一组固定提议（RPN，${P_k}$，表1（c））上的目标检测结果，在COCO的minival数据集上进行评估。</span></li><li>Object detection results using Faster R-CNN [29] evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8"> 使用Faster R-CNN[29]在COCOminival数据集上评估的目标检测结果。</span></li><li>More object detection results using Faster R-CNN and our FPNs, evaluated on <font color=forestgreen>minival</font>.<span style="font-size:80%;opacity:0.8"> 使用Faster R-CNN和我们的FPN在minival上的更多目标检测结果。</span></li><li>This increases AP on <font color=forestgreen>minival</font> to 35.6, without sharing features.<span style="font-size:80%;opacity:0.8"> 这将minival上的AP增加到了35.6，没有共享特征。</span></li><li>Some results were not available on the test-std set, so we also include the test-dev results (and for Multipath [40] on <font color=forestgreen>minival</font>).<span style="font-size:80%;opacity:0.8"> 一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> test-std </td> <td>  </td> <td> 
<ul><li>We also report final results on the standard test set (<font color=forestgreen>test-std</font>) [21] which has no disclosed labels.<span style="font-size:80%;opacity:0.8"> 我们还报告了在没有公开标签的标准测试集（test-std）[21]上的最终结果。</span></li><li>Some results were not available on the <font color=forestgreen>test-std</font> set, so we also include the test-dev results (and for Multipath [40] on minival).<span style="font-size:80%;opacity:0.8"> 一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> ImageNet1k </td> <td>  </td> <td> 
<ul><li>As is common practice [12], all network backbones are pre-trained on the <font color=forestgreen>ImageNet1k</font> classification set [33] and then fine-tuned on the detection dataset.<span style="font-size:80%;opacity:0.8"> 正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> reimplementation </td> <td>  </td> <td> 
<ul><li>Our code is a <font color=forestgreen>reimplementation</font> of py-faster-rcnn using Caffe2.<span style="font-size:80%;opacity:0.8"> 我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> py-faster-rcnn </td> <td>  </td> <td> 
<ul><li>Our code is a reimplementation of <font color=forestgreen>py-faster-rcnn</font> using Caffe2.<span style="font-size:80%;opacity:0.8"> 我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> Caffe </td> <td>  </td> <td> 
<ul><li>Our code is a reimplementation of py-faster-rcnn using <font color=forestgreen>Caffe</font>2.<span style="font-size:80%;opacity:0.8"> 我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> resize </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The input image is <font color=orangered>resized</font> such that its shorter side has 800 pixels.<span style="font-size:80%;opacity:0.8"> 输入图像的大小调整为其较短边有800像素。</span></li><li>The input image is <font color=orangered>resized</font> such that its shorter side has 800 pixels.<span style="font-size:80%;opacity:0.8"> 调整大小输入图像，使其较短边为800像素。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> synchronize </td> <td> [ˈsɪŋkrənaɪz] </td> <td> 
<ul><li>We adopt <font color=orangered>synchronized</font> SGD training on 8 GPUs.<span style="font-size:80%;opacity:0.8"> 我们采用8个GPU进行同步SGD训练。</span></li><li><font color=orangered>Synchronized</font> SGD is used to train the model on 8 GPUs.<span style="font-size:80%;opacity:0.8"> 同步SGD用于在8个GPU上训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>We adopt synchronized <font color=orangered>SGD</font> training on 8 GPUs.<span style="font-size:80%;opacity:0.8"> 我们采用8个GPU进行同步SGD训练。</span></li><li>Synchronized <font color=orangered>SGD</font> is used to train the model on 8 GPUs.<span style="font-size:80%;opacity:0.8"> 同步SGD用于在8个GPU上训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8"> 我们使用0.0001的权重衰减和0.9的动量。</span></li><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8"> 我们使用0.0001的权重衰减和0.9的动量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> enrichment </td> <td> [ɪn'rɪtʃmənt] </td> <td> 
<ul><li>How important is top-down <font color=orangered>enrichment</font>? Table 1(d) shows the results of our feature pyramid without the top-down pathway.<span style="font-size:80%;opacity:0.8"> 自上而下的改进有多重要？表1（d）显示了没有自上而下路径的特征金字塔的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> par </td> <td> [pɑ:(r)] </td> <td> 
<ul><li>The results in Table 1(d) are just on <font color=orangered>par</font> with the RPN baseline and lag far behind ours.<span style="font-size:80%;opacity:0.8"> 表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> lag </td> <td> [læg] </td> <td> 
<ul><li>The results in Table 1(d) are just on par with the RPN baseline and <font color=orangered>lag</font> far behind ours.<span style="font-size:80%;opacity:0.8"> 表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> conjecture </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that this is because there are large semantic gaps between different levels on the bottom-up pyramid (Fig. 1(b)), especially for very deep ResNets.<span style="font-size:80%;opacity:0.8"> 我们推测这是因为自下而上的金字塔（图1（b））的不同层次之间存在较大的语义差距，尤其是对于非常深的ResNets。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> variant </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>We have also evaluated a <font color=orangered>variant</font> of Table 1(d) without sharing the parameters of the heads, but observed similarly degraded performance.<span style="font-size:80%;opacity:0.8">  我们还评估了表1（d）的一个变体，但没有分享磁头的参数，但观察到类似的性能下降。</span></li><li>This <font color=orangered>variant</font> (Table 1(f)) is better than the baseline but inferior to our approach.<span style="font-size:80%;opacity:0.8"> 这个变体（表1（f））比基线要好，但不如我们的方法。</span></li><li>Despite the good accuracy of this <font color=orangered>variant</font>, it is based on the RPN proposals of ${P_k}$ and has thus already benefited from the pyramid representation.<span style="font-size:80%;opacity:0.8"> 尽管这个变体具有很好的准确性，但它是基于${P_k}$的RPN提议的，因此已经从金字塔表示中受益。</span></li><li>DeepMask and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ <font color=orangered>variants</font>).<span style="font-size:80%;opacity:0.8"> DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> level-specific </td> <td> [!≈ ˈlevl spəˈsɪfɪk] </td> <td> 
<ul><li>This issue cannot be simply remedied by <font color=orangered>level-specific</font> heads.<span style="font-size:80%;opacity:0.8"> 这个问题不能简单地由特定级别的负责人来解决。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> downsampled </td> <td>  </td> <td> 
<ul><li>But we argue that the locations of these features are not precise, because these maps have been <font color=forestgreen>downsampled</font> and upsampled several times.<span style="font-size:80%;opacity:0.8"> 但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> highest-resolution </td> <td> [!≈ haɪɪst ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the <font color=orangered>highest-resolution</font>, strongly semantic feature maps of $P_2$ (i.e., the finest level in our pyramids).<span style="font-size:80%;opacity:0.8"> 金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> i.e. </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly semantic feature maps of $P_2$ (<font color=orangered>i.e.</font>, the finest level in our pyramids).<span style="font-size:80%;opacity:0.8"> 金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> orthogonal </td> <td> [ɔ:'θɒgənl] </td> <td> 
<ul><li>It gets an AP of 28.8, indicating that the 2-fc head does not give us any <font color=orangered>orthogonal</font> advantage over the baseline in Table 2(a).<span style="font-size:80%;opacity:0.8"> 它得到了28.8的AP，表明2-fc头部没有给我们带来任何超过表2（a）中基线的正交优势。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> sub-section </td> <td> ['sʌbs'ekʃn] </td> <td> 
<ul><li>Table 2(d) and (e) show that removing top-down connections or removing lateral connections leads to inferior results, similar to what we have observed in the above <font color=orangered>sub-section</font> for RPN.<span style="font-size:80%;opacity:0.8"> 表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> noteworthy </td> <td> [ˈnəʊtwɜ:ði] </td> <td> 
<ul><li>It is <font color=orangered>noteworthy</font> that removing top-down connections (Table 2(d)) significantly degrades the accuracy, suggesting that Fast R-CNN suffers from using the low-level features at the high-resolution maps.<span style="font-size:80%;opacity:0.8"> 值得注意的是，去除自上而下的连接（表2（d））显著降低了准确性，表明Fast R-CNN在高分辨率映射中使用了低级特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> warping-like </td> <td> [!≈ 'wɔ:pɪŋ laɪk] </td> <td> 
<ul><li>We argue that this is because RoI pooling is a <font color=orangered>warping-like</font> operation, which is less sensitive to the region’s scales.<span style="font-size:80%;opacity:0.8"> 我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> reproduction </td> <td> [ˌri:prəˈdʌkʃn] </td> <td> 
<ul><li>Table 3(a) shows our <font color=orangered>reproduction</font> of the baseline Faster R-CNN system as described in [16].<span style="font-size:80%;opacity:0.8"> 表3（a）显示了我们再现[16]中描述的Faster R-CNN系统的基线。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> convergence </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 RoIs per image which accelerate <font color=orangered>convergence</font>, in contrast to 64 RoIs in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8"> 我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> FPN-based </td> <td>  </td> <td> 
<ul><li>With feature sharing, our <font color=forestgreen>FPN-based</font> Faster R-CNN system has inference time of 0.148 seconds per image on a single NVIDIA M40 GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8"> 通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> NVIDIA </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.148 seconds per image on a single <font color=orangered>NVIDIA</font> M40 GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8"> 通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li><li>^† Runtimes are measured on an <font color=orangered>NVIDIA</font> M40 GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8"> ^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> M40 </td> <td>  </td> <td> 
<ul><li>With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.148 seconds per image on a single NVIDIA <font color=forestgreen>M40</font> GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8"> 通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li><li>^† Runtimes are measured on an NVIDIA <font color=forestgreen>M40</font> GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8"> ^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> leaderboard </td> <td> ['li:dərbɔ:d] </td> <td> 
<ul><li>This model is the one we submitted to the COCO detection <font color=orangered>leaderboard</font>, shown in Table 4.<span style="font-size:80%;opacity:0.8"> 该模型是我们提交给COCO检测排行榜的模型，如表4所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> feature-sharing </td> <td> [!≈ ˈfi:tʃə(r) 'ʃeərɪŋ] </td> <td> 
<ul><li>We have not evaluated its <font color=orangered>feature-sharing</font> version due to limited time, which should be slightly better as implied by Table 5.<span style="font-size:80%;opacity:0.8"> 由于时间有限，我们尚未评估其特征共享版本，这应该稍微好一些，如表5所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> test-dev </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Some results were not available on the test-std set, so we also include the <font color=orangered>test-dev</font> results (and for Multipath [40] on minival).<span style="font-size:80%;opacity:0.8"> 一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li><li>On the <font color=orangered>test-dev</font> set, our method increases over the existing best results by 0.5 points of AP (36.2 vs. 35.7) and 3.4 points of AP@0.5 (59.1 vs.<span style="font-size:80%;opacity:0.8"> 没有添加额外的东西，我们的单模型提交就已经超越了这些强大的，经过严格设计的竞争对手。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> Multipath </td> <td> ['mʌltɪpæθ] </td> <td> 
<ul><li>Some results were not available on the test-std set, so we also include the test-dev results (and for <font color=orangered>Multipath</font> [40] on minival).<span style="font-size:80%;opacity:0.8"> 一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> AttractioNet </td> <td>  </td> <td> 
<ul><li>^§: This entry of <font color=forestgreen>AttractioNet</font> [10] adopts VGG-16 for proposals and Wide ResNet [39] for object detection, so is not strictly a single-model result.<span style="font-size:80%;opacity:0.8"> ^§：AttractioNet[10]的输入采用VGG-16进行目标提议，用Wide ResNet[39]进行目标检测，因此它不是严格意义上的单模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> G-RMI </td> <td>  </td> <td> 
<ul><li>Table 4 compares our method with the single-model results of the COCO competition winners, including the 2016 winner <font color=forestgreen>G-RMI</font> and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has surpassed these strong, heavily engineered competitors.<span style="font-size:80%;opacity:0.8"> 表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> small-scale </td> <td> [ˈsmɔ:lˈskeɪl] </td> <td> 
<ul><li>It is worth noting that our method does not rely on image pyramids and only uses a single input image scale, but still has outstanding AP on <font color=orangered>small-scale</font> objects.<span style="font-size:80%;opacity:0.8"> 值得注意的是，我们的方法不依赖图像金字塔，只使用单个输入图像尺度，但在小型目标上仍然具有出色的AP。</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> iterative </td> <td> ['ɪtərətɪv] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as <font color=orangered>iterative</font> regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> mining </td> <td> [ˈmaɪnɪŋ] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative <font color=orangered>mining</font> [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> augmentation </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data <font color=orangered>augmentation</font> [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> complementary </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are <font color=orangered>complementary</font> to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8"> 此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> DeepMask </td> <td>  </td> <td> 
<ul><li>In this section we use FPNs to generate segmentation proposals, following the <font color=forestgreen>DeepMask</font>/SharpMask framework [27, 28].<span style="font-size:80%;opacity:0.8"> 在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li><font color=forestgreen>DeepMask</font>/SharpMask were trained on image crops for predicting instance segments and object/non-object scores.<span style="font-size:80%;opacity:0.8"> DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。</span></li><li><font color=forestgreen>DeepMask</font>, SharpMask, and FPN use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8"> DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li><font color=forestgreen>DeepMask</font> and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ variants).<span style="font-size:80%;opacity:0.8"> DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li><li>We also report comparisons to <font color=forestgreen>DeepMask</font> [27], Sharp-Mask [28], and InstanceFCN [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8"> 我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> convolutionally </td> <td> [!≈ kɒnvə'lu:ʃənəli] </td> <td> 
<ul><li>At inference time, these models are run <font color=orangered>convolutionally</font> to generate dense proposals in an image.<span style="font-size:80%;opacity:0.8"> 在推断时，这些模型是卷积运行的，以在图像中生成密集的提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> setup </td> <td> ['setʌp] </td> <td> 
<ul><li>We use a fully convolutional <font color=orangered>setup</font> for both training and inference.<span style="font-size:80%;opacity:0.8"> 我们对训练和推断都使用全卷积设置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> Additionally </td> <td> [ə'dɪʃənəlɪ] </td> <td> 
<ul><li><font color=orangered>Additionally</font>, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8"> 此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> Instance-FCN </td> <td>  </td> <td> 
<ul><li>DeepMask, SharpMask, and FPN use ResNet-50 while <font color=forestgreen>Instance-FCN</font> uses VGG-16.<span style="font-size:80%;opacity:0.8"> DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> zoom </td> <td> [zu:m] </td> <td> 
<ul><li>DeepMask and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘<font color=orangered>zoom</font>’ variants).<span style="font-size:80%;opacity:0.8"> DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> runtime </td> <td> [rʌn'taɪm] </td> <td> 
<ul><li>^† <font color=orangered>Runtimes</font> are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8"> ^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> InstanceFCN </td> <td>  </td> <td> 
<ul><li>^† Runtimes are measured on an NVIDIA M40 GPU, except the <font color=forestgreen>InstanceFCN</font> timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8"> ^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li><li>We also report comparisons to DeepMask [27], Sharp-Mask [28], and <font color=forestgreen>InstanceFCN</font> [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8"> 我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> K40 </td> <td>  </td> <td> 
<ul><li>^† Runtimes are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower <font color=forestgreen>K40</font>.<span style="font-size:80%;opacity:0.8"> ^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> Sharp-Mask </td> <td> [!≈ ʃɑ:p mɑ:sk] </td> <td> 
<ul><li>We also report comparisons to DeepMask [27], <font color=orangered>Sharp-Mask</font> [28], and InstanceFCN [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8"> 我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> computationally </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>Existing mask proposal methods [27, 28, 4] are based on densely sampled image pyramids (e.g., scaled by 2^{\lbrace −2:0.5:1 \rbrace} in [27, 28]), making them <font color=orangered>computationally</font> expensive.<span style="font-size:80%;opacity:0.8"> 现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为2^{\lbrace −2:0.5:1 \rbrace}），使得它们是计算昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> substantially </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Our approach, based on FPNs, is <font color=orangered>substantially</font> faster (our models run at 6 to 7 FPS).<span style="font-size:80%;opacity:0.8"> 我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> FPN<br>(30) </td> <td> [!≈ ef pi: en] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (<font color=orangered>FPN</font>), shows significant improvement as a generic feature extractor in several applications.<span style="font-size:80%;opacity:0.8">这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>Using <font color=orangered>FPN</font> in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners.<span style="font-size:80%;opacity:0.8">在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。</span></li><li>(d) Our proposed Feature Pyramid Network (<font color=orangered>FPN</font>) is fast like (b) and (c), but more accurate.<span style="font-size:80%;opacity:0.8">（d）我们提出的特征金字塔网络（FPN）与（b）和（c）类似，但更准确。</span></li><li>We evaluate our method, called a Feature Pyramid Network (<font color=orangered>FPN</font>), in various systems for detection and segmentation [11, 29, 27].<span style="font-size:80%;opacity:0.8">我们评估了我们称为特征金字塔网络（FPN）的方法，其在各种系统中用于检测和分割[11，29，27]。</span></li><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on <font color=orangered>FPN</font> and a basic Faster R-CNN detector [29], surpassing all existing heavily-engineered single-model entries of competition winners.<span style="font-size:80%;opacity:0.8">没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li><li>In ablation experiments, we find that for bounding box proposals, <font color=orangered>FPN</font> significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8">在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>As a result, <font color=orangered>FPNs</font> are able to achieve higher accuracy than all existing state-of-the-art methods.<span style="font-size:80%;opacity:0.8">因此，FPN能够比所有现有的最先进方法获得更高的准确度。</span></li><li>We also generalize <font color=orangered>FPNs</font> to instance segmentation proposals in Sec.6.<span style="font-size:80%;opacity:0.8">在第6节中我们还将FPN泛化到实例细分提议。</span></li><li>We adapt RPN by replacing the single-scale feature map with our <font color=orangered>FPN</font>.<span style="font-size:80%;opacity:0.8">我们通过用我们的FPN替换单尺度特征映射来适应RPN。</span></li><li>With the above adaptations, RPN can be naturally trained and tested with our <font color=orangered>FPN</font>, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8">通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>To use it with our <font color=orangered>FPN</font>, we need to assign RoIs of different scales to the pyramid levels.<span style="font-size:80%;opacity:0.8">要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</span></li><li>Training RPN with <font color=orangered>FPN</font> on 8 GPUs takes about 8 hours on COCO.<span style="font-size:80%;opacity:0.8">使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</span></li><li>Placing <font color=orangered>FPN</font> in RPN improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale RPN baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8">将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>As a results, <font color=orangered>FPN</font> has an $AR^1k$ score 10 points higher than Table 1(e).<span style="font-size:80%;opacity:0.8">因此，FPN的$AR^1k$的得分比表1（e）高10个点。</span></li><li>Next we investigate <font color=orangered>FPN</font> for region-based (non-sliding window) detectors.<span style="font-size:80%;opacity:0.8">接下来我们研究基于区域（非滑动窗口）检测器的FPN。</span></li><li>Training Fast R-CNN with <font color=orangered>FPN</font> takes about 10 hours on the COCO dataset.<span style="font-size:80%;opacity:0.8">使用FPN在COCO数据集上训练Fast R-CNN需要约10小时。</span></li><li>To better investigate <font color=orangered>FPN</font>’s effects on the region-based detector alone, we conduct ablations of Fast R-CNN on a fixed set of proposals.<span style="font-size:80%;opacity:0.8">为了更好地调查FPN对仅基于区域的检测器的影响，我们在一组固定的提议上进行Fast R-CNN的消融。</span></li><li>We choose to freeze the proposals as computed by RPN on <font color=orangered>FPN</font> (Table 1(c)), because it has good performance on small objects that are to be recognized by the detector.<span style="font-size:80%;opacity:0.8">我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。</span></li><li>Table 2(c) shows the results of our <font color=orangered>FPN</font> in Fast R-CNN.<span style="font-size:80%;opacity:0.8">表2（c）显示了Fast R-CNN中我们的FPN结果。</span></li><li>Under controlled settings, our <font color=orangered>FPN</font> (Table 3(c)) is better than this strong baseline by 2.3 points AP and 3.8 points AP@0.5.<span style="font-size:80%;opacity:0.8">在受控的环境下，我们的FPN（表3（c））比这个强劲的基线要好2.3个点的AP和3.8个点的AP@0.5。</span></li><li>More object detection results using Faster R-CNN and our <font color=orangered>FPNs</font>, evaluated on minival.<span style="font-size:80%;opacity:0.8">使用Faster R-CNN和我们的FPN在minival上的更多目标检测结果。</span></li><li>Our method introduces small extra cost by the extra layers in the <font color=orangered>FPN</font>, but has a lighter weight head.<span style="font-size:80%;opacity:0.8">我们的方法通过FPN中的额外层引入了较小的额外成本，但具有更轻的头部。</span></li><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to <font color=orangered>FPNs</font> and should boost accuracy further.<span style="font-size:80%;opacity:0.8">此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li><li>Recently, <font color=orangered>FPN</font> has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and keypoint estimation.<span style="font-size:80%;opacity:0.8">最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。</span></li><li>In this section we use <font color=orangered>FPNs</font> to generate segmentation proposals, following the DeepMask/SharpMask framework [27, 28].<span style="font-size:80%;opacity:0.8">在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li>It is easy to adapt <font color=orangered>FPN</font> to generate mask proposals.<span style="font-size:80%;opacity:0.8">改编FPN生成掩码提议很容易。</span></li><li><font color=orangered>FPN</font> for object segment proposals.<span style="font-size:80%;opacity:0.8">目标分割提议的FPN。</span></li><li>Our baseline <font color=orangered>FPN</font> model with a single 5×5 MLP achieves an AR of 43.4.<span style="font-size:80%;opacity:0.8">我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。</span></li><li>DeepMask, SharpMask, and <font color=orangered>FPN</font> use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8">DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li>Our approach, based on <font color=orangered>FPNs</font>, is substantially faster (our models run at 6 to 7 FPS).<span style="font-size:80%;opacity:0.8">我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> RPN<br>(27) </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window proposers (Region Proposal Network, <font color=orangered>RPN</font> for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8">由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li><li>In the following we adopt our method in <font color=orangered>RPN</font> [29] for bounding box proposal generation and in Fast R-CNN [11] for object detection.<span style="font-size:80%;opacity:0.8">在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。</span></li><li>4.1. Feature Pyramid Networks for <font color=orangered>RPN</font><span style="font-size:80%;opacity:0.8">4.1. RPN的特征金字塔网络</span></li><li><font color=orangered>RPN</font> [29] is a sliding-window class-agnostic object detector.<span style="font-size:80%;opacity:0.8">RPN[29]是一个滑动窗口类不可知的目标检测器。</span></li><li>In the original <font color=orangered>RPN</font> design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression.<span style="font-size:80%;opacity:0.8">在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>We adapt <font color=orangered>RPN</font> by replacing the single-scale feature map with our FPN.<span style="font-size:80%;opacity:0.8">我们通过用我们的FPN替换单尺度特征映射来适应RPN。</span></li><li>With the above adaptations, <font color=orangered>RPN</font> can be naturally trained and tested with our FPN, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8">通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>5.1. Region Proposal with <font color=orangered>RPN</font><span style="font-size:80%;opacity:0.8">5.1. 区域提议与RPN</span></li><li>For all <font color=orangered>RPN</font> experiments (including baselines), we include the anchor boxes that are outside the image for training, which is unlike [29] where these anchor boxes are ignored.<span style="font-size:80%;opacity:0.8">对于所有的RPN实验（包括基准数据集），我们都包含了图像外部的锚盒来进行训练，这不同于[29]中的忽略这些锚盒。</span></li><li>Training <font color=orangered>RPN</font> with FPN on 8 GPUs takes about 8 hours on COCO.<span style="font-size:80%;opacity:0.8">使用具有FPN的RPN在8个GPU上训练COCO数据集需要约8小时。</span></li><li>Bounding box proposal results using <font color=orangered>RPN</font> [29], evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8">使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li><li>For fair comparisons with original <font color=orangered>RPNs</font>[29], we run two baselines (Table 1(a, b)) using the single-scale map of $C_4$ (the same as [16]) or $C_5$, both using the same hyper-parameters as ours, including using 5 scale anchors of $\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$.<span style="font-size:80%;opacity:0.8">为了与原始RPNs[29]进行公平比较，我们使用$C_4$(与[16]相同)或$C_5$的单尺度映射运行了两个基线（表1（a，b）），都使用与我们相同的超参数，包括使用5种尺度锚点$\lbrace 32^2, 64^2, 128^2, 256^2, 512^2 \rbrace$。</span></li><li>Placing FPN in <font color=orangered>RPN</font> improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale RPN baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8">将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>Placing FPN in RPN improves $AR^{1k}$ to 56.3 (Table 1 (c)), which is 8.0 points increase over the single-scale <font color=orangered>RPN</font> baseline (Table 1 (a)).<span style="font-size:80%;opacity:0.8">将FPN放在RPN中可将$AR^{1k}$提高到56.3（表1（c）），这比单尺度RPN基线（表1（a））增加了8.0个点。</span></li><li>Our pyramid representation greatly improves <font color=orangered>RPN</font>’s robustness to object scale variation.<span style="font-size:80%;opacity:0.8">我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</span></li><li>The results in Table 1(d) are just on par with the <font color=orangered>RPN</font> baseline and lag far behind ours.<span style="font-size:80%;opacity:0.8">表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li><li><font color=orangered>RPN</font> is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale variance.<span style="font-size:80%;opacity:0.8">RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li><li>We choose to freeze the proposals as computed by <font color=orangered>RPN</font> on FPN (Table 1(c)), because it has good performance on small objects that are to be recognized by the detector.<span style="font-size:80%;opacity:0.8">我们选择冻结RPN在FPN上计算的提议（表1（c）），因为它在能被检测器识别的小目标上具有良好的性能。</span></li><li>For simplicity we do not share features between Fast R-CNN and <font color=orangered>RPN</font>, except when specified.<span style="font-size:80%;opacity:0.8">为了简单起见，我们不在Fast R-CNN和RPN之间共享特征，除非指定。</span></li><li>Object detection results using Fast R-CNN [11] on a fixed set of proposals (<font color=orangered>RPN</font>, ${P_k}$, Table 1(c)), evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8">使用Fast R-CNN[11]在一组固定提议（RPN，${P_k}$，表1（c））上的目标检测结果，在COCO的minival数据集上进行评估。</span></li><li>Table 2(d) and (e) show that removing top-down connections or removing lateral connections leads to inferior results, similar to what we have observed in the above sub-section for <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8">表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li><li>Despite the good accuracy of this variant, it is based on the <font color=orangered>RPN</font> proposals of ${P_k}$ and has thus already benefited from the pyramid representation.<span style="font-size:80%;opacity:0.8">尽管这个变体具有很好的准确性，但它是基于${P_k}$的RPN提议的，因此已经从金字塔表示中受益。</span></li><li>But in a Faster R-CNN system [29], the <font color=orangered>RPN</font> and Fast R-CNN must use the same network backbone in order to make feature sharing possible.<span style="font-size:80%;opacity:0.8">但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用相同的骨干网络来实现特征共享。</span></li><li>Table 3 shows the comparisons between our method and two baselines, all using consistent backbone architectures for <font color=orangered>RPN</font> and Fast R-CNN.<span style="font-size:80%;opacity:0.8">表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。</span></li><li>The backbone network for <font color=orangered>RPN</font> are consistent with Fast R-CNN.<span style="font-size:80%;opacity:0.8">RPN与Fast R-CNN的骨干网络是一致的。</span></li><li>In the above, for simplicity we do not share the features between <font color=orangered>RPN</font> and Fast R-CNN.<span style="font-size:80%;opacity:0.8">在上面，为了简单起见，我们不共享RPN和Fast R-CNN之间的特征。</span></li><li>The two MLPs play a similar role as anchors in <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8">这两个MLP在RPN中扮演着类似于锚点的角色。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> ConvNet<br>(17) </td> <td>  </td> <td> 
<ul><li>(c) An alternative is to reuse the pyramidal feature hierarchy computed by a <font color=forestgreen>ConvNet</font> as if it were a featurized image pyramid.<span style="font-size:80%;opacity:0.8">（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li>For recognition tasks, engineered features have largely been replaced with features computed by deep convolutional networks (<font color=forestgreen>ConvNets</font>) [19, 20].<span style="font-size:80%;opacity:0.8">对于识别任务，工程特征大部分已经被深度卷积网络（ConvNets）[19，20]计算的特征所取代。</span></li><li>Aside from being capable of representing higher-level semantics, <font color=forestgreen>ConvNets</font> are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8">除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>A deep <font color=forestgreen>ConvNet</font> computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape.<span style="font-size:80%;opacity:0.8">深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a <font color=forestgreen>ConvNet</font>’s pyramidal feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8">单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>The goal of this paper is to naturally leverage the pyramidal shape of a <font color=forestgreen>ConvNet</font>’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8">本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>Before HOG and SIFT, early work on face detection with <font color=forestgreen>ConvNets</font> [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8">在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li><li>Deep <font color=forestgreen>ConvNet</font> object detectors.<span style="font-size:80%;opacity:0.8">Deep ConvNet目标检测器。</span></li><li>With the development of modern deep <font color=forestgreen>ConvNets</font> [19], object detectors like OverFeat [34] and R-CNN [12] showed dramatic improvements in accuracy.<span style="font-size:80%;opacity:0.8">随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。</span></li><li>OverFeat adopted a strategy similar to early neural network face detectors by applying a <font color=forestgreen>ConvNet</font> as a sliding window detector on an image pyramid.<span style="font-size:80%;opacity:0.8">OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。</span></li><li>R-CNN adopted a region proposal-based strategy [37] in which each proposal was scale-normalized before classifying with a <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8">R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。</span></li><li>A number of recent approaches improve detection and segmentation by using different layers in a <font color=forestgreen>ConvNet</font>.<span style="font-size:80%;opacity:0.8">一些最近的方法通过使用ConvNet中的不同层来改进检测和分割。</span></li><li>Our goal is to leverage a <font color=forestgreen>ConvNet</font>’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8">我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>The bottom-up pathway is the feed-forward computation of the backbone <font color=forestgreen>ConvNet</font>, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8">自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>Our method is a generic solution for building feature pyramids inside deep <font color=forestgreen>ConvNets</font>.<span style="font-size:80%;opacity:0.8">我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。</span></li><li>We have presented a clean and simple framework for building feature pyramids inside <font color=forestgreen>ConvNets</font>.<span style="font-size:80%;opacity:0.8">我们提出了一个干净而简单的框架，用于在ConvNets内部构建特征金字塔。</span></li><li>Finally, our study suggests that despite the strong representational power of deep <font color=forestgreen>ConvNets</font> and their implicit robustness to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8">最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> lateral<br>(15) </td> <td> [ˈlætərəl] </td> <td> 
<ul><li>A top-down architecture with <font color=orangered>lateral</font> connections is developed for building high-level semantic feature maps at all scales.<span style="font-size:80%;opacity:0.8">开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down pathway and <font color=orangered>lateral</font> connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8">为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>There are recent methods exploiting <font color=orangered>lateral</font>/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>The construction of our pyramid involves a bottom-up pathway, a top-down pathway, and <font color=orangered>lateral</font> connections, as introduced in the following.<span style="font-size:80%;opacity:0.8">如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>Top-down pathway and <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8">自顶向下的路径和横向连接。</span></li><li>These features are then enhanced with features from the bottom-up pathway via <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8">这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。</span></li><li>Each <font color=orangered>lateral</font> connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down pathway.<span style="font-size:80%;opacity:0.8">每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>A building block illustrating the <font color=orangered>lateral</font> connection and the top-down pathway, merged by addition.<span style="font-size:80%;opacity:0.8">构建模块说明了横向连接和自顶向下路径，通过加法合并。</span></li><li>The columns “<font color=orangered>lateral</font>” and “top-down” denote the presence of lateral and top-down connections, respectively.<span style="font-size:80%;opacity:0.8">列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。</span></li><li>The columns “lateral” and “top-down” denote the presence of <font color=orangered>lateral</font> and top-down connections, respectively.<span style="font-size:80%;opacity:0.8">列“lateral”和“top-down”分别表示横向连接和自上而下连接的存在。</span></li><li>With this modification, the 1×1 <font color=orangered>lateral</font> connections followed by 3×3 convolutions are attached to the bottom-up pyramid.<span style="font-size:80%;opacity:0.8">通过这种修改，将1×1横向连接和后面的3×3卷积添加到自下而上的金字塔中。</span></li><li>How important are <font color=orangered>lateral</font> connections? Table 1(e) shows the ablation results of a top-down feature pyramid without the 1×1 lateral connections.<span style="font-size:80%;opacity:0.8">横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>How important are lateral connections? Table 1(e) shows the ablation results of a top-down feature pyramid without the 1×1 <font color=orangered>lateral</font> connections.<span style="font-size:80%;opacity:0.8">横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>More precise locations of features can be directly passed from the finer levels of the bottom-up maps via the <font color=orangered>lateral</font> connections to the top-down maps.<span style="font-size:80%;opacity:0.8">更精确的特征位置可以通过横向连接直接从自下而上映射的更精细层级传递到自上而下的映射。</span></li><li>Table 2(d) and (e) show that removing top-down connections or removing <font color=orangered>lateral</font> connections leads to inferior results, similar to what we have observed in the above sub-section for RPN.<span style="font-size:80%;opacity:0.8">表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> semantic<br>(15) </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>A top-down architecture with lateral connections is developed for building high-level <font color=orangered>semantic</font> feature maps at all scales.<span style="font-size:80%;opacity:0.8">开发了一种具有横向连接的自顶向下架构，用于在所有尺度上构建高级语义特征映射。</span></li><li>Aside from being capable of representing higher-level <font color=orangered>semantics</font>, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8">除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>This in-network feature hierarchy produces feature maps of different spatial resolutions, but introduces large <font color=orangered>semantic</font> gaps caused by different depths.<span style="font-size:80%;opacity:0.8">这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。</span></li><li>The goal of this paper is to naturally leverage the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong <font color=orangered>semantics</font> at all scales.<span style="font-size:80%;opacity:0.8">本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>The result is a feature pyramid that has rich <font color=orangered>semantics</font> at all levels and is built quickly from a single input image scale.<span style="font-size:80%;opacity:0.8">其结果是一个特征金字塔，在所有级别都具有丰富的语义，并且可以从单个输入图像尺度上进行快速构建。</span></li><li>FCN [24] sums partial scores for each category over multiple scales to compute <font color=orangered>semantic</font> segmentations.<span style="font-size:80%;opacity:0.8">FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。</span></li><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and <font color=orangered>semantic</font> levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has <font color=orangered>semantics</font> from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8">我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>Our goal is to leverage a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level <font color=orangered>semantics</font> throughout.<span style="font-size:80%;opacity:0.8">我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>The bottom-up feature map is of lower-level <font color=orangered>semantics</font>, but its activations are more accurately localized as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8">自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li><li>The good performance of sharing parameters indicates that all levels of our pyramid share similar <font color=orangered>semantic</font> levels.<span style="font-size:80%;opacity:0.8">共享参数的良好性能表明我们的金字塔的所有层级共享相似的语义级别。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single higher-level feature map is not enough because there is a trade-off between coarser resolutions and stronger <font color=orangered>semantics</font>.<span style="font-size:80%;opacity:0.8">表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li><li>We conjecture that this is because there are large <font color=orangered>semantic</font> gaps between different levels on the bottom-up pyramid (Fig. 1(b)), especially for very deep ResNets.<span style="font-size:80%;opacity:0.8">我们推测这是因为自下而上的金字塔（图1（b））的不同层次之间存在较大的语义差距，尤其是对于非常深的ResNets。</span></li><li>This top-down pyramid has strong <font color=orangered>semantic</font> features and fine resolutions.<span style="font-size:80%;opacity:0.8">这个自顶向下的金字塔具有强大的语义特征和良好的分辨率。</span></li><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly <font color=orangered>semantic</font> feature maps of $P_2$ (i.e., the finest level in our pyramids).<span style="font-size:80%;opacity:0.8">金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> roi<br>(13) </td> <td> [rwɑ:] </td> <td> 
<ul><li>Fast R-CNN [11] is a region-based object detector in which Region-of-Interest (<font color=orangered>RoI</font>) pooling is used to extract features.<span style="font-size:80%;opacity:0.8">Fast R-CNN[11]是一个基于区域的目标检测器，利用感兴趣区域（RoI）池化来提取特征。</span></li><li>To use it with our FPN, we need to assign <font color=orangered>RoIs</font> of different scales to the pyramid levels.<span style="font-size:80%;opacity:0.8">要将其与我们的FPN一起使用，我们需要为金字塔等级分配不同尺度的RoI。</span></li><li>Formally, we assign an <font color=orangered>RoI</font> of width $w$ and height $h$ (on the input image to the network) to the level $P_k$ of our feature pyramid by:<span style="font-size:80%;opacity:0.8">在形式上，我们通过以下公式将宽度为$w$和高度为$h$（在网络上的输入图像上）的RoI分配到特征金字塔的级别$P_k$上：</span></li><li>Here 224 is the canonical ImageNet pre-training size, and $k_0$ is the target level on which an <font color=orangered>RoI</font> with $w\times h=224^2$ should be mapped into.<span style="font-size:80%;opacity:0.8">这里224是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。</span></li><li>(1) means that if the <font color=orangered>RoI</font>’s scale becomes smaller (say, 1/2 of 224), it should be mapped into a finer-resolution level (say, $k=3$).<span style="font-size:80%;opacity:0.8">直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如k=3）。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all <font color=orangered>RoIs</font> of all levels.<span style="font-size:80%;opacity:0.8">我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li><li>So unlike [16], we simply adopt <font color=orangered>RoI</font> pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected (fc) layers (each followed by ReLU) before the final classification and bounding box regression layers.<span style="font-size:80%;opacity:0.8">因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（fc）层（每层后都接ReLU层）。</span></li><li>Each mini-batch involves 2 image per GPU and 512 <font color=orangered>RoIs</font> per image.<span style="font-size:80%;opacity:0.8">每个小批量数据包括每个GPU2张图像和每张图像上512个RoI。</span></li><li>We use 2000 <font color=orangered>RoIs</font> per image for training and 1000 for testing.<span style="font-size:80%;opacity:0.8">我们每张图像使用2000个RoIs进行训练，1000个RoI进行测试。</span></li><li>As a ResNet-based Fast R-CNN baseline, following [16], we adopt <font color=orangered>RoI</font> pooling with an output size of 14×14 and attach all conv5 layers as the hidden layers of the head.<span style="font-size:80%;opacity:0.8">作为基于ResNet的Fast R-CNN基线，遵循[16]，我们采用输出尺寸为14×14的RoI池化，并将所有conv5层作为头部的隐藏层。</span></li><li>We argue that this is because <font color=orangered>RoI</font> pooling is a warping-like operation, which is less sensitive to the region’s scales.<span style="font-size:80%;opacity:0.8">我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。</span></li><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 <font color=orangered>RoIs</font> per image which accelerate convergence, in contrast to 64 RoIs in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8">我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 RoIs per image which accelerate convergence, in contrast to 64 <font color=orangered>RoIs</font> in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8">我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> featurize<br>(12) </td> <td> ['fi:tʃәraiz] </td> <td> 
<ul><li>Feature pyramids built upon image pyramids (for short we call these <font color=orangered>featurized</font> image pyramids) form the basis of a standard solution [1] (Fig. 1(a)).<span style="font-size:80%;opacity:0.8">建立在图像金字塔之上的特征金字塔（我们简称为特征化图像金字塔）构成了标准解决方案的基础[1]（图1（a））。</span></li><li>(c) An alternative is to reuse the pyramidal feature hierarchy computed by a ConvNet as if it were a <font color=orangered>featurized</font> image pyramid.<span style="font-size:80%;opacity:0.8">（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li><font color=orangered>Featurized</font> image pyramids were heavily used in the era of hand-engineered features [5, 25].<span style="font-size:80%;opacity:0.8">特征化图像金字塔在手工设计的时代被大量使用[5，25]。</span></li><li>All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on <font color=orangered>featurized</font> image pyramids (e.g., [16, 35]).<span style="font-size:80%;opacity:0.8">在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。</span></li><li>For these reasons, Fast and Faster R-CNN [11, 29] opt to not use <font color=orangered>featurized</font> image pyramids under default settings.<span style="font-size:80%;opacity:0.8">出于这些原因，Fast和Faster R-CNN[11，29]选择在默认设置下不使用特征化图像金字塔。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s pyramidal feature hierarchy as if it were a <font color=orangered>featurized</font> image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8">单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>In other words, we show how to create in-network feature pyramids that can be used to replace <font color=orangered>featurized</font> image pyramids without sacrificing representational power, speed, or memory.<span style="font-size:80%;opacity:0.8">换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li><li>Our model echoes a <font color=orangered>featurized</font> image pyramid, which has not been explored in these works.<span style="font-size:80%;opacity:0.8">我们的模型反映了一个特征化的图像金字塔，这在这些研究中还没有探索过。</span></li><li>There has also been significant interest in computing <font color=orangered>featurized</font> image pyramids quickly.<span style="font-size:80%;opacity:0.8">这对快速计算特征化图像金字塔也很有意义。</span></li><li>Although these methods adopt architectures with pyramidal shapes, they are unlike <font color=orangered>featurized</font> image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2.<span style="font-size:80%;opacity:0.8">尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。</span></li><li>Because all levels of the pyramid use shared classifiers/regressors as in a traditional <font color=orangered>featurized</font> image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps.<span style="font-size:80%;opacity:0.8">由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为d）。</span></li><li>This advantage is analogous to that of using a <font color=orangered>featurized</font> image pyramid, where a common head classifier can be applied to features computed at any image scale.<span style="font-size:80%;opacity:0.8">这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> pathway<br>(12) </td> <td> [ˈpɑ:θweɪ] </td> <td> 
<ul><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, semantically weak features via a top-down <font color=orangered>pathway</font> and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8">为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>The construction of our pyramid involves a bottom-up <font color=orangered>pathway</font>, a top-down pathway, and lateral connections, as introduced in the following.<span style="font-size:80%;opacity:0.8">如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>The construction of our pyramid involves a bottom-up pathway, a top-down <font color=orangered>pathway</font>, and lateral connections, as introduced in the following.<span style="font-size:80%;opacity:0.8">如下所述，我们的金字塔结构包括自下而上的路径，自上而下的路径和横向连接。</span></li><li>Bottom-up <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8">自下而上的路径。</span></li><li>The bottom-up <font color=orangered>pathway</font> is the feed-forward computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8">自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>Top-down <font color=orangered>pathway</font> and lateral connections.<span style="font-size:80%;opacity:0.8">自顶向下的路径和横向连接。</span></li><li>The top-down <font color=orangered>pathway</font> hallucinates higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8">自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li><li>These features are then enhanced with features from the bottom-up <font color=orangered>pathway</font> via lateral connections.<span style="font-size:80%;opacity:0.8">这些特征随后通过来自自下而上路径上的特征经由横向连接进行增强。</span></li><li>Each lateral connection merges feature maps of the same spatial size from the bottom-up <font color=orangered>pathway</font> and the top-down pathway.<span style="font-size:80%;opacity:0.8">每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>Each lateral connection merges feature maps of the same spatial size from the bottom-up pathway and the top-down <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8">每个横向连接合并来自自下而上路径和自顶向下路径的具有相同空间大小的特征映射。</span></li><li>A building block illustrating the lateral connection and the top-down <font color=orangered>pathway</font>, merged by addition.<span style="font-size:80%;opacity:0.8">构建模块说明了横向连接和自顶向下路径，通过加法合并。</span></li><li>How important is top-down enrichment? Table 1(d) shows the results of our feature pyramid without the top-down <font color=orangered>pathway</font>.<span style="font-size:80%;opacity:0.8">自上而下的改进有多重要？表1（d）显示了没有自上而下路径的特征金字塔的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> MLP<br>(10) </td> <td> [!≈ em el pi:] </td> <td> 
<ul><li>Note that compared to the standard conv5 head, our 2-fc <font color=orangered>MLP</font> head is lighter weight and faster.<span style="font-size:80%;opacity:0.8">请注意，与标准的conv5头部相比，我们的2-fc MLP头部更轻更快。</span></li><li>Table 2(b) is a baseline exploiting an <font color=orangered>MLP</font> head with 2 hidden fc layers, similar to the head in our architecture.<span style="font-size:80%;opacity:0.8">表2（b）是利用MLP头部的基线，其具有2个隐藏的fc层，类似于我们的架构中的头部。</span></li><li>On top of each level of the feature pyramid, we apply a small 5×5 <font color=orangered>MLP</font> to predict 14×14 masks and object scores in a fully convolutional fashion, see Fig. 4.<span style="font-size:80%;opacity:0.8">在特征金字塔的每个层级上，我们应用一个小的5×5MLP以全卷积方式预测14×14掩码和目标分数，参见图4。</span></li><li>Additionally, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second <font color=orangered>MLP</font> of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8">此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>The two <font color=orangered>MLPs</font> play a similar role as anchors in RPN.<span style="font-size:80%;opacity:0.8">这两个MLP在RPN中扮演着类似于锚点的角色。</span></li><li>We apply a small <font color=orangered>MLP</font> on 5x5 windows to generate dense object segments with output dimension of 14x14.<span style="font-size:80%;opacity:0.8">我们在5x5窗口上应用一个小的MLP来生成输出尺寸为14x14的密集目标块。</span></li><li>Half octaves are handled by an <font color=orangered>MLP</font> on 7x7 windows ($7 \approx 5 \sqrt 2$), not shown here.<span style="font-size:80%;opacity:0.8">半个组由MLP在7x7窗口（ $7 \ approx 5 \ sqrt 2 $）处理，此处未展示。</span></li><li>Our baseline FPN model with a single 5×5 <font color=orangered>MLP</font> achieves an AR of 43.4.<span style="font-size:80%;opacity:0.8">我们的具有单个5×5MLP的基线FPN模型达到了43.4的AR。</span></li><li>Switching to a slightly larger 7×7 <font color=orangered>MLP</font> leaves accuracy largely unchanged.<span style="font-size:80%;opacity:0.8">切换到稍大的7×7MLP，精度基本保持不变。</span></li><li>Using both <font color=orangered>MLPs</font> together increases accuracy to 45.7 AR.<span style="font-size:80%;opacity:0.8">同时使用两个MLP将精度提高到了45.7的AR。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> pyramidal<br>(9) </td> <td> ['pɪrəmɪdl] </td> <td> 
<ul><li>In this paper, we exploit the inherent multi-scale, <font color=orangered>pyramidal</font> hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost.<span style="font-size:80%;opacity:0.8">在本文中，我们利用深度卷积网络内在的多尺度、金字塔分级来构造具有很少额外成本的特征金字塔。</span></li><li>(c) An alternative is to reuse the <font color=orangered>pyramidal</font> feature hierarchy computed by a ConvNet as if it were a featurized image pyramid.<span style="font-size:80%;opacity:0.8">（c）另一种方法是重用ConvNet计算的金字塔特征层次结构，就好像它是一个特征化的图像金字塔。</span></li><li>A deep ConvNet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, <font color=orangered>pyramidal</font> shape.<span style="font-size:80%;opacity:0.8">深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li><li>The Single Shot Detector (SSD) [22] is one of the first attempts at using a ConvNet’s <font color=orangered>pyramidal</font> feature hierarchy as if it were a featurized image pyramid (Fig. 1(c)).<span style="font-size:80%;opacity:0.8">单次检测器（SSD）[22]是首先尝试使用ConvNet的金字塔特征层级中的一个，好像它是一个特征化的图像金字塔（图1（c））。</span></li><li>The goal of this paper is to naturally leverage the <font color=orangered>pyramidal</font> shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8">本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>Although these methods adopt architectures with <font color=orangered>pyramidal</font> shapes, they are unlike featurized image pyramids [5, 7, 34] where predictions are made independently at all levels, see Fig. 2.<span style="font-size:80%;opacity:0.8">尽管这些方法采用的是金字塔形状的架构，但它们不同于特征化的图像金字塔[5，7，34]，其中所有层次上的预测都是独立进行的，参见图2。</span></li><li>In fact, for the <font color=orangered>pyramidal</font> architecture in Fig. 2 (top), image pyramids are still needed to recognize objects across multiple scales [28].<span style="font-size:80%;opacity:0.8">事实上，对于图2（顶部）中的金字塔结构，图像金字塔仍然需要跨多个尺度上识别目标[28]。</span></li><li>Our goal is to leverage a ConvNet’s <font color=orangered>pyramidal</font> feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8">我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li><li>This architecture simulates the effect of reusing the <font color=orangered>pyramidal</font> feature hierarchy (Fig. 1(b)).<span style="font-size:80%;opacity:0.8">该架构模拟了重用金字塔特征层次结构的效果（图1（b））。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> e.g.<br>(9) </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (<font color=orangered>e.g.</font>, 10 scales per octave).<span style="font-size:80%;opacity:0.8">它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li><li>All recent top entries in the ImageNet [33] and COCO [21] detection challenges use multi-scale testing on featurized image pyramids (<font color=orangered>e.g.</font>, [16, 35]).<span style="font-size:80%;opacity:0.8">在ImageNet[33]和COCO[21]检测挑战中，最近的所有排名靠前的输入都使用了针对特征化图像金字塔的多尺度测试（例如[16，35]）。</span></li><li>Inference time increases considerably (<font color=orangered>e.g.</font>, by four times [11]), making this approach impractical for real applications.<span style="font-size:80%;opacity:0.8">推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。</span></li><li>But to avoid using low-level features SSD foregoes reusing already computed layers and instead builds the pyramid starting from high up in the network (<font color=orangered>e.g.</font>, conv4_3 of VGG nets [36]) and then by adding several new layers.<span style="font-size:80%;opacity:0.8">但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。</span></li><li>On the contrary, our method leverages the architecture as a feature pyramid where predictions (<font color=orangered>e.g.</font>, object detections) are independently made on each level (Fig. 2 bottom).<span style="font-size:80%;opacity:0.8">相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。</span></li><li>Top: a top-down architecture with skip connections, where predictions are made on the finest level (<font color=orangered>e.g.</font>, [28]).<span style="font-size:80%;opacity:0.8">顶部：带有跳跃连接的自顶向下的架构，在最好的级别上进行预测（例如，[28]）。</span></li><li>This process is independent of the backbone convolutional architectures (<font color=orangered>e.g.</font>, [19, 36, 16]), and in this paper we present results using ResNets [16].<span style="font-size:80%;opacity:0.8">这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。</span></li><li>We have experimented with more sophisticated blocks (<font color=orangered>e.g.</font>, using multi-layer residual blocks [16] as the connections) and observed marginally better results.<span style="font-size:80%;opacity:0.8">我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li><li>Existing mask proposal methods [27, 28, 4] are based on densely sampled image pyramids (<font color=orangered>e.g.</font>, scaled by 2^{\lbrace −2:0.5:1 \rbrace} in [27, 28]), making them computationally expensive.<span style="font-size:80%;opacity:0.8">现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为2^{\lbrace −2:0.5:1 \rbrace}），使得它们是计算昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> bounding<br>(8) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>In ablation experiments, we find that for <font color=orangered>bounding</font> box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8">在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>In the following we adopt our method in RPN [29] for <font color=orangered>bounding</font> box proposal generation and in Fast R-CNN [11] for object detection.<span style="font-size:80%;opacity:0.8">在下面，我们采用我们的方法在RPN[29]中进行边界框提议生成，并在Fast R-CNN[11]中进行目标检测。</span></li><li>In the original RPN design, a small subnetwork is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and <font color=orangered>bounding</font> box regression.<span style="font-size:80%;opacity:0.8">在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>The object/non-object criterion and <font color=orangered>bounding</font> box regression target are defined with respect to a set of reference boxes called anchors [29].<span style="font-size:80%;opacity:0.8">目标/非目标标准和边界框回归目标的定义是关于一组称为锚点的参考框的[29]。</span></li><li>We assign training labels to the anchors based on their Intersection-over-Union (IoU) ratios with ground-truth <font color=orangered>bounding</font> boxes as in [29].<span style="font-size:80%;opacity:0.8">如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and <font color=orangered>bounding</font> box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8">我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li><li>So unlike [16], we simply adopt RoI pooling to extract 7×7 features, and attach two hidden 1,024-d fully-connected (fc) layers (each followed by ReLU) before the final classification and <font color=orangered>bounding</font> box regression layers.<span style="font-size:80%;opacity:0.8">因此，与[16]不同，我们只是采用RoI池化提取7×7特征，并在最终的分类层和边界框回归层之前附加两个隐藏单元为1024维的全连接（fc）层（每层后都接ReLU层）。</span></li><li><font color=orangered>Bounding</font> box proposal results using RPN [29], evaluated on the COCO minival set.<span style="font-size:80%;opacity:0.8">使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> minival<br>(7) </td> <td>  </td> <td> 
<ul><li>We train using the union of 80k train images and a 35k subset of val images (trainval35k [2]), and report ablations on a 5k subset of val images (<font color=forestgreen>minival</font>).<span style="font-size:80%;opacity:0.8">我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>Bounding box proposal results using RPN [29], evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8">使用RPN[29]的边界框提议结果，在COCO的minival数据集上进行评估。</span></li><li>Object detection results using Fast R-CNN [11] on a fixed set of proposals (RPN, ${P_k}$, Table 1(c)), evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8">使用Fast R-CNN[11]在一组固定提议（RPN，${P_k}$，表1（c））上的目标检测结果，在COCO的minival数据集上进行评估。</span></li><li>Object detection results using Faster R-CNN [29] evaluated on the COCO <font color=forestgreen>minival</font> set.<span style="font-size:80%;opacity:0.8">使用Faster R-CNN[29]在COCOminival数据集上评估的目标检测结果。</span></li><li>More object detection results using Faster R-CNN and our FPNs, evaluated on <font color=forestgreen>minival</font>.<span style="font-size:80%;opacity:0.8">使用Faster R-CNN和我们的FPN在minival上的更多目标检测结果。</span></li><li>This increases AP on <font color=forestgreen>minival</font> to 35.6, without sharing features.<span style="font-size:80%;opacity:0.8">这将minival上的AP增加到了35.6，没有共享特征。</span></li><li>Some results were not available on the test-std set, so we also include the test-dev results (and for Multipath [40] on <font color=forestgreen>minival</font>).<span style="font-size:80%;opacity:0.8">一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> backbone<br>(6) </td> <td> [ˈbækbəʊn] </td> <td> 
<ul><li>This process is independent of the <font color=orangered>backbone</font> convolutional architectures (e.g., [19, 36, 16]), and in this paper we present results using ResNets [16].<span style="font-size:80%;opacity:0.8">这个过程独立于主卷积体系结构（例如[19，36，16]），在本文中，我们呈现了使用ResNets[16]的结果。</span></li><li>The bottom-up pathway is the feed-forward computation of the <font color=orangered>backbone</font> ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8">自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li><li>As is common practice [12], all network <font color=orangered>backbones</font> are pre-trained on the ImageNet1k classification set [33] and then fine-tuned on the detection dataset.<span style="font-size:80%;opacity:0.8">正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。</span></li><li>But in a Faster R-CNN system [29], the RPN and Fast R-CNN must use the same network <font color=orangered>backbone</font> in order to make feature sharing possible.<span style="font-size:80%;opacity:0.8">但是在Faster R-CNN系统中[29]，RPN和Fast R-CNN必须使用相同的骨干网络来实现特征共享。</span></li><li>Table 3 shows the comparisons between our method and two baselines, all using consistent <font color=orangered>backbone</font> architectures for RPN and Fast R-CNN.<span style="font-size:80%;opacity:0.8">表3显示了我们的方法和两个基线之间的比较，所有这些RPN和Fast R-CNN都使用一致的骨干架构。</span></li><li>The <font color=orangered>backbone</font> network for RPN are consistent with Fast R-CNN.<span style="font-size:80%;opacity:0.8">RPN与Fast R-CNN的骨干网络是一致的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> semantically<br>(5) </td> <td> [sɪ'mæntɪklɪ] </td> <td> 
<ul><li>In this figure, feature maps are indicate by blue outlines and thicker outlines denote <font color=orangered>semantically</font> stronger features.<span style="font-size:80%;opacity:0.8">在该图中，特征映射用蓝色轮廓表示，较粗的轮廓表示语义上较强的特征。</span></li><li>The principle advantage of featurizing each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are <font color=orangered>semantically</font> strong, including the high-resolution levels.<span style="font-size:80%;opacity:0.8">对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, <font color=orangered>semantically</font> strong features with high-resolution, semantically weak features via a top-down pathway and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8">为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>To achieve this goal, we rely on an architecture that combines low-resolution, semantically strong features with high-resolution, <font color=orangered>semantically</font> weak features via a top-down pathway and lateral connections (Fig. 1(d)).<span style="font-size:80%;opacity:0.8">为了实现这个目标，我们所依赖的架构将低分辨率、强语义的特征与高分辨率、弱语义的特征通过自顶向下的路径和横向连接相结合。（图1（d））。</span></li><li>The top-down pathway hallucinates higher resolution features by upsampling spatially coarser, but <font color=orangered>semantically</font> stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8">自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> ablation<br>(5) </td> <td> [əˈbleɪʃn] </td> <td> 
<ul><li>In <font color=orangered>ablation</font> experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8">在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We train using the union of 80k train images and a 35k subset of val images (trainval35k [2]), and report <font color=orangered>ablations</font> on a 5k subset of val images (minival).<span style="font-size:80%;opacity:0.8">我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>5.1.1 <font color=orangered>Ablation</font> Experiments<span style="font-size:80%;opacity:0.8">5.1.1 消融实验</span></li><li>How important are lateral connections? Table 1(e) shows the <font color=orangered>ablation</font> results of a top-down feature pyramid without the 1×1 lateral connections.<span style="font-size:80%;opacity:0.8">横向连接有多重要？表1（e）显示了没有1×1横向连接的自顶向下特征金字塔的消融结果。</span></li><li>To better investigate FPN’s effects on the region-based detector alone, we conduct <font color=orangered>ablations</font> of Fast R-CNN on a fixed set of proposals.<span style="font-size:80%;opacity:0.8">为了更好地调查FPN对仅基于区域的检测器的影响，我们在一组固定的提议上进行Fast R-CNN的消融。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> SharpMask<br>(5) </td> <td>  </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and <font color=forestgreen>SharpMask</font> [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>In this section we use FPNs to generate segmentation proposals, following the DeepMask/<font color=forestgreen>SharpMask</font> framework [27, 28].<span style="font-size:80%;opacity:0.8">在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li>DeepMask/<font color=forestgreen>SharpMask</font> were trained on image crops for predicting instance segments and object/non-object scores.<span style="font-size:80%;opacity:0.8">DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。</span></li><li>DeepMask, <font color=forestgreen>SharpMask</font>, and FPN use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8">DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li>DeepMask and <font color=forestgreen>SharpMask</font> performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ variants).<span style="font-size:80%;opacity:0.8">DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> DeepMask<br>(5) </td> <td>  </td> <td> 
<ul><li>In this section we use FPNs to generate segmentation proposals, following the <font color=forestgreen>DeepMask</font>/SharpMask framework [27, 28].<span style="font-size:80%;opacity:0.8">在本节中，我们使用FPN生成分割建议，遵循DeepMask/SharpMask框架[27，28]。</span></li><li><font color=forestgreen>DeepMask</font>/SharpMask were trained on image crops for predicting instance segments and object/non-object scores.<span style="font-size:80%;opacity:0.8">DeepMask/SharpMask在裁剪图像上进行训练，可以预测实例块和目标/非目标分数。</span></li><li><font color=forestgreen>DeepMask</font>, SharpMask, and FPN use ResNet-50 while Instance-FCN uses VGG-16.<span style="font-size:80%;opacity:0.8">DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li><li><font color=forestgreen>DeepMask</font> and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ variants).<span style="font-size:80%;opacity:0.8">DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li><li>We also report comparisons to <font color=forestgreen>DeepMask</font> [27], Sharp-Mask [28], and InstanceFCN [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8">我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> generic<br>(4) </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a <font color=orangered>generic</font> feature extractor in several applications.<span style="font-size:80%;opacity:0.8">这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>Our method is a <font color=orangered>generic</font> solution for building feature pyramids inside deep ConvNets.<span style="font-size:80%;opacity:0.8">我们的方法是在深度ConvNets内部构建特征金字塔的通用解决方案。</span></li><li>Our method is a <font color=orangered>generic</font> pyramid representation and can be used in applications other than object detection.<span style="font-size:80%;opacity:0.8">我们的方法是一种通用金字塔表示，可用于除目标检测之外的其他应用。</span></li><li>These results demonstrate that our model is a <font color=orangered>generic</font> feature extractor and can replace image pyramids for other multi-scale detection problems.<span style="font-size:80%;opacity:0.8">这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> octave<br>(4) </td> <td> [ˈɒktɪv] </td> <td> 
<ul><li>They were so critical that object detectors like DPM [7] required dense scale sampling to achieve good results (e.g., 10 scales per <font color=orangered>octave</font>).<span style="font-size:80%;opacity:0.8">它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li><li>Additionally, motivated by the use of 2 scales per <font color=orangered>octave</font> in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8">此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>Additionally, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half <font color=orangered>octaves</font>.<span style="font-size:80%;opacity:0.8">此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li><li>Half <font color=orangered>octaves</font> are handled by an MLP on 7x7 windows ($7 \approx 5 \sqrt 2$), not shown here.<span style="font-size:80%;opacity:0.8">半个组由MLP在7x7窗口（ $7 \ approx 5 \ sqrt 2 $）处理，此处未展示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> robustness<br>(4) </td> <td> [rəʊ'bʌstnəs] </td> <td> 
<ul><li>But even with this <font color=orangered>robustness</font>, pyramids are still needed to get the most accurate results.<span style="font-size:80%;opacity:0.8">但即使有这种鲁棒性，金字塔仍然需要得到最准确的结果。</span></li><li>Our pyramid representation greatly improves RPN’s <font color=orangered>robustness</font> to object scale variation.<span style="font-size:80%;opacity:0.8">我们的金字塔表示大大提高了RPN对目标尺度变化的鲁棒性。</span></li><li>RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its <font color=orangered>robustness</font> to scale variance.<span style="font-size:80%;opacity:0.8">RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li><li>Finally, our study suggests that despite the strong representational power of deep ConvNets and their implicit <font color=orangered>robustness</font> to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8">最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> leverage<br>(4) </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>The goal of this paper is to naturally <font color=orangered>leverage</font> the pyramidal shape of a ConvNet’s feature hierarchy while creating a feature pyramid that has strong semantics at all scales.<span style="font-size:80%;opacity:0.8">本文的目标是自然地利用ConvNet特征层级的金字塔形状，同时创建一个在所有尺度上都具有强大语义的特征金字塔。</span></li><li>On the contrary, our method <font color=orangered>leverages</font> the architecture as a feature pyramid where predictions (e.g., object detections) are independently made on each level (Fig. 2 bottom).<span style="font-size:80%;opacity:0.8">相反，我们的方法利用这个架构作为特征金字塔，其中预测（例如目标检测）在每个级别上独立进行（图2底部）。</span></li><li>Bottom: our model that has a similar structure but <font color=orangered>leverages</font> it as a feature pyramid, with predictions made independently at all levels.<span style="font-size:80%;opacity:0.8">底部：我们的模型具有类似的结构，但将其用作特征金字塔，并在各个层级上独立进行预测。</span></li><li>Our goal is to <font color=orangered>leverage</font> a ConvNet’s pyramidal feature hierarchy, which has semantics from low to high levels, and build a feature pyramid with high-level semantics throughout.<span style="font-size:80%;opacity:0.8">我们的目标是利用ConvNet的金字塔特征层级，该层次结构具有从低到高的语义，并在整个过程中构建具有高级语义的特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> SIFT<br>(4) </td> <td> [sɪft] </td> <td> 
<ul><li><font color=orangered>SIFT</font> features [25] were originally extracted at scale-space extrema and used for feature point matching.<span style="font-size:80%;opacity:0.8">SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li><li>HOG features [5], and later <font color=orangered>SIFT</font> features as well, were computed densely over entire image pyramids.<span style="font-size:80%;opacity:0.8">HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。</span></li><li>These HOG and <font color=orangered>SIFT</font> pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more.<span style="font-size:80%;opacity:0.8">这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。</span></li><li>Before HOG and <font color=orangered>SIFT</font>, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8">在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> trainval35k<br>(4) </td> <td>  </td> <td> 
<ul><li>We train using the union of 80k train images and a 35k subset of val images (<font color=forestgreen>trainval35k</font> [2]), and report ablations on a 5k subset of val images (minival).<span style="font-size:80%;opacity:0.8">我们训练使用80k张训练图像和35k大小的验证图像子集（trainval35k[2]）的联合，并报告了在5k大小的验证图像子集（minival）上的消融实验。</span></li><li>All models are trained on <font color=forestgreen>trainval35k</font>.<span style="font-size:80%;opacity:0.8">所有模型都是通过trainval35k训练的。</span></li><li>Models are trained on the <font color=forestgreen>trainval35k</font> set.<span style="font-size:80%;opacity:0.8">模型在trainval35k数据集上训练。</span></li><li>Models are trained on the <font color=forestgreen>trainval35k</font> set and use ResNet-50. ^†Provided by authors of [16].<span style="font-size:80%;opacity:0.8">模型在trainval35k数据集上训练并使用ResNet-50。^†由[16]的作者提供。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> variant<br>(4) </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>We have also evaluated a <font color=orangered>variant</font> of Table 1(d) without sharing the parameters of the heads, but observed similarly degraded performance.<span style="font-size:80%;opacity:0.8"> 我们还评估了表1（d）的一个变体，但没有分享磁头的参数，但观察到类似的性能下降。</span></li><li>This <font color=orangered>variant</font> (Table 1(f)) is better than the baseline but inferior to our approach.<span style="font-size:80%;opacity:0.8">这个变体（表1（f））比基线要好，但不如我们的方法。</span></li><li>Despite the good accuracy of this <font color=orangered>variant</font>, it is based on the RPN proposals of ${P_k}$ and has thus already benefited from the pyramid representation.<span style="font-size:80%;opacity:0.8">尽管这个变体具有很好的准确性，但它是基于${P_k}$的RPN提议的，因此已经从金字塔表示中受益。</span></li><li>DeepMask and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘zoom’ <font color=orangered>variants</font>).<span style="font-size:80%;opacity:0.8">DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> surpass<br>(3) </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, <font color=orangered>surpassing</font> all existing single-model entries including those from the COCO 2016 challenge winners.<span style="font-size:80%;opacity:0.8">在一个基本的Faster R-CNN系统中使用FPN，没有任何不必要的东西，我们的方法可以在COCO检测基准数据集上取得最先进的单模型结果，结果超过了所有现有的单模型输入，包括COCO 2016挑战赛的获奖者。</span></li><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], <font color=orangered>surpassing</font> all existing heavily-engineered single-model entries of competition winners.<span style="font-size:80%;opacity:0.8">没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li><li>Table 4 compares our method with the single-model results of the COCO competition winners, including the 2016 winner G-RMI and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has <font color=orangered>surpassed</font> these strong, heavily engineered competitors.<span style="font-size:80%;opacity:0.8">表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> representational<br>(3) </td> <td> [ˌreprɪzenˈteɪʃnl] </td> <td> 
<ul><li>The high-resolution maps have low-level features that harm their <font color=orangered>representational</font> capacity for object recognition.<span style="font-size:80%;opacity:0.8">高分辨率映射具有损害其目标识别表示能力的低级特征。</span></li><li>In other words, we show how to create in-network feature pyramids that can be used to replace featurized image pyramids without sacrificing <font color=orangered>representational</font> power, speed, or memory.<span style="font-size:80%;opacity:0.8">换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li><li>Finally, our study suggests that despite the strong <font color=orangered>representational</font> power of deep ConvNets and their implicit robustness to scale variation, it is still critical to explicitly address multi-scale problems using pyramid representations.<span style="font-size:80%;opacity:0.8">最后，我们的研究表明，尽管深层ConvNets具有强大的表示能力以及它们对尺度变化的隐式鲁棒性，但使用金字塔表示对于明确地解决多尺度问题仍然至关重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> COCO-style<br>(3) </td> <td> [!≈ 'kəʊkəʊ staɪl] </td> <td> 
<ul><li>In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the <font color=orangered>COCO-style</font> Average Precision (AP) by 2.3 points and PASCAL-style AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8">在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We evaluate the <font color=orangered>COCO-style</font> Average Recall (AR) and AR on small, medium, and large objects ($AR_s$, $AR_m$, and $AR_l$) following the definitions in [21].<span style="font-size:80%;opacity:0.8">根据[21]中的定义，我们评估了COCO类型的平均召回率（AR）和在小型，中型和大型目标($AR_s$, $AR_m$, and $AR_lv)上的AR。</span></li><li>We evaluate object detection by the <font color=orangered>COCO-style</font> Average Precision (AP) and PASCAL-style AP (at a single IoU threshold of 0.5).<span style="font-size:80%;opacity:0.8">我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> HOG<br>(3) </td> <td> [hɒg] </td> <td> 
<ul><li><font color=orangered>HOG</font> features [5], and later SIFT features as well, were computed densely over entire image pyramids.<span style="font-size:80%;opacity:0.8">HOG特征[5]，以及后来的SIFT特征，都是在整个图像金字塔上密集计算的。</span></li><li>These <font color=orangered>HOG</font> and SIFT pyramids have been used in numerous works for image classification, object detection, human pose estimation, and more.<span style="font-size:80%;opacity:0.8">这些HOG和SIFT金字塔已在许多工作中得到了应用，用于图像分类，目标检测，人体姿势估计等。</span></li><li>Before <font color=orangered>HOG</font> and SIFT, early work on face detection with ConvNets [38, 32] computed shallow networks over image pyramids to detect faces across scales.<span style="font-size:80%;opacity:0.8">在HOG和SIFT之前，使用ConvNet[38，32]的早期人脸检测工作计算了图像金字塔上的浅网络，以检测跨尺度的人脸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> residual<br>(3) </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>Specifically, for ResNets [16] we use the feature activations output by each stage’s last <font color=orangered>residual</font> block.<span style="font-size:80%;opacity:0.8">具体而言，对于ResNets[16]，我们使用每个阶段的最后一个残差块输出的特征激活。</span></li><li>We denote the output of these last <font color=orangered>residual</font> blocks as $\lbrace C_2 , C_3 , C_4 , C_5 \rbrace$ for conv2, conv3, conv4, and conv5 outputs, and note that they have strides of {4, 8, 16, 32} pixels with respect to the input image.<span style="font-size:80%;opacity:0.8">对于conv2，conv3，conv4和conv5输出，我们将这些最后残差块的输出表示为$\lbrace C_2, C_3, C_4, C_5 \rbrace$，并注意相对于输入图像它们的步长为{4，8，16，32}个像素。</span></li><li>We have experimented with more sophisticated blocks (e.g., using multi-layer <font color=orangered>residual</font> blocks [16] as the connections) and observed marginally better results.<span style="font-size:80%;opacity:0.8">我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> extractor<br>(2) </td> <td> [ɪkˈstræktə(r)] </td> <td> 
<ul><li>This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature <font color=orangered>extractor</font> in several applications.<span style="font-size:80%;opacity:0.8">这种称为特征金字塔网络（FPN）的架构在几个应用程序中作为通用特征提取器表现出了显著的改进。</span></li><li>These results demonstrate that our model is a generic feature <font color=orangered>extractor</font> and can replace image pyramids for other multi-scale detection problems.<span style="font-size:80%;opacity:0.8">这些结果表明，我们的模型是一个通用的特征提取器，可以替代图像金字塔以用于其他多尺度检测问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> FPS<br>(2) </td> <td> ['efp'i:'es] </td> <td> 
<ul><li>In addition, our method can run at 6 <font color=orangered>FPS</font> on a GPU and thus is a practical and accurate solution to multi-scale object detection.<span style="font-size:80%;opacity:0.8">此外，我们的方法可以在GPU上以6FPS运行，因此是多尺度目标检测的实用和准确的解决方案。</span></li><li>Our approach, based on FPNs, is substantially faster (our models run at 6 to 7 <font color=orangered>FPS</font>).<span style="font-size:80%;opacity:0.8">我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> hand-engineered<br>(2) </td> <td> [!≈ hænd 'endʒɪn'ɪərd] </td> <td> 
<ul><li>Featurized image pyramids were heavily used in the era of <font color=orangered>hand-engineered</font> features [5, 25].<span style="font-size:80%;opacity:0.8">特征化图像金字塔在手工设计的时代被大量使用[5，25]。</span></li><li><font color=orangered>Hand-engineered</font> features and early neural networks.<span style="font-size:80%;opacity:0.8">手工设计特征和早期神经网络。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> higher-level<br>(2) </td> <td> [!≈ ˈhaɪə(r) ˈlevl] </td> <td> 
<ul><li>Aside from being capable of representing <font color=orangered>higher-level</font> semantics, ConvNets are also more robust to variance in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8">除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single <font color=orangered>higher-level</font> feature map is not enough because there is a trade-off between coarser resolutions and stronger semantics.<span style="font-size:80%;opacity:0.8">表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> variance<br>(2) </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>Aside from being capable of representing higher-level semantics, ConvNets are also more robust to <font color=orangered>variance</font> in scale and thus facilitate recognition from features computed on a single input scale [15, 11, 29] (Fig. 1(b)).<span style="font-size:80%;opacity:0.8">除了能够表示更高级别的语义，ConvNets对于尺度变化也更加鲁棒，从而有助于从单一输入尺度上计算的特征进行识别[15，11，29]（图1（b））。</span></li><li>RPN is a sliding window detector with a fixed window size, so scanning over pyramid levels can increase its robustness to scale <font color=orangered>variance</font>.<span style="font-size:80%;opacity:0.8">RPN是一个具有固定窗口大小的滑动窗口检测器，因此在金字塔层级上扫描可以增加其对尺度变化的鲁棒性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> featurizing<br>(2) </td> <td>  </td> <td> 
<ul><li>The principle advantage of <font color=forestgreen>featurizing</font> each level of an image pyramid is that it produces a multi-scale feature representation in which all levels are semantically strong, including the high-resolution levels.<span style="font-size:80%;opacity:0.8">对图像金字塔的每个层次进行特征化的主要优势在于它产生了多尺度的特征表示，其中所有层次上在语义上都很强，包括高分辨率层。</span></li><li>Nevertheless, <font color=forestgreen>featurizing</font> each level of an image pyramid has obvious limitations.<span style="font-size:80%;opacity:0.8">尽管如此，特征化图像金字塔的每个层次都具有明显的局限性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> in-network<br>(2) </td> <td> [!≈ ɪn ˈnetwɜ:k] </td> <td> 
<ul><li>This <font color=orangered>in-network</font> feature hierarchy produces feature maps of different spatial resolutions, but introduces large semantic gaps caused by different depths.<span style="font-size:80%;opacity:0.8">这种网内特征层级产生不同空间分辨率的特征映射，但引入了由不同深度引起的较大的语义差异。</span></li><li>In other words, we show how to create <font color=orangered>in-network</font> feature pyramids that can be used to replace featurized image pyramids without sacrificing representational power, speed, or memory.<span style="font-size:80%;opacity:0.8">换句话说，我们展示了如何创建网络中的特征金字塔，可以用来代替特征化的图像金字塔，而不牺牲表示能力，速度或内存。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> PASCAL-style<br>(2) </td> <td> [!≈ 'pæskәl staɪl] </td> <td> 
<ul><li>In ablation experiments, we find that for bounding box proposals, FPN significantly increases the Average Recall (AR) by 8.0 points; for object detection, it improves the COCO-style Average Precision (AP) by 2.3 points and <font color=orangered>PASCAL-style</font> AP by 3.8 points, over a strong single-scale baseline of Faster R-CNN on ResNets [16].<span style="font-size:80%;opacity:0.8">在消融实验中，我们发现对于边界框提议，FPN将平均召回率（AR）显著增加了8个百分点；对于目标检测，它将COCO型的平均精度（AP）提高了2.3个百分点，PASCAL型AP提高了3.8个百分点，超过了ResNet[16]上Faster R-CNN强大的单尺度基准线。</span></li><li>We evaluate object detection by the COCO-style Average Precision (AP) and <font color=orangered>PASCAL-style</font> AP (at a single IoU threshold of 0.5).<span style="font-size:80%;opacity:0.8">我们通过COCO类型的平均精度（AP）和PASCAL类型的AP（单个IoU阈值为0.5）来评估目标检测。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> OverFeat<br>(2) </td> <td>  </td> <td> 
<ul><li>With the development of modern deep ConvNets [19], object detectors like <font color=forestgreen>OverFeat</font> [34] and R-CNN [12] showed dramatic improvements in accuracy.<span style="font-size:80%;opacity:0.8">随着现代深度卷积网络[19]的发展，像OverFeat[34]和R-CNN[12]这样的目标检测器在精度上显示出了显著的提高。</span></li><li><font color=forestgreen>OverFeat</font> adopted a strategy similar to early neural network face detectors by applying a ConvNet as a sliding window detector on an image pyramid.<span style="font-size:80%;opacity:0.8">OverFeat采用了一种类似于早期神经网络人脸检测器的策略，通过在图像金字塔上应用ConvNet作为滑动窗口检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> trade-off<br>(2) </td> <td> [ˈtreɪdˌɔ:f, -ˌɔf] </td> <td> 
<ul><li>Recent and more accurate detection methods like Fast R-CNN [11] and Faster R-CNN [29] advocate using features computed from a single scale, because it offers a good <font color=orangered>trade-off</font> between accuracy and speed.<span style="font-size:80%;opacity:0.8">最近更准确的检测方法，如Fast R-CNN[11]和Faster R-CNN[29]提倡使用从单一尺度计算出的特征，因为它提供了精确度和速度之间的良好折衷。</span></li><li>Table 1 (b) shows no advantage over (a), indicating that a single higher-level feature map is not enough because there is a <font color=orangered>trade-off</font> between coarser resolutions and stronger semantics.<span style="font-size:80%;opacity:0.8">表1（b）显示没有优于（a），这表明单个更高级别的特征映射是不够的，因为存在在较粗分辨率和较强语义之间的权衡。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> FCN<br>(2) </td> <td> [!≈ ef si: en] </td> <td> 
<ul><li><font color=orangered>FCN</font> [24] sums partial scores for each category over multiple scales to compute semantic segmentations.<span style="font-size:80%;opacity:0.8">FCN[24]将多个尺度上的每个类别的部分分数相加以计算语义分割。</span></li><li>Ghiasi et al. [8] present a Laplacian pyramid presentation for <font color=orangered>FCNs</font> to progressively refine segmentation.<span style="font-size:80%;opacity:0.8">Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> keypoint<br>(2) </td> <td> [ki:'pɔɪnt] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for <font color=orangered>keypoint</font> estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li><li>Recently, FPN has enabled new top results in all tracks of the COCO competition, including detection, instance segmentation, and <font color=orangered>keypoint</font> estimation.<span style="font-size:80%;opacity:0.8">最近，FPN在COCO竞赛的所有方面都取得了新的最佳结果，包括检测，实例分割和关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> upsampled<br>(2) </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>upsampled</font> map is then merged with the corresponding bottom-up map (which undergoes a 1×1 convolutional layer to reduce channel dimensions) by element-wise addition.<span style="font-size:80%;opacity:0.8">然后通过按元素相加，将上采样映射与相应的自下而上映射（其经过1×1卷积层来减少通道维度）合并。</span></li><li>But we argue that the locations of these features are not precise, because these maps have been downsampled and <font color=forestgreen>upsampled</font> several times.<span style="font-size:80%;opacity:0.8">但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> regressor<br>(2) </td> <td> [rɪ'gresə(r)] </td> <td> 
<ul><li>Because all levels of the pyramid use shared classifiers/<font color=orangered>regressors</font> as in a traditional featurized image pyramid, we fix the feature dimension (numbers of channels, denoted as d) in all the feature maps.<span style="font-size:80%;opacity:0.8">由于金字塔的所有层都像传统的特征图像金字塔一样使用共享分类器/回归器，因此我们在所有特征映射中固定特征维度（通道数记为d）。</span></li><li>We attach predictor heads (in Fast R-CNN the heads are class-specific classifiers and bounding box <font color=orangered>regressors</font>) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8">我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> marginally<br>(2) </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>We have experimented with more sophisticated blocks (e.g., using multi-layer residual blocks [16] as the connections) and observed <font color=orangered>marginally</font> better results.<span style="font-size:80%;opacity:0.8">我们已经尝试了更复杂的块（例如，使用多层残差块[16]作为连接）并观察到稍微更好的结果。</span></li><li>Its result (33.4 AP) is <font color=orangered>marginally</font> worse than that of using all pyramid levels (33.9 AP, Table 2(c)).<span style="font-size:80%;opacity:0.8">其结果（33.4 AP）略低于使用所有金字塔等级（33.9 AP，表2（c））的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> subnetwork<br>(2) </td> <td>  </td> <td> 
<ul><li>In the original RPN design, a small <font color=forestgreen>subnetwork</font> is evaluated on dense 3×3 sliding windows, on top of a single-scale convolutional feature map, performing object/non-object binary classification and bounding box regression.<span style="font-size:80%;opacity:0.8">在原始的RPN设计中，一个小型子网络在密集的3×3滑动窗口，单尺度卷积特征映射上进行评估，执行目标/非目标的二分类和边界框回归。</span></li><li>In [16], a ResNet’s conv5 layers (a 9-layer deep <font color=forestgreen>subnetwork</font>) are adopted as the head on top of the conv4 features, but our method has already harnessed conv5 to construct the feature pyramid.<span style="font-size:80%;opacity:0.8">在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> analogous<br>(2) </td> <td> [əˈnæləgəs] </td> <td> 
<ul><li>This advantage is <font color=orangered>analogous</font> to that of using a featurized image pyramid, where a common head classifier can be applied to features computed at any image scale.<span style="font-size:80%;opacity:0.8">这个优点类似于使用特征图像金字塔的优点，其中可以将常见头部分类器应用于在任何图像尺度下计算的特征。</span></li><li><font color=orangered>Analogous</font> to the ResNet-based Faster R-CNN system [16] that uses $C_4$ as the single-scale feature map, we set $k_0$ to 4. Intuitively, Eqn.<span style="font-size:80%;opacity:0.8">类似于基于ResNet的Faster R-CNN系统[16]使用$C_4$作为单尺度特征映射，我们将$k_0$设置为4。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> adaptation<br>(2) </td> <td> [ˌædæpˈteɪʃn] </td> <td> 
<ul><li>With the above <font color=orangered>adaptations</font>, RPN can be naturally trained and tested with our FPN, in the same fashion as in [29].<span style="font-size:80%;opacity:0.8">通过上述改编，RPN可以自然地通过我们的FPN进行训练和测试，与[29]中的方式相同。</span></li><li>Based on these <font color=orangered>adaptations</font>, we can train and test Fast R-CNN on top of the feature pyramid.<span style="font-size:80%;opacity:0.8">基于这些改编，我们可以在特征金字塔之上训练和测试Fast R-CNN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> canonical<br>(2) </td> <td> [kəˈnɒnɪkl] </td> <td> 
<ul><li>Here 224 is the <font color=orangered>canonical</font> ImageNet pre-training size, and $k_0$ is the target level on which an RoI with $w\times h=224^2$ should be mapped into.<span style="font-size:80%;opacity:0.8">这里224是规范的ImageNet预训练大小，而$k_0$是大小为$w \times h=224^2$的RoI应该映射到的目标级别。</span></li><li>Both the corresponding image region size (light orange) and <font color=orangered>canonical</font> object size (dark orange) are shown.<span style="font-size:80%;opacity:0.8">显示了相应的图像区域大小（浅橙色）和典型目标大小（深橙色）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> test-std<br>(2) </td> <td>  </td> <td> 
<ul><li>We also report final results on the standard test set (<font color=forestgreen>test-std</font>) [21] which has no disclosed labels.<span style="font-size:80%;opacity:0.8">我们还报告了在没有公开标签的标准测试集（test-std）[21]上的最终结果。</span></li><li>Some results were not available on the <font color=forestgreen>test-std</font> set, so we also include the test-dev results (and for Multipath [40] on minival).<span style="font-size:80%;opacity:0.8">一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> resize<br>(2) </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The input image is <font color=orangered>resized</font> such that its shorter side has 800 pixels.<span style="font-size:80%;opacity:0.8">输入图像的大小调整为其较短边有800像素。</span></li><li>The input image is <font color=orangered>resized</font> such that its shorter side has 800 pixels.<span style="font-size:80%;opacity:0.8">调整大小输入图像，使其较短边为800像素。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> synchronize<br>(2) </td> <td> [ˈsɪŋkrənaɪz] </td> <td> 
<ul><li>We adopt <font color=orangered>synchronized</font> SGD training on 8 GPUs.<span style="font-size:80%;opacity:0.8">我们采用8个GPU进行同步SGD训练。</span></li><li><font color=orangered>Synchronized</font> SGD is used to train the model on 8 GPUs.<span style="font-size:80%;opacity:0.8">同步SGD用于在8个GPU上训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> SGD<br>(2) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>We adopt synchronized <font color=orangered>SGD</font> training on 8 GPUs.<span style="font-size:80%;opacity:0.8">我们采用8个GPU进行同步SGD训练。</span></li><li>Synchronized <font color=orangered>SGD</font> is used to train the model on 8 GPUs.<span style="font-size:80%;opacity:0.8">同步SGD用于在8个GPU上训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> momentum<br>(2) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8">我们使用0.0001的权重衰减和0.9的动量。</span></li><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8">我们使用0.0001的权重衰减和0.9的动量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> NVIDIA<br>(2) </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.148 seconds per image on a single <font color=orangered>NVIDIA</font> M40 GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8">通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li><li>^† Runtimes are measured on an <font color=orangered>NVIDIA</font> M40 GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8">^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> M40<br>(2) </td> <td>  </td> <td> 
<ul><li>With feature sharing, our FPN-based Faster R-CNN system has inference time of 0.148 seconds per image on a single NVIDIA <font color=forestgreen>M40</font> GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8">通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li><li>^† Runtimes are measured on an NVIDIA <font color=forestgreen>M40</font> GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8">^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> test-dev<br>(2) </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Some results were not available on the test-std set, so we also include the <font color=orangered>test-dev</font> results (and for Multipath [40] on minival).<span style="font-size:80%;opacity:0.8">一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li><li>On the <font color=orangered>test-dev</font> set, our method increases over the existing best results by 0.5 points of AP (36.2 vs. 35.7) and 3.4 points of AP@0.5 (59.1 vs.<span style="font-size:80%;opacity:0.8">没有添加额外的东西，我们的单模型提交就已经超越了这些强大的，经过严格设计的竞争对手。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> InstanceFCN<br>(2) </td> <td>  </td> <td> 
<ul><li>^† Runtimes are measured on an NVIDIA M40 GPU, except the <font color=forestgreen>InstanceFCN</font> timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8">^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li><li>We also report comparisons to DeepMask [27], Sharp-Mask [28], and <font color=forestgreen>InstanceFCN</font> [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8">我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> scale-invariant<br>(1) </td> <td> [!≈ skeɪl ɪnˈveəriənt] </td> <td> 
<ul><li>These pyramids are <font color=orangered>scale-invariant</font> in the sense that an object’s scale change is offset by shifting its level in the pyramid.<span style="font-size:80%;opacity:0.8">这些金字塔是尺度不变的，因为目标的尺度变化是通过在金字塔中移动它的层级来抵消的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> DPM<br>(1) </td> <td> [!≈ di: pi: em] </td> <td> 
<ul><li>They were so critical that object detectors like <font color=orangered>DPM</font> [7] required dense scale sampling to achieve good results (e.g., 10 scales per octave).<span style="font-size:80%;opacity:0.8">它们非常关键，以至于像DPM[7]这样的目标检测器需要密集的尺度采样才能获得好的结果（例如每组10个尺度，octave含义参考SIFT特征）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> impractical<br>(1) </td> <td> [ɪmˈpræktɪkl] </td> <td> 
<ul><li>Inference time increases considerably (e.g., by four times [11]), making this approach <font color=orangered>impractical</font> for real applications.<span style="font-size:80%;opacity:0.8">推断时间显著增加（例如，四倍[11]），使得这种方法在实际应用中不切实际。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> infeasible<br>(1) </td> <td> [ɪn'fi:zəbl] </td> <td> 
<ul><li>Moreover, training deep networks end-to-end on an image pyramid is <font color=orangered>infeasible</font> in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an inconsistency between train/test-time inference.<span style="font-size:80%;opacity:0.8">此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> inconsistency<br>(1) </td> <td> [ˌɪnkən'sɪstənsɪ] </td> <td> 
<ul><li>Moreover, training deep networks end-to-end on an image pyramid is infeasible in terms of memory, and so, if exploited, image pyramids are used only at test time [15, 11, 16, 35], which creates an <font color=orangered>inconsistency</font> between train/test-time inference.<span style="font-size:80%;opacity:0.8">此外，在图像金字塔上端对端地训练深度网络在内存方面是不可行的，所以如果被采用，图像金字塔仅在测试时被使用[15，11，16，35]，这造成了训练/测试时推断的不一致性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> subsampling<br>(1) </td> <td>  </td> <td> 
<ul><li>A deep ConvNet computes a feature hierarchy layer by layer, and with <font color=forestgreen>subsampling</font> layers the feature hierarchy has an inherent multi-scale, pyramidal shape.<span style="font-size:80%;opacity:0.8">深层ConvNet逐层计算特征层级，而对于下采样层，特征层级具有内在的多尺度金字塔形状。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> Ideally<br>(1) </td> <td> [aɪ'di:əlɪ] </td> <td> 
<ul><li><font color=orangered>Ideally</font>, the SSD-style pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost.<span style="font-size:80%;opacity:0.8">理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> SSD-style<br>(1) </td> <td>  </td> <td> 
<ul><li>Ideally, the <font color=forestgreen>SSD-style</font> pyramid would reuse the multi-scale feature maps from different layers computed in the forward pass and thus come free of cost.<span style="font-size:80%;opacity:0.8">理想情况下，SSD风格的金字塔将重用正向传递中从不同层中计算的多尺度特征映射，因此是零成本的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> forego<br>(1) </td> <td> [fɔ:ˈɡəu] </td> <td> 
<ul><li>But to avoid using low-level features SSD <font color=orangered>foregoes</font> reusing already computed layers and instead builds the pyramid starting from high up in the network (e.g., conv4_3 of VGG nets [36]) and then by adding several new layers.<span style="font-size:80%;opacity:0.8">但为了避免使用低级特征，SSD放弃重用已经计算好的图层，而从网络中的最高层开始构建金字塔（例如，VGG网络的conv4_3[36]），然后添加几个新层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> higher-resolution<br>(1) </td> <td> [!≈ ˈhaɪə(r) ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>Thus it misses the opportunity to reuse the <font color=orangered>higher-resolution</font> maps of the feature hierarchy.<span style="font-size:80%;opacity:0.8">因此它错过了重用特征层级的更高分辨率映射的机会。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> heavily-engineered<br>(1) </td> <td> [!≈ ˈhevɪli 'endʒɪn'ɪərd] </td> <td> 
<ul><li>Without bells and whistles, we report a state-of-the-art single-model result on the challenging COCO detection benchmark [21] simply based on FPN and a basic Faster R-CNN detector [29], surpassing all existing <font color=orangered>heavily-engineered</font> single-model entries of competition winners.<span style="font-size:80%;opacity:0.8">没有任何不必要的东西，我们在具有挑战性的COCO检测基准数据集上报告了最新的单模型结果，仅仅基于FPN和基本的Faster R-CNN检测器[29]，就超过了竞赛获奖者所有现存的严重工程化的单模型竞赛输入。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> consistently<br>(1) </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>In addition, our pyramid structure can be trained end-to-end with all scales and is used <font color=orangered>consistently</font> at train/test time, which would be memory-infeasible using image pyramids.<span style="font-size:80%;opacity:0.8">另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> memory-infeasible<br>(1) </td> <td> [!≈ ˈmeməri ɪn'fi:zəbl] </td> <td> 
<ul><li>In addition, our pyramid structure can be trained end-to-end with all scales and is used consistently at train/test time, which would be <font color=orangered>memory-infeasible</font> using image pyramids.<span style="font-size:80%;opacity:0.8">另外，我们的金字塔结构可以通过所有尺度进行端对端培训，并且在训练/测试时一致地使用，这在使用图像金字塔时是内存不可行的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> scale-space<br>(1) </td> <td> [!≈ skeɪl speɪs] </td> <td> 
<ul><li>SIFT features [25] were originally extracted at <font color=orangered>scale-space</font> extrema and used for feature point matching.<span style="font-size:80%;opacity:0.8">SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> extrema<br>(1) </td> <td> [ɪks'tri:mə] </td> <td> 
<ul><li>SIFT features [25] were originally extracted at scale-space <font color=orangered>extrema</font> and used for feature point matching.<span style="font-size:80%;opacity:0.8">SIFT特征[25]最初是从尺度空间极值中提取的，用于特征点匹配。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> sparsely<br>(1) </td> <td> [spɑ:slɪ] </td> <td> 
<ul><li>Dollar et al. [6] demonstrated fast pyramid computation by first computing a <font color=orangered>sparsely</font> sampled (in scale) pyramid and then interpolating missing levels.<span style="font-size:80%;opacity:0.8">Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> interpolate<br>(1) </td> <td> [ɪnˈtɜ:pəleɪt] </td> <td> 
<ul><li>Dollar et al. [6] demonstrated fast pyramid computation by first computing a sparsely sampled (in scale) pyramid and then <font color=orangered>interpolating</font> missing levels.<span style="font-size:80%;opacity:0.8">Dollar等人[6]通过先计算一个稀疏采样（尺度）金字塔，然后插入缺失的层级，从而演示了快速金字塔计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> scale-normalized<br>(1) </td> <td> [!≈ skeɪl 'nɔ:məlaɪzd] </td> <td> 
<ul><li>R-CNN adopted a region proposal-based strategy [37] in which each proposal was <font color=orangered>scale-normalized</font> before classifying with a ConvNet.<span style="font-size:80%;opacity:0.8">R-CNN采用了基于区域提议的策略[37]，其中每个提议在用ConvNet进行分类之前都进行了尺度归一化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> SPPnet<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SPPnet</font> [15] demonstrated that such region-based detectors could be applied much more efficiently on feature maps extracted on a single image scale.<span style="font-size:80%;opacity:0.8">SPPnet[15]表明，这种基于区域的检测器可以更有效地应用于在单个图像尺度上提取的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> hypercolumn<br>(1) </td> <td> [haɪpə'kɒləm] </td> <td> 
<ul><li><font color=orangered>Hypercolumns</font> [13] uses a similar method for object instance segmentation.<span style="font-size:80%;opacity:0.8">Hypercolumns[13]使用类似的方法进行目标实例分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> HyperNet<br>(1) </td> <td>  </td> <td> 
<ul><li>Several other approaches (<font color=forestgreen>HyperNet</font> [18], ParseNet [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8">在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> ParseNet<br>(1) </td> <td>  </td> <td> 
<ul><li>Several other approaches (HyperNet [18], <font color=forestgreen>ParseNet</font> [23], and ION [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8">在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> ION<br>(1) </td> <td> [ˈaɪən] </td> <td> 
<ul><li>Several other approaches (HyperNet [18], ParseNet [23], and <font color=orangered>ION</font> [2]) concatenate features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8">在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> concatenate<br>(1) </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>Several other approaches (HyperNet [18], ParseNet [23], and ION [2]) <font color=orangered>concatenate</font> features of multiple layers before computing predictions, which is equivalent to summing transformed features.<span style="font-size:80%;opacity:0.8">在计算预测之前，其他几种方法（HyperNet[18]，ParseNet[23]和ION[2]）将多个层的特征连接起来，这相当于累加转换后的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> MS-CNN<br>(1) </td> <td>  </td> <td> 
<ul><li>SSD [22] and <font color=forestgreen>MS-CNN</font> [3] predict objects at multiple layers of the feature hierarchy without combining features or scores.<span style="font-size:80%;opacity:0.8">SSD[22]和MS-CNN[3]可预测特征层级中多个层的目标，而不需要组合特征或分数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> U-Net<br>(1) </td> <td> [!≈ ju: net] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including <font color=orangered>U-Net</font> [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> Recombinator<br>(1) </td> <td> [riːkəm'bɪnətə] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, <font color=orangered>Recombinator</font> networks [17] for face detection, and Stacked Hourglass networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Hourglass<br>(1) </td> <td> [ˈaʊəglɑ:s] </td> <td> 
<ul><li>There are recent methods exploiting lateral/skip connections that associate low-level feature maps across resolutions and semantic levels, including U-Net [31] and SharpMask [28] for segmentation, Recombinator networks [17] for face detection, and Stacked <font color=orangered>Hourglass</font> networks [26] for keypoint estimation.<span style="font-size:80%;opacity:0.8">最近有一些方法利用横向/跳跃连接将跨分辨率和语义层次的低级特征映射关联起来，包括用于分割的U-Net[31]和SharpMask[28]，Recombinator网络[17]用于人脸检测以及Stacked Hourglass网络[26]用于关键点估计。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> Ghiasi<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Ghiasi</font> et al. [8] present a Laplacian pyramid presentation for FCNs to progressively refine segmentation.<span style="font-size:80%;opacity:0.8">Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> Laplacian<br>(1) </td> <td> [lɑ:'plɑ:siәn] </td> <td> 
<ul><li>Ghiasi et al. [8] present a <font color=orangered>Laplacian</font> pyramid presentation for FCNs to progressively refine segmentation.<span style="font-size:80%;opacity:0.8">Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> progressively<br>(1) </td> <td> [prəˈgresɪvli] </td> <td> 
<ul><li>Ghiasi et al. [8] present a Laplacian pyramid presentation for FCNs to <font color=orangered>progressively</font> refine segmentation.<span style="font-size:80%;opacity:0.8">Ghiasi等人[8]为FCN提出拉普拉斯金字塔表示，以逐步细化分割。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> general-purpose<br>(1) </td> <td> ['dʒenrəl 'pɜ:pəs] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is <font color=orangered>general-purpose</font> and in this paper we focus on sliding window proposers (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8">由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> proposer<br>(1) </td> <td> [prəˈpəʊzə(r)] </td> <td> 
<ul><li>The resulting Feature Pyramid Network is general-purpose and in this paper we focus on sliding window <font color=orangered>proposers</font> (Region Proposal Network, RPN for short) [29] and region-based detectors (Fast R-CNN) [11].<span style="font-size:80%;opacity:0.8">由此产生的特征金字塔网络是通用的，在本文中，我们侧重于滑动窗口提议（Region Proposal Network，简称RPN）[29]和基于区域的检测器（Fast R-CNN）[11]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> Sec.6.<br>(1) </td> <td>  </td> <td> 
<ul><li>We also generalize FPNs to instance segmentation proposals in <font color=forestgreen>Sec.6.</font><span style="font-size:80%;opacity:0.8">在第6节中我们还将FPN泛化到实例细分提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> arbitrary<br>(1) </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>Our method takes a single-scale image of an <font color=orangered>arbitrary</font> size as input, and outputs proportionally sized feature maps at multiple levels, in a fully convolutional fashion.<span style="font-size:80%;opacity:0.8">我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> proportionally<br>(1) </td> <td> [prə'pɔ:ʃənlɪ] </td> <td> 
<ul><li>Our method takes a single-scale image of an arbitrary size as input, and outputs <font color=orangered>proportionally</font> sized feature maps at multiple levels, in a fully convolutional fashion.<span style="font-size:80%;opacity:0.8">我们的方法以任意大小的单尺度图像作为输入，并以全卷积的方式输出多层适当大小的特征映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> feed-forward<br>(1) </td> <td> ['fi:df'ɔ:wəd] </td> <td> 
<ul><li>The bottom-up pathway is the <font color=orangered>feed-forward</font> computation of the backbone ConvNet, which computes a feature hierarchy consisting of feature maps at several scales with a scaling step of 2.<span style="font-size:80%;opacity:0.8">自下向上的路径是主ConvNet的前馈计算，其计算由尺度步长为2的多尺度特征映射组成的特征层级。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> hallucinate<br>(1) </td> <td> [həˈlu:sɪneɪt] </td> <td> 
<ul><li>The top-down pathway <font color=orangered>hallucinates</font> higher resolution features by upsampling spatially coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8">自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> spatially<br>(1) </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>The top-down pathway hallucinates higher resolution features by upsampling <font color=orangered>spatially</font> coarser, but semantically stronger, feature maps from higher pyramid levels.<span style="font-size:80%;opacity:0.8">自顶向下的路径通过上采样空间上更粗糙但在语义上更强的来自较高金字塔等级的特征映射来幻化更高分辨率的特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> lower-level<br>(1) </td> <td> [!≈ ˈləʊə(r) ˈlevl] </td> <td> 
<ul><li>The bottom-up feature map is of <font color=orangered>lower-level</font> semantics, but its activations are more accurately localized as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8">自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> localized<br>(1) </td> <td> [ˈləʊkəlaɪzd] </td> <td> 
<ul><li>The bottom-up feature map is of lower-level semantics, but its activations are more accurately <font color=orangered>localized</font> as it was subsampled fewer times.<span style="font-size:80%;opacity:0.8">自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> subsampled<br>(1) </td> <td>  </td> <td> 
<ul><li>The bottom-up feature map is of lower-level semantics, but its activations are more accurately localized as it was <font color=forestgreen>subsampled</font> fewer times.<span style="font-size:80%;opacity:0.8">自下而上的特征映射具有较低级别的语义，但其激活可以更精确地定位，因为它被下采样的次数更少。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> coarser-resolution<br>(1) </td> <td> [!≈ kɔ:sə ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>With a <font color=orangered>coarser-resolution</font> feature map, we upsample the spatial resolution by a factor of 2 (using nearest neighbor upsampling for simplicity).<span style="font-size:80%;opacity:0.8">使用较粗糙分辨率的特征映射，我们将空间分辨率上采样为2倍（为了简单起见，使用最近邻上采样）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> iterated<br>(1) </td> <td> [ˈɪtəˌreɪtid] </td> <td> 
<ul><li>This process is <font color=orangered>iterated</font> until the finest resolution map is generated.<span style="font-size:80%;opacity:0.8">迭代这个过程，直到生成最佳分辨率映射。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> append<br>(1) </td> <td> [əˈpend] </td> <td> 
<ul><li>Finally, we <font color=orangered>append</font> a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling.<span style="font-size:80%;opacity:0.8">最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> alias<br>(1) </td> <td> [ˈeɪliəs] </td> <td> 
<ul><li>Finally, we append a 3 × 3 convolution on each merged map to generate the final feature map, which is to reduce the <font color=orangered>aliasing</font> effect of upsampling.<span style="font-size:80%;opacity:0.8">最后，我们在每个合并的映射上添加一个3×3卷积来生成最终的特征映射，这是为了减少上采样的混叠效应。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> non-linearity<br>(1) </td> <td> ['nɒnlaɪn'ərɪtɪ] </td> <td> 
<ul><li>There are no <font color=orangered>non-linearities</font> in these extra layers, which we have empirically found to have minor impacts.<span style="font-size:80%;opacity:0.8">在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> empirically<br>(1) </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li>There are no non-linearities in these extra layers, which we have <font color=orangered>empirically</font> found to have minor impacts.<span style="font-size:80%;opacity:0.8">在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> minor<br>(1) </td> <td> [ˈmaɪnə(r)] </td> <td> 
<ul><li>There are no non-linearities in these extra layers, which we have empirically found to have <font color=orangered>minor</font> impacts.<span style="font-size:80%;opacity:0.8">在这些额外的层中没有非线性，我们在实验中发现这些影响很小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> minimal<br>(1) </td> <td> [ˈmɪnɪməl] </td> <td> 
<ul><li>To demonstrate the simplicity and effectiveness of our method, we make <font color=orangered>minimal</font> modifications to the original systems of [29, 11] when adapting them to our feature pyramid.<span style="font-size:80%;opacity:0.8">为了证明我们方法的简洁性和有效性，我们对[29，11]的原始系统进行最小修改，使其适应我们的特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> class-agnostic<br>(1) </td> <td> [!≈ klɑ:s ægˈnɒstɪk] </td> <td> 
<ul><li>RPN [29] is a sliding-window <font color=orangered>class-agnostic</font> object detector.<span style="font-size:80%;opacity:0.8">RPN[29]是一个滑动窗口类不可知的目标检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> Intersection-over-Union<br>(1) </td> <td> [!≈ ˌɪntəˈsekʃn ˈəʊvə(r) ˈju:niən] </td> <td> 
<ul><li>We assign training labels to the anchors based on their <font color=orangered>Intersection-over-Union</font> (IoU) ratios with ground-truth bounding boxes as in [29].<span style="font-size:80%;opacity:0.8">如[29]，我们根据锚点和实际边界框的交并比（IoU）比例将训练标签分配给锚点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> finer-resolution<br>(1) </td> <td> [!≈ 'faɪnə ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>(1) means that if the RoI’s scale becomes smaller (say, 1/2 of 224), it should be mapped into a <font color=orangered>finer-resolution</font> level (say, $k=3$).<span style="font-size:80%;opacity:0.8">直觉上，方程（1）意味着如果RoI的尺寸变小了（比如224的1/2），它应该被映射到一个更精细的分辨率级别（比如k=3）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> predictor<br>(1) </td> <td> [prɪˈdɪktə(r)] </td> <td> 
<ul><li>We attach <font color=orangered>predictor</font> heads (in Fast R-CNN the heads are class-specific classifiers and bounding box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8">我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> class-specific<br>(1) </td> <td> [!≈ klɑ:s spəˈsɪfɪk] </td> <td> 
<ul><li>We attach predictor heads (in Fast R-CNN the heads are <font color=orangered>class-specific</font> classifiers and bounding box regressors) to all RoIs of all levels.<span style="font-size:80%;opacity:0.8">我们在所有级别的所有RoI中附加预测器头部（在Fast R-CNN中，预测器头部是特定类别的分类器和边界框回归器）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> harness<br>(1) </td> <td> [ˈhɑ:nɪs] </td> <td> 
<ul><li>In [16], a ResNet’s conv5 layers (a 9-layer deep subnetwork) are adopted as the head on top of the conv4 features, but our method has already <font color=orangered>harnessed</font> conv5 to construct the feature pyramid.<span style="font-size:80%;opacity:0.8">在[16]中，ResNet的conv5层（9层深的子网络）被用作conv4特征之上的头部，但我们的方法已经利用了conv5来构建特征金字塔。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> ImageNet1k<br>(1) </td> <td>  </td> <td> 
<ul><li>As is common practice [12], all network backbones are pre-trained on the <font color=forestgreen>ImageNet1k</font> classification set [33] and then fine-tuned on the detection dataset.<span style="font-size:80%;opacity:0.8">正如通常的做法[12]，所有的网络骨干都是在ImageNet1k分类集[33]上预先训练好的，然后在检测数据集上进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> reimplementation<br>(1) </td> <td>  </td> <td> 
<ul><li>Our code is a <font color=forestgreen>reimplementation</font> of py-faster-rcnn using Caffe2.<span style="font-size:80%;opacity:0.8">我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> py-faster-rcnn<br>(1) </td> <td>  </td> <td> 
<ul><li>Our code is a reimplementation of <font color=forestgreen>py-faster-rcnn</font> using Caffe2.<span style="font-size:80%;opacity:0.8">我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> Caffe<br>(1) </td> <td>  </td> <td> 
<ul><li>Our code is a reimplementation of py-faster-rcnn using <font color=forestgreen>Caffe</font>2.<span style="font-size:80%;opacity:0.8">我们的代码是使用Caffe2重新实现py-faster-rcnn。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> enrichment<br>(1) </td> <td> [ɪn'rɪtʃmənt] </td> <td> 
<ul><li>How important is top-down <font color=orangered>enrichment</font>? Table 1(d) shows the results of our feature pyramid without the top-down pathway.<span style="font-size:80%;opacity:0.8">自上而下的改进有多重要？表1（d）显示了没有自上而下路径的特征金字塔的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> par<br>(1) </td> <td> [pɑ:(r)] </td> <td> 
<ul><li>The results in Table 1(d) are just on <font color=orangered>par</font> with the RPN baseline and lag far behind ours.<span style="font-size:80%;opacity:0.8">表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> lag<br>(1) </td> <td> [læg] </td> <td> 
<ul><li>The results in Table 1(d) are just on par with the RPN baseline and <font color=orangered>lag</font> far behind ours.<span style="font-size:80%;opacity:0.8">表1（d）中的结果与RPN基线相当，并且远远落后于我们的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> conjecture<br>(1) </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that this is because there are large semantic gaps between different levels on the bottom-up pyramid (Fig. 1(b)), especially for very deep ResNets.<span style="font-size:80%;opacity:0.8">我们推测这是因为自下而上的金字塔（图1（b））的不同层次之间存在较大的语义差距，尤其是对于非常深的ResNets。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> level-specific<br>(1) </td> <td> [!≈ ˈlevl spəˈsɪfɪk] </td> <td> 
<ul><li>This issue cannot be simply remedied by <font color=orangered>level-specific</font> heads.<span style="font-size:80%;opacity:0.8">这个问题不能简单地由特定级别的负责人来解决。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> downsampled<br>(1) </td> <td>  </td> <td> 
<ul><li>But we argue that the locations of these features are not precise, because these maps have been <font color=forestgreen>downsampled</font> and upsampled several times.<span style="font-size:80%;opacity:0.8">但是我们认为这些特征的位置并不精确，因为这些映射已经进行了多次下采样和上采样。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> highest-resolution<br>(1) </td> <td> [!≈ haɪɪst ˌrezəˈlu:ʃn] </td> <td> 
<ul><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the <font color=orangered>highest-resolution</font>, strongly semantic feature maps of $P_2$ (i.e., the finest level in our pyramids).<span style="font-size:80%;opacity:0.8">金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> i.e.<br>(1) </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>How important are pyramid representations? Instead of resorting to pyramid representations, one can attach the head to the highest-resolution, strongly semantic feature maps of $P_2$ (<font color=orangered>i.e.</font>, the finest level in our pyramids).<span style="font-size:80%;opacity:0.8">金字塔表示有多重要？可以将头部附加到$P_2$的最高分辨率的强语义特征映射上（即我们金字塔中的最好层级），而不采用金字塔表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> orthogonal<br>(1) </td> <td> [ɔ:'θɒgənl] </td> <td> 
<ul><li>It gets an AP of 28.8, indicating that the 2-fc head does not give us any <font color=orangered>orthogonal</font> advantage over the baseline in Table 2(a).<span style="font-size:80%;opacity:0.8">它得到了28.8的AP，表明2-fc头部没有给我们带来任何超过表2（a）中基线的正交优势。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> sub-section<br>(1) </td> <td> ['sʌbs'ekʃn] </td> <td> 
<ul><li>Table 2(d) and (e) show that removing top-down connections or removing lateral connections leads to inferior results, similar to what we have observed in the above <font color=orangered>sub-section</font> for RPN.<span style="font-size:80%;opacity:0.8">表2（d）和（e）表明，去除自上而下的连接或去除横向连接会导致较差的结果，类似于我们在上面的RPN小节中观察到的结果。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> noteworthy<br>(1) </td> <td> [ˈnəʊtwɜ:ði] </td> <td> 
<ul><li>It is <font color=orangered>noteworthy</font> that removing top-down connections (Table 2(d)) significantly degrades the accuracy, suggesting that Fast R-CNN suffers from using the low-level features at the high-resolution maps.<span style="font-size:80%;opacity:0.8">值得注意的是，去除自上而下的连接（表2（d））显著降低了准确性，表明Fast R-CNN在高分辨率映射中使用了低级特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> warping-like<br>(1) </td> <td> [!≈ 'wɔ:pɪŋ laɪk] </td> <td> 
<ul><li>We argue that this is because RoI pooling is a <font color=orangered>warping-like</font> operation, which is less sensitive to the region’s scales.<span style="font-size:80%;opacity:0.8">我们认为这是因为RoI池化是一种扭曲式的操作，对区域尺度较不敏感。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> reproduction<br>(1) </td> <td> [ˌri:prəˈdʌkʃn] </td> <td> 
<ul><li>Table 3(a) shows our <font color=orangered>reproduction</font> of the baseline Faster R-CNN system as described in [16].<span style="font-size:80%;opacity:0.8">表3（a）显示了我们再现[16]中描述的Faster R-CNN系统的基线。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> convergence<br>(1) </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>We find the following implementations contribute to the gap: (i) We use an image scale of 800 pixels instead of 600 in [11, 16]; (ii) We train with 512 RoIs per image which accelerate <font color=orangered>convergence</font>, in contrast to 64 RoIs in [11, 16]; (iii) We use 5 scale anchors instead of 4 in [16] (adding $32^2$); (iv) At test time we use 1000 proposals per image instead of 300 in [16].<span style="font-size:80%;opacity:0.8">我们发现以下实现有助于缩小差距：（i）我们使用800像素的图像尺度，而不是[11，16]中的600像素；（ii）与[11，16]中的64个ROI相比，我们训练时每张图像有512个ROIs，可以加速收敛；（iii）我们使用5个尺度的锚点，而不是[16]中的4个（添加$32^2$）；（iv）在测试时，我们每张图像使用1000个提议，而不是[16]中的300个。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> FPN-based<br>(1) </td> <td>  </td> <td> 
<ul><li>With feature sharing, our <font color=forestgreen>FPN-based</font> Faster R-CNN system has inference time of 0.148 seconds per image on a single NVIDIA M40 GPU for ResNet-50, and 0.172 seconds for ResNet-101.<span style="font-size:80%;opacity:0.8">通过特征共享，我们的基于FPN的Faster R-CNN系统使用ResNet-50在单个NVIDIA M40 GPU上每张图像的推断时间为0.148秒，使用ResNet-101的时间为0.172秒。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> leaderboard<br>(1) </td> <td> ['li:dərbɔ:d] </td> <td> 
<ul><li>This model is the one we submitted to the COCO detection <font color=orangered>leaderboard</font>, shown in Table 4.<span style="font-size:80%;opacity:0.8">该模型是我们提交给COCO检测排行榜的模型，如表4所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> feature-sharing<br>(1) </td> <td> [!≈ ˈfi:tʃə(r) 'ʃeərɪŋ] </td> <td> 
<ul><li>We have not evaluated its <font color=orangered>feature-sharing</font> version due to limited time, which should be slightly better as implied by Table 5.<span style="font-size:80%;opacity:0.8">由于时间有限，我们尚未评估其特征共享版本，这应该稍微好一些，如表5所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> Multipath<br>(1) </td> <td> ['mʌltɪpæθ] </td> <td> 
<ul><li>Some results were not available on the test-std set, so we also include the test-dev results (and for <font color=orangered>Multipath</font> [40] on minival).<span style="font-size:80%;opacity:0.8">一些在test-std数据集上的结果是不可获得的，因此我们也包括了在test-dev上的结果（和Multipath[40]在minival上的结果）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> AttractioNet<br>(1) </td> <td>  </td> <td> 
<ul><li>^§: This entry of <font color=forestgreen>AttractioNet</font> [10] adopts VGG-16 for proposals and Wide ResNet [39] for object detection, so is not strictly a single-model result.<span style="font-size:80%;opacity:0.8">^§：AttractioNet[10]的输入采用VGG-16进行目标提议，用Wide ResNet[39]进行目标检测，因此它不是严格意义上的单模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 138 </td> <td> G-RMI<br>(1) </td> <td>  </td> <td> 
<ul><li>Table 4 compares our method with the single-model results of the COCO competition winners, including the 2016 winner <font color=forestgreen>G-RMI</font> and the 2015 winner Faster R-CNN+++. Without adding bells and whistles, our single-model entry has surpassed these strong, heavily engineered competitors.<span style="font-size:80%;opacity:0.8">表4将我们方法的单模型结果与COCO竞赛获胜者的结果进行了比较，其中包括2016年冠军G-RMI和2015年冠军Faster R-CNN+++。</span></li></ul>
 </td>
</tr>
<tr>
<td> 139 </td> <td> small-scale<br>(1) </td> <td> [ˈsmɔ:lˈskeɪl] </td> <td> 
<ul><li>It is worth noting that our method does not rely on image pyramids and only uses a single input image scale, but still has outstanding AP on <font color=orangered>small-scale</font> objects.<span style="font-size:80%;opacity:0.8">值得注意的是，我们的方法不依赖图像金字塔，只使用单个输入图像尺度，但在小型目标上仍然具有出色的AP。</span></li></ul>
 </td>
</tr>
<tr>
<td> 140 </td> <td> iterative<br>(1) </td> <td> ['ɪtərətɪv] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as <font color=orangered>iterative</font> regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8">此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 141 </td> <td> mining<br>(1) </td> <td> [ˈmaɪnɪŋ] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative <font color=orangered>mining</font> [35], context modeling [16], stronger data augmentation [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8">此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 142 </td> <td> augmentation<br>(1) </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data <font color=orangered>augmentation</font> [22], etc. These improvements are complementary to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8">此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 143 </td> <td> complementary<br>(1) </td> <td> [ˌkɒmplɪˈmentri] </td> <td> 
<ul><li>Moreover, our method does not exploit many popular improvements, such as iterative regression [9], hard negative mining [35], context modeling [16], stronger data augmentation [22], etc. These improvements are <font color=orangered>complementary</font> to FPNs and should boost accuracy further.<span style="font-size:80%;opacity:0.8">此外，我们的方法没有利用许多流行的改进，如迭代回归[9]，难例挖掘[35]，上下文建模[16]，更强大的数据增强[22]等。这些改进与FPN互补，应该会进一步提高准确度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 144 </td> <td> convolutionally<br>(1) </td> <td> [!≈ kɒnvə'lu:ʃənəli] </td> <td> 
<ul><li>At inference time, these models are run <font color=orangered>convolutionally</font> to generate dense proposals in an image.<span style="font-size:80%;opacity:0.8">在推断时，这些模型是卷积运行的，以在图像中生成密集的提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 145 </td> <td> setup<br>(1) </td> <td> ['setʌp] </td> <td> 
<ul><li>We use a fully convolutional <font color=orangered>setup</font> for both training and inference.<span style="font-size:80%;opacity:0.8">我们对训练和推断都使用全卷积设置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 146 </td> <td> Additionally<br>(1) </td> <td> [ə'dɪʃənəlɪ] </td> <td> 
<ul><li><font color=orangered>Additionally</font>, motivated by the use of 2 scales per octave in the image pyramid of [27, 28], we use a second MLP of input size 7×7 to handle half octaves.<span style="font-size:80%;opacity:0.8">此外，由于在[27,28]的图像金字塔中每组使用2个尺度，我们使用输入大小为7×7的第二个MLP来处理半个组。</span></li></ul>
 </td>
</tr>
<tr>
<td> 147 </td> <td> Instance-FCN<br>(1) </td> <td>  </td> <td> 
<ul><li>DeepMask, SharpMask, and FPN use ResNet-50 while <font color=forestgreen>Instance-FCN</font> uses VGG-16.<span style="font-size:80%;opacity:0.8">DeepMask，SharpMask和FPN使用ResNet-50，而Instance-FCN使用VGG-16。</span></li></ul>
 </td>
</tr>
<tr>
<td> 148 </td> <td> zoom<br>(1) </td> <td> [zu:m] </td> <td> 
<ul><li>DeepMask and SharpMask performance is computed with models available from https://github.com/facebookresearch/deepmask (both are the ‘<font color=orangered>zoom</font>’ variants).<span style="font-size:80%;opacity:0.8">DeepMask和SharpMask性能计算的模型是从https://github.com/facebookresearch/deepmask上获得的（都是‘zoom’变体）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 149 </td> <td> runtime<br>(1) </td> <td> [rʌn'taɪm] </td> <td> 
<ul><li>^† <font color=orangered>Runtimes</font> are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower K40.<span style="font-size:80%;opacity:0.8">^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 150 </td> <td> K40<br>(1) </td> <td>  </td> <td> 
<ul><li>^† Runtimes are measured on an NVIDIA M40 GPU, except the InstanceFCN timing which is based on the slower <font color=forestgreen>K40</font>.<span style="font-size:80%;opacity:0.8">^†运行时间是在NVIDIA M40 GPU上测量的，除了基于较慢的K40的InstanceFCN。</span></li></ul>
 </td>
</tr>
<tr>
<td> 151 </td> <td> Sharp-Mask<br>(1) </td> <td> [!≈ ʃɑ:p mɑ:sk] </td> <td> 
<ul><li>We also report comparisons to DeepMask [27], <font color=orangered>Sharp-Mask</font> [28], and InstanceFCN [4], the previous state of the art methods in mask proposal generation.<span style="font-size:80%;opacity:0.8">我们还报告了与DeepMask[27]，Sharp-Mask[28]和InstanceFCN[4]的比较，这是以前的掩模提议生成中的先进方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 152 </td> <td> computationally<br>(1) </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>Existing mask proposal methods [27, 28, 4] are based on densely sampled image pyramids (e.g., scaled by 2^{\lbrace −2:0.5:1 \rbrace} in [27, 28]), making them <font color=orangered>computationally</font> expensive.<span style="font-size:80%;opacity:0.8">现有的掩码提议方法[27，28，4]是基于密集采样的图像金字塔的（例如，[27，28]中的缩放为2^{\lbrace −2:0.5:1 \rbrace}），使得它们是计算昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 153 </td> <td> substantially<br>(1) </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Our approach, based on FPNs, is <font color=orangered>substantially</font> faster (our models run at 6 to 7 FPS).<span style="font-size:80%;opacity:0.8">我们的方法基于FPN，速度明显加快（我们的模型运行速度为6至7FPS）。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>