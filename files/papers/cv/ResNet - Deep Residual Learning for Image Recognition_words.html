<html>
<head>
<meta charset="utf-8">
<title> ResNet - Deep Residual Learning for Image Recognition </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> residual </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>Deep <font color=orangered>Residual</font> Learning for Image Recognition<span style="font-size:80%;opacity:0.8"> 深度残差学习在图像识别中的应用</span></li><li>We present a <font color=orangered>residual</font> learning framework to ease the training of networks that are substantially deeper than those used previously.<span style="font-size:80%;opacity:0.8"> 我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。</span></li><li>We explicitly reformulate the layers as learning <font color=orangered>residual</font> functions with reference to the layer inputs, instead of learning unreferenced functions.<span style="font-size:80%;opacity:0.8"> 我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>We provide comprehensive empirical evidence showing that these <font color=orangered>residual</font> networks are easier to optimize, and can gain accuracy from considerably increased depth.<span style="font-size:80%;opacity:0.8"> 我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。</span></li><li>On the ImageNet dataset we evaluate <font color=orangered>residual</font> nets with a depth of up to 152 layers——8× deeper than VGG nets [40] but still having lower complexity.<span style="font-size:80%;opacity:0.8"> 在ImageNet数据集上我们评估了深度高达152层的残差网络——比VGG[40]深8倍但仍具有较低的复杂度。</span></li><li>An ensemble of these <font color=orangered>residual</font> nets achieves 3.57% error on the ImageNet test set.<span style="font-size:80%;opacity:0.8"> 这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。</span></li><li>Deep <font color=orangered>residual</font> nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8"> 深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li><li>In this paper, we address the degradation problem by introducing a deep <font color=orangered>residual</font> learning framework.<span style="font-size:80%;opacity:0.8"> 在本文中，我们通过引入深度残差学习框架解决了退化问题。</span></li><li>Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a <font color=orangered>residual</font> mapping.<span style="font-size:80%;opacity:0.8"> 我们明确地让这些层拟合残差映射，而不是希望每几个堆叠的层直接拟合期望的基础映射。</span></li><li>We hypothesize that it is easier to optimize the <font color=orangered>residual</font> mapping than to optimize the original, unreferenced mapping.<span style="font-size:80%;opacity:0.8"> 我们假设残差映射比原始的、未参考的映射更容易优化。</span></li><li>To the extreme, if an identity mapping were optimal, it would be easier to push the <font color=orangered>residual</font> to zero than to fit an identity mapping by a stack of nonlinear layers.<span style="font-size:80%;opacity:0.8"> 在极端情况下，如果一个恒等映射是最优的，那么将残差置为零比通过一堆非线性层来拟合恒等映射更容易。</span></li><li>Figure 2. <font color=orangered>Residual</font> learning: a building block.<span style="font-size:80%;opacity:0.8"> 图2. 残差学习：构建块</span></li><li>We show that: 1) Our extremely deep <font color=orangered>residual</font> nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.<span style="font-size:80%;opacity:0.8"> 我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li><li>We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep <font color=orangered>residual</font> nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.<span style="font-size:80%;opacity:0.8"> 我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li><li>On the ImageNet classification dataset [35], we obtain excellent results by extremely deep <font color=orangered>residual</font> nets.<span style="font-size:80%;opacity:0.8"> 在ImageNet分类数据集[35]中，我们通过非常深的残差网络获得了很好的结果。</span></li><li>Our 152-layer <font color=orangered>residual</font> net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [40].<span style="font-size:80%;opacity:0.8"> 我们的152层残差网络是ImageNet上最深的网络，同时还具有比VGG网络[40]更低的复杂性。</span></li><li>This strong evidence shows that the <font color=orangered>residual</font> learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8"> 坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li><li><font color=orangered>Residual</font> Representations.<span style="font-size:80%;opacity:0.8"> 残差表示。</span></li><li>In image recognition, VLAD [18] is a representation that encodes by the <font color=orangered>residual</font> vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.<span style="font-size:80%;opacity:0.8"> 在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li><li>For vector quantization, encoding <font color=orangered>residual</font> vectors [17] is shown to be more effective than encoding original vectors.<span style="font-size:80%;opacity:0.8"> 对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。</span></li><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the <font color=orangered>residual</font> solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li><li>An alternative to Multigrid is hierarchical basis preconditioning [44, 45], which relies on variables that represent <font color=orangered>residual</font> vectors between two scales.<span style="font-size:80%;opacity:0.8"> Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li><li>It has been shown [3, 44, 45] that these solvers converge much faster than standard solvers that are unaware of the <font color=orangered>residual</font> nature of the solutions.<span style="font-size:80%;opacity:0.8"> 已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>On the contrary, our formulation always learns <font color=orangered>residual</font> functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned.<span style="font-size:80%;opacity:0.8"> 相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。</span></li><li>On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional <font color=orangered>residual</font> functions to be learned.<span style="font-size:80%;opacity:0.8"> 相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。</span></li><li>3. Deep <font color=orangered>Residual</font> Learning<span style="font-size:80%;opacity:0.8"> 3. 深度残差学习</span></li><li>3.1. <font color=orangered>Residual</font> Learning<span style="font-size:80%;opacity:0.8"> 3.1. 残差学习</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the <font color=orangered>residual</font> functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>So rather than expect stacked layers to approximate $H(x)$, we explicitly let these layers approximate a <font color=orangered>residual</font> function $F(x) := H(x) − x$.<span style="font-size:80%;opacity:0.8"> 因此，我们明确让这些层近似参数函数 $F(x) := H(x) − x$，而不是期望堆叠层近似$H(x)$。</span></li><li>With the <font color=orangered>residual</font> learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8"> 通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>We show by experiments (Fig. 7) that the learned <font color=orangered>residual</font> functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.<span style="font-size:80%;opacity:0.8"> 我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li><li>We adopt <font color=orangered>residual</font> learning to every few stacked layers.<span style="font-size:80%;opacity:0.8"> 我们每隔几个堆叠层采用残差学习。</span></li><li>The function $F(x, {W_i})$ represents the <font color=orangered>residual</font> mapping to be learned.<span style="font-size:80%;opacity:0.8"> 函数$F(x, {W_i})$表示要学习的残差映射。</span></li><li>This is not only attractive in practice but also important in our comparisons between plain and <font color=orangered>residual</font> networks.<span style="font-size:80%;opacity:0.8"> 这不仅在实践中有吸引力，而且在简单网络和残差网络的比较中也很重要。</span></li><li>We can fairly compare plain/<font color=orangered>residual</font> networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).<span style="font-size:80%;opacity:0.8"> 我们可以公平地比较同时具有相同数量的参数，相同深度，宽度和计算成本的简单/残差网络（除了不可忽略的元素加法之外）。</span></li><li>The form of the <font color=orangered>residual</font> function F is flexible.<span style="font-size:80%;opacity:0.8"> 残差函数F的形式是可变的。</span></li><li>A deeper <font color=orangered>residual</font> function F for ImageNet.<span style="font-size:80%;opacity:0.8"> ImageNet的深度残差函数F。</span></li><li>We have tested various plain/<font color=orangered>residual</font> nets, and have observed consistent phenomena.<span style="font-size:80%;opacity:0.8"> 我们测试了各种简单/残差网络，并观察到了一致的现象。</span></li><li>Right: a <font color=orangered>residual</font> network with 34 parameter layers (3.6 billion FLOPs).<span style="font-size:80%;opacity:0.8"> 右：具有34个参数层的残差网络（36亿FLOPs）。</span></li><li><font color=orangered>Residual</font> Network.<span style="font-size:80%;opacity:0.8"> 残差网络。</span></li><li>Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart <font color=orangered>residual</font> version.<span style="font-size:80%;opacity:0.8">  基于上述的简单网络，我们插入快捷连接（图3，右），将网络转换为其对应的残差版本。</span></li><li>We initialize the weights as in [12] and train all plain/<font color=orangered>residual</font> nets from scratch.<span style="font-size:80%;opacity:0.8"> 我们按照[12]的方法初始化权重，从零开始训练所有的简单/残差网络。</span></li><li>In this plot, the <font color=orangered>residual</font> networks have no extra parameter compared to their plain counterparts.<span style="font-size:80%;opacity:0.8"> 在本图中，残差网络与对应的简单网络相比没有额外的参数。</span></li><li><font color=orangered>Residual</font> Networks.<span style="font-size:80%;opacity:0.8"> 残差网络。</span></li><li>Next we evaluate 18-layer and 34-layer <font color=orangered>residual</font> nets (ResNets).<span style="font-size:80%;opacity:0.8"> 接下来我们评估18层和34层残差网络（ResNets）。</span></li><li>First, the situation is reversed with <font color=orangered>residual</font> learning —— the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%).<span style="font-size:80%;opacity:0.8"> 首先，残留学习的情况变了——34层ResNet比18层ResNet更好（2.8％）。</span></li><li>This comparison verifies the effectiveness of <font color=orangered>residual</font> learning on extremely deep systems.<span style="font-size:80%;opacity:0.8"> 这种比较证实了在极深系统中残差学习的有效性。</span></li><li>Last, we also note that the 18-layer plain/<font color=orangered>residual</font> nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).<span style="font-size:80%;opacity:0.8"> 最后，我们还注意到18层的简单/残差网络同样地准确（表2），但18层ResNet收敛更快（图4右和左）。</span></li><li>We argue that this is because the zero-padded dimensions in A indeed have no <font color=orangered>residual</font> learning.<span style="font-size:80%;opacity:0.8"> 我们认为这是因为A中的零填充确实没有残差学习。</span></li><li>For each <font color=orangered>residual</font> function F , we use a stack of 3 layers instead of 2 (Fig. 5).<span style="font-size:80%;opacity:0.8"> 对于每个残差函数F，我们使用3层堆叠而不是2层（图5）。</span></li><li>The plain/<font color=orangered>residual</font> architectures follow the form in Fig. 3 (middle/right).<span style="font-size:80%;opacity:0.8"> 简单/残差架构遵循图3（中/右）的形式。</span></li><li>On this dataset we use identity shortcuts in all cases (i.e., option A), so our <font color=orangered>residual</font> models have exactly the same depth, width, and number of parameters as the plain counterparts.<span style="font-size:80%;opacity:0.8"> 在这个数据集上，我们在所有案例中都使用恒等快捷连接（即选项A），因此我们的残差模型与对应的简单模型具有完全相同的深度，宽度和参数数量。</span></li><li>For ResNets, this analysis reveals the response strength of the <font color=orangered>residual</font> functions.<span style="font-size:80%;opacity:0.8"> 对于ResNets，该分析揭示了残差函数的响应强度。</span></li><li>These results support our basic motivation (Sec.3.1) that the <font color=orangered>residual</font> functions might be generally closer to zero than the non-residual functions.<span style="font-size:80%;opacity:0.8"> 这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li><li>Based on deep <font color=orangered>residual</font> nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8"> 基于深度残差网络，我们在ILSVRC & COCO 2015竞赛的几个任务中获得了第一名，分别是：ImageNet检测，ImageNet定位，COCO检测，COCO分割。</span></li><li>These improvements are based on deep features and thus should benefit from <font color=orangered>residual</font> learning.<span style="font-size:80%;opacity:0.8"> 这些改进基于深度特征，因此应受益于剩余学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> substantially </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>We present a residual learning framework to ease the training of networks that are <font color=orangered>substantially</font> deeper than those used previously.<span style="font-size:80%;opacity:0.8"> 我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。</span></li><li>We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results <font color=orangered>substantially</font> better than previous networks.<span style="font-size:80%;opacity:0.8"> 我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> reformulate </td> <td> [ˌri:ˈfɔ:mjuleɪt] </td> <td> 
<ul><li>We explicitly <font color=orangered>reformulate</font> the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.<span style="font-size:80%;opacity:0.8"> 我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] <font color=orangered>reformulates</font> the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> unreferenced </td> <td> [!≈ ʌn'refrənst] </td> <td> 
<ul><li>We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning <font color=orangered>unreferenced</font> functions.<span style="font-size:80%;opacity:0.8"> 我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>We hypothesize that it is easier to optimize the residual mapping than to optimize the original, <font color=orangered>unreferenced</font> mapping.<span style="font-size:80%;opacity:0.8"> 我们假设残差映射比原始的、未参考的映射更容易优化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> empirical </td> <td> [ɪmˈpɪrɪkl] </td> <td> 
<ul><li>We provide comprehensive <font color=orangered>empirical</font> evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.<span style="font-size:80%;opacity:0.8"> 我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> ensemble </td> <td> [ɒnˈsɒmbl] </td> <td> 
<ul><li>An <font color=orangered>ensemble</font> of these residual nets achieves 3.57% error on the ImageNet test set.<span style="font-size:80%;opacity:0.8"> 这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。</span></li><li>Our <font color=orangered>ensemble</font> has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition.<span style="font-size:80%;opacity:0.8"> 我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。</span></li><li>This single-model result outperforms all previous <font color=orangered>ensemble</font> results (Table 5).<span style="font-size:80%;opacity:0.8"> 这种单一模型的结果胜过以前的所有综合结果（表5）。</span></li><li>We combine six models of different depth to form an <font color=orangered>ensemble</font> (only with two 152-layer ones at the time of submitting).<span style="font-size:80%;opacity:0.8"> 我们结合了六种不同深度的模型，形成一个集合（在提交时仅有两个152层）。</span></li><li>Error rates (%) of <font color=orangered>ensembles</font>.<span style="font-size:80%;opacity:0.8"> 模型综合的错误率(%)。</span></li><li><font color=orangered>Ensemble</font>.<span style="font-size:80%;opacity:0.8"> 集成。</span></li><li>In Faster R-CNN, the system is designed to learn region proposals and also object classifiers, so an <font color=orangered>ensemble</font> can be used to boost both tasks.<span style="font-size:80%;opacity:0.8"> 在Faster R-CNN中，该系统被设计来进行区域建议和对象分类器学习，因此可以使用集成来增强这两个任务。</span></li><li>We use an <font color=orangered>ensemble</font> for proposing regions, and the union set of proposals are processed by an ensemble of per-region classifiers.<span style="font-size:80%;opacity:0.8"> 我们使用集成来给出区域建议，并且建议的联合集由每个区域分类器的集成来处理。</span></li><li>We use an ensemble for proposing regions, and the union set of proposals are processed by an <font color=orangered>ensemble</font> of per-region classifiers.<span style="font-size:80%;opacity:0.8"> 我们使用集成来给出区域建议，并且建议的联合集由每个区域分类器的集成来处理。</span></li><li>Table 9 shows our result based on an <font color=orangered>ensemble</font> of 3 networks.<span style="font-size:80%;opacity:0.8"> 表9显示了基于3个网络的集成的结果。</span></li><li>Our single model with ResNet-101 has 58.8% mAP and our <font color=orangered>ensemble</font> of 3 models has 62.1% mAP on the DET test set (Table 12).<span style="font-size:80%;opacity:0.8"> 我们使用ResNet-101的单个模型具有58.8%的mAP，而我们的3个模型的集合在DET测试集上具有62.1%的mAP(表12)。</span></li><li>Using an <font color=orangered>ensemble</font> of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set.<span style="font-size:80%;opacity:0.8"> 使用网络集成进行分类和定位，我们在测试集上实现了9.0%的前5位定位误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> ILSVRC </td> <td> [!≈ aɪ el es vi: ɑ:(r) si:] </td> <td> 
<ul><li>This result won the 1st place on the <font color=orangered>ILSVRC</font> 2015 classification task.<span style="font-size:80%;opacity:0.8"> 这个结果在ILSVRC 2015分类任务上赢得了第一名。</span></li><li>Deep residual nets are foundations of our submissions to <font color=orangered>ILSVRC</font> & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8"> 深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li><li>Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the <font color=orangered>ILSVRC</font> 2015 classification competition.<span style="font-size:80%;opacity:0.8"> 我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。</span></li><li>The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in <font color=orangered>ILSVRC</font> & COCO 2015 competitions.<span style="font-size:80%;opacity:0.8"> 极深的表示在其它识别任务中也有极好的泛化性能，并带领我们在进一步赢得了第一名：包括ILSVRC & COCO 2015竞赛中的ImageNet检测，ImageNet定位，COCO检测和COCO分割。</span></li><li>This entry won the 1st place in <font color=orangered>ILSVRC</font> 2015.<span style="font-size:80%;opacity:0.8"> 这次提交在2015年ILSVRC中荣获了第一名。</span></li><li>Based on deep residual nets, we won the 1st places in several tracks in <font color=orangered>ILSVRC</font> & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8"> 基于深度残差网络，我们在ILSVRC & COCO 2015竞赛的几个任务中获得了第一名，分别是：ImageNet检测，ImageNet定位，COCO检测，COCO分割。</span></li><li>We have experimented with ResNet-50/101 at the time of the <font color=orangered>ILSVRC</font> & COCO 2015 detection competitions.<span style="font-size:80%;opacity:0.8"> 在ILSVRC和COCO 2015检测比赛期间，我们已经用ResNet-50/101进行了实验。</span></li><li>We do not use other <font color=orangered>ILSVRC</font> 2015 data.<span style="font-size:80%;opacity:0.8"> 我们不使用其他ILSVRC 2015数据。</span></li><li>This result won the 1st place in the ImageNet detection task in <font color=orangered>ILSVRC</font> 2015, surpassing the second place by 8.5 points (absolute).<span style="font-size:80%;opacity:0.8"> 这一结果在ILSVRC 2015的ImageNet检测任务中获得第一名，超过第二名8.5个百分点(绝对)。</span></li><li>This number significantly outperforms the <font color=orangered>ILSVRC</font> 14 results (Table 14), showing a 64% relative reduction of error.<span style="font-size:80%;opacity:0.8"> 这个数字明显优于ILSVRC 14的结果(表14)，显示了64%的相对误差减少。</span></li><li>This result won the 1st place in the ImageNet localization task in <font color=orangered>ILSVRC</font> 2015.<span style="font-size:80%;opacity:0.8"> 此结果在ILSVRC 2015的ImageNet定位任务中获得第一名。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> CIFAR </td> <td> [!≈ si: aɪ ef eɪ ɑ:(r)] </td> <td> 
<ul><li>We also present analysis on <font color=orangered>CIFAR</font>-10 with 100 and 1000 layers.<span style="font-size:80%;opacity:0.8"> 我们也在CIFAR-10上分析了100层和1000层的残差网络。</span></li><li>Figure 1. Training error (left) and test error (right) on <font color=orangered>CIFAR</font>-10 with 20-layer and 56-layer “plain” networks.<span style="font-size:80%;opacity:0.8"> 图1 20层和56层的“简单”网络在CIFAR-10上的训练误差（左）和测试误差（右）。</span></li><li>Similar phenomena are also shown on the <font color=orangered>CIFAR</font>-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset.<span style="font-size:80%;opacity:0.8"> CIFAR-10数据集上[20]也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。</span></li><li>Standard deviations (std) of layer responses on <font color=orangered>CIFAR</font>-10.<span style="font-size:80%;opacity:0.8"> 层响应在CIFAR-10上的标准差（std）。</span></li><li>4.2. <font color=orangered>CIFAR</font>-10 and Analysis<span style="font-size:80%;opacity:0.8"> 4.2. CIFAR-10和分析</span></li><li>We conducted more studies on the <font color=orangered>CIFAR</font>-10 dataset [20], which consists of 50k training images and 10k testing images in 10 classes.<span style="font-size:80%;opacity:0.8"> 我们对CIFAR-10数据集[20]进行了更多的研究，其中包括10个类别中的5万张训练图像和1万张测试图像。</span></li><li>Training on <font color=orangered>CIFAR</font>-10.<span style="font-size:80%;opacity:0.8"> 在CIFAR-10上训练。</span></li><li>Classification error on the <font color=orangered>CIFAR</font>-10 test set.<span style="font-size:80%;opacity:0.8"> 在CIFAR-10测试集上的分类误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> submission </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>Deep residual nets are foundations of our <font color=orangered>submissions</font> to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8"> 深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> breakthrough </td> <td> [ˈbreɪkθru:] </td> <td> 
<ul><li>Deep convolutional neural networks [22, 21] have led to a series of <font color=orangered>breakthroughs</font> for image classification [21, 49, 39].<span style="font-size:80%;opacity:0.8"> 深度卷积神经网络[22, 21]导致了图像分类[21, 49, 39]的一系列突破。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> notorious </td> <td> [nəʊˈtɔ:riəs] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the <font color=orangered>notorious</font> problem of vanishing/exploding gradients [14, 1, 8], which hamper convergence from the beginning.<span style="font-size:80%;opacity:0.8"> 在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> hamper </td> <td> [ˈhæmpə(r)] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which <font color=orangered>hamper</font> convergence from the beginning.<span style="font-size:80%;opacity:0.8"> 在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> convergence </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which hamper <font color=orangered>convergence</font> from the beginning.<span style="font-size:80%;opacity:0.8"> 在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li><li>We conjecture that the deep plain nets may have exponentially low <font color=orangered>convergence</font> rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8"> 我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li><li>In this case, the ResNet eases the optimization by providing faster <font color=orangered>convergence</font> at the early stage.<span style="font-size:80%;opacity:0.8"> 在这种情况下，ResNet通过在早期提供更快的收敛简便了优化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> normalize </td> <td> [ˈnɔ:məlaɪz] </td> <td> 
<ul><li>This problem, however, has been largely addressed by <font color=orangered>normalized</font> initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8"> 然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> stochastic </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for <font color=orangered>stochastic</font> gradient descent (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8"> 然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li><li>As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for <font color=orangered>stochastic</font> training.<span style="font-size:80%;opacity:0.8"> 结果，Fast R-CNN[7]的以图像为中心的训练产生小变化的样本，这可能不是随机训练所需要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> descent </td> <td> [dɪˈsent] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient <font color=orangered>descent</font> (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8"> 然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (<font color=orangered>SGD</font>) with backpropagation [22].<span style="font-size:80%;opacity:0.8"> 然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li><li>The entire network can still be trained end-to-end by <font color=orangered>SGD</font> with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8"> 整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>We use <font color=orangered>SGD</font> with a mini-batch size of 256.<span style="font-size:80%;opacity:0.8"> 我们使用批大小为256的SGD方法。</span></li><li>When the net is “not overly deep” (18 layers here), the current <font color=orangered>SGD</font> solver is still able to find good solutions to the plain net.<span style="font-size:80%;opacity:0.8"> 当网络“不过度深”时（18层），目前的SGD求解器仍能在简单网络中找到好的解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> saturated </td> <td> [ˈsætʃəreɪtɪd] </td> <td> 
<ul><li>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets <font color=orangered>saturated</font> (which might be unsurprising) and then degrades rapidly.<span style="font-size:80%;opacity:0.8"> 当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> unsurprising </td> <td> [ˌʌnsəˈpraɪzɪŋ] </td> <td> 
<ul><li>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be <font color=orangered>unsurprising</font>) and then degrades rapidly.<span style="font-size:80%;opacity:0.8"> 当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> solver </td> <td> [ˈsɒlvə(r)] </td> <td> 
<ul><li>But experiments show that our current <font color=orangered>solvers</font> on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).<span style="font-size:80%;opacity:0.8"> 但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。</span></li><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the <font color=orangered>solvers</font>.<span style="font-size:80%;opacity:0.8"> 整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>It has been shown [3, 44, 45] that these <font color=orangered>solvers</font> converge much faster than standard solvers that are unaware of the residual nature of the solutions.<span style="font-size:80%;opacity:0.8"> 已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>It has been shown [3, 44, 45] that these solvers converge much faster than standard <font color=orangered>solvers</font> that are unaware of the residual nature of the solutions.<span style="font-size:80%;opacity:0.8"> 已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>The degradation problem suggests that the <font color=orangered>solvers</font> might have difficulties in approximating identity mappings by multiple nonlinear layers.<span style="font-size:80%;opacity:0.8"> 退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。</span></li><li>With the residual learning reformulation, if identity mappings are optimal, the <font color=orangered>solvers</font> may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8"> 通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the <font color=orangered>solver</font> to find the perturbations with reference to an identity mapping, than to learn the function as a new one.<span style="font-size:80%;opacity:0.8"> 如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。</span></li><li>In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the <font color=orangered>solver</font> works to some extent.<span style="font-size:80%;opacity:0.8"> 实际上，34层简单网络仍能取得有竞争力的准确率（表3），这表明在某种程度上来说求解器仍工作。</span></li><li>When the net is “not overly deep” (18 layers here), the current SGD <font color=orangered>solver</font> is still able to find good solutions to the plain net.<span style="font-size:80%;opacity:0.8"> 当网络“不过度深”时（18层），目前的SGD求解器仍能在简单网络中找到好的解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> comparably </td> <td> ['kɒmpərəblɪ] </td> <td> 
<ul><li>But experiments show that our current solvers on hand are unable to find solutions that are <font color=orangered>comparably</font> good or better than the constructed solution (or unable to do so in feasible time).<span style="font-size:80%;opacity:0.8"> 但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。</span></li><li>Last, we also note that the 18-layer plain/residual nets are <font color=orangered>comparably</font> accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).<span style="font-size:80%;opacity:0.8"> 最后，我们还注意到18层的简单/残差网络同样地准确（表2），但18层ResNet收敛更快（图4右和左）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> recast </td> <td> [ˌri:ˈkɑ:st] </td> <td> 
<ul><li>The original mapping is <font color=orangered>recast</font> into $F(x) + x$.<span style="font-size:80%;opacity:0.8"> 原始的映射重写为$F(x) + x$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> hypothesize </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>We <font color=orangered>hypothesize</font> that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping.<span style="font-size:80%;opacity:0.8"> 我们假设残差映射比原始的、未参考的映射更容易优化。</span></li><li>If one <font color=orangered>hypothesizes</font> that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to <font color=orangered>hypothesize</font> that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>Although both forms should be able to asymptotically approximate the desired functions (as <font color=orangered>hypothesized</font>), the ease of learning might be different.<span style="font-size:80%;opacity:0.8"> 尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> feedforward </td> <td> [fi:d'fɔ:wəd] </td> <td> 
<ul><li>The formulation of $F (x) + x$ can be realized by <font color=orangered>feedforward</font> neural networks with “shortcut connections” (Fig. 2).<span style="font-size:80%;opacity:0.8"> 公式$F (x) + x$可以通过带有“快捷连接”的前向神经网络（图2）来实现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> e.g. </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (<font color=orangered>e.g.</font>, Caffe [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8"> 整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>In addition, highway networks have not demonstrated accuracy gains with extremely increased depth (<font color=orangered>e.g.</font>, over 100 layers).<span style="font-size:80%;opacity:0.8"> 此外，高速网络还没有证实极度增加的深度（例如，超过100个层）带来的准确性收益。</span></li><li>(1). If this is not the case (<font color=orangered>e.g.</font>, when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions:<span style="font-size:80%;opacity:0.8"> 如果不是这种情况（例如，当更改输入/输出通道时），我们可以通过快捷连接执行线性投影$W_s$来匹配维度：</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> Caffe </td> <td>  </td> <td> 
<ul><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., <font color=forestgreen>Caffe</font> [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8"> 整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> akin </td> <td> [əˈkɪn] </td> <td> 
<ul><li>Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just <font color=orangered>akin</font> to a particular dataset.<span style="font-size:80%;opacity:0.8"> CIFAR-10数据集上[20]也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> generic </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>This strong evidence shows that the residual learning principle is <font color=orangered>generic</font>, and we expect that it is applicable in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8"> 坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> applicable </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>This strong evidence shows that the residual learning principle is generic, and we expect that it is <font color=orangered>applicable</font> in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8"> 坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li><li>We also note that although the above notations are about fully-connected layers for simplicity, they are <font color=orangered>applicable</font> to convolutional layers.<span style="font-size:80%;opacity:0.8"> 我们还注意到，为了简单起见，尽管上述符号是关于全连接层的，但它们同样适用于卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> VLAD </td> <td> [vlæd] </td> <td> 
<ul><li>In image recognition, <font color=orangered>VLAD</font> [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.<span style="font-size:80%;opacity:0.8"> 在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li><li>In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of <font color=orangered>VLAD</font>.<span style="font-size:80%;opacity:0.8"> 在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> probabilistic </td> <td> [ˌprɒbəbɪˈlɪstɪk] </td> <td> 
<ul><li>In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a <font color=orangered>probabilistic</font> version [18] of VLAD.<span style="font-size:80%;opacity:0.8"> 在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> retrieval </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>Both of them are powerful shallow representations for image <font color=orangered>retrieval</font> and classification [4, 47].<span style="font-size:80%;opacity:0.8"> 它们都是图像检索和图像分类[4,47]中强大的浅层表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> quantization </td> <td> [ˌkwɒntɪ'zeɪʃən] </td> <td> 
<ul><li>For vector <font color=orangered>quantization</font>, encoding residual vectors [17] is shown to be more effective than encoding original vectors.<span style="font-size:80%;opacity:0.8"> 对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> graphics </td> <td> [ˈgræfɪks] </td> <td> 
<ul><li>In low-level vision and computer <font color=orangered>graphics</font>, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> Differential </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial <font color=orangered>Differential</font> Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> PDEs </td> <td>  </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial Differential Equations (<font color=forestgreen>PDEs</font>), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> Multigrid </td> <td> ['mʌltɪgrɪd] </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used <font color=orangered>Multigrid</font> method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8"> 在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li><li>An alternative to <font color=orangered>Multigrid</font> is hierarchical basis preconditioning [44, 45], which relies on variables that represent residual vectors between two scales.<span style="font-size:80%;opacity:0.8"> Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> preconditioning </td> <td> [pri:kən'dɪʃnɪŋ] </td> <td> 
<ul><li>An alternative to Multigrid is hierarchical basis <font color=orangered>preconditioning</font> [44, 45], which relies on variables that represent residual vectors between two scales.<span style="font-size:80%;opacity:0.8"> Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li><li>These methods suggest that a good reformulation or <font color=orangered>preconditioning</font> can simplify the optimization.<span style="font-size:80%;opacity:0.8"> 这些方法表明，良好的重构或预处理可以简化优化。</span></li><li>We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable <font color=orangered>preconditioning</font>.<span style="font-size:80%;opacity:0.8"> 我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> reformulation </td> <td> [ˌri:ˌfɔ:mjʊ'leɪʃn] </td> <td> 
<ul><li>These methods suggest that a good <font color=orangered>reformulation</font> or preconditioning can simplify the optimization.<span style="font-size:80%;opacity:0.8"> 这些方法表明，良好的重构或预处理可以简化优化。</span></li><li>This <font color=orangered>reformulation</font> is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left).<span style="font-size:80%;opacity:0.8"> 关于退化问题的反直觉现象激发了这种重构（图1左）。</span></li><li>With the residual learning <font color=orangered>reformulation</font>, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8"> 通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>In real cases, it is unlikely that identity mappings are optimal, but our <font color=orangered>reformulation</font> may help to precondition the problem.<span style="font-size:80%;opacity:0.8"> 在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> perceptron </td> <td> [pəˈseptrɒn] </td> <td> 
<ul><li>An early practice of training multi-layer <font color=orangered>perceptrons</font> (MLPs) is to add a linear layer connected from the network input to the output [33, 48].<span style="font-size:80%;opacity:0.8"> 训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出[33,48]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> MLPs </td> <td>  </td> <td> 
<ul><li>An early practice of training multi-layer perceptrons (<font color=forestgreen>MLPs</font>) is to add a linear layer connected from the network input to the output [33, 48].<span style="font-size:80%;opacity:0.8"> 训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出[33,48]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> auxiliary </td> <td> [ɔ:gˈzɪliəri] </td> <td> 
<ul><li>In [43, 24], a few intermediate layers are directly connected to <font color=orangered>auxiliary</font> classifiers for addressing vanishing/exploding gradients.<span style="font-size:80%;opacity:0.8"> 在[43,24]中，一些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> propagate </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>The papers of [38, 37, 31, 46] propose methods for centering layer responses, gradients, and <font color=orangered>propagated</font> errors, implemented by shortcut connections.<span style="font-size:80%;opacity:0.8"> 论文[38,37,31,46]提出了通过快捷连接实现层间响应，梯度和传播误差的方法。</span></li><li>These plain networks are trained with BN [16], which ensures forward <font color=orangered>propagated</font> signals to have non-zero variances.<span style="font-size:80%;opacity:0.8"> 这些简单网络使用BN[16]训练，这保证了前向传播信号有非零方差。</span></li><li>We also verify that the backward <font color=orangered>propagated</font> gradients exhibit healthy norms with BN.<span style="font-size:80%;opacity:0.8"> 我们还验证了反向传播的梯度，结果显示其符合BN的正常标准。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> inception </td> <td> [ɪnˈsepʃn] </td> <td> 
<ul><li>In [43], an “<font color=orangered>inception</font>” layer is composed of a shortcut branch and a few deeper branches.<span style="font-size:80%;opacity:0.8"> 在[43]中，一个“inception”层由一个快捷分支和一些更深的分支组成。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> Concurrent </td> <td> [kənˈkʌrənt] </td> <td> 
<ul><li><font color=orangered>Concurrent</font> with our work, “highway networks” [41, 42] present shortcut connections with gating functions [15].<span style="font-size:80%;opacity:0.8"> 和我们同时进行的工作，“highway networks” [41, 42]提出了门功能[15]的快捷连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> data-dependent </td> <td> ['deɪtədɪp'endənt] </td> <td> 
<ul><li>These gates are <font color=orangered>data-dependent</font> and have parameters, in contrast to our identity shortcuts that are parameter-free.<span style="font-size:80%;opacity:0.8"> 这些门是数据相关且有参数的，与我们不具有参数的恒等快捷连接相反。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> non-residual </td> <td> [!≈ nɒn rɪˈzɪdjuəl] </td> <td> 
<ul><li>When a gated shortcut is “closed” (approaching zero), the layers in highway networks represent <font color=orangered>non-residual</font> functions.<span style="font-size:80%;opacity:0.8"> 当门控快捷连接“关闭”（接近零）时，高速网络中的层表示非残差函数。</span></li><li>These results support our basic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the <font color=orangered>non-residual</font> functions.<span style="font-size:80%;opacity:0.8"> 这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> asymptotically </td> <td>  </td> <td> 
<ul><li>If one hypothesizes that multiple nonlinear layers can <font color=forestgreen>asymptotically</font> approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can <font color=forestgreen>asymptotically</font> approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>Although both forms should be able to <font color=forestgreen>asymptotically</font> approximate the desired functions (as hypothesized), the ease of learning might be different.<span style="font-size:80%;opacity:0.8"> 尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> i.e. </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, <font color=orangered>i.e.</font>, $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8"> 假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>We adopt the second nonlinearity after the addition (<font color=orangered>i.e.</font>, $\sigma(y)$, see Fig. 2).<span style="font-size:80%;opacity:0.8"> 在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。</span></li><li>On this dataset we use identity shortcuts in all cases (<font color=orangered>i.e.</font>, option A), so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.<span style="font-size:80%;opacity:0.8"> 在这个数据集上，我们在所有案例中都使用恒等快捷连接（即选项A），因此我们的残差模型与对应的简单模型具有完全相同的深度，宽度和参数数量。</span></li><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (<font color=orangered>i.e.</font>, conv1, conv2_x, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8"> 我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li><li>We train the COCO models with an 8-GPU implementation, and thus the RPN step has a mini-batch size of 8 images (<font color=orangered>i.e.</font>, 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images.<span style="font-size:80%;opacity:0.8"> 我们用8-GPU实现训练COCO模型，因此RPN步骤具有8个图像的小批量大小(即，每个GPU 1个)，而Fast R-CNN步骤具有16个图像的小批量大小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> counterintuitive </td> <td> [kaʊntərɪn'tju:ɪtɪv] </td> <td> 
<ul><li>This reformulation is motivated by the <font color=orangered>counterintuitive</font> phenomena about the degradation problem (Fig. 1, left).<span style="font-size:80%;opacity:0.8"> 关于退化问题的反直觉现象激发了这种重构（图1左）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> mapping </td> <td> [ˈmæpiŋ] </td> <td> 
<ul><li>As we discussed in the introduction, if the added layers can be constructed as identity <font color=orangered>mappings</font>, a deeper model should have training error no greater than its shallower counterpart.<span style="font-size:80%;opacity:0.8"> 正如我们在引言中讨论的那样，如果添加的层可以被构建为恒等映射，更深模型的训练误差应该不大于它对应的更浅版本。</span></li><li>The degradation problem suggests that the solvers might have difficulties in approximating identity <font color=orangered>mappings</font> by multiple nonlinear layers.<span style="font-size:80%;opacity:0.8"> 退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。</span></li><li>With the residual learning reformulation, if identity <font color=orangered>mappings</font> are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8"> 通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity <font color=orangered>mappings</font>.<span style="font-size:80%;opacity:0.8"> 通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>In real cases, it is unlikely that identity <font color=orangered>mappings</font> are optimal, but our reformulation may help to precondition the problem.<span style="font-size:80%;opacity:0.8"> 在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li><li>We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity <font color=orangered>mappings</font> provide reasonable preconditioning.<span style="font-size:80%;opacity:0.8"> 我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> precondition </td> <td> [ˌpri:kənˈdɪʃn] </td> <td> 
<ul><li>In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to <font color=orangered>precondition</font> the problem.<span style="font-size:80%;opacity:0.8"> 在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> perturbation </td> <td> [ˌpɜ:təˈbeɪʃn] </td> <td> 
<ul><li>If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the <font color=orangered>perturbations</font> with reference to an identity mapping, than to learn the function as a new one.<span style="font-size:80%;opacity:0.8"> 如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> deviation </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>Standard <font color=orangered>deviations</font> (std) of layer responses on CIFAR-10.<span style="font-size:80%;opacity:0.8"> 层响应在CIFAR-10上的标准差（std）。</span></li><li>Fig. 7 shows the standard <font color=orangered>deviations</font> (std) of the layer responses.<span style="font-size:80%;opacity:0.8"> 图7显示了层响应的标准偏差（std）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> nonlinearity </td> <td> [nɒnlɪnɪ'ærɪtɪ] </td> <td> 
<ul><li>The responses are the outputs of each 3×3 layer, after BN and before <font color=orangered>nonlinearity</font>.<span style="font-size:80%;opacity:0.8"> 这些响应是每个3×3层的输出，在BN之后非线性之前。</span></li><li>We adopt the second <font color=orangered>nonlinearity</font> after the addition (i.e., $\sigma(y)$, see Fig. 2).<span style="font-size:80%;opacity:0.8"> 在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。</span></li><li>The responses are the outputs of each 3×3 layer, after BN and before other <font color=orangered>nonlinearity</font> (ReLU/addition).<span style="font-size:80%;opacity:0.8"> 这些响应每个3×3层的输出，在BN之后和其他非线性（ReLU/加法）之前。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> notation </td> <td> [nəʊˈteɪʃn] </td> <td> 
<ul><li>For the example in Fig. 2 that has two layers, $F = W_2 \sigma(W_1x)$ in which $\sigma$ denotes ReLU [29] and the biases are omitted for simplifying <font color=orangered>notations</font>.<span style="font-size:80%;opacity:0.8"> 图2中的例子有两层，$F = W_2 \sigma(W_1x)$中$\sigma$表示ReLU[29]，为了简化写法忽略偏置项。</span></li><li>We also note that although the above <font color=orangered>notations</font> are about fully-connected layers for simplicity, they are applicable to convolutional layers.<span style="font-size:80%;opacity:0.8"> 我们还注意到，为了简单起见，尽管上述符号是关于全连接层的，但它们同样适用于卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> halve </td> <td> [hɑ:v] </td> <td> 
<ul><li>The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is <font color=orangered>halved</font>, the number of filters is doubled so as to preserve the time complexity per layer.<span style="font-size:80%;opacity:0.8"> 卷积层主要有3×3的滤波器，并遵循两个简单的设计规则：（i）对于相同的输出特征图尺寸，层具有相同数量的滤波器；（ii）如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> flop </td> <td> [flɒp] </td> <td> 
<ul><li>Left: the VGG-19 model [40] (19.6 billion <font color=orangered>FLOPs</font>) as a reference.<span style="font-size:80%;opacity:0.8"> 左：作为参考的VGG-19模型40。</span></li><li>Middle: a plain network with 34 parameter layers (3.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8"> 中：具有34个参数层的简单网络（36亿FLOPs）。</span></li><li>Right: a residual network with 34 parameter layers (3.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8"> 右：具有34个参数层的残差网络（36亿FLOPs）。</span></li><li>Our 34-layer baseline has 3.6 billion <font color=orangered>FLOPs</font> (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8"> 我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li><li>Our 34-layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8"> 我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li><li>This model has 3.8 billion <font color=orangered>FLOPs</font>.<span style="font-size:80%;opacity:0.8"> 该模型有38亿FLOP。</span></li><li>Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion <font color=orangered>FLOPs</font>) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8"> 值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li><li>Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8"> 值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> variant </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Table 1 shows more details and other <font color=orangered>variants</font>.<span style="font-size:80%;opacity:0.8"> 表1显示了更多细节和其它变种。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> multiply-add </td> <td> [!≈ ˈmʌltɪplaɪ æd] </td> <td> 
<ul><li>Our 34-layer baseline has 3.6 billion FLOPs (<font color=orangered>multiply-adds</font>), which is only 18% of VGG-19 (19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8"> 我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> resize </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The image is <font color=orangered>resized</font> with its shorter side randomly sampled in [256, 480] for scale augmentation [40].<span style="font-size:80%;opacity:0.8"> 调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强[40]。</span></li><li>For best results, we adopt the fully-convolutional form as in [40, 12], and average the scores at multiple scales (images are <font color=orangered>resized</font> such that the shorter side is in {224, 256, 384, 480, 640}).<span style="font-size:80%;opacity:0.8"> 对于最好的结果，我们采用如[40, 12]中的全卷积形式，并在多尺度上对分数进行平均（图像归一化，短边位于{224, 256, 384, 480, 640}中）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> augmentation </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>The image is resized with its shorter side randomly sampled in [256, 480] for scale <font color=orangered>augmentation</font> [40].<span style="font-size:80%;opacity:0.8"> 调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强[40]。</span></li><li>The standard color <font color=orangered>augmentation</font> in [21] is used.<span style="font-size:80%;opacity:0.8"> 使用了[21]中的标准颜色增强。</span></li><li>We follow the simple data <font color=orangered>augmentation</font> in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip.<span style="font-size:80%;opacity:0.8"> 我们按照[24]中的简单数据增强进行训练：每边填充4个像素，并从填充图像或其水平翻转图像中随机采样32×32的裁剪图像。</span></li><li>All methods are with data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8"> 所有的方法都使用了数据增强。</span></li><li>As in our ImageNet classification training (Sec. 3.4), we randomly sample $224\times 224$ crops for data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8"> 与我们的ImageNet分类训练(参见3.4节)，我们随机抽取$224\times 224$剪裁区域进行数据增强。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> flip </td> <td> [flɪp] </td> <td> 
<ul><li>A 224×224 crop is randomly sampled from an image or its horizontal <font color=orangered>flip</font>, with the per-pixel mean subtracted [21].<span style="font-size:80%;opacity:0.8"> 224×224裁剪是从图像或其水平翻转中随机采样，并逐像素减去均值[21]。</span></li><li>We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal <font color=orangered>flip</font>.<span style="font-size:80%;opacity:0.8"> 我们按照[24]中的简单数据增强进行训练：每边填充4个像素，并从填充图像或其水平翻转图像中随机采样32×32的裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> plateau </td> <td> [ˈplætəʊ] </td> <td> 
<ul><li>The learning rate starts from 0.1 and is divided by 10 when the error <font color=orangered>plateaus</font>, and the models are trained for up to $60 \times 10^4$ iterations.<span style="font-size:80%;opacity:0.8"> 学习速度从0.1开始，当误差稳定时学习率除以10，并且模型训练高达$60 \times 10^4$次迭代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8"> 我们使用的权重衰减为0.0001，动量为0.9。</span></li><li>We use a weight decay of 0.0001 and <font color=orangered>momentum</font> of 0.9, and adopt the weight initialization in [12] and BN [16] but with no dropout.<span style="font-size:80%;opacity:0.8"> 我们使用的权重衰减为0.0001和动量为0.9，并采用[12]和BN[16]中的权重初始化，但没有使用丢弃。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> variance </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero <font color=orangered>variances</font>.<span style="font-size:80%;opacity:0.8"> 这些简单网络使用BN[16]训练，这保证了前向传播信号有非零方差。</span></li><li>For the usage of BN layers, after pre-training, we compute the BN statistics (means and <font color=orangered>variances</font>) for each layer on the ImageNet training set.<span style="font-size:80%;opacity:0.8"> 对于BN层的使用，在预培训之后，我们计算ImageNet训练集上每一层的BN统计(均值和方差)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> conjecture </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8"> 我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> exponentially </td> <td> [ˌekspə'nenʃəlɪ] </td> <td> 
<ul><li>We conjecture that the deep plain nets may have <font color=orangered>exponentially</font> low convergence rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8"> 我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> generalizable </td> <td> ['dʒenərəlaɪzəbl] </td> <td> 
<ul><li>More importantly, the 34-layer ResNet exhibits considerably lower training error and is <font color=orangered>generalizable</font> to the validation data.<span style="font-size:80%;opacity:0.8"> 更重要的是，34层ResNet显示出较低的训练误差，并且可以泛化到验证数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> marginally </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>C is <font color=orangered>marginally</font> better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts.<span style="font-size:80%;opacity:0.8"> 选项C比B稍好，我们把这归因于许多（十三）投影快捷连接引入了额外参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> remarkably </td> <td> [rɪ'mɑ:kəblɪ] </td> <td> 
<ul><li><font color=orangered>Remarkably</font>, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8"> 值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li><li>Most <font color=orangered>remarkably</font>, on the challenging COCO dataset we obtain a 6.0% increase in COCO’s standard metric (mAP@[. 5, . 95]), which is a 28% relative improvement.<span style="font-size:80%;opacity:0.8"> 最显著的是，在有挑战性的COCO数据集中，COCO的标准度量指标（mAP@[.5，.95]）增长了6.0％，相对改善了28％。</span></li><li><font color=orangered>Remarkably</font>, the mAP@[.5, . 95]’s absolute increase (6.0%) is nearly as big as mAP@. 5’s (6.9%).<span style="font-size:80%;opacity:0.8"> 值得注意的是，MAP@[.5，.95]的绝对增长(6.0%)几乎与MAP@.5的(6.9%)一样大。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> intentionally </td> <td> [ɪn'tenʃənəlɪ] </td> <td> 
<ul><li>Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we <font color=orangered>intentionally</font> use simple architectures as follows.<span style="font-size:80%;opacity:0.8"> 我们的焦点在于极深网络的行为，但不是推动最先进的结果，所以我们有意使用如下的简单架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> subsampling </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>subsampling</font> is performed by convolutions with a stride of 2.<span style="font-size:80%;opacity:0.8"> 下采样由步长为2的卷积进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> MNIST </td> <td> [!≈ em en aɪ es ti:] </td> <td> 
<ul><li>This phenomenon is similar to that on ImageNet (Fig. 4, left) and on <font color=orangered>MNIST</font> (see [41]), suggesting that such an optimization difficulty is a fundamental problem.<span style="font-size:80%;opacity:0.8"> 这种现象类似于ImageNet中（图4，左）和MNIST中（请看[41]）的现象，表明这种优化困难是一个基本的问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> dash </td> <td> [dæʃ] </td> <td> 
<ul><li><font color=orangered>Dashed</font> lines denote training error, and bold lines denote testing error.<span style="font-size:80%;opacity:0.8"> 虚线表示训练误差，粗线表示测试误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> FitNet </td> <td>  </td> <td> 
<ul><li>It has fewer parameters than other deep and thin networks such as <font color=forestgreen>FitNet</font> [34] and Highway [41] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6).<span style="font-size:80%;opacity:0.8"> 它与其它的深且窄的网络例如FitNet[34]和Highway41相比有更少的参数，但结果仍在目前最好的结果之间（6.43%，表6）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> Sec.3. </td> <td>  </td> <td> 
<ul><li>These results support our basic motivation (<font color=forestgreen>Sec.3.</font>1) that the residual functions might be generally closer to zero than the non-residual functions.<span style="font-size:80%;opacity:0.8"> 这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> maxout </td> <td>  </td> <td> 
<ul><li>Strong regularization such as <font color=forestgreen>maxout</font> [9] or dropout [13] is applied to obtain the best results ([9, 25, 24, 34]) on this dataset.<span style="font-size:80%;opacity:0.8"> 在这个数据集应用强大的正则化，如maxout[9]或者dropout[13]来获得最佳结果（[9,25,24,34]）。</span></li><li>In this paper, we use no <font color=forestgreen>maxout</font>/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization.<span style="font-size:80%;opacity:0.8"> 在本文中，我们不使用maxout/dropout，只是简单地通过设计深且窄的架构简单地进行正则化，而不会分散集中在优化难点上的注意力。</span></li><li>Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using <font color=forestgreen>maxout</font> layers.<span style="font-size:80%;opacity:0.8"> 在[12，7]中，通过从特征金字塔中选择尺度，以及在[33]中，通过使用maxout层，已经开发了多尺度训练/测试。</span></li><li>RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by <font color=forestgreen>maxout</font> as in [33].<span style="font-size:80%;opacity:0.8"> RoI池化和随后的层在这两个比例的特征映射上执行[33]，这两个比例由maxout合并，如[33]中所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> PASCAL </td> <td> ['pæskәl] </td> <td> 
<ul><li>4.3. Object Detection on <font color=orangered>PASCAL</font> and MS COCO<span style="font-size:80%;opacity:0.8"> 4.3. 在PASCAL和MS COCO上的目标检测</span></li><li>Table 7 and 8 show the object detection baseline results on <font color=orangered>PASCAL</font> VOC 2007 and 2012 [5] and COCO [26].<span style="font-size:80%;opacity:0.8"> 表7和表8显示了PASCAL VOC 2007和2012[5]以及COCO[26]的目标检测基准结果。</span></li><li>Object detection mAP (%) on the <font color=orangered>PASCAL</font> VOC 2007/2012 test sets using baseline Faster R-CNN.<span style="font-size:80%;opacity:0.8"> 在PASCAL VOC 2007/2012测试集上使用基准Faster R-CNN的目标检测mAP(%)。</span></li><li>Following [7, 32], for the <font color=orangered>PASCAL</font> VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8"> 在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>For the <font color=orangered>PASCAL</font> VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07++12”).<span style="font-size:80%;opacity:0.8"> 对于Pascal VOC 2012测试集，我们使用VOC 2007中的10k trainval+测试图像和VOC 2012中的16k trainval图像进行培训(“07++12”)。</span></li><li>We evaluate the <font color=orangered>PASCAL</font> VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = . 5:. 05:. 95).<span style="font-size:80%;opacity:0.8"> 我们评估Pascal VOC度量(MAP@IOU=0.5)和标准COCO度量(MAP@IOU=.5：.05：.95)。</span></li><li>Our detection system for COCO is similar to that for <font color=orangered>PASCAL</font> VOC.<span style="font-size:80%;opacity:0.8"> 我们针对COCO的检测系统与针对Pascal VOC的检测系统类似。</span></li><li>Table 10. Detection results on the <font color=orangered>PASCAL</font> VOC 2007 test set.<span style="font-size:80%;opacity:0.8"> 表10.Pascal VOC 2007测试集上的检测结果。</span></li><li>Table 11. Detection results on the <font color=orangered>PASCAL</font> VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4).<span style="font-size:80%;opacity:0.8"> 表11.PASCAL VOC 2012测试集(http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4)的检测结果。</span></li><li>We revisit the <font color=orangered>PASCAL</font> VOC dataset based on the above model.<span style="font-size:80%;opacity:0.8"> 我们基于上述模型重新访问PASCAL VOC数据集。</span></li><li>With the single model on the COCO dataset (55.7% mAP@. 5 in Table 9), we fine-tune this model on the <font color=orangered>PASCAL</font> VOC sets.<span style="font-size:80%;opacity:0.8"> 使用COCO数据集上的单个模型(表9中的55.7%map@.5)，我们在PASCAL VOC集上微调此模型。</span></li><li>By doing so, we achieve 85.6% mAP on <font color=orangered>PASCAL</font> VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11). The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8"> 通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li><li>By doing so, we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on <font color=orangered>PASCAL</font> VOC 2012 (Table 11). The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8"> 通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li><li>By doing so, we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11). The result on <font color=orangered>PASCAL</font> VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8"> 通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> NoC </td> <td>  </td> <td> 
<ul><li>We adopt the idea of “Networks on Conv feature maps” (<font color=forestgreen>NoC</font>) [33] to address this issue.<span style="font-size:80%;opacity:0.8"> 我们采用“Conv功能图上的网络”(NOC)[33]的思想来解决这个问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> full-image </td> <td> [!≈ fʊl ˈɪmɪdʒ] </td> <td> 
<ul><li>We compute the <font color=orangered>full-image</font> shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8"> 我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li><li>Given the <font color=orangered>full-image</font> conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the RoI.<span style="font-size:80%;opacity:0.8"> 给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> conv2_x </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, <font color=forestgreen>conv2_x</font>, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8"> 我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> conv3_x </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, <font color=forestgreen>conv3_x</font>, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8"> 我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> conv4_x </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, conv3_x, and <font color=forestgreen>conv4_x</font>, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8"> 我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> analogous </td> <td> [əˈnæləgəs] </td> <td> 
<ul><li>We consider these layers as <font color=orangered>analogous</font> to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels).<span style="font-size:80%;opacity:0.8"> 我们认为这些层类似于VGG-16中的13个conv层，通过这样做，ResNet和VGG-16都具有相同总跨度(16像素)的conv特征地图。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> RPN </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>These layers are shared by a region proposal network (<font color=orangered>RPN</font>, generating 300 proposals) [32] and a Fast R-CNN detection network [7].<span style="font-size:80%;opacity:0.8"> 这些层由区域建议网络(RPN，生成300个建议)[32]和快速R-CNN检测网络[7]共享。</span></li><li>We train the COCO models with an 8-GPU implementation, and thus the <font color=orangered>RPN</font> step has a mini-batch size of 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images.<span style="font-size:80%;opacity:0.8"> 我们用8-GPU实现训练COCO模型，因此RPN步骤具有8个图像的小批量大小(即，每个GPU 1个)，而Fast R-CNN步骤具有16个图像的小批量大小。</span></li><li>The <font color=orangered>RPN</font> step and Fast RCNN step are both trained for 240k iterations with a learning rate of 0.001 and then for 80k iterations with 0.0001.<span style="font-size:80%;opacity:0.8"> RPN步骤和快速RCNN步骤都以0.001的学习率为240k迭代进行训练，然后以0.0001的学习率为80k迭代进行训练。</span></li><li>In addition, we have performed multi-scale testing only for the Fast R-CNN step (but not yet for the <font color=orangered>RPN</font> step).<span style="font-size:80%;opacity:0.8"> 此外，我们仅针对Fast R-CNN步骤(但尚未针对RPN步骤)执行了多尺度测试。</span></li><li>Our localization algorithm is based on the <font color=orangered>RPN</font> framework of [32] with a few modifications.<span style="font-size:80%;opacity:0.8"> 我们的定位算法是基于[32]的RPN框架，并做了一些修改。</span></li><li>Unlike the way in [32] that is category-agnostic, our <font color=orangered>RPN</font> for localization is designed in a per-class form.<span style="font-size:80%;opacity:0.8"> 与[32]中与类别无关的方式不同，我们的定位RPN是以每个类的形式设计的。</span></li><li>This <font color=orangered>RPN</font> ends with two sibling $1\times1$ convolutional layers for binary classification (cls) and box regression (reg), as in [32].<span style="font-size:80%;opacity:0.8"> 此RPN以两个兄弟$1 \times 1$卷积层结束，用于二元分类(cls)和边框回归(reg)，如[32]中所示。</span></li><li>Under the same setting, our <font color=orangered>RPN</font> method using ResNet-101 net significantly reduces the center-crop error to 13.3%.<span style="font-size:80%;opacity:0.8"> 在相同的设置下，我们使用ResNet-101网络的RPN方法显著地将中心裁剪误差降低到13.3%。</span></li><li>The above results are only based on the proposal network (<font color=orangered>RPN</font>) in Faster R-CNN [32].<span style="font-size:80%;opacity:0.8"> 上述结果仅基于Faster R-CNN中的提议网络(RPN)[32]。</span></li><li>We apply the per-class <font color=orangered>RPN</font> trained as above on the training images to predict bounding boxes for the ground truth class.<span style="font-size:80%;opacity:0.8"> 我们在训练图像上应用如上所述训练的每类RPN来预测真实类别的边界框。</span></li><li>For testing, the <font color=orangered>RPN</font> generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals’ scores and box positions.<span style="font-size:80%;opacity:0.8"> 对于测试，RPN为每个预测类生成得分最高的200个建议，并且R-CNN网络用于更新这些建议的分数和边框位置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> RoI </td> <td> [rwɑ:] </td> <td> 
<ul><li><font color=orangered>RoI</font> pooling [7] is performed before conv5 1.<span style="font-size:80%;opacity:0.8"> ROI池化[7]在Conv5 1之前执行。</span></li><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “<font color=orangered>RoI</font>” pooling using the entire image’s bounding box as the RoI.<span style="font-size:80%;opacity:0.8"> 给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the <font color=orangered>RoI</font>.<span style="font-size:80%;opacity:0.8"> 给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li><font color=orangered>RoI</font> pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33].<span style="font-size:80%;opacity:0.8"> RoI池化和随后的层在这两个比例的特征映射上执行[33]，这两个比例由maxout合并，如[33]中所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> RoI-pooled </td> <td> [!≈ rwɑ: 'pu:ld] </td> <td> 
<ul><li>On this <font color=orangered>RoI-pooled</font> feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16’s fc layers.<span style="font-size:80%;opacity:0.8"> 在此ROI池化特征上，每个区域都采用Cont5 x和UP的所有层，扮演VGG-16的FC层的角色。</span></li><li>But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar <font color=orangered>RoI-pooled</font> features.<span style="font-size:80%;opacity:0.8"> 但我们注意到，在这个数据集上，一个图像通常包含单个主导对象，建议区域彼此高度重叠，因此具有非常相似的RoI池化特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> trainval </td> <td>  </td> <td> 
<ul><li>Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k <font color=forestgreen>trainval</font> images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8"> 在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k <font color=forestgreen>trainval</font> images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8"> 在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>For the PASCAL VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k <font color=forestgreen>trainval</font> images in VOC 2012 for training (“07++12”).<span style="font-size:80%;opacity:0.8"> 对于Pascal VOC 2012测试集，我们使用VOC 2007中的10k trainval+测试图像和VOC 2012中的16k trainval图像进行培训(“07++12”)。</span></li><li>Using validation data. Next we use the 80k+40k <font color=forestgreen>trainval</font> set for training and the 20k test-dev set for evaluation.<span style="font-size:80%;opacity:0.8"> 使用验证数据。接下来，我们使用80k+40k trainval集合进行训练，使用20k test-dev集合进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> completeness </td> <td> [kəm'pli:tnəs] </td> <td> 
<ul><li>For <font color=orangered>completeness</font>, we report the improvements made for the competitions.<span style="font-size:80%;opacity:0.8"> 为了完整起见，我们报告了为竞赛所做的改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> refinement </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>Box <font color=orangered>refinement</font>.<span style="font-size:80%;opacity:0.8"> 边框细调。</span></li><li>Our box <font color=orangered>refinement</font> partially follows the iterative localization in [6].<span style="font-size:80%;opacity:0.8"> 我们的边框细调部分遵循[6]中的迭代定位。</span></li><li>Box <font color=orangered>refinement</font> improves mAP by about 2 points (Table 9).<span style="font-size:80%;opacity:0.8"> 边框细化将mAP提高约2个百分点(表9)。</span></li><li>The system “baseline+++” include box <font color=orangered>refinement</font>, context, and multi-scale testing in Table 9.<span style="font-size:80%;opacity:0.8"> 系统“Baseline+++”包括表9中的边框细化、上下文和多尺度测试。</span></li><li>The system “baseline+++” include box <font color=orangered>refinement</font>, context, and multi-scale testing in Table 9.<span style="font-size:80%;opacity:0.8"> 系统“Baseline+++”包括表9中的边框细化、上下文和多尺度测试。</span></li><li>The improvements of box <font color=orangered>refinement</font>, context, and multi-scale testing are also adopted.<span style="font-size:80%;opacity:0.8"> 还采用了边框细化、上下文和多尺度测试的改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> iterative </td> <td> ['ɪtərətɪv] </td> <td> 
<ul><li>Our box refinement partially follows the <font color=orangered>iterative</font> localization in [6].<span style="font-size:80%;opacity:0.8"> 我们的边框细调部分遵循[6]中的迭代定位。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> regress </td> <td> [rɪˈgres] </td> <td> 
<ul><li>In Faster R-CNN, the final output is a <font color=orangered>regressed</font> box that is different from its proposal box.<span style="font-size:80%;opacity:0.8"> 在Faster R-CNN中，最终输出是一个与其建议边框不同的回归边框。</span></li><li>So for inference, we pool a new feature from the <font color=orangered>regressed</font> box and obtain a new classification score and a new regressed box.<span style="font-size:80%;opacity:0.8"> 因此，为了进行推理，我们从回归边框中汇集了一个新的特征，并获得了一个新的分类分数和一个新的回归边框。</span></li><li>So for inference, we pool a new feature from the regressed box and obtain a new classification score and a new <font color=orangered>regressed</font> box.<span style="font-size:80%;opacity:0.8"> 因此，为了进行推理，我们从回归边框中汇集了一个新的特征，并获得了一个新的分类分数和一个新的回归边框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> suppression </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>Non-maximum <font color=orangered>suppression</font> (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6].<span style="font-size:80%;opacity:0.8"> 使用IOU阈值0.3[8]，将非最大抑制(NMS)应用于预测边框的并集[8]，然后进行边框投票[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> NMS </td> <td> [!≈ en em es] </td> <td> 
<ul><li>Non-maximum suppression (<font color=orangered>NMS</font>) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6].<span style="font-size:80%;opacity:0.8"> 使用IOU阈值0.3[8]，将非最大抑制(NMS)应用于预测边框的并集[8]，然后进行边框投票[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s <font color=orangered>bounding</font> box as the RoI.<span style="font-size:80%;opacity:0.8"> 给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li>Following [40, 41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting <font color=orangered>bounding</font> boxes based on the predicted classes.<span style="font-size:80%;opacity:0.8"> 在[40，41]之后，我们假设首先采用图像级分类器来预测图像的类别标签，并且定位算法仅考虑基于预测的类别预测边界框。</span></li><li>We adopt the “per-class regression” (PCR) strategy [40, 41], learning a <font color=orangered>bounding</font> box regressor for each class.<span style="font-size:80%;opacity:0.8"> 我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li><li>As in [32], our <font color=orangered>bounding</font> box regression is with reference to multiple translation-invariant “anchor” boxes at each position.<span style="font-size:80%;opacity:0.8"> 与[32]中一样，我们的边界框回归是参考每个位置的多个平移不变的“锚点”框。</span></li><li>We apply the per-class RPN trained as above on the training images to predict <font color=orangered>bounding</font> boxes for the ground truth class.<span style="font-size:80%;opacity:0.8"> 我们在训练图像上应用如上所述训练的每类RPN来预测真实类别的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> post-RoI </td> <td> [!≈ pəʊst rwɑ:] </td> <td> 
<ul><li>This pooled feature is fed into the <font color=orangered>post-RoI</font> layers to obtain a global context feature.<span style="font-size:80%;opacity:0.8"> 该池化特征被馈送到ROI后层中以获得全局上下文特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> concatenate </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>This global feature is <font color=orangered>concatenated</font> with the original per-region feature, followed by the sibling classification and box regression layers.<span style="font-size:80%;opacity:0.8"> 此全局特征与原始每个区域特征相连接，然后是同级分类和边框回归层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> test-dev </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Using validation data. Next we use the 80k+40k trainval set for training and the 20k <font color=orangered>test-dev</font> set for evaluation.<span style="font-size:80%;opacity:0.8"> 使用验证数据。接下来，我们使用80k+40k trainval集合进行训练，使用20k test-dev集合进行评估。</span></li><li>The mAP is 59.0% and 37.4% on the <font color=orangered>test-dev</font> set.<span style="font-size:80%;opacity:0.8"> 测试开发集上的mAP为59.0%和37.4%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> testdev </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>testdev</font> set has no publicly available ground truth and the result is reported by the evaluation server.<span style="font-size:80%;opacity:0.8"> testdev集合没有公开可用的真实值，结果由评估服务器报告。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> DET </td> <td> [!≈ di: i: ti:] </td> <td> 
<ul><li>The ImageNet Detection (<font color=orangered>DET</font>) task involves 200 object categories.<span style="font-size:80%;opacity:0.8"> ImageNet检测(DET)任务涉及200个对象类别。</span></li><li>Our object detection algorithm for ImageNet <font color=orangered>DET</font> is the same as that for MS COCO in Table 9.<span style="font-size:80%;opacity:0.8"> 我们针对ImageNet Det的对象检测算法与表9中针对MS Coco的对象检测算法相同。</span></li><li>The networks are pretrained on the 1000-class ImageNet classification set, and are fine-tuned on the <font color=orangered>DET</font> data.<span style="font-size:80%;opacity:0.8"> 网络在1000类ImageNet分类集上进行了预训练，并在DET数据上进行了微调。</span></li><li>We fine-tune the detection models using the <font color=orangered>DET</font> training set and the val1 set.<span style="font-size:80%;opacity:0.8"> 我们使用DET训练集和val1集来微调检测模型。</span></li><li>Our single model with ResNet-101 has 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the <font color=orangered>DET</font> test set (Table 12).<span style="font-size:80%;opacity:0.8"> 我们使用ResNet-101的单个模型具有58.8%的mAP，而我们的3个模型的集合在DET测试集上具有62.1%的mAP(表12)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> pretrained </td> <td>  </td> <td> 
<ul><li>The networks are <font color=forestgreen>pretrained</font> on the 1000-class ImageNet classification set, and are fine-tuned on the DET data.<span style="font-size:80%;opacity:0.8"> 网络在1000类ImageNet分类集上进行了预训练，并在DET数据上进行了微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> surpass </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>This result won the 1st place in the ImageNet detection task in ILSVRC 2015, <font color=orangered>surpassing</font> the second place by 8.5 points (absolute).<span style="font-size:80%;opacity:0.8"> 这一结果在ILSVRC 2015的ImageNet检测任务中获得第一名，超过第二名8.5个百分点(绝对)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> LOC </td> <td> [!≈ el əu si:] </td> <td> 
<ul><li>In the column of “<font color=orangered>LOC</font> error on GT class” ([41]), the ground truth class is used.<span style="font-size:80%;opacity:0.8"> 在“GT类上的LOC错误”([41])列中，使用Ground True类。</span></li><li>The ImageNet Localization (<font color=orangered>LOC</font>) task [36] requires to classify and localize the objects.<span style="font-size:80%;opacity:0.8"> ImageNet定位(LOC)任务[36]需要对对象进行分类和定位。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> GT </td> <td> [dʒi:'ti:] </td> <td> 
<ul><li>In the column of “LOC error on <font color=orangered>GT</font> class” ([41]), the ground truth class is used.<span style="font-size:80%;opacity:0.8"> 在“GT类上的LOC错误”([41])列中，使用Ground True类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> image-level </td> <td> [!≈ ˈɪmɪdʒ ˈlevl] </td> <td> 
<ul><li>Following [40, 41], we assume that the <font color=orangered>image-level</font> classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes.<span style="font-size:80%;opacity:0.8"> 在[40，41]之后，我们假设首先采用图像级分类器来预测图像的类别标签，并且定位算法仅考虑基于预测的类别预测边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> PCR </td> <td> [!≈ pi: si: ɑ:(r)] </td> <td> 
<ul><li>We adopt the “per-class regression” (<font color=orangered>PCR</font>) strategy [40, 41], learning a bounding box regressor for each class.<span style="font-size:80%;opacity:0.8"> 我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> regressor </td> <td> [rɪ'gresə(r)] </td> <td> 
<ul><li>We adopt the “per-class regression” (PCR) strategy [40, 41], learning a bounding box <font color=orangered>regressor</font> for each class.<span style="font-size:80%;opacity:0.8"> 我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li><li>Specifically, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not being an object class; the reg layer has a $1000 \times 4$-d output consisting of box <font color=orangered>regressors</font> for 1000 classes.<span style="font-size:80%;opacity:0.8"> 具体地说，cls层有一个1000-d的输出，并且每个维度都是用于预测是否是对象类的二元逻辑回归；reg层有一个$1000 \times 4$-d的输出，由1000个类的边框回归组成。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> category-agnostic </td> <td> [!≈ ˈkætəgəri ægˈnɒstɪk] </td> <td> 
<ul><li>Unlike the way in [32] that is <font color=orangered>category-agnostic</font>, our RPN for localization is designed in a per-class form.<span style="font-size:80%;opacity:0.8"> 与[32]中与类别无关的方式不同，我们的定位RPN是以每个类的形式设计的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> translation-invariant </td> <td> [!≈ trænsˈleɪʃn ɪnˈveəriənt] </td> <td> 
<ul><li>As in [32], our bounding box regression is with reference to multiple <font color=orangered>translation-invariant</font> “anchor” boxes at each position.<span style="font-size:80%;opacity:0.8"> 与[32]中一样，我们的边界框回归是参考每个位置的多个平移不变的“锚点”框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> fully-convolutionally </td> <td> [!≈ ˈfʊli !≈ kɒnvə'lu:ʃənəli] </td> <td> 
<ul><li>For testing, the network is applied on the image <font color=orangered>fully-convolutionally</font>.<span style="font-size:80%;opacity:0.8"> 为了测试，将网络完全卷积地应用于图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> oracle </td> <td> [ˈɒrəkl] </td> <td> 
<ul><li>Following [41], we first perform “<font color=orangered>oracle</font>” testing using the ground truth class as the classification prediction.<span style="font-size:80%;opacity:0.8"> 在[41]之后，我们首先使用真实分类作为分类预测来执行“oracle”测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> center-crop </td> <td> [!≈ 'sentə krɒp] </td> <td> 
<ul><li>VGG’s paper [41] reports a <font color=orangered>center-crop</font> error of 33.1% (Table 13) using ground truth classes.<span style="font-size:80%;opacity:0.8"> VGG的论文[41]报告了使用真实类别的中心裁剪误差为33.1%(表13)。</span></li><li>Under the same setting, our RPN method using ResNet-101 net significantly reduces the <font color=orangered>center-crop</font> error to 13.3%.<span style="font-size:80%;opacity:0.8"> 在相同的设置下，我们使用ResNet-101网络的RPN方法显著地将中心裁剪误差降低到13.3%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> image-centric </td> <td> [!≈ ˈɪmɪdʒ 'sentrɪk] </td> <td> 
<ul><li>As a result, the <font color=orangered>image-centric</font> training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training.<span style="font-size:80%;opacity:0.8"> 结果，Fast R-CNN[7]的以图像为中心的训练产生小变化的样本，这可能不是随机训练所需要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> RoI-centric </td> <td> [!≈ rwɑ: 'sentrɪk] </td> <td> 
<ul><li>Motivated by this, in our current experiment we use the original RCNN [8] that is <font color=orangered>RoI-centric</font>, in place of Fast R-CNN.<span style="font-size:80%;opacity:0.8"> 受此启发，在我们当前的实验中，我们使用以投资回报为中心的原始RCNN[8]，而不是Fast R-CNN。</span></li><li>This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the <font color=orangered>RoI-centric</font> fashion.<span style="font-size:80%;opacity:0.8"> 这个R-CNN网络在训练集上使用以RoI为中心的大小为256的批量进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> class-dependent </td> <td> [!≈ klɑ:s dɪˈpendənt] </td> <td> 
<ul><li>These predicted boxes play a role of <font color=orangered>class-dependent</font> proposals.<span style="font-size:80%;opacity:0.8"> 这些预测框起到了和类别相关建议的作用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> warp </td> <td> [wɔ:p] </td> <td> 
<ul><li>The image region is cropped from a proposal, <font color=orangered>warped</font> to $224 \times 224$ pixels, and fed into the classification network as in R-CNN [8].<span style="font-size:80%;opacity:0.8"> 图像区域从提案中裁剪，扭曲到$224 \times 224$像素，并像R-CNN[8]那样馈送到分类网络中。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> residual<br>(56) </td> <td> [rɪˈzɪdjuəl] </td> <td> 
<ul><li>Deep <font color=orangered>Residual</font> Learning for Image Recognition<span style="font-size:80%;opacity:0.8">深度残差学习在图像识别中的应用</span></li><li>We present a <font color=orangered>residual</font> learning framework to ease the training of networks that are substantially deeper than those used previously.<span style="font-size:80%;opacity:0.8">我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。</span></li><li>We explicitly reformulate the layers as learning <font color=orangered>residual</font> functions with reference to the layer inputs, instead of learning unreferenced functions.<span style="font-size:80%;opacity:0.8">我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>We provide comprehensive empirical evidence showing that these <font color=orangered>residual</font> networks are easier to optimize, and can gain accuracy from considerably increased depth.<span style="font-size:80%;opacity:0.8">我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。</span></li><li>On the ImageNet dataset we evaluate <font color=orangered>residual</font> nets with a depth of up to 152 layers——8× deeper than VGG nets [40] but still having lower complexity.<span style="font-size:80%;opacity:0.8">在ImageNet数据集上我们评估了深度高达152层的残差网络——比VGG[40]深8倍但仍具有较低的复杂度。</span></li><li>An ensemble of these <font color=orangered>residual</font> nets achieves 3.57% error on the ImageNet test set.<span style="font-size:80%;opacity:0.8">这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。</span></li><li>Deep <font color=orangered>residual</font> nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8">深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li><li>In this paper, we address the degradation problem by introducing a deep <font color=orangered>residual</font> learning framework.<span style="font-size:80%;opacity:0.8">在本文中，我们通过引入深度残差学习框架解决了退化问题。</span></li><li>Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a <font color=orangered>residual</font> mapping.<span style="font-size:80%;opacity:0.8">我们明确地让这些层拟合残差映射，而不是希望每几个堆叠的层直接拟合期望的基础映射。</span></li><li>We hypothesize that it is easier to optimize the <font color=orangered>residual</font> mapping than to optimize the original, unreferenced mapping.<span style="font-size:80%;opacity:0.8">我们假设残差映射比原始的、未参考的映射更容易优化。</span></li><li>To the extreme, if an identity mapping were optimal, it would be easier to push the <font color=orangered>residual</font> to zero than to fit an identity mapping by a stack of nonlinear layers.<span style="font-size:80%;opacity:0.8">在极端情况下，如果一个恒等映射是最优的，那么将残差置为零比通过一堆非线性层来拟合恒等映射更容易。</span></li><li>Figure 2. <font color=orangered>Residual</font> learning: a building block.<span style="font-size:80%;opacity:0.8">图2. 残差学习：构建块</span></li><li>We show that: 1) Our extremely deep <font color=orangered>residual</font> nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.<span style="font-size:80%;opacity:0.8">我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li><li>We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep <font color=orangered>residual</font> nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks.<span style="font-size:80%;opacity:0.8">我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li><li>On the ImageNet classification dataset [35], we obtain excellent results by extremely deep <font color=orangered>residual</font> nets.<span style="font-size:80%;opacity:0.8">在ImageNet分类数据集[35]中，我们通过非常深的残差网络获得了很好的结果。</span></li><li>Our 152-layer <font color=orangered>residual</font> net is the deepest network ever presented on ImageNet, while still having lower complexity than VGG nets [40].<span style="font-size:80%;opacity:0.8">我们的152层残差网络是ImageNet上最深的网络，同时还具有比VGG网络[40]更低的复杂性。</span></li><li>This strong evidence shows that the <font color=orangered>residual</font> learning principle is generic, and we expect that it is applicable in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8">坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li><li><font color=orangered>Residual</font> Representations.<span style="font-size:80%;opacity:0.8">残差表示。</span></li><li>In image recognition, VLAD [18] is a representation that encodes by the <font color=orangered>residual</font> vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.<span style="font-size:80%;opacity:0.8">在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li><li>For vector quantization, encoding <font color=orangered>residual</font> vectors [17] is shown to be more effective than encoding original vectors.<span style="font-size:80%;opacity:0.8">对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。</span></li><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the <font color=orangered>residual</font> solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li><li>An alternative to Multigrid is hierarchical basis preconditioning [44, 45], which relies on variables that represent <font color=orangered>residual</font> vectors between two scales.<span style="font-size:80%;opacity:0.8">Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li><li>It has been shown [3, 44, 45] that these solvers converge much faster than standard solvers that are unaware of the <font color=orangered>residual</font> nature of the solutions.<span style="font-size:80%;opacity:0.8">已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>On the contrary, our formulation always learns <font color=orangered>residual</font> functions; our identity shortcuts are never closed, and all information is always passed through, with additional residual functions to be learned.<span style="font-size:80%;opacity:0.8">相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。</span></li><li>On the contrary, our formulation always learns residual functions; our identity shortcuts are never closed, and all information is always passed through, with additional <font color=orangered>residual</font> functions to be learned.<span style="font-size:80%;opacity:0.8">相反，我们的公式总是学习残差函数；我们的恒等快捷连接永远不会关闭，所有的信息总是通过，还有额外的残差函数要学习。</span></li><li>3. Deep <font color=orangered>Residual</font> Learning<span style="font-size:80%;opacity:0.8">3. 深度残差学习</span></li><li>3.1. <font color=orangered>Residual</font> Learning<span style="font-size:80%;opacity:0.8">3.1. 残差学习</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the <font color=orangered>residual</font> functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>So rather than expect stacked layers to approximate $H(x)$, we explicitly let these layers approximate a <font color=orangered>residual</font> function $F(x) := H(x) − x$.<span style="font-size:80%;opacity:0.8">因此，我们明确让这些层近似参数函数 $F(x) := H(x) − x$，而不是期望堆叠层近似$H(x)$。</span></li><li>With the <font color=orangered>residual</font> learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8">通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>We show by experiments (Fig. 7) that the learned <font color=orangered>residual</font> functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning.<span style="font-size:80%;opacity:0.8">我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li><li>We adopt <font color=orangered>residual</font> learning to every few stacked layers.<span style="font-size:80%;opacity:0.8">我们每隔几个堆叠层采用残差学习。</span></li><li>The function $F(x, {W_i})$ represents the <font color=orangered>residual</font> mapping to be learned.<span style="font-size:80%;opacity:0.8">函数$F(x, {W_i})$表示要学习的残差映射。</span></li><li>This is not only attractive in practice but also important in our comparisons between plain and <font color=orangered>residual</font> networks.<span style="font-size:80%;opacity:0.8">这不仅在实践中有吸引力，而且在简单网络和残差网络的比较中也很重要。</span></li><li>We can fairly compare plain/<font color=orangered>residual</font> networks that simultaneously have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).<span style="font-size:80%;opacity:0.8">我们可以公平地比较同时具有相同数量的参数，相同深度，宽度和计算成本的简单/残差网络（除了不可忽略的元素加法之外）。</span></li><li>The form of the <font color=orangered>residual</font> function F is flexible.<span style="font-size:80%;opacity:0.8">残差函数F的形式是可变的。</span></li><li>A deeper <font color=orangered>residual</font> function F for ImageNet.<span style="font-size:80%;opacity:0.8">ImageNet的深度残差函数F。</span></li><li>We have tested various plain/<font color=orangered>residual</font> nets, and have observed consistent phenomena.<span style="font-size:80%;opacity:0.8">我们测试了各种简单/残差网络，并观察到了一致的现象。</span></li><li>Right: a <font color=orangered>residual</font> network with 34 parameter layers (3.6 billion FLOPs).<span style="font-size:80%;opacity:0.8">右：具有34个参数层的残差网络（36亿FLOPs）。</span></li><li><font color=orangered>Residual</font> Network.<span style="font-size:80%;opacity:0.8">残差网络。</span></li><li>Based on the above plain network, we insert shortcut connections (Fig. 3, right) which turn the network into its counterpart <font color=orangered>residual</font> version.<span style="font-size:80%;opacity:0.8"> 基于上述的简单网络，我们插入快捷连接（图3，右），将网络转换为其对应的残差版本。</span></li><li>We initialize the weights as in [12] and train all plain/<font color=orangered>residual</font> nets from scratch.<span style="font-size:80%;opacity:0.8">我们按照[12]的方法初始化权重，从零开始训练所有的简单/残差网络。</span></li><li>In this plot, the <font color=orangered>residual</font> networks have no extra parameter compared to their plain counterparts.<span style="font-size:80%;opacity:0.8">在本图中，残差网络与对应的简单网络相比没有额外的参数。</span></li><li><font color=orangered>Residual</font> Networks.<span style="font-size:80%;opacity:0.8">残差网络。</span></li><li>Next we evaluate 18-layer and 34-layer <font color=orangered>residual</font> nets (ResNets).<span style="font-size:80%;opacity:0.8">接下来我们评估18层和34层残差网络（ResNets）。</span></li><li>First, the situation is reversed with <font color=orangered>residual</font> learning —— the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%).<span style="font-size:80%;opacity:0.8">首先，残留学习的情况变了——34层ResNet比18层ResNet更好（2.8％）。</span></li><li>This comparison verifies the effectiveness of <font color=orangered>residual</font> learning on extremely deep systems.<span style="font-size:80%;opacity:0.8">这种比较证实了在极深系统中残差学习的有效性。</span></li><li>Last, we also note that the 18-layer plain/<font color=orangered>residual</font> nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).<span style="font-size:80%;opacity:0.8">最后，我们还注意到18层的简单/残差网络同样地准确（表2），但18层ResNet收敛更快（图4右和左）。</span></li><li>We argue that this is because the zero-padded dimensions in A indeed have no <font color=orangered>residual</font> learning.<span style="font-size:80%;opacity:0.8">我们认为这是因为A中的零填充确实没有残差学习。</span></li><li>For each <font color=orangered>residual</font> function F , we use a stack of 3 layers instead of 2 (Fig. 5).<span style="font-size:80%;opacity:0.8">对于每个残差函数F，我们使用3层堆叠而不是2层（图5）。</span></li><li>The plain/<font color=orangered>residual</font> architectures follow the form in Fig. 3 (middle/right).<span style="font-size:80%;opacity:0.8">简单/残差架构遵循图3（中/右）的形式。</span></li><li>On this dataset we use identity shortcuts in all cases (i.e., option A), so our <font color=orangered>residual</font> models have exactly the same depth, width, and number of parameters as the plain counterparts.<span style="font-size:80%;opacity:0.8">在这个数据集上，我们在所有案例中都使用恒等快捷连接（即选项A），因此我们的残差模型与对应的简单模型具有完全相同的深度，宽度和参数数量。</span></li><li>For ResNets, this analysis reveals the response strength of the <font color=orangered>residual</font> functions.<span style="font-size:80%;opacity:0.8">对于ResNets，该分析揭示了残差函数的响应强度。</span></li><li>These results support our basic motivation (Sec.3.1) that the <font color=orangered>residual</font> functions might be generally closer to zero than the non-residual functions.<span style="font-size:80%;opacity:0.8">这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li><li>Based on deep <font color=orangered>residual</font> nets, we won the 1st places in several tracks in ILSVRC & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8">基于深度残差网络，我们在ILSVRC & COCO 2015竞赛的几个任务中获得了第一名，分别是：ImageNet检测，ImageNet定位，COCO检测，COCO分割。</span></li><li>These improvements are based on deep features and thus should benefit from <font color=orangered>residual</font> learning.<span style="font-size:80%;opacity:0.8">这些改进基于深度特征，因此应受益于剩余学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> PASCAL<br>(14) </td> <td> ['pæskәl] </td> <td> 
<ul><li>4.3. Object Detection on <font color=orangered>PASCAL</font> and MS COCO<span style="font-size:80%;opacity:0.8">4.3. 在PASCAL和MS COCO上的目标检测</span></li><li>Table 7 and 8 show the object detection baseline results on <font color=orangered>PASCAL</font> VOC 2007 and 2012 [5] and COCO [26].<span style="font-size:80%;opacity:0.8">表7和表8显示了PASCAL VOC 2007和2012[5]以及COCO[26]的目标检测基准结果。</span></li><li>Object detection mAP (%) on the <font color=orangered>PASCAL</font> VOC 2007/2012 test sets using baseline Faster R-CNN.<span style="font-size:80%;opacity:0.8">在PASCAL VOC 2007/2012测试集上使用基准Faster R-CNN的目标检测mAP(%)。</span></li><li>Following [7, 32], for the <font color=orangered>PASCAL</font> VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8">在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>For the <font color=orangered>PASCAL</font> VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07++12”).<span style="font-size:80%;opacity:0.8">对于Pascal VOC 2012测试集，我们使用VOC 2007中的10k trainval+测试图像和VOC 2012中的16k trainval图像进行培训(“07++12”)。</span></li><li>We evaluate the <font color=orangered>PASCAL</font> VOC metric (mAP @ IoU = 0.5) and the standard COCO metric (mAP @ IoU = . 5:. 05:. 95).<span style="font-size:80%;opacity:0.8">我们评估Pascal VOC度量(MAP@IOU=0.5)和标准COCO度量(MAP@IOU=.5：.05：.95)。</span></li><li>Our detection system for COCO is similar to that for <font color=orangered>PASCAL</font> VOC.<span style="font-size:80%;opacity:0.8">我们针对COCO的检测系统与针对Pascal VOC的检测系统类似。</span></li><li>Table 10. Detection results on the <font color=orangered>PASCAL</font> VOC 2007 test set.<span style="font-size:80%;opacity:0.8">表10.Pascal VOC 2007测试集上的检测结果。</span></li><li>Table 11. Detection results on the <font color=orangered>PASCAL</font> VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4).<span style="font-size:80%;opacity:0.8">表11.PASCAL VOC 2012测试集(http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=4)的检测结果。</span></li><li>We revisit the <font color=orangered>PASCAL</font> VOC dataset based on the above model.<span style="font-size:80%;opacity:0.8">我们基于上述模型重新访问PASCAL VOC数据集。</span></li><li>With the single model on the COCO dataset (55.7% mAP@. 5 in Table 9), we fine-tune this model on the <font color=orangered>PASCAL</font> VOC sets.<span style="font-size:80%;opacity:0.8">使用COCO数据集上的单个模型(表9中的55.7%map@.5)，我们在PASCAL VOC集上微调此模型。</span></li><li>By doing so, we achieve 85.6% mAP on <font color=orangered>PASCAL</font> VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11). The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8">通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li><li>By doing so, we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on <font color=orangered>PASCAL</font> VOC 2012 (Table 11). The result on PASCAL VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8">通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li><li>By doing so, we achieve 85.6% mAP on PASCAL VOC 2007 (Table 10) and 83.8% on PASCAL VOC 2012 (Table 11). The result on <font color=orangered>PASCAL</font> VOC 2012 is 10 points higher than the previous state-of-the-art result [6].<span style="font-size:80%;opacity:0.8">通过这样做，我们在PASCAL VOC 2007(表10)和Pascal VOC 2012(表11)上实现了85.6%的MAP和83.8%的MAP。在PASCAL VOC 2012上的结果比之前最先进的结果高出10个百分点[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> ensemble<br>(12) </td> <td> [ɒnˈsɒmbl] </td> <td> 
<ul><li>An <font color=orangered>ensemble</font> of these residual nets achieves 3.57% error on the ImageNet test set.<span style="font-size:80%;opacity:0.8">这些残差网络的集合在ImageNet测试集上取得了3.57%的错误率。</span></li><li>Our <font color=orangered>ensemble</font> has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the ILSVRC 2015 classification competition.<span style="font-size:80%;opacity:0.8">我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。</span></li><li>This single-model result outperforms all previous <font color=orangered>ensemble</font> results (Table 5).<span style="font-size:80%;opacity:0.8">这种单一模型的结果胜过以前的所有综合结果（表5）。</span></li><li>We combine six models of different depth to form an <font color=orangered>ensemble</font> (only with two 152-layer ones at the time of submitting).<span style="font-size:80%;opacity:0.8">我们结合了六种不同深度的模型，形成一个集合（在提交时仅有两个152层）。</span></li><li>Error rates (%) of <font color=orangered>ensembles</font>.<span style="font-size:80%;opacity:0.8">模型综合的错误率(%)。</span></li><li><font color=orangered>Ensemble</font>.<span style="font-size:80%;opacity:0.8">集成。</span></li><li>In Faster R-CNN, the system is designed to learn region proposals and also object classifiers, so an <font color=orangered>ensemble</font> can be used to boost both tasks.<span style="font-size:80%;opacity:0.8">在Faster R-CNN中，该系统被设计来进行区域建议和对象分类器学习，因此可以使用集成来增强这两个任务。</span></li><li>We use an <font color=orangered>ensemble</font> for proposing regions, and the union set of proposals are processed by an ensemble of per-region classifiers.<span style="font-size:80%;opacity:0.8">我们使用集成来给出区域建议，并且建议的联合集由每个区域分类器的集成来处理。</span></li><li>We use an ensemble for proposing regions, and the union set of proposals are processed by an <font color=orangered>ensemble</font> of per-region classifiers.<span style="font-size:80%;opacity:0.8">我们使用集成来给出区域建议，并且建议的联合集由每个区域分类器的集成来处理。</span></li><li>Table 9 shows our result based on an <font color=orangered>ensemble</font> of 3 networks.<span style="font-size:80%;opacity:0.8">表9显示了基于3个网络的集成的结果。</span></li><li>Our single model with ResNet-101 has 58.8% mAP and our <font color=orangered>ensemble</font> of 3 models has 62.1% mAP on the DET test set (Table 12).<span style="font-size:80%;opacity:0.8">我们使用ResNet-101的单个模型具有58.8%的mAP，而我们的3个模型的集合在DET测试集上具有62.1%的mAP(表12)。</span></li><li>Using an <font color=orangered>ensemble</font> of networks for both classification and localization, we achieve a top-5 localization error of 9.0% on the test set.<span style="font-size:80%;opacity:0.8">使用网络集成进行分类和定位，我们在测试集上实现了9.0%的前5位定位误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> ILSVRC<br>(11) </td> <td> [!≈ aɪ el es vi: ɑ:(r) si:] </td> <td> 
<ul><li>This result won the 1st place on the <font color=orangered>ILSVRC</font> 2015 classification task.<span style="font-size:80%;opacity:0.8">这个结果在ILSVRC 2015分类任务上赢得了第一名。</span></li><li>Deep residual nets are foundations of our submissions to <font color=orangered>ILSVRC</font> & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8">深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li><li>Our ensemble has 3.57% top-5 error on the ImageNet test set, and won the 1st place in the <font color=orangered>ILSVRC</font> 2015 classification competition.<span style="font-size:80%;opacity:0.8">我们的模型集合在ImageNet测试集上有3.57% top-5的错误率，并在ILSVRC 2015分类比赛中获得了第一名。</span></li><li>The extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation in <font color=orangered>ILSVRC</font> & COCO 2015 competitions.<span style="font-size:80%;opacity:0.8">极深的表示在其它识别任务中也有极好的泛化性能，并带领我们在进一步赢得了第一名：包括ILSVRC & COCO 2015竞赛中的ImageNet检测，ImageNet定位，COCO检测和COCO分割。</span></li><li>This entry won the 1st place in <font color=orangered>ILSVRC</font> 2015.<span style="font-size:80%;opacity:0.8">这次提交在2015年ILSVRC中荣获了第一名。</span></li><li>Based on deep residual nets, we won the 1st places in several tracks in <font color=orangered>ILSVRC</font> & COCO 2015 competitions: ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8">基于深度残差网络，我们在ILSVRC & COCO 2015竞赛的几个任务中获得了第一名，分别是：ImageNet检测，ImageNet定位，COCO检测，COCO分割。</span></li><li>We have experimented with ResNet-50/101 at the time of the <font color=orangered>ILSVRC</font> & COCO 2015 detection competitions.<span style="font-size:80%;opacity:0.8">在ILSVRC和COCO 2015检测比赛期间，我们已经用ResNet-50/101进行了实验。</span></li><li>We do not use other <font color=orangered>ILSVRC</font> 2015 data.<span style="font-size:80%;opacity:0.8">我们不使用其他ILSVRC 2015数据。</span></li><li>This result won the 1st place in the ImageNet detection task in <font color=orangered>ILSVRC</font> 2015, surpassing the second place by 8.5 points (absolute).<span style="font-size:80%;opacity:0.8">这一结果在ILSVRC 2015的ImageNet检测任务中获得第一名，超过第二名8.5个百分点(绝对)。</span></li><li>This number significantly outperforms the <font color=orangered>ILSVRC</font> 14 results (Table 14), showing a 64% relative reduction of error.<span style="font-size:80%;opacity:0.8">这个数字明显优于ILSVRC 14的结果(表14)，显示了64%的相对误差减少。</span></li><li>This result won the 1st place in the ImageNet localization task in <font color=orangered>ILSVRC</font> 2015.<span style="font-size:80%;opacity:0.8">此结果在ILSVRC 2015的ImageNet定位任务中获得第一名。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> RPN<br>(11) </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>These layers are shared by a region proposal network (<font color=orangered>RPN</font>, generating 300 proposals) [32] and a Fast R-CNN detection network [7].<span style="font-size:80%;opacity:0.8">这些层由区域建议网络(RPN，生成300个建议)[32]和快速R-CNN检测网络[7]共享。</span></li><li>We train the COCO models with an 8-GPU implementation, and thus the <font color=orangered>RPN</font> step has a mini-batch size of 8 images (i.e., 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images.<span style="font-size:80%;opacity:0.8">我们用8-GPU实现训练COCO模型，因此RPN步骤具有8个图像的小批量大小(即，每个GPU 1个)，而Fast R-CNN步骤具有16个图像的小批量大小。</span></li><li>The <font color=orangered>RPN</font> step and Fast RCNN step are both trained for 240k iterations with a learning rate of 0.001 and then for 80k iterations with 0.0001.<span style="font-size:80%;opacity:0.8">RPN步骤和快速RCNN步骤都以0.001的学习率为240k迭代进行训练，然后以0.0001的学习率为80k迭代进行训练。</span></li><li>In addition, we have performed multi-scale testing only for the Fast R-CNN step (but not yet for the <font color=orangered>RPN</font> step).<span style="font-size:80%;opacity:0.8">此外，我们仅针对Fast R-CNN步骤(但尚未针对RPN步骤)执行了多尺度测试。</span></li><li>Our localization algorithm is based on the <font color=orangered>RPN</font> framework of [32] with a few modifications.<span style="font-size:80%;opacity:0.8">我们的定位算法是基于[32]的RPN框架，并做了一些修改。</span></li><li>Unlike the way in [32] that is category-agnostic, our <font color=orangered>RPN</font> for localization is designed in a per-class form.<span style="font-size:80%;opacity:0.8">与[32]中与类别无关的方式不同，我们的定位RPN是以每个类的形式设计的。</span></li><li>This <font color=orangered>RPN</font> ends with two sibling $1\times1$ convolutional layers for binary classification (cls) and box regression (reg), as in [32].<span style="font-size:80%;opacity:0.8">此RPN以两个兄弟$1 \times 1$卷积层结束，用于二元分类(cls)和边框回归(reg)，如[32]中所示。</span></li><li>Under the same setting, our <font color=orangered>RPN</font> method using ResNet-101 net significantly reduces the center-crop error to 13.3%.<span style="font-size:80%;opacity:0.8">在相同的设置下，我们使用ResNet-101网络的RPN方法显著地将中心裁剪误差降低到13.3%。</span></li><li>The above results are only based on the proposal network (<font color=orangered>RPN</font>) in Faster R-CNN [32].<span style="font-size:80%;opacity:0.8">上述结果仅基于Faster R-CNN中的提议网络(RPN)[32]。</span></li><li>We apply the per-class <font color=orangered>RPN</font> trained as above on the training images to predict bounding boxes for the ground truth class.<span style="font-size:80%;opacity:0.8">我们在训练图像上应用如上所述训练的每类RPN来预测真实类别的边界框。</span></li><li>For testing, the <font color=orangered>RPN</font> generates the highest scored 200 proposals for each predicted class, and the R-CNN network is used to update these proposals’ scores and box positions.<span style="font-size:80%;opacity:0.8">对于测试，RPN为每个预测类生成得分最高的200个建议，并且R-CNN网络用于更新这些建议的分数和边框位置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> solver<br>(9) </td> <td> [ˈsɒlvə(r)] </td> <td> 
<ul><li>But experiments show that our current <font color=orangered>solvers</font> on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).<span style="font-size:80%;opacity:0.8">但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。</span></li><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the <font color=orangered>solvers</font>.<span style="font-size:80%;opacity:0.8">整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>It has been shown [3, 44, 45] that these <font color=orangered>solvers</font> converge much faster than standard solvers that are unaware of the residual nature of the solutions.<span style="font-size:80%;opacity:0.8">已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>It has been shown [3, 44, 45] that these solvers converge much faster than standard <font color=orangered>solvers</font> that are unaware of the residual nature of the solutions.<span style="font-size:80%;opacity:0.8">已经被证明[3,44,45]这些求解器比不知道解的残差性质的标准求解器收敛得更快。</span></li><li>The degradation problem suggests that the <font color=orangered>solvers</font> might have difficulties in approximating identity mappings by multiple nonlinear layers.<span style="font-size:80%;opacity:0.8">退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。</span></li><li>With the residual learning reformulation, if identity mappings are optimal, the <font color=orangered>solvers</font> may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8">通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the <font color=orangered>solver</font> to find the perturbations with reference to an identity mapping, than to learn the function as a new one.<span style="font-size:80%;opacity:0.8">如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。</span></li><li>In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the <font color=orangered>solver</font> works to some extent.<span style="font-size:80%;opacity:0.8">实际上，34层简单网络仍能取得有竞争力的准确率（表3），这表明在某种程度上来说求解器仍工作。</span></li><li>When the net is “not overly deep” (18 layers here), the current SGD <font color=orangered>solver</font> is still able to find good solutions to the plain net.<span style="font-size:80%;opacity:0.8">当网络“不过度深”时（18层），目前的SGD求解器仍能在简单网络中找到好的解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> CIFAR<br>(8) </td> <td> [!≈ si: aɪ ef eɪ ɑ:(r)] </td> <td> 
<ul><li>We also present analysis on <font color=orangered>CIFAR</font>-10 with 100 and 1000 layers.<span style="font-size:80%;opacity:0.8">我们也在CIFAR-10上分析了100层和1000层的残差网络。</span></li><li>Figure 1. Training error (left) and test error (right) on <font color=orangered>CIFAR</font>-10 with 20-layer and 56-layer “plain” networks.<span style="font-size:80%;opacity:0.8">图1 20层和56层的“简单”网络在CIFAR-10上的训练误差（左）和测试误差（右）。</span></li><li>Similar phenomena are also shown on the <font color=orangered>CIFAR</font>-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just akin to a particular dataset.<span style="font-size:80%;opacity:0.8">CIFAR-10数据集上[20]也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。</span></li><li>Standard deviations (std) of layer responses on <font color=orangered>CIFAR</font>-10.<span style="font-size:80%;opacity:0.8">层响应在CIFAR-10上的标准差（std）。</span></li><li>4.2. <font color=orangered>CIFAR</font>-10 and Analysis<span style="font-size:80%;opacity:0.8">4.2. CIFAR-10和分析</span></li><li>We conducted more studies on the <font color=orangered>CIFAR</font>-10 dataset [20], which consists of 50k training images and 10k testing images in 10 classes.<span style="font-size:80%;opacity:0.8">我们对CIFAR-10数据集[20]进行了更多的研究，其中包括10个类别中的5万张训练图像和1万张测试图像。</span></li><li>Training on <font color=orangered>CIFAR</font>-10.<span style="font-size:80%;opacity:0.8">在CIFAR-10上训练。</span></li><li>Classification error on the <font color=orangered>CIFAR</font>-10 test set.<span style="font-size:80%;opacity:0.8">在CIFAR-10测试集上的分类误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> flop<br>(8) </td> <td> [flɒp] </td> <td> 
<ul><li>Left: the VGG-19 model [40] (19.6 billion <font color=orangered>FLOPs</font>) as a reference.<span style="font-size:80%;opacity:0.8">左：作为参考的VGG-19模型40。</span></li><li>Middle: a plain network with 34 parameter layers (3.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8">中：具有34个参数层的简单网络（36亿FLOPs）。</span></li><li>Right: a residual network with 34 parameter layers (3.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8">右：具有34个参数层的残差网络（36亿FLOPs）。</span></li><li>Our 34-layer baseline has 3.6 billion <font color=orangered>FLOPs</font> (multiply-adds), which is only 18% of VGG-19 (19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8">我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li><li>Our 34-layer baseline has 3.6 billion FLOPs (multiply-adds), which is only 18% of VGG-19 (19.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8">我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li><li>This model has 3.8 billion <font color=orangered>FLOPs</font>.<span style="font-size:80%;opacity:0.8">该模型有38亿FLOP。</span></li><li>Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion <font color=orangered>FLOPs</font>) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8">值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li><li>Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion <font color=orangered>FLOPs</font>).<span style="font-size:80%;opacity:0.8">值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> mapping<br>(6) </td> <td> [ˈmæpiŋ] </td> <td> 
<ul><li>As we discussed in the introduction, if the added layers can be constructed as identity <font color=orangered>mappings</font>, a deeper model should have training error no greater than its shallower counterpart.<span style="font-size:80%;opacity:0.8">正如我们在引言中讨论的那样，如果添加的层可以被构建为恒等映射，更深模型的训练误差应该不大于它对应的更浅版本。</span></li><li>The degradation problem suggests that the solvers might have difficulties in approximating identity <font color=orangered>mappings</font> by multiple nonlinear layers.<span style="font-size:80%;opacity:0.8">退化问题表明求解器通过多个非线性层来近似恒等映射可能有困难。</span></li><li>With the residual learning reformulation, if identity <font color=orangered>mappings</font> are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8">通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity <font color=orangered>mappings</font>.<span style="font-size:80%;opacity:0.8">通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>In real cases, it is unlikely that identity <font color=orangered>mappings</font> are optimal, but our reformulation may help to precondition the problem.<span style="font-size:80%;opacity:0.8">在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li><li>We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity <font color=orangered>mappings</font> provide reasonable preconditioning.<span style="font-size:80%;opacity:0.8">我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> refinement<br>(6) </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>Box <font color=orangered>refinement</font>.<span style="font-size:80%;opacity:0.8">边框细调。</span></li><li>Our box <font color=orangered>refinement</font> partially follows the iterative localization in [6].<span style="font-size:80%;opacity:0.8">我们的边框细调部分遵循[6]中的迭代定位。</span></li><li>Box <font color=orangered>refinement</font> improves mAP by about 2 points (Table 9).<span style="font-size:80%;opacity:0.8">边框细化将mAP提高约2个百分点(表9)。</span></li><li>The system “baseline+++” include box <font color=orangered>refinement</font>, context, and multi-scale testing in Table 9.<span style="font-size:80%;opacity:0.8">系统“Baseline+++”包括表9中的边框细化、上下文和多尺度测试。</span></li><li>The system “baseline+++” include box <font color=orangered>refinement</font>, context, and multi-scale testing in Table 9.<span style="font-size:80%;opacity:0.8">系统“Baseline+++”包括表9中的边框细化、上下文和多尺度测试。</span></li><li>The improvements of box <font color=orangered>refinement</font>, context, and multi-scale testing are also adopted.<span style="font-size:80%;opacity:0.8">还采用了边框细化、上下文和多尺度测试的改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> i.e.<br>(5) </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, <font color=orangered>i.e.</font>, $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>We adopt the second nonlinearity after the addition (<font color=orangered>i.e.</font>, $\sigma(y)$, see Fig. 2).<span style="font-size:80%;opacity:0.8">在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。</span></li><li>On this dataset we use identity shortcuts in all cases (<font color=orangered>i.e.</font>, option A), so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.<span style="font-size:80%;opacity:0.8">在这个数据集上，我们在所有案例中都使用恒等快捷连接（即选项A），因此我们的残差模型与对应的简单模型具有完全相同的深度，宽度和参数数量。</span></li><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (<font color=orangered>i.e.</font>, conv1, conv2_x, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8">我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li><li>We train the COCO models with an 8-GPU implementation, and thus the RPN step has a mini-batch size of 8 images (<font color=orangered>i.e.</font>, 1 per GPU) and the Fast R-CNN step has a mini-batch size of 16 images.<span style="font-size:80%;opacity:0.8">我们用8-GPU实现训练COCO模型，因此RPN步骤具有8个图像的小批量大小(即，每个GPU 1个)，而Fast R-CNN步骤具有16个图像的小批量大小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> augmentation<br>(5) </td> <td> [ˌɔ:ɡmen'teɪʃn] </td> <td> 
<ul><li>The image is resized with its shorter side randomly sampled in [256, 480] for scale <font color=orangered>augmentation</font> [40].<span style="font-size:80%;opacity:0.8">调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强[40]。</span></li><li>The standard color <font color=orangered>augmentation</font> in [21] is used.<span style="font-size:80%;opacity:0.8">使用了[21]中的标准颜色增强。</span></li><li>We follow the simple data <font color=orangered>augmentation</font> in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal flip.<span style="font-size:80%;opacity:0.8">我们按照[24]中的简单数据增强进行训练：每边填充4个像素，并从填充图像或其水平翻转图像中随机采样32×32的裁剪图像。</span></li><li>All methods are with data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8">所有的方法都使用了数据增强。</span></li><li>As in our ImageNet classification training (Sec. 3.4), we randomly sample $224\times 224$ crops for data <font color=orangered>augmentation</font>.<span style="font-size:80%;opacity:0.8">与我们的ImageNet分类训练(参见3.4节)，我们随机抽取$224\times 224$剪裁区域进行数据增强。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> bounding<br>(5) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s <font color=orangered>bounding</font> box as the RoI.<span style="font-size:80%;opacity:0.8">给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li>Following [40, 41], we assume that the image-level classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting <font color=orangered>bounding</font> boxes based on the predicted classes.<span style="font-size:80%;opacity:0.8">在[40，41]之后，我们假设首先采用图像级分类器来预测图像的类别标签，并且定位算法仅考虑基于预测的类别预测边界框。</span></li><li>We adopt the “per-class regression” (PCR) strategy [40, 41], learning a <font color=orangered>bounding</font> box regressor for each class.<span style="font-size:80%;opacity:0.8">我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li><li>As in [32], our <font color=orangered>bounding</font> box regression is with reference to multiple translation-invariant “anchor” boxes at each position.<span style="font-size:80%;opacity:0.8">与[32]中一样，我们的边界框回归是参考每个位置的多个平移不变的“锚点”框。</span></li><li>We apply the per-class RPN trained as above on the training images to predict <font color=orangered>bounding</font> boxes for the ground truth class.<span style="font-size:80%;opacity:0.8">我们在训练图像上应用如上所述训练的每类RPN来预测真实类别的边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> DET<br>(5) </td> <td> [!≈ di: i: ti:] </td> <td> 
<ul><li>The ImageNet Detection (<font color=orangered>DET</font>) task involves 200 object categories.<span style="font-size:80%;opacity:0.8">ImageNet检测(DET)任务涉及200个对象类别。</span></li><li>Our object detection algorithm for ImageNet <font color=orangered>DET</font> is the same as that for MS COCO in Table 9.<span style="font-size:80%;opacity:0.8">我们针对ImageNet Det的对象检测算法与表9中针对MS Coco的对象检测算法相同。</span></li><li>The networks are pretrained on the 1000-class ImageNet classification set, and are fine-tuned on the <font color=orangered>DET</font> data.<span style="font-size:80%;opacity:0.8">网络在1000类ImageNet分类集上进行了预训练，并在DET数据上进行了微调。</span></li><li>We fine-tune the detection models using the <font color=orangered>DET</font> training set and the val1 set.<span style="font-size:80%;opacity:0.8">我们使用DET训练集和val1集来微调检测模型。</span></li><li>Our single model with ResNet-101 has 58.8% mAP and our ensemble of 3 models has 62.1% mAP on the <font color=orangered>DET</font> test set (Table 12).<span style="font-size:80%;opacity:0.8">我们使用ResNet-101的单个模型具有58.8%的mAP，而我们的3个模型的集合在DET测试集上具有62.1%的mAP(表12)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> SGD<br>(4) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (<font color=orangered>SGD</font>) with backpropagation [22].<span style="font-size:80%;opacity:0.8">然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li><li>The entire network can still be trained end-to-end by <font color=orangered>SGD</font> with backpropagation, and can be easily implemented using common libraries (e.g., Caffe [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8">整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>We use <font color=orangered>SGD</font> with a mini-batch size of 256.<span style="font-size:80%;opacity:0.8">我们使用批大小为256的SGD方法。</span></li><li>When the net is “not overly deep” (18 layers here), the current <font color=orangered>SGD</font> solver is still able to find good solutions to the plain net.<span style="font-size:80%;opacity:0.8">当网络“不过度深”时（18层），目前的SGD求解器仍能在简单网络中找到好的解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> hypothesize<br>(4) </td> <td> [haɪˈpɒθəsaɪz] </td> <td> 
<ul><li>We <font color=orangered>hypothesize</font> that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping.<span style="font-size:80%;opacity:0.8">我们假设残差映射比原始的、未参考的映射更容易优化。</span></li><li>If one <font color=orangered>hypothesizes</font> that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to <font color=orangered>hypothesize</font> that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>Although both forms should be able to asymptotically approximate the desired functions (as <font color=orangered>hypothesized</font>), the ease of learning might be different.<span style="font-size:80%;opacity:0.8">尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> reformulation<br>(4) </td> <td> [ˌri:ˌfɔ:mjʊ'leɪʃn] </td> <td> 
<ul><li>These methods suggest that a good <font color=orangered>reformulation</font> or preconditioning can simplify the optimization.<span style="font-size:80%;opacity:0.8">这些方法表明，良好的重构或预处理可以简化优化。</span></li><li>This <font color=orangered>reformulation</font> is motivated by the counterintuitive phenomena about the degradation problem (Fig. 1, left).<span style="font-size:80%;opacity:0.8">关于退化问题的反直觉现象激发了这种重构（图1左）。</span></li><li>With the residual learning <font color=orangered>reformulation</font>, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.<span style="font-size:80%;opacity:0.8">通过残差学习的重构，如果恒等映射是最优的，求解器可能简单地将多个非线性连接的权重推向零来接近恒等映射。</span></li><li>In real cases, it is unlikely that identity mappings are optimal, but our <font color=orangered>reformulation</font> may help to precondition the problem.<span style="font-size:80%;opacity:0.8">在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> maxout<br>(4) </td> <td>  </td> <td> 
<ul><li>Strong regularization such as <font color=forestgreen>maxout</font> [9] or dropout [13] is applied to obtain the best results ([9, 25, 24, 34]) on this dataset.<span style="font-size:80%;opacity:0.8">在这个数据集应用强大的正则化，如maxout[9]或者dropout[13]来获得最佳结果（[9,25,24,34]）。</span></li><li>In this paper, we use no <font color=forestgreen>maxout</font>/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization.<span style="font-size:80%;opacity:0.8">在本文中，我们不使用maxout/dropout，只是简单地通过设计深且窄的架构简单地进行正则化，而不会分散集中在优化难点上的注意力。</span></li><li>Multi-scale training/testing has been developed in [12, 7] by selecting a scale from a feature pyramid, and in [33] by using <font color=forestgreen>maxout</font> layers.<span style="font-size:80%;opacity:0.8">在[12，7]中，通过从特征金字塔中选择尺度，以及在[33]中，通过使用maxout层，已经开发了多尺度训练/测试。</span></li><li>RoI pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by <font color=forestgreen>maxout</font> as in [33].<span style="font-size:80%;opacity:0.8">RoI池化和随后的层在这两个比例的特征映射上执行[33]，这两个比例由maxout合并，如[33]中所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> RoI<br>(4) </td> <td> [rwɑ:] </td> <td> 
<ul><li><font color=orangered>RoI</font> pooling [7] is performed before conv5 1.<span style="font-size:80%;opacity:0.8">ROI池化[7]在Conv5 1之前执行。</span></li><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “<font color=orangered>RoI</font>” pooling using the entire image’s bounding box as the RoI.<span style="font-size:80%;opacity:0.8">给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li>Given the full-image conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the <font color=orangered>RoI</font>.<span style="font-size:80%;opacity:0.8">给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li><li><font color=orangered>RoI</font> pooling and subsequent layers are performed on the feature maps of these two scales [33], which are merged by maxout as in [33].<span style="font-size:80%;opacity:0.8">RoI池化和随后的层在这两个比例的特征映射上执行[33]，这两个比例由maxout合并，如[33]中所示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> trainval<br>(4) </td> <td>  </td> <td> 
<ul><li>Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k <font color=forestgreen>trainval</font> images in VOC 2007 and 16k trainval images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8">在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>Following [7, 32], for the PASCAL VOC 2007 test set, we use the 5k trainval images in VOC 2007 and 16k <font color=forestgreen>trainval</font> images in VOC 2012 for training (“07+12”).<span style="font-size:80%;opacity:0.8">在[7，32]之后，对于Pascal VOC 2007测试集，我们使用VOC 2007中的5k Trainval图像和VOC 2012中的16k Trainval图像进行培训(“07+12”)。</span></li><li>For the PASCAL VOC 2012 test set, we use the 10k trainval+test images in VOC 2007 and 16k <font color=forestgreen>trainval</font> images in VOC 2012 for training (“07++12”).<span style="font-size:80%;opacity:0.8">对于Pascal VOC 2012测试集，我们使用VOC 2007中的10k trainval+测试图像和VOC 2012中的16k trainval图像进行培训(“07++12”)。</span></li><li>Using validation data. Next we use the 80k+40k <font color=forestgreen>trainval</font> set for training and the 20k test-dev set for evaluation.<span style="font-size:80%;opacity:0.8">使用验证数据。接下来，我们使用80k+40k trainval集合进行训练，使用20k test-dev集合进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> convergence<br>(3) </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which hamper <font color=orangered>convergence</font> from the beginning.<span style="font-size:80%;opacity:0.8">在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li><li>We conjecture that the deep plain nets may have exponentially low <font color=orangered>convergence</font> rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8">我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li><li>In this case, the ResNet eases the optimization by providing faster <font color=orangered>convergence</font> at the early stage.<span style="font-size:80%;opacity:0.8">在这种情况下，ResNet通过在早期提供更快的收敛简便了优化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> e.g.<br>(3) </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (<font color=orangered>e.g.</font>, Caffe [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8">整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li><li>In addition, highway networks have not demonstrated accuracy gains with extremely increased depth (<font color=orangered>e.g.</font>, over 100 layers).<span style="font-size:80%;opacity:0.8">此外，高速网络还没有证实极度增加的深度（例如，超过100个层）带来的准确性收益。</span></li><li>(1). If this is not the case (<font color=orangered>e.g.</font>, when changing the input/output channels), we can perform a linear projection $W_s$ by the shortcut connections to match the dimensions:<span style="font-size:80%;opacity:0.8">如果不是这种情况（例如，当更改输入/输出通道时），我们可以通过快捷连接执行线性投影$W_s$来匹配维度：</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> preconditioning<br>(3) </td> <td> [pri:kən'dɪʃnɪŋ] </td> <td> 
<ul><li>An alternative to Multigrid is hierarchical basis <font color=orangered>preconditioning</font> [44, 45], which relies on variables that represent residual vectors between two scales.<span style="font-size:80%;opacity:0.8">Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li><li>These methods suggest that a good reformulation or <font color=orangered>preconditioning</font> can simplify the optimization.<span style="font-size:80%;opacity:0.8">这些方法表明，良好的重构或预处理可以简化优化。</span></li><li>We show by experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable <font color=orangered>preconditioning</font>.<span style="font-size:80%;opacity:0.8">我们通过实验（图7）显示学习的残差函数通常有更小的响应，表明恒等映射提供了合理的预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> propagate<br>(3) </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>The papers of [38, 37, 31, 46] propose methods for centering layer responses, gradients, and <font color=orangered>propagated</font> errors, implemented by shortcut connections.<span style="font-size:80%;opacity:0.8">论文[38,37,31,46]提出了通过快捷连接实现层间响应，梯度和传播误差的方法。</span></li><li>These plain networks are trained with BN [16], which ensures forward <font color=orangered>propagated</font> signals to have non-zero variances.<span style="font-size:80%;opacity:0.8">这些简单网络使用BN[16]训练，这保证了前向传播信号有非零方差。</span></li><li>We also verify that the backward <font color=orangered>propagated</font> gradients exhibit healthy norms with BN.<span style="font-size:80%;opacity:0.8">我们还验证了反向传播的梯度，结果显示其符合BN的正常标准。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> asymptotically<br>(3) </td> <td>  </td> <td> 
<ul><li>If one hypothesizes that multiple nonlinear layers can <font color=forestgreen>asymptotically</font> approximate complicated functions, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions, then it is equivalent to hypothesize that they can <font color=forestgreen>asymptotically</font> approximate the residual functions, i.e., $H(x) − x$ (assuming that the input and output are of the same dimensions).<span style="font-size:80%;opacity:0.8">假设多个非线性层可以渐近地近似复杂函数，它等价于假设它们可以渐近地近似残差函数，即$H(x) − x$(假设输入输出是相同维度)。</span></li><li>Although both forms should be able to <font color=forestgreen>asymptotically</font> approximate the desired functions (as hypothesized), the ease of learning might be different.<span style="font-size:80%;opacity:0.8">尽管两种形式应该都能渐近地近似要求的函数（如假设），但学习的难易程度可能是不同的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> nonlinearity<br>(3) </td> <td> [nɒnlɪnɪ'ærɪtɪ] </td> <td> 
<ul><li>The responses are the outputs of each 3×3 layer, after BN and before <font color=orangered>nonlinearity</font>.<span style="font-size:80%;opacity:0.8">这些响应是每个3×3层的输出，在BN之后非线性之前。</span></li><li>We adopt the second <font color=orangered>nonlinearity</font> after the addition (i.e., $\sigma(y)$, see Fig. 2).<span style="font-size:80%;opacity:0.8">在相加之后我们采纳了第二种非线性（即$\sigma(y)$，看图2）。</span></li><li>The responses are the outputs of each 3×3 layer, after BN and before other <font color=orangered>nonlinearity</font> (ReLU/addition).<span style="font-size:80%;opacity:0.8">这些响应每个3×3层的输出，在BN之后和其他非线性（ReLU/加法）之前。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> remarkably<br>(3) </td> <td> [rɪ'mɑ:kəblɪ] </td> <td> 
<ul><li><font color=orangered>Remarkably</font>, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8">值得注意的是，尽管深度显著增加，但152层ResNet（113亿FLOP）仍然比VGG-16/19网络（153/196亿FLOP）具有更低的复杂度。</span></li><li>Most <font color=orangered>remarkably</font>, on the challenging COCO dataset we obtain a 6.0% increase in COCO’s standard metric (mAP@[. 5, . 95]), which is a 28% relative improvement.<span style="font-size:80%;opacity:0.8">最显著的是，在有挑战性的COCO数据集中，COCO的标准度量指标（mAP@[.5，.95]）增长了6.0％，相对改善了28％。</span></li><li><font color=orangered>Remarkably</font>, the mAP@[.5, . 95]’s absolute increase (6.0%) is nearly as big as mAP@. 5’s (6.9%).<span style="font-size:80%;opacity:0.8">值得注意的是，MAP@[.5，.95]的绝对增长(6.0%)几乎与MAP@.5的(6.9%)一样大。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> regress<br>(3) </td> <td> [rɪˈgres] </td> <td> 
<ul><li>In Faster R-CNN, the final output is a <font color=orangered>regressed</font> box that is different from its proposal box.<span style="font-size:80%;opacity:0.8">在Faster R-CNN中，最终输出是一个与其建议边框不同的回归边框。</span></li><li>So for inference, we pool a new feature from the <font color=orangered>regressed</font> box and obtain a new classification score and a new regressed box.<span style="font-size:80%;opacity:0.8">因此，为了进行推理，我们从回归边框中汇集了一个新的特征，并获得了一个新的分类分数和一个新的回归边框。</span></li><li>So for inference, we pool a new feature from the regressed box and obtain a new classification score and a new <font color=orangered>regressed</font> box.<span style="font-size:80%;opacity:0.8">因此，为了进行推理，我们从回归边框中汇集了一个新的特征，并获得了一个新的分类分数和一个新的回归边框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> substantially<br>(2) </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>We present a residual learning framework to ease the training of networks that are <font color=orangered>substantially</font> deeper than those used previously.<span style="font-size:80%;opacity:0.8">我们提出了一种残差学习框架来减轻网络训练，这些网络比以前使用的网络更深。</span></li><li>We show that: 1) Our extremely deep residual nets are easy to optimize, but the counterpart “plain” nets (that simply stack layers) exhibit higher training error when the depth increases; 2) Our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results <font color=orangered>substantially</font> better than previous networks.<span style="font-size:80%;opacity:0.8">我们发现：1）我们极深的残差网络易于优化，但当深度增加时，对应的“简单”网络（简单堆叠层）表现出更高的训练误差；2）我们的深度残差网络可以从大大增加的深度中轻松获得准确性收益，生成的结果实质上比以前的网络更好。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> reformulate<br>(2) </td> <td> [ˌri:ˈfɔ:mjuleɪt] </td> <td> 
<ul><li>We explicitly <font color=orangered>reformulate</font> the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.<span style="font-size:80%;opacity:0.8">我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] <font color=orangered>reformulates</font> the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> unreferenced<br>(2) </td> <td> [!≈ ʌn'refrənst] </td> <td> 
<ul><li>We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning <font color=orangered>unreferenced</font> functions.<span style="font-size:80%;opacity:0.8">我们明确地将层变为学习关于层输入的残差函数，而不是学习未参考的函数。</span></li><li>We hypothesize that it is easier to optimize the residual mapping than to optimize the original, <font color=orangered>unreferenced</font> mapping.<span style="font-size:80%;opacity:0.8">我们假设残差映射比原始的、未参考的映射更容易优化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> stochastic<br>(2) </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for <font color=orangered>stochastic</font> gradient descent (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8">然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li><li>As a result, the image-centric training of Fast R-CNN [7] generates samples of small variations, which may not be desired for <font color=orangered>stochastic</font> training.<span style="font-size:80%;opacity:0.8">结果，Fast R-CNN[7]的以图像为中心的训练产生小变化的样本，这可能不是随机训练所需要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> comparably<br>(2) </td> <td> ['kɒmpərəblɪ] </td> <td> 
<ul><li>But experiments show that our current solvers on hand are unable to find solutions that are <font color=orangered>comparably</font> good or better than the constructed solution (or unable to do so in feasible time).<span style="font-size:80%;opacity:0.8">但是实验表明，我们目前现有的解决方案无法找到与构建的解决方案相比相对不错或更好的解决方案（或在合理的时间内无法实现）。</span></li><li>Last, we also note that the 18-layer plain/residual nets are <font color=orangered>comparably</font> accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left).<span style="font-size:80%;opacity:0.8">最后，我们还注意到18层的简单/残差网络同样地准确（表2），但18层ResNet收敛更快（图4右和左）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> applicable<br>(2) </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>This strong evidence shows that the residual learning principle is generic, and we expect that it is <font color=orangered>applicable</font> in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8">坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li><li>We also note that although the above notations are about fully-connected layers for simplicity, they are <font color=orangered>applicable</font> to convolutional layers.<span style="font-size:80%;opacity:0.8">我们还注意到，为了简单起见，尽管上述符号是关于全连接层的，但它们同样适用于卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> VLAD<br>(2) </td> <td> [vlæd] </td> <td> 
<ul><li>In image recognition, <font color=orangered>VLAD</font> [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD.<span style="font-size:80%;opacity:0.8">在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li><li>In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of <font color=orangered>VLAD</font>.<span style="font-size:80%;opacity:0.8">在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> Multigrid<br>(2) </td> <td> ['mʌltɪgrɪd] </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial Differential Equations (PDEs), the widely used <font color=orangered>Multigrid</font> method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li><li>An alternative to <font color=orangered>Multigrid</font> is hierarchical basis preconditioning [44, 45], which relies on variables that represent residual vectors between two scales.<span style="font-size:80%;opacity:0.8">Multigrid的替代方法是层次化基础预处理[44,45]，它依赖于表示两个尺度之间残差向量的变量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> non-residual<br>(2) </td> <td> [!≈ nɒn rɪˈzɪdjuəl] </td> <td> 
<ul><li>When a gated shortcut is “closed” (approaching zero), the layers in highway networks represent <font color=orangered>non-residual</font> functions.<span style="font-size:80%;opacity:0.8">当门控快捷连接“关闭”（接近零）时，高速网络中的层表示非残差函数。</span></li><li>These results support our basic motivation (Sec.3.1) that the residual functions might be generally closer to zero than the <font color=orangered>non-residual</font> functions.<span style="font-size:80%;opacity:0.8">这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> deviation<br>(2) </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>Standard <font color=orangered>deviations</font> (std) of layer responses on CIFAR-10.<span style="font-size:80%;opacity:0.8">层响应在CIFAR-10上的标准差（std）。</span></li><li>Fig. 7 shows the standard <font color=orangered>deviations</font> (std) of the layer responses.<span style="font-size:80%;opacity:0.8">图7显示了层响应的标准偏差（std）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> notation<br>(2) </td> <td> [nəʊˈteɪʃn] </td> <td> 
<ul><li>For the example in Fig. 2 that has two layers, $F = W_2 \sigma(W_1x)$ in which $\sigma$ denotes ReLU [29] and the biases are omitted for simplifying <font color=orangered>notations</font>.<span style="font-size:80%;opacity:0.8">图2中的例子有两层，$F = W_2 \sigma(W_1x)$中$\sigma$表示ReLU[29]，为了简化写法忽略偏置项。</span></li><li>We also note that although the above <font color=orangered>notations</font> are about fully-connected layers for simplicity, they are applicable to convolutional layers.<span style="font-size:80%;opacity:0.8">我们还注意到，为了简单起见，尽管上述符号是关于全连接层的，但它们同样适用于卷积层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> resize<br>(2) </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The image is <font color=orangered>resized</font> with its shorter side randomly sampled in [256, 480] for scale augmentation [40].<span style="font-size:80%;opacity:0.8">调整图像大小，其较短的边在[256,480]之间进行随机采样，用于尺度增强[40]。</span></li><li>For best results, we adopt the fully-convolutional form as in [40, 12], and average the scores at multiple scales (images are <font color=orangered>resized</font> such that the shorter side is in {224, 256, 384, 480, 640}).<span style="font-size:80%;opacity:0.8">对于最好的结果，我们采用如[40, 12]中的全卷积形式，并在多尺度上对分数进行平均（图像归一化，短边位于{224, 256, 384, 480, 640}中）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> flip<br>(2) </td> <td> [flɪp] </td> <td> 
<ul><li>A 224×224 crop is randomly sampled from an image or its horizontal <font color=orangered>flip</font>, with the per-pixel mean subtracted [21].<span style="font-size:80%;opacity:0.8">224×224裁剪是从图像或其水平翻转中随机采样，并逐像素减去均值[21]。</span></li><li>We follow the simple data augmentation in [24] for training: 4 pixels are padded on each side, and a 32×32 crop is randomly sampled from the padded image or its horizontal <font color=orangered>flip</font>.<span style="font-size:80%;opacity:0.8">我们按照[24]中的简单数据增强进行训练：每边填充4个像素，并从填充图像或其水平翻转图像中随机采样32×32的裁剪图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> momentum<br>(2) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We use a weight decay of 0.0001 and a <font color=orangered>momentum</font> of 0.9.<span style="font-size:80%;opacity:0.8">我们使用的权重衰减为0.0001，动量为0.9。</span></li><li>We use a weight decay of 0.0001 and <font color=orangered>momentum</font> of 0.9, and adopt the weight initialization in [12] and BN [16] but with no dropout.<span style="font-size:80%;opacity:0.8">我们使用的权重衰减为0.0001和动量为0.9，并采用[12]和BN[16]中的权重初始化，但没有使用丢弃。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> variance<br>(2) </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero <font color=orangered>variances</font>.<span style="font-size:80%;opacity:0.8">这些简单网络使用BN[16]训练，这保证了前向传播信号有非零方差。</span></li><li>For the usage of BN layers, after pre-training, we compute the BN statistics (means and <font color=orangered>variances</font>) for each layer on the ImageNet training set.<span style="font-size:80%;opacity:0.8">对于BN层的使用，在预培训之后，我们计算ImageNet训练集上每一层的BN统计(均值和方差)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> full-image<br>(2) </td> <td> [!≈ fʊl ˈɪmɪdʒ] </td> <td> 
<ul><li>We compute the <font color=orangered>full-image</font> shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8">我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li><li>Given the <font color=orangered>full-image</font> conv feature map, we pool a feature by global Spatial Pyramid Pooling [12] (with a “single-level” pyramid) which can be implemented as “RoI” pooling using the entire image’s bounding box as the RoI.<span style="font-size:80%;opacity:0.8">给定全图像conv特征图，我们通过全局空间金字塔池化[12](使用“单层”金字塔)汇聚一个特征，这可以实现为使用整个图像的边界框作为ROI的“ROI”池化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> RoI-pooled<br>(2) </td> <td> [!≈ rwɑ: 'pu:ld] </td> <td> 
<ul><li>On this <font color=orangered>RoI-pooled</font> feature, all layers of conv5 x and up are adopted for each region, playing the roles of VGG-16’s fc layers.<span style="font-size:80%;opacity:0.8">在此ROI池化特征上，每个区域都采用Cont5 x和UP的所有层，扮演VGG-16的FC层的角色。</span></li><li>But we notice that on this dataset, one image usually contains a single dominate object, and the proposal regions highly overlap with each other and thus have very similar <font color=orangered>RoI-pooled</font> features.<span style="font-size:80%;opacity:0.8">但我们注意到，在这个数据集上，一个图像通常包含单个主导对象，建议区域彼此高度重叠，因此具有非常相似的RoI池化特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> test-dev<br>(2) </td> <td> [!≈ test dev] </td> <td> 
<ul><li>Using validation data. Next we use the 80k+40k trainval set for training and the 20k <font color=orangered>test-dev</font> set for evaluation.<span style="font-size:80%;opacity:0.8">使用验证数据。接下来，我们使用80k+40k trainval集合进行训练，使用20k test-dev集合进行评估。</span></li><li>The mAP is 59.0% and 37.4% on the <font color=orangered>test-dev</font> set.<span style="font-size:80%;opacity:0.8">测试开发集上的mAP为59.0%和37.4%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> LOC<br>(2) </td> <td> [!≈ el əu si:] </td> <td> 
<ul><li>In the column of “<font color=orangered>LOC</font> error on GT class” ([41]), the ground truth class is used.<span style="font-size:80%;opacity:0.8">在“GT类上的LOC错误”([41])列中，使用Ground True类。</span></li><li>The ImageNet Localization (<font color=orangered>LOC</font>) task [36] requires to classify and localize the objects.<span style="font-size:80%;opacity:0.8">ImageNet定位(LOC)任务[36]需要对对象进行分类和定位。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> regressor<br>(2) </td> <td> [rɪ'gresə(r)] </td> <td> 
<ul><li>We adopt the “per-class regression” (PCR) strategy [40, 41], learning a bounding box <font color=orangered>regressor</font> for each class.<span style="font-size:80%;opacity:0.8">我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li><li>Specifically, the cls layer has a 1000-d output, and each dimension is binary logistic regression for predicting being or not being an object class; the reg layer has a $1000 \times 4$-d output consisting of box <font color=orangered>regressors</font> for 1000 classes.<span style="font-size:80%;opacity:0.8">具体地说，cls层有一个1000-d的输出，并且每个维度都是用于预测是否是对象类的二元逻辑回归；reg层有一个$1000 \times 4$-d的输出，由1000个类的边框回归组成。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> center-crop<br>(2) </td> <td> [!≈ 'sentə krɒp] </td> <td> 
<ul><li>VGG’s paper [41] reports a <font color=orangered>center-crop</font> error of 33.1% (Table 13) using ground truth classes.<span style="font-size:80%;opacity:0.8">VGG的论文[41]报告了使用真实类别的中心裁剪误差为33.1%(表13)。</span></li><li>Under the same setting, our RPN method using ResNet-101 net significantly reduces the <font color=orangered>center-crop</font> error to 13.3%.<span style="font-size:80%;opacity:0.8">在相同的设置下，我们使用ResNet-101网络的RPN方法显著地将中心裁剪误差降低到13.3%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> RoI-centric<br>(2) </td> <td> [!≈ rwɑ: 'sentrɪk] </td> <td> 
<ul><li>Motivated by this, in our current experiment we use the original RCNN [8] that is <font color=orangered>RoI-centric</font>, in place of Fast R-CNN.<span style="font-size:80%;opacity:0.8">受此启发，在我们当前的实验中，我们使用以投资回报为中心的原始RCNN[8]，而不是Fast R-CNN。</span></li><li>This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the <font color=orangered>RoI-centric</font> fashion.<span style="font-size:80%;opacity:0.8">这个R-CNN网络在训练集上使用以RoI为中心的大小为256的批量进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> empirical<br>(1) </td> <td> [ɪmˈpɪrɪkl] </td> <td> 
<ul><li>We provide comprehensive <font color=orangered>empirical</font> evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.<span style="font-size:80%;opacity:0.8">我们提供了全面的经验证据说明这些残差网络很容易优化，并可以显著增加深度来提高准确性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> submission<br>(1) </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>Deep residual nets are foundations of our <font color=orangered>submissions</font> to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.<span style="font-size:80%;opacity:0.8">深度残差网络是我们向ILSVRC和COCO 2015竞赛提交的基础，我们也赢得了ImageNet检测任务，ImageNet定位任务，COCO检测和COCO分割任务的第一名。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> breakthrough<br>(1) </td> <td> [ˈbreɪkθru:] </td> <td> 
<ul><li>Deep convolutional neural networks [22, 21] have led to a series of <font color=orangered>breakthroughs</font> for image classification [21, 49, 39].<span style="font-size:80%;opacity:0.8">深度卷积神经网络[22, 21]导致了图像分类[21, 49, 39]的一系列突破。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> notorious<br>(1) </td> <td> [nəʊˈtɔ:riəs] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the <font color=orangered>notorious</font> problem of vanishing/exploding gradients [14, 1, 8], which hamper convergence from the beginning.<span style="font-size:80%;opacity:0.8">在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> hamper<br>(1) </td> <td> [ˈhæmpə(r)] </td> <td> 
<ul><li>Driven by the significance of depth, a question arises: Is learning better networks as easy as stacking more layers? An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [14, 1, 8], which <font color=orangered>hamper</font> convergence from the beginning.<span style="font-size:80%;opacity:0.8">在深度重要性的推动下，出现了一个问题：学些更好的网络是否像堆叠更多的层一样容易？回答这个问题的一个障碍是梯度消失/爆炸[14, 1, 8]这个众所周知的问题，它从一开始就阻碍了收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> normalize<br>(1) </td> <td> [ˈnɔ:məlaɪz] </td> <td> 
<ul><li>This problem, however, has been largely addressed by <font color=orangered>normalized</font> initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8">然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> descent<br>(1) </td> <td> [dɪˈsent] </td> <td> 
<ul><li>This problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient <font color=orangered>descent</font> (SGD) with backpropagation [22].<span style="font-size:80%;opacity:0.8">然而，这个问题通过标准初始化[23, 8, 36, 12]和中间标准化层[16]在很大程度上已经解决，这使得数十层的网络能通过具有反向传播的随机梯度下降（SGD）开始收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> saturated<br>(1) </td> <td> [ˈsætʃəreɪtɪd] </td> <td> 
<ul><li>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets <font color=orangered>saturated</font> (which might be unsurprising) and then degrades rapidly.<span style="font-size:80%;opacity:0.8">当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> unsurprising<br>(1) </td> <td> [ˌʌnsəˈpraɪzɪŋ] </td> <td> 
<ul><li>When deeper networks are able to start converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated (which might be <font color=orangered>unsurprising</font>) and then degrades rapidly.<span style="font-size:80%;opacity:0.8">当更深的网络能够开始收敛时，暴露了一个退化问题：随着网络深度的增加，准确率达到饱和（这可能并不奇怪）然后迅速下降。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> recast<br>(1) </td> <td> [ˌri:ˈkɑ:st] </td> <td> 
<ul><li>The original mapping is <font color=orangered>recast</font> into $F(x) + x$.<span style="font-size:80%;opacity:0.8">原始的映射重写为$F(x) + x$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> feedforward<br>(1) </td> <td> [fi:d'fɔ:wəd] </td> <td> 
<ul><li>The formulation of $F (x) + x$ can be realized by <font color=orangered>feedforward</font> neural networks with “shortcut connections” (Fig. 2).<span style="font-size:80%;opacity:0.8">公式$F (x) + x$可以通过带有“快捷连接”的前向神经网络（图2）来实现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> Caffe<br>(1) </td> <td>  </td> <td> 
<ul><li>The entire network can still be trained end-to-end by SGD with backpropagation, and can be easily implemented using common libraries (e.g., <font color=forestgreen>Caffe</font> [19]) without modifying the solvers.<span style="font-size:80%;opacity:0.8">整个网络仍然可以由带有反向传播的SGD进行端到端的训练，并且可以使用公共库（例如，Caffe [19]）轻松实现，而无需修改求解器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> akin<br>(1) </td> <td> [əˈkɪn] </td> <td> 
<ul><li>Similar phenomena are also shown on the CIFAR-10 set [20], suggesting that the optimization difficulties and the effects of our method are not just <font color=orangered>akin</font> to a particular dataset.<span style="font-size:80%;opacity:0.8">CIFAR-10数据集上[20]也显示出类似的现象，这表明了优化的困难以及我们的方法的影响不仅仅是针对一个特定的数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> generic<br>(1) </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>This strong evidence shows that the residual learning principle is <font color=orangered>generic</font>, and we expect that it is applicable in other vision and non-vision problems.<span style="font-size:80%;opacity:0.8">坚实的证据表明残差学习准则是通用的，并且我们期望它适用于其它的视觉和非视觉问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> probabilistic<br>(1) </td> <td> [ˌprɒbəbɪˈlɪstɪk] </td> <td> 
<ul><li>In image recognition, VLAD [18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a <font color=orangered>probabilistic</font> version [18] of VLAD.<span style="font-size:80%;opacity:0.8">在图像识别中，VLAD[18]是一种通过关于字典的残差向量进行编码的表示形式，Fisher矢量[30]可以表示为VLAD的概率版本[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> retrieval<br>(1) </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>Both of them are powerful shallow representations for image <font color=orangered>retrieval</font> and classification [4, 47].<span style="font-size:80%;opacity:0.8">它们都是图像检索和图像分类[4,47]中强大的浅层表示。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> quantization<br>(1) </td> <td> [ˌkwɒntɪ'zeɪʃən] </td> <td> 
<ul><li>For vector <font color=orangered>quantization</font>, encoding residual vectors [17] is shown to be more effective than encoding original vectors.<span style="font-size:80%;opacity:0.8">对于矢量量化，编码残差矢量[17]被证明比编码原始矢量更有效。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> graphics<br>(1) </td> <td> [ˈgræfɪks] </td> <td> 
<ul><li>In low-level vision and computer <font color=orangered>graphics</font>, for solving Partial Differential Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> Differential<br>(1) </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial <font color=orangered>Differential</font> Equations (PDEs), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> PDEs<br>(1) </td> <td>  </td> <td> 
<ul><li>In low-level vision and computer graphics, for solving Partial Differential Equations (<font color=forestgreen>PDEs</font>), the widely used Multigrid method [3] reformulates the system as subproblems at multiple scales, where each subproblem is responsible for the residual solution between a coarser and a finer scale.<span style="font-size:80%;opacity:0.8">在低级视觉和计算机图形学中，为了求解偏微分方程（PDE），广泛使用的Multigrid方法[3]将系统重构为在多个尺度上的子问题，其中每个子问题负责较粗尺度和较细尺度的残差解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> perceptron<br>(1) </td> <td> [pəˈseptrɒn] </td> <td> 
<ul><li>An early practice of training multi-layer <font color=orangered>perceptrons</font> (MLPs) is to add a linear layer connected from the network input to the output [33, 48].<span style="font-size:80%;opacity:0.8">训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出[33,48]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> MLPs<br>(1) </td> <td>  </td> <td> 
<ul><li>An early practice of training multi-layer perceptrons (<font color=forestgreen>MLPs</font>) is to add a linear layer connected from the network input to the output [33, 48].<span style="font-size:80%;opacity:0.8">训练多层感知机（MLP）的早期实践是添加一个线性层来连接网络的输入和输出[33,48]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> auxiliary<br>(1) </td> <td> [ɔ:gˈzɪliəri] </td> <td> 
<ul><li>In [43, 24], a few intermediate layers are directly connected to <font color=orangered>auxiliary</font> classifiers for addressing vanishing/exploding gradients.<span style="font-size:80%;opacity:0.8">在[43,24]中，一些中间层直接连接到辅助分类器，用于解决梯度消失/爆炸。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> inception<br>(1) </td> <td> [ɪnˈsepʃn] </td> <td> 
<ul><li>In [43], an “<font color=orangered>inception</font>” layer is composed of a shortcut branch and a few deeper branches.<span style="font-size:80%;opacity:0.8">在[43]中，一个“inception”层由一个快捷分支和一些更深的分支组成。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> Concurrent<br>(1) </td> <td> [kənˈkʌrənt] </td> <td> 
<ul><li><font color=orangered>Concurrent</font> with our work, “highway networks” [41, 42] present shortcut connections with gating functions [15].<span style="font-size:80%;opacity:0.8">和我们同时进行的工作，“highway networks” [41, 42]提出了门功能[15]的快捷连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> data-dependent<br>(1) </td> <td> ['deɪtədɪp'endənt] </td> <td> 
<ul><li>These gates are <font color=orangered>data-dependent</font> and have parameters, in contrast to our identity shortcuts that are parameter-free.<span style="font-size:80%;opacity:0.8">这些门是数据相关且有参数的，与我们不具有参数的恒等快捷连接相反。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> counterintuitive<br>(1) </td> <td> [kaʊntərɪn'tju:ɪtɪv] </td> <td> 
<ul><li>This reformulation is motivated by the <font color=orangered>counterintuitive</font> phenomena about the degradation problem (Fig. 1, left).<span style="font-size:80%;opacity:0.8">关于退化问题的反直觉现象激发了这种重构（图1左）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> precondition<br>(1) </td> <td> [ˌpri:kənˈdɪʃn] </td> <td> 
<ul><li>In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to <font color=orangered>precondition</font> the problem.<span style="font-size:80%;opacity:0.8">在实际情况下，恒等映射不太可能是最优的，但是我们的重构可能有助于对问题进行预处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> perturbation<br>(1) </td> <td> [ˌpɜ:təˈbeɪʃn] </td> <td> 
<ul><li>If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the <font color=orangered>perturbations</font> with reference to an identity mapping, than to learn the function as a new one.<span style="font-size:80%;opacity:0.8">如果最优函数比零映射更接近于恒等映射，则求解器应该更容易找到关于恒等映射的抖动，而不是将该函数作为新函数来学习。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> halve<br>(1) </td> <td> [hɑ:v] </td> <td> 
<ul><li>The convolutional layers mostly have 3×3 filters and follow two simple design rules: (i) for the same output feature map size, the layers have the same number of filters; and (ii) if the feature map size is <font color=orangered>halved</font>, the number of filters is doubled so as to preserve the time complexity per layer.<span style="font-size:80%;opacity:0.8">卷积层主要有3×3的滤波器，并遵循两个简单的设计规则：（i）对于相同的输出特征图尺寸，层具有相同数量的滤波器；（ii）如果特征图尺寸减半，则滤波器数量加倍，以便保持每层的时间复杂度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> variant<br>(1) </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Table 1 shows more details and other <font color=orangered>variants</font>.<span style="font-size:80%;opacity:0.8">表1显示了更多细节和其它变种。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> multiply-add<br>(1) </td> <td> [!≈ ˈmʌltɪplaɪ æd] </td> <td> 
<ul><li>Our 34-layer baseline has 3.6 billion FLOPs (<font color=orangered>multiply-adds</font>), which is only 18% of VGG-19 (19.6 billion FLOPs).<span style="font-size:80%;opacity:0.8">我们的34层基准有36亿FLOP(乘加)，仅是VGG-19（196亿FLOP）的18%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> plateau<br>(1) </td> <td> [ˈplætəʊ] </td> <td> 
<ul><li>The learning rate starts from 0.1 and is divided by 10 when the error <font color=orangered>plateaus</font>, and the models are trained for up to $60 \times 10^4$ iterations.<span style="font-size:80%;opacity:0.8">学习速度从0.1开始，当误差稳定时学习率除以10，并且模型训练高达$60 \times 10^4$次迭代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> conjecture<br>(1) </td> <td> [kənˈdʒektʃə(r)] </td> <td> 
<ul><li>We <font color=orangered>conjecture</font> that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8">我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> exponentially<br>(1) </td> <td> [ˌekspə'nenʃəlɪ] </td> <td> 
<ul><li>We conjecture that the deep plain nets may have <font color=orangered>exponentially</font> low convergence rates, which impact the reducing of the training error.<span style="font-size:80%;opacity:0.8">我们推测深度简单网络可能有指数级低收敛特性，这影响了训练误差的降低。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> generalizable<br>(1) </td> <td> ['dʒenərəlaɪzəbl] </td> <td> 
<ul><li>More importantly, the 34-layer ResNet exhibits considerably lower training error and is <font color=orangered>generalizable</font> to the validation data.<span style="font-size:80%;opacity:0.8">更重要的是，34层ResNet显示出较低的训练误差，并且可以泛化到验证数据。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> marginally<br>(1) </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>C is <font color=orangered>marginally</font> better than B, and we attribute this to the extra parameters introduced by many (thirteen) projection shortcuts.<span style="font-size:80%;opacity:0.8">选项C比B稍好，我们把这归因于许多（十三）投影快捷连接引入了额外参数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> intentionally<br>(1) </td> <td> [ɪn'tenʃənəlɪ] </td> <td> 
<ul><li>Our focus is on the behaviors of extremely deep networks, but not on pushing the state-of-the-art results, so we <font color=orangered>intentionally</font> use simple architectures as follows.<span style="font-size:80%;opacity:0.8">我们的焦点在于极深网络的行为，但不是推动最先进的结果，所以我们有意使用如下的简单架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> subsampling<br>(1) </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>subsampling</font> is performed by convolutions with a stride of 2.<span style="font-size:80%;opacity:0.8">下采样由步长为2的卷积进行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> MNIST<br>(1) </td> <td> [!≈ em en aɪ es ti:] </td> <td> 
<ul><li>This phenomenon is similar to that on ImageNet (Fig. 4, left) and on <font color=orangered>MNIST</font> (see [41]), suggesting that such an optimization difficulty is a fundamental problem.<span style="font-size:80%;opacity:0.8">这种现象类似于ImageNet中（图4，左）和MNIST中（请看[41]）的现象，表明这种优化困难是一个基本的问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> dash<br>(1) </td> <td> [dæʃ] </td> <td> 
<ul><li><font color=orangered>Dashed</font> lines denote training error, and bold lines denote testing error.<span style="font-size:80%;opacity:0.8">虚线表示训练误差，粗线表示测试误差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> FitNet<br>(1) </td> <td>  </td> <td> 
<ul><li>It has fewer parameters than other deep and thin networks such as <font color=forestgreen>FitNet</font> [34] and Highway [41] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6).<span style="font-size:80%;opacity:0.8">它与其它的深且窄的网络例如FitNet[34]和Highway41相比有更少的参数，但结果仍在目前最好的结果之间（6.43%，表6）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> Sec.3.<br>(1) </td> <td>  </td> <td> 
<ul><li>These results support our basic motivation (<font color=forestgreen>Sec.3.</font>1) that the residual functions might be generally closer to zero than the non-residual functions.<span style="font-size:80%;opacity:0.8">这些结果支持了我们的基本动机（第3.1节），残差函数通常具有比非残差函数更接近零。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> NoC<br>(1) </td> <td>  </td> <td> 
<ul><li>We adopt the idea of “Networks on Conv feature maps” (<font color=forestgreen>NoC</font>) [33] to address this issue.<span style="font-size:80%;opacity:0.8">我们采用“Conv功能图上的网络”(NOC)[33]的思想来解决这个问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> conv2_x<br>(1) </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, <font color=forestgreen>conv2_x</font>, conv3_x, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8">我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> conv3_x<br>(1) </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, <font color=forestgreen>conv3_x</font>, and conv4_x, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8">我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> conv4_x<br>(1) </td> <td>  </td> <td> 
<ul><li>We compute the full-image shared conv feature maps using those layers whose strides on the image are no greater than 16 pixels (i.e., conv1, conv2_x, conv3_x, and <font color=forestgreen>conv4_x</font>, totally 91 conv layers in ResNet-101; Table 1).<span style="font-size:80%;opacity:0.8">我们使用图像上步长不大于16像素的那些层来计算全图像共享conv特征映射(即，conv1，conv2_x，conv3_x和conv_4x，在ResNet-101中总共91个conv层；表1)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> analogous<br>(1) </td> <td> [əˈnæləgəs] </td> <td> 
<ul><li>We consider these layers as <font color=orangered>analogous</font> to the 13 conv layers in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels).<span style="font-size:80%;opacity:0.8">我们认为这些层类似于VGG-16中的13个conv层，通过这样做，ResNet和VGG-16都具有相同总跨度(16像素)的conv特征地图。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> completeness<br>(1) </td> <td> [kəm'pli:tnəs] </td> <td> 
<ul><li>For <font color=orangered>completeness</font>, we report the improvements made for the competitions.<span style="font-size:80%;opacity:0.8">为了完整起见，我们报告了为竞赛所做的改进。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> iterative<br>(1) </td> <td> ['ɪtərətɪv] </td> <td> 
<ul><li>Our box refinement partially follows the <font color=orangered>iterative</font> localization in [6].<span style="font-size:80%;opacity:0.8">我们的边框细调部分遵循[6]中的迭代定位。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> suppression<br>(1) </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>Non-maximum <font color=orangered>suppression</font> (NMS) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6].<span style="font-size:80%;opacity:0.8">使用IOU阈值0.3[8]，将非最大抑制(NMS)应用于预测边框的并集[8]，然后进行边框投票[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> NMS<br>(1) </td> <td> [!≈ en em es] </td> <td> 
<ul><li>Non-maximum suppression (<font color=orangered>NMS</font>) is applied on the union set of predicted boxes using an IoU threshold of 0.3 [8], followed by box voting [6].<span style="font-size:80%;opacity:0.8">使用IOU阈值0.3[8]，将非最大抑制(NMS)应用于预测边框的并集[8]，然后进行边框投票[6]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> post-RoI<br>(1) </td> <td> [!≈ pəʊst rwɑ:] </td> <td> 
<ul><li>This pooled feature is fed into the <font color=orangered>post-RoI</font> layers to obtain a global context feature.<span style="font-size:80%;opacity:0.8">该池化特征被馈送到ROI后层中以获得全局上下文特征。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> concatenate<br>(1) </td> <td> [kɒn'kætɪneɪt] </td> <td> 
<ul><li>This global feature is <font color=orangered>concatenated</font> with the original per-region feature, followed by the sibling classification and box regression layers.<span style="font-size:80%;opacity:0.8">此全局特征与原始每个区域特征相连接，然后是同级分类和边框回归层。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> testdev<br>(1) </td> <td>  </td> <td> 
<ul><li>The <font color=forestgreen>testdev</font> set has no publicly available ground truth and the result is reported by the evaluation server.<span style="font-size:80%;opacity:0.8">testdev集合没有公开可用的真实值，结果由评估服务器报告。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> pretrained<br>(1) </td> <td>  </td> <td> 
<ul><li>The networks are <font color=forestgreen>pretrained</font> on the 1000-class ImageNet classification set, and are fine-tuned on the DET data.<span style="font-size:80%;opacity:0.8">网络在1000类ImageNet分类集上进行了预训练，并在DET数据上进行了微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> surpass<br>(1) </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>This result won the 1st place in the ImageNet detection task in ILSVRC 2015, <font color=orangered>surpassing</font> the second place by 8.5 points (absolute).<span style="font-size:80%;opacity:0.8">这一结果在ILSVRC 2015的ImageNet检测任务中获得第一名，超过第二名8.5个百分点(绝对)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> GT<br>(1) </td> <td> [dʒi:'ti:] </td> <td> 
<ul><li>In the column of “LOC error on <font color=orangered>GT</font> class” ([41]), the ground truth class is used.<span style="font-size:80%;opacity:0.8">在“GT类上的LOC错误”([41])列中，使用Ground True类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> image-level<br>(1) </td> <td> [!≈ ˈɪmɪdʒ ˈlevl] </td> <td> 
<ul><li>Following [40, 41], we assume that the <font color=orangered>image-level</font> classifiers are first adopted for predicting the class labels of an image, and the localization algorithm only accounts for predicting bounding boxes based on the predicted classes.<span style="font-size:80%;opacity:0.8">在[40，41]之后，我们假设首先采用图像级分类器来预测图像的类别标签，并且定位算法仅考虑基于预测的类别预测边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> PCR<br>(1) </td> <td> [!≈ pi: si: ɑ:(r)] </td> <td> 
<ul><li>We adopt the “per-class regression” (<font color=orangered>PCR</font>) strategy [40, 41], learning a bounding box regressor for each class.<span style="font-size:80%;opacity:0.8">我们采用“逐类回归”(PCR)策略[40，41]，学习每个类的边界框回归器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> category-agnostic<br>(1) </td> <td> [!≈ ˈkætəgəri ægˈnɒstɪk] </td> <td> 
<ul><li>Unlike the way in [32] that is <font color=orangered>category-agnostic</font>, our RPN for localization is designed in a per-class form.<span style="font-size:80%;opacity:0.8">与[32]中与类别无关的方式不同，我们的定位RPN是以每个类的形式设计的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> translation-invariant<br>(1) </td> <td> [!≈ trænsˈleɪʃn ɪnˈveəriənt] </td> <td> 
<ul><li>As in [32], our bounding box regression is with reference to multiple <font color=orangered>translation-invariant</font> “anchor” boxes at each position.<span style="font-size:80%;opacity:0.8">与[32]中一样，我们的边界框回归是参考每个位置的多个平移不变的“锚点”框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> fully-convolutionally<br>(1) </td> <td> [!≈ ˈfʊli !≈ kɒnvə'lu:ʃənəli] </td> <td> 
<ul><li>For testing, the network is applied on the image <font color=orangered>fully-convolutionally</font>.<span style="font-size:80%;opacity:0.8">为了测试，将网络完全卷积地应用于图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> oracle<br>(1) </td> <td> [ˈɒrəkl] </td> <td> 
<ul><li>Following [41], we first perform “<font color=orangered>oracle</font>” testing using the ground truth class as the classification prediction.<span style="font-size:80%;opacity:0.8">在[41]之后，我们首先使用真实分类作为分类预测来执行“oracle”测试。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> image-centric<br>(1) </td> <td> [!≈ ˈɪmɪdʒ 'sentrɪk] </td> <td> 
<ul><li>As a result, the <font color=orangered>image-centric</font> training of Fast R-CNN [7] generates samples of small variations, which may not be desired for stochastic training.<span style="font-size:80%;opacity:0.8">结果，Fast R-CNN[7]的以图像为中心的训练产生小变化的样本，这可能不是随机训练所需要的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> class-dependent<br>(1) </td> <td> [!≈ klɑ:s dɪˈpendənt] </td> <td> 
<ul><li>These predicted boxes play a role of <font color=orangered>class-dependent</font> proposals.<span style="font-size:80%;opacity:0.8">这些预测框起到了和类别相关建议的作用。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> warp<br>(1) </td> <td> [wɔ:p] </td> <td> 
<ul><li>The image region is cropped from a proposal, <font color=orangered>warped</font> to $224 \times 224$ pixels, and fed into the classification network as in R-CNN [8].<span style="font-size:80%;opacity:0.8">图像区域从提案中裁剪，扭曲到$224 \times 224$像素，并像R-CNN[8]那样馈送到分类网络中。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>