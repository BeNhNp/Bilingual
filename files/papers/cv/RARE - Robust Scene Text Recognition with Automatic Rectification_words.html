<html>
<head>
<meta charset="utf-8">
<title> RARE - Robust Scene Text Recognition with Automatic Rectification </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> rectification </td> <td> [ˌrektɪfɪ'keɪʃn] </td> <td> 
<ul><li>Robust Scene Text Recognition with Automatic <font color=orangered>Rectification</font><span style="font-size:80%;opacity:0.8"> 具有自动校正的可靠场景文本识别器</span></li><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text recognizer with Automatic <font color=orangered>REctification</font>), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8"> 与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Zhang et al. [42] propose a character <font color=orangered>rectification</font> method that leverages the low-rank structures of text.<span style="font-size:80%;opacity:0.8"> 张等人 [42]提出了一种利用文本的低等级结构的字符整理方法。</span></li><li>Moreover, it does not require extra annotations for the <font color=orangered>rectification</font> process, since the STN is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8"> 此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>To validate the effectiveness of the <font color=orangered>rectification</font> scheme, we evaluate RARE on the task of perspective text recognition.<span style="font-size:80%;opacity:0.8"> 为了验证整合方案的有效性，我们评估了RARE对透视文本识别的任务。</span></li><li>Our <font color=orangered>rectification</font> scheme can significantly alleviate this problem.<span style="font-size:80%;opacity:0.8"> 我们的整改计划可以显着缓解这一问题。</span></li><li>Figure 9. Examples showing the <font color=orangered>rectifications</font> our model makes and the recognition results.<span style="font-size:80%;opacity:0.8"> 图9. 示例显示了我们的模型所做的纠正和识别结果。</span></li><li>In Fig. 9, we demonstrate the effect of <font color=orangered>rectification</font> through some examples.<span style="font-size:80%;opacity:0.8"> 在图9中，我们通过一些例子演示了整改的效果。</span></li><li>Generally, the <font color=orangered>rectification</font> made by the STN is not perfect, but it alleviates the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8"> 一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li><li>Traditional solutions typically use a separate text <font color=orangered>rectification</font> component.<span style="font-size:80%;opacity:0.8"> 传统的解决方案通常使用单独的文本校正组件。</span></li><li>The extensive experimental results show that 1) without geometric supervision, the learned model can automatically generate more “readable” images for both human and the sequence recognition network; 2) the proposed text <font color=orangered>rectification</font> method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8"> 大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> unsolved </td> <td> [ˌʌnˈsɒlvd] </td> <td> 
<ul><li>Recognizing text in natural images is a challenging task with many <font color=orangered>unsolved</font> problems.<span style="font-size:80%;opacity:0.8"> 识别自然图像中的文本是一项具有挑战性的任务，存在许多未解决的问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> placement </td> <td> [ˈpleɪsmənt] </td> <td> 
<ul><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character <font color=orangered>placement</font>, etc. We propose RARE (Robust text recognizer with Automatic REctification), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8"> 与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Due to its irregular character <font color=orangered>placement</font>, recognizing curved text is very challenging.<span style="font-size:80%;opacity:0.8"> 由于其不规则的字符放置，识别弯曲文本是非常具有挑战性的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> recognizer </td> <td> ['rekəgnaɪzə] </td> <td> 
<ul><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text <font color=orangered>recognizer</font> with Automatic REctification), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8"> 与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Usually, a text <font color=orangered>recognizer</font> works best when its input images contain tightly-bounded regular text.<span style="font-size:80%;opacity:0.8"> 通常，文本识别器在其输入图像包含紧密有界的常规文本时效果最佳。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to rectify input images into ones that are more “readable” by <font color=orangered>recognizers</font>.<span style="font-size:80%;opacity:0.8"> 这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>The STN is able to rectify images that contain these types of irregular text, making them more readable for the following <font color=orangered>recognizer</font>.<span style="font-size:80%;opacity:0.8"> STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>1, we see that the SRN-only model is also a very competitive <font color=orangered>recognizer</font>, achieving higher or competitive performance on most of the benchmarks.<span style="font-size:80%;opacity:0.8">  1，我们看到仅SRN模型也是一个非常有竞争力的识别器，在大多数基准测试中实现了更高或更具竞争力的性能。</span></li><li>In addition, the spatial transformer network is connected to an attention-based sequence <font color=orangered>recognizer</font>, allowing us to train the whole model end-to-end.<span style="font-size:80%;opacity:0.8"> 此外，空间变换器网络连接到基于注意力的序列识别器，允许我们端到端地训练整个模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> transformer </td> <td> [trænsˈfɔ:mə(r)] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial <font color=orangered>Transformer</font> Network (STN) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8"> RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial <font color=orangered>transformer</font> network (STN) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8"> 图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial <font color=orangered>Transformer</font> Network [18] (STN) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8"> 具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>3.1. Spatial <font color=orangered>Transformer</font> Network<span style="font-size:80%;opacity:0.8"> 3.1 空间变换网络</span></li><li>Spatial <font color=orangered>Transformer</font> Network The localization network of STN has 4 convolution layers, each followed by a $2 \times 2$ max-pooling layer.<span style="font-size:80%;opacity:0.8"> 空间变换器网络STN的定位网络有4个卷积层，每个卷层都有一个$2 \times 2$最大池层。</span></li><li>We address this problem in a more feasible and elegant way by adopting a differentiable spatial <font color=orangered>transformer</font> network module.<span style="font-size:80%;opacity:0.8"> 我们通过采用可区分的空间变换网络模块，以一种更可行和更优雅的方式解决了这个问题。</span></li><li>In addition, the spatial <font color=orangered>transformer</font> network is connected to an attention-based sequence recognizer, allowing us to train the whole model end-to-end.<span style="font-size:80%;opacity:0.8"> 此外，空间变换器网络连接到基于注意力的序列识别器，允许我们端到端地训练整个模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> STN </td> <td> [!≈ es ti: en] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial Transformer Network (<font color=orangered>STN</font>) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8"> RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial transformer network (<font color=orangered>STN</font>) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8"> 图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>The <font color=orangered>STN</font> transforms an input image to a rectified image, while the SRN recognizes text.<span style="font-size:80%;opacity:0.8"> STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial Transformer Network [18] (<font color=orangered>STN</font>) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8"> 具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>In the <font color=orangered>STN</font>, an input image is spatially transformed into a rectified image.<span style="font-size:80%;opacity:0.8"> 在STN中，输入图像在空间上变换成校正后的图像。</span></li><li>Ideally, the <font color=orangered>STN</font> produces an image that contains regular text, which is a more appropriate input for the SRN than the original one.<span style="font-size:80%;opacity:0.8"> 在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>Consequently, for the <font color=orangered>STN</font>, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>In practice, the training eventually makes the <font color=orangered>STN</font> tend to produce images that contain regular text, which are desirable inputs for the SRN.<span style="font-size:80%;opacity:0.8"> 在实践中，训练最终会使STN倾向于产生包含常规文本的图像，这些图像正是SRN的理想输入。</span></li><li>Second, our model extends the <font color=orangered>STN</font> framework [18] with an attention-based model.<span style="font-size:80%;opacity:0.8"> 第二，我们的模型扩展了以注意为基础的STN框架的模型。</span></li><li>The original <font color=orangered>STN</font> is only tested on plain convolutional neural networks.<span style="font-size:80%;opacity:0.8"> 原本的STN仅在普通卷积神经网络上进行测试。</span></li><li>Moreover, it does not require extra annotations for the rectification process, since the <font color=orangered>STN</font> is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8"> 此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>The <font color=orangered>STN</font> transforms an input image I to a rectified image $I^\prime$ with a predicted TPS transformation.<span style="font-size:80%;opacity:0.8"> STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>A distinctive property of <font color=orangered>STN</font> is that its sampler is differentiable.<span style="font-size:80%;opacity:0.8"> STN的一个独特属性是其采样器是可微分的。</span></li><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the <font color=orangered>STN</font> can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8"> 因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>Structure of the <font color=orangered>STN</font>. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The sampler produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8"> 定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>Instead, the training of the localization network is completely supervised by the gradients propagated by the other parts of the <font color=orangered>STN</font>, following the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8"> 相反，定位网络的训练完全受到STN其他部分传播的梯度的监督，遵循反向传播算法[22]。</span></li><li>The <font color=orangered>STN</font> is able to rectify images that contain these types of irregular text, making them more readable for the following recognizer.<span style="font-size:80%;opacity:0.8"> STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>Figure 4. The <font color=orangered>STN</font> rectifies images that contain several types of irregular text.<span style="font-size:80%;opacity:0.8"> 图4. STN重新构建包含多种不规则文本的图像。</span></li><li>The <font color=orangered>STN</font> can deal with several types of irregular text, including (a) loosely-bounded text; (b) multi-oriented text; (c) perspective text; (d) curved text.<span style="font-size:80%;opacity:0.8"> STN可以处理几种类型的不规则文本，包括（a）松散有界的文本; （b）多方面文本; （c）透视文本; （d）弯曲文本。</span></li><li>where the probability $p(\cdot)$ is computed by Eq. 8, $\theta$ is the parameters of both <font color=orangered>STN</font> and SRN.<span style="font-size:80%;opacity:0.8"> 其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>Spatial Transformer Network The localization network of <font color=orangered>STN</font> has 4 convolution layers, each followed by a $2 \times 2$ max-pooling layer.<span style="font-size:80%;opacity:0.8"> 空间变换器网络STN的定位网络有4个卷积层，每个卷层都有一个$2 \times 2$最大池层。</span></li><li>The output size of the <font color=orangered>STN</font> is also $100 \times 32$.<span style="font-size:80%;opacity:0.8"> STN的输出大小也是$100 \times 32$。</span></li><li>fiducial points predicted by the <font color=orangered>STN</font> are plotted on input images in green crosses.<span style="font-size:80%;opacity:0.8"> 由STN预测的基准点被绘制在绿色十字架的输入图像上。</span></li><li>We see that the <font color=orangered>STN</font> tends to place fiducial points along upper and lower edges of scene text, and<span style="font-size:80%;opacity:0.8"> 我们看到STN倾向于沿场景文本的上下边缘放置特定点，并且</span></li><li>Generally, the rectification made by the <font color=orangered>STN</font> is not perfect, but it alleviates the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8"> 一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> SRN </td> <td> [!≈ es ɑ:(r) en] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial Transformer Network (STN) and a Sequence Recognition Network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8"> RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more “readable” image for the following <font color=orangered>SRN</font>, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial transformer network (STN) and a sequence recognition network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8"> 图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>The STN transforms an input image to a rectified image, while the <font color=orangered>SRN</font> recognizes text.<span style="font-size:80%;opacity:0.8"> STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial Transformer Network [18] (STN) and a Sequence Recognition Network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8"> 具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>Ideally, the STN produces an image that contains regular text, which is a more appropriate input for the <font color=orangered>SRN</font> than the original one.<span style="font-size:80%;opacity:0.8"> 在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>Motivated by this, for the <font color=orangered>SRN</font> we construct an attention-based model [4] that recognizes text in a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 受此启发，我们构建了SRN，这是一种在序列识别中采用了注意力的模型。</span></li><li>The <font color=orangered>SRN</font> consists of an encoder and a decoder.<span style="font-size:80%;opacity:0.8"> SRN由编码器和解码器构成。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>In practice, the training eventually makes the STN tend to produce images that contain regular text, which are desirable inputs for the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8"> 在实践中，训练最终会使STN倾向于产生包含常规文本的图像，这些图像正是SRN的理想输入。</span></li><li>Third, our model adopts a convolutional-recurrent structure in the encoder of the <font color=orangered>SRN</font>, thus is a novel variant of the attention-based model [4].<span style="font-size:80%;opacity:0.8"> 第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li><li>Moreover, it does not require extra annotations for the rectification process, since the STN is supervised by the <font color=orangered>SRN</font> during training.<span style="font-size:80%;opacity:0.8"> 此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>The input to the <font color=orangered>SRN</font> is a rectified image $I^\prime$ , which ideally contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8"> SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li><li>In our model, the <font color=orangered>SRN</font> is an attention-based model [4, 8], which directly recognizes a sequence from an input image.<span style="font-size:80%;opacity:0.8"> 在我们的模型中，SRN是一种基于注意力的模型[4,8]，它直接识别来自输入图像的序列。</span></li><li>The <font color=orangered>SRN</font> consists of an encoder and a decoder.<span style="font-size:80%;opacity:0.8">  SRN由编码器和解码器组成。</span></li><li>Structure of the <font color=orangered>SRN</font>, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer BLSTM network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8"> 编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li><li>The <font color=orangered>SRN</font> directly maps a input sequence to another sequence.<span style="font-size:80%;opacity:0.8"> SRN直接将输入序列映射到另一个序列。</span></li><li>where the probability $p(\cdot)$ is computed by Eq. 8, $\theta$ is the parameters of both STN and <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8"> 其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>Sequence Recognition Network In the <font color=orangered>SRN</font>, the encoder has 7 convolutional layers, whose {filter size, number of filters, stride, padding size} are respectively {3,64,1,1}, {3,128,1,1}, {3,256,1,1}, {3,256,1,1,}, {3,512,1,1}, {3,512,1,1}, and {2,512,1,0}.<span style="font-size:80%;opacity:0.8"> 序列识别网络在SRN中，编码器有7个卷积层，其{滤波器大小，滤波器数量，步幅，填充大小}分别为{3,64,1,1}，{3,128,1,1}，{3,256 ，1,1}，{3,256,1,1，}，{3,512,1,1}，{3,512,1,1}和{2,512,1,0}。</span></li><li>We also train and test a model that contains only the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8"> 我们还训练和测试仅包含SRN的模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> rectify </td> <td> [ˈrektɪfaɪ] </td> <td> 
<ul><li>In testing, an image is firstly <font color=orangered>rectified</font> via a predicted Thin-Plate-Spline (TPS) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>The STN transforms an input image to a <font color=orangered>rectified</font> image, while the SRN recognizes text.<span style="font-size:80%;opacity:0.8"> STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to <font color=orangered>rectify</font> input images into ones that are more “readable” by recognizers.<span style="font-size:80%;opacity:0.8"> 这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>In the STN, an input image is spatially transformed into a <font color=orangered>rectified</font> image.<span style="font-size:80%;opacity:0.8"> 在STN中，输入图像在空间上变换成校正后的图像。</span></li><li>The transformation is a thinplate-spline [6] (TPS) transformation, whose nonlinearity allows us to <font color=orangered>rectify</font> various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8"> STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li><li>Phan et al. propose to explicitly <font color=orangered>rectify</font> perspective distortions via SIFT [23] descriptor matching.<span style="font-size:80%;opacity:0.8"> 潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li><li>Our method <font color=orangered>rectifies</font> several types of irregular text in a unified way.<span style="font-size:80%;opacity:0.8"> 我们的方法以统一的方式重新定义了几种不规则文本。</span></li><li>The STN transforms an input image I to a <font color=orangered>rectified</font> image $I^\prime$ with a predicted TPS transformation.<span style="font-size:80%;opacity:0.8"> STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>The sampler takes both the grid and the input image, it produces a <font color=orangered>rectified</font> image $I^\prime$ by sampling on the grid points.<span style="font-size:80%;opacity:0.8"> 采样器同时采用网格和输入图像，通过对网格点进行采样，生成一个矫正的图像$I^\prime$。</span></li><li>Structure of the STN. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The sampler produces a <font color=orangered>rectified</font> image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8"> 定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>As illustrated in fig. 3, the base fiducial points are evenly distributed along the top and bottom edge of a <font color=orangered>rectified</font> image $I^\prime$.<span style="font-size:80%;opacity:0.8"> 如图3所示，基本金属点沿着矫正图像$I^\prime$的顶部和底部边缘均匀分布。</span></li><li>The grid of pixels on a <font color=orangered>rectified</font> image $I^\prime$ is denoted by $P^\prime = {p_i^\prime}_{i=1,\cdots,N}$ , where $p_i^\prime = {x_i^\prime, y_i^\prime}^T$ is the x,y coordinates of the i-th pixel, N is the number of pixels.<span style="font-size:80%;opacity:0.8"> 矫正图像上的像素网格由$P^\prime = {p_i^\prime}_{i=1,\cdots,N}$表示，其中$p_i^\prime = {x_i^\prime, y_i^\prime}^T$是第i个像素的x，y坐标，N是像素数。</span></li><li>By setting all pixel values, we get the <font color=orangered>rectified</font> image $T^\prime$ :<span style="font-size:80%;opacity:0.8"> 通过设置所有像素值，我们得到了矫正的图像$T^\prime$：</span></li><li>The ﬂexibility of the TPS transformation allows us to transform irregular text images into <font color=orangered>rectified</font> images that contain regular text.<span style="font-size:80%;opacity:0.8"> TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li><li>The STN is able to <font color=orangered>rectify</font> images that contain these types of irregular text, making them more readable for the following recognizer.<span style="font-size:80%;opacity:0.8"> STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>The input to the SRN is a <font color=orangered>rectified</font> image $I^\prime$ , which ideally contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8"> SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li><li>Figure 4. The STN <font color=orangered>rectifies</font> images that contain several types of irregular text.<span style="font-size:80%;opacity:0.8"> 图4. STN重新构建包含多种不规则文本的图像。</span></li><li>The middle column is the <font color=orangered>rectified</font> images (we use gray-scale images for recognition).<span style="font-size:80%;opacity:0.8"> 中间一列是校正后的图像(我们使用灰度图像进行识别)。</span></li><li>Our model <font color=orangered>rectifies</font> images that contain curved text before recognizing them.<span style="font-size:80%;opacity:0.8"> 我们的模型在识别包含弯曲文本的图像之前对其进行校正。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> Thin-Plate-Spline </td> <td> [!≈ θɪn pleɪt splaɪn] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted <font color=orangered>Thin-Plate-Spline</font> (TPS) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> TPS </td> <td> [!≈ ti: pi: es] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (<font color=orangered>TPS</font>) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>The transformation is a thinplate-spline [6] (<font color=orangered>TPS</font>) transformation, whose nonlinearity allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8"> STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li><li>The <font color=orangered>TPS</font> transformation is configured by a set of fiducial points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8"> TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the <font color=orangered>TPS</font> fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>The STN transforms an input image I to a rectified image $I^\prime$ with a predicted <font color=orangered>TPS</font> transformation.<span style="font-size:80%;opacity:0.8"> STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>Then, inside the grid generator, it calculates the <font color=orangered>TPS</font> transformation parameters from the fiducial points, and generates a sampling grid on I.<span style="font-size:80%;opacity:0.8"> 然后，在网格生成器内部，它从各个点计算TPS变换参数，并在I上生成采样网格。</span></li><li>The grid generator estimates the <font color=orangered>TPS</font> transformation parameters, and generates a sampling grid.<span style="font-size:80%;opacity:0.8"> 网格生成器估计TPS变换参数，并生成采样网格。</span></li><li>Figure 3. fiducial points and the <font color=orangered>TPS</font> transformation.<span style="font-size:80%;opacity:0.8"> 图3.基准点和TPS转换。</span></li><li>The parameters of the <font color=orangered>TPS</font> transformation is represented by a matrix $T \in \Re^{2 \times (K+3)}$ , which is computed by<span style="font-size:80%;opacity:0.8"> TPS变换的参数由矩阵表示，其由下式计算</span></li><li>The ﬂexibility of the <font color=orangered>TPS</font> transformation allows us to transform irregular text images into rectified images that contain regular text.<span style="font-size:80%;opacity:0.8"> TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> readable </td> <td> [ˈri:dəbl] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more “<font color=orangered>readable</font>” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8"> 在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to rectify input images into ones that are more “<font color=orangered>readable</font>” by recognizers.<span style="font-size:80%;opacity:0.8"> 这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>The STN is able to rectify images that contain these types of irregular text, making them more <font color=orangered>readable</font> for the following recognizer.<span style="font-size:80%;opacity:0.8"> STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>The extensive experimental results show that 1) without geometric supervision, the learned model can automatically generate more “<font color=orangered>readable</font>” images for both human and the sequence recognition network; 2) the proposed text rectification method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8"> 大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> trainable </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>RARE is end-to-end <font color=orangered>trainable</font>, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems.<span style="font-size:80%;opacity:0.8"> RARE是端到端的可训练的，只需要图像和相关的文本标签，便于在实际系统中训练和部署模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> e.g. </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>In natural scenes, text appears on various kinds of objects, <font color=orangered>e.g.</font> road signs, billboards, and product packaging.<span style="font-size:80%;opacity:0.8"> 在自然场景中，文本出现在各种对象上，例如道路标志，广告牌和产品包装。</span></li><li>However, on very large lexicons, <font color=orangered>e.g.</font> the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>In the future, we plan to address the end-to-end scene text reading problem through the combination of RARE with a scene text detection method, <font color=orangered>e.g.</font> [43].<span style="font-size:80%;opacity:0.8"> 未来，我们计划通过将RARE与场景文本检测方法相结合来解决端到端场景文本阅读问题，例如[43]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> billboard </td> <td> [ˈbɪlbɔ:d] </td> <td> 
<ul><li>In natural scenes, text appears on various kinds of objects, e.g. road signs, <font color=orangered>billboards</font>, and product packaging.<span style="font-size:80%;opacity:0.8"> 在自然场景中，文本出现在各种对象上，例如道路标志，广告牌和产品包装。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> semantic </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>It carries rich and high-level <font color=orangered>semantic</font> information that is important for image understanding.<span style="font-size:80%;opacity:0.8"> 它携带丰富的高级语义信息，这对于图像理解非常重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> real-world </td> <td> [!≈ ˈri:əl wɜ:ld] </td> <td> 
<ul><li>Recognizing text in images facilitates many <font color=orangered>real-world</font> applications, such as geolocation, driverless car, and image-based machine translation.<span style="font-size:80%;opacity:0.8"> 识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> geolocation </td> <td> [dʒɪɒləʊ'keɪʃn] </td> <td> 
<ul><li>Recognizing text in images facilitates many real-world applications, such as <font color=orangered>geolocation</font>, driverless car, and image-based machine translation.<span style="font-size:80%;opacity:0.8"> 识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> driverless </td> <td> [d'raɪvərles] </td> <td> 
<ul><li>Recognizing text in images facilitates many real-world applications, such as geolocation, <font color=orangered>driverless</font> car, and image-based machine translation.<span style="font-size:80%;opacity:0.8"> 识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> side-view </td> <td> ['saɪdvj'u:] </td> <td> 
<ul><li>For example, some scene text is perspective text [29], which is caused by <font color=orangered>side-view</font> camera angles; some has curved shapes, meaning that its characters are placed along curves rather than straight lines.<span style="font-size:80%;opacity:0.8"> 例如，一些场景文本是透视文本[29]，它是由侧视摄像机角度引起的;有些具有弯曲的形状，这意味着它的角色沿着曲线而不是直线放置。</span></li><li>In fig. 4, we show some common types of irregular text, including a) loosely-bounded text, which resulted by imperfect text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by <font color=orangered>side-view</font> camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8"> 在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> frontal </td> <td> [ˈfrʌntl] </td> <td> 
<ul><li>We call such text irregular text, in contrast to regular text which is horizontal and <font color=orangered>frontal</font>.<span style="font-size:80%;opacity:0.8"> 我们将此类文本称为不规则文本，与常规文本（水平和正面）形成对比。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> Schematic </td> <td> [ski:ˈmætɪk] </td> <td> 
<ul><li>Figure 1. <font color=orangered>Schematic</font> overview of RARE, which consists a spatial transformer network (STN) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8"> 图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> jointly </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>The two networks are <font color=orangered>jointly</font> trained by the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8"> 这两个网络由反向传播算法共同训练[22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> dash </td> <td> [dæʃ] </td> <td> 
<ul><li>The <font color=orangered>dashed</font> lines represent the ﬂows of the back-propagated gradients.<span style="font-size:80%;opacity:0.8"> 虚线表示反向传播的梯度的流动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> ow </td> <td> [aʊ] </td> <td> 
<ul><li>The dashed lines represent the ﬂ<font color=orangered>ows</font> of the back-propagated gradients.<span style="font-size:80%;opacity:0.8"> 虚线表示反向传播的梯度的流动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> back-propagated </td> <td> [!≈ bæk ˈprɔpəɡeitid] </td> <td> 
<ul><li>The dashed lines represent the ﬂows of the <font color=orangered>back-propagated</font> gradients.<span style="font-size:80%;opacity:0.8"> 虚线表示反向传播的梯度的流动。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials <font color=orangered>back-propagated</font> by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> tightly-bounded </td> <td> [!≈ ˈtaɪtli 'baʊndɪd] </td> <td> 
<ul><li>Usually, a text recognizer works best when its input images contain <font color=orangered>tightly-bounded</font> regular text.<span style="font-size:80%;opacity:0.8"> 通常，文本识别器在其输入图像包含紧密有界的常规文本时效果最佳。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> spatially </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>In the STN, an input image is <font color=orangered>spatially</font> transformed into a rectified image.<span style="font-size:80%;opacity:0.8"> 在STN中，输入图像在空间上变换成校正后的图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> ideally </td> <td> [aɪ'di:əlɪ] </td> <td> 
<ul><li><font color=orangered>Ideally</font>, the STN produces an image that contains regular text, which is a more appropriate input for the SRN than the original one.<span style="font-size:80%;opacity:0.8"> 在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>The input to the SRN is a rectified image $I^\prime$ , which <font color=orangered>ideally</font> contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8"> SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> thinplate-spline </td> <td>  </td> <td> 
<ul><li>The transformation is a <font color=forestgreen>thinplate-spline</font> [6] (TPS) transformation, whose nonlinearity allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8"> STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> nonlinearity </td> <td> [nɒnlɪnɪ'ærɪtɪ] </td> <td> 
<ul><li>The transformation is a thinplate-spline [6] (TPS) transformation, whose <font color=orangered>nonlinearity</font> allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8"> STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> configure </td> <td> [kənˈfɪgə(r)] </td> <td> 
<ul><li>The TPS transformation is <font color=orangered>configured</font> by a set of fiducial points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8"> TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> fiducial </td> <td> [fɪ'dju:ʃjəl] </td> <td> 
<ul><li>The TPS transformation is configured by a set of <font color=orangered>fiducial</font> points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8"> TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS <font color=orangered>fiducial</font> points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>As illustrated in fig. 2, it first predicts a set of <font color=orangered>fiducial</font> points via its localization network.<span style="font-size:80%;opacity:0.8"> 如图2所示，它首先通过其定位网络预测一组特定点。</span></li><li>Then, inside the grid generator, it calculates the TPS transformation parameters from the <font color=orangered>fiducial</font> points, and generates a sampling grid on I.<span style="font-size:80%;opacity:0.8"> 然后，在网格生成器内部，它从各个点计算TPS变换参数，并在I上生成采样网格。</span></li><li>Structure of the STN. The localization network localizes a set of <font color=orangered>fiducial</font> points C, with which the grid generator generates a sampling grid P. The sampler produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8"> 定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>The localization network localizes K <font color=orangered>fiducial</font> points by directly regressing their $x,y$ -coordinates.<span style="font-size:80%;opacity:0.8"> 本地化网络通过直接回归其 $x,y$-coordinates来定位K个点。</span></li><li>The coordinates are denoted by $C=[c_1,\cdots, c_K] \in R^{2 \times K}$ , whose k-th column $c_k=[x_k,y_k]^T$ contains the coordinates of the k-th <font color=orangered>fiducial</font> point.<span style="font-size:80%;opacity:0.8"> 坐标由 $C=[c_1,\cdots, c_K] \in R^{2 \times K}$表示，其第k列包含第k个特定点的坐标。</span></li><li>The network localizes <font color=orangered>fiducial</font> points based on global image contexts.<span style="font-size:80%;opacity:0.8"> 网络基于全局图像上下文定位各个点。</span></li><li>It is expected to capture the overall text shape of an input image, and localizes <font color=orangered>fiducial</font> points accordingly.<span style="font-size:80%;opacity:0.8"> 期望捕获输入图像的整体文本形状，并相应地定位各个点。</span></li><li>It should be emphasized that we do not annotate coordinates of <font color=orangered>fiducial</font> points for any sample.<span style="font-size:80%;opacity:0.8"> 应该强调的是，我们不会为任何样本注释各个点的坐标。</span></li><li>We first define another set of <font color=orangered>fiducial</font> points, called the base fiducial points, denoted by $C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$.<span style="font-size:80%;opacity:0.8"> 我们首先定义了另一组特殊点，称为基本点，由$C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$表示。</span></li><li>We first define another set of fiducial points, called the base <font color=orangered>fiducial</font> points, denoted by $C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$.<span style="font-size:80%;opacity:0.8"> 我们首先定义了另一组特殊点，称为基本点，由$C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$表示。</span></li><li>As illustrated in fig. 3, the base <font color=orangered>fiducial</font> points are evenly distributed along the top and bottom edge of a rectified image $I^\prime$.<span style="font-size:80%;opacity:0.8"> 如图3所示，基本金属点沿着矫正图像$I^\prime$的顶部和底部边缘均匀分布。</span></li><li>Figure 3. <font color=orangered>fiducial</font> points and the TPS transformation.<span style="font-size:80%;opacity:0.8"> 图3.基准点和TPS转换。</span></li><li>Green markers on the left image are the <font color=orangered>fiducial</font> points C.<span style="font-size:80%;opacity:0.8"> 左侧图像上的绿色标记是C点。</span></li><li>Cyan markers on the right image are the base <font color=orangered>fiducial</font> points $C^\prime$.<span style="font-size:80%;opacity:0.8"> 右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li><li>where $d_{i,k}$ is the euclidean distance between $p^\prime_i$ and the $k$-th base <font color=orangered>fiducial</font> point $c^\prime_k$ .<span style="font-size:80%;opacity:0.8"> 其中$d_{i,k}$是$p^\prime_i$和第k个基本点$c^\prime_k$之间的欧氏距离。</span></li><li>Green markers are the predicted <font color=orangered>fiducial</font> points on the input images.<span style="font-size:80%;opacity:0.8"> 绿色标记是输入图像上预测的特定点。</span></li><li>Figure 6. Some initialization patterns for the <font color=orangered>fiducial</font> points.<span style="font-size:80%;opacity:0.8"> 图6.各个点的一些初始化模式。</span></li><li>The initial biases are set to such values that yield the <font color=orangered>fiducial</font> points pattern displayed in fig. 6. a.<span style="font-size:80%;opacity:0.8"> 初始偏差设置为这样的值，产生图6.a中显示的金属点图案。</span></li><li>We set the number of <font color=orangered>fiducial</font> points to $K=20$ , meaning that the localization network outputs a 40-dimensional vector.<span style="font-size:80%;opacity:0.8"> 我们将多个点的数量设置为$K=20$，这意味着定位网络输出一个40维向量。</span></li><li>The left column is the input images, where green crosses are the predicted <font color=orangered>fiducial</font> points.<span style="font-size:80%;opacity:0.8"> 左列是输入图像，其中绿色十字是预测的基准点。</span></li><li><font color=orangered>fiducial</font> points predicted by the STN are plotted on input images in green crosses.<span style="font-size:80%;opacity:0.8"> 由STN预测的基准点被绘制在绿色十字架的输入图像上。</span></li><li>We see that the STN tends to place <font color=orangered>fiducial</font> points along upper and lower edges of scene text, and<span style="font-size:80%;opacity:0.8"> 我们看到STN倾向于沿场景文本的上下边缘放置特定点，并且</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> regress </td> <td> [rɪˈgres] </td> <td> 
<ul><li>The TPS transformation is configured by a set of fiducial points, whose coordinates are <font color=orangered>regressed</font> by a convolutional neural network.<span style="font-size:80%;opacity:0.8"> TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>The localization network localizes K fiducial points by directly <font color=orangered>regressing</font> their $x,y$ -coordinates.<span style="font-size:80%;opacity:0.8"> 本地化网络通过直接回归其 $x,y$-coordinates来定位K个点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> sequential </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>It bares some resemblance to a <font color=orangered>sequential</font> signal.<span style="font-size:80%;opacity:0.8"> 它有点类似于顺序信号。</span></li><li>Given an input image, the encoder generates a <font color=orangered>sequential</font> feature representation, which is a sequence of feature vectors.<span style="font-size:80%;opacity:0.8"> 编码器将输入的图像表示成序列的特征，即一系列的特征向量。</span></li><li>Su and Lu [34] extract <font color=orangered>sequential</font> image representation, which is a sequence of HOG [10] descriptors, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8"> Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>We extract a <font color=orangered>sequential</font> representation from $I^\prime$ , and recognize a word from it.<span style="font-size:80%;opacity:0.8"> 我们从$I^\prime$中提取顺序表示，并从中识别出一个单词。</span></li><li>The encoder extracts a <font color=orangered>sequential</font> representation from the input image $I^\prime$.<span style="font-size:80%;opacity:0.8"> 编码器从输入图像$I^\prime$中提取顺序表示。</span></li><li>The decoder recurrently generates a sequence conditioned on the <font color=orangered>sequential</font> representation, by decoding the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8"> 解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li><li>A na¨ıve approach for extracting a <font color=orangered>sequential</font> representation for $I^\prime$ is to take local image patches from left to right, and describe each of them with a CNN.<span style="font-size:80%;opacity:0.8"> 用于提取$I^\prime$的顺序表示的一种简单方法是从左到右获取图像中的局部图像块，并用CNN描述每个图像块。</span></li><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer BLSTM network to extract a <font color=orangered>sequential</font> representation (h) for the input image.<span style="font-size:80%;opacity:0.8"> 编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> recurrently </td> <td> [rɪ'kʌrəntlɪ] </td> <td> 
<ul><li>The decoder <font color=orangered>recurrently</font> generates a character sequence conditioning on the input sequence, by decoding the relevant contents which are determined by its attention mechanism at each step.<span style="font-size:80%;opacity:0.8"> 解码器会根据注意力机制进行解码，循序地生成识别出的字符序列。</span></li><li>The decoder <font color=orangered>recurrently</font> generates a sequence conditioned on the sequential representation, by decoding the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8"> 解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li><li>The decoder <font color=orangered>recurrently</font> generates a sequence of characters, conditioned on the sequence produced by the encoder.<span style="font-size:80%;opacity:0.8"> 解码器以编码器产生的序列为条件，反复生成一系列字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> decode </td> <td> [ˌdi:ˈkəʊd] </td> <td> 
<ul><li>The decoder recurrently generates a character sequence conditioning on the input sequence, by <font color=orangered>decoding</font> the relevant contents which are determined by its attention mechanism at each step.<span style="font-size:80%;opacity:0.8"> 解码器会根据注意力机制进行解码，循序地生成识别出的字符序列。</span></li><li>The decoder recurrently generates a sequence conditioned on the sequential representation, by <font color=orangered>decoding</font> the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8"> 解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> geometric </td> <td> [ˌdʒi:əˈmetrɪk] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any <font color=orangered>geometric</font> ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>The extensive experimental results show that 1) without <font color=orangered>geometric</font> supervision, the learned model can automatically generate more “readable” images for both human and the sequence recognition network; 2) the proposed text rectification method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8"> 大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> i.e. </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any geometric ground truth, <font color=orangered>i.e.</font> the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, <font color=orangered>i.e.</font> receptive field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8"> 根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li><li>where $l_{t-1}$ is the $t-1$-th ground-truth label in training, while in testing, it is the label predicted in the previous step, <font color=orangered>i.e.</font> $\hat l_{t-1}$ .<span style="font-size:80%;opacity:0.8"> 其中$l_{t-1}$是训练中第$t-1$个真实标签，而在测试中，它是上一步中预测的标签，即$\hat l_{t-1}$。</span></li><li>When a test image is associated with a lexicon, <font color=orangered>i.e.</font> a set of words for selection, the recognition process is to pick the word with the highest posterior conditional probability:<span style="font-size:80%;opacity:0.8"> 当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> differential </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error <font color=orangered>differentials</font> back-propagated by the SRN.<span style="font-size:80%;opacity:0.8"> 因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the STN can back-propagate error <font color=orangered>differentials</font> and gets trained.<span style="font-size:80%;opacity:0.8"> 因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> convolutional-recurrent </td> <td> [!≈ kɒnvə'lu:ʃənəl rɪˈkʌrənt] </td> <td> 
<ul><li>Third, our model adopts a <font color=orangered>convolutional-recurrent</font> structure in the encoder of the SRN, thus is a novel variant of the attention-based model [4].<span style="font-size:80%;opacity:0.8"> 第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li><li>3.2.1 Encoder: <font color=orangered>Convolutional-Recurrent</font> Network<span style="font-size:80%;opacity:0.8"> 3.2.1编码器：卷积 - 循环网络</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> variant </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Third, our model adopts a convolutional-recurrent structure in the encoder of the SRN, thus is a novel <font color=orangered>variant</font> of the attention-based model [4].<span style="font-size:80%;opacity:0.8"> 第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> Hough </td> <td> [hɒk] </td> <td> 
<ul><li>Among the traditional methods, many adopt bottom-up approaches, where individual characters are firstly detected using sliding window [36, 35], connected components [28], or <font color=orangered>Hough</font> voting [39].<span style="font-size:80%;opacity:0.8"> 在传统方法中，许多方法采用自下而上的方法，其中首先使用滑动窗口[36,35]，连通组件[28]或霍夫投票[39]来检测单个字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> lexicon </td> <td> [ˈleksɪkən] </td> <td> 
<ul><li>Following that, the detected characters are integrated into words by means of dynamic programming, <font color=orangered>lexicon</font> search [35], etc. Other work adopts top-down approaches, where text is directly recognized from entire input images, rather than detecting and recognizing individual characters.<span style="font-size:80%;opacity:0.8"> 之后，通过动态编程，词典搜索[35]等将检测到的字符集成到单词中。其他工作采用自上而下的方法，其中文本直接从整个输入图像中识别，而不是检测和识别单个字符。</span></li><li>3.4. Recognizing With a <font color=orangered>Lexicon</font><span style="font-size:80%;opacity:0.8"> 3.4 用词典识别</span></li><li>When a test image is associated with a <font color=orangered>lexicon</font>, i.e. a set of words for selection, the recognition process is to pick the word with the highest posterior conditional probability:<span style="font-size:80%;opacity:0.8"> 当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li><li>However, on very large <font color=orangered>lexicons</font>, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>We adopt an efficient approximate search scheme on large <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8"> 我们在大词典上采用了有效的近似搜索方案。</span></li><li>We first construct a prefix tree over a given <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> 我们首先在给定的词典上构建一个pre fi x树。</span></li><li>Since the tree depth is at most the length of the longest word in the <font color=orangered>lexicon</font>, this search process takes much less computation than the precise search.<span style="font-size:80%;opacity:0.8"> 由于树深度最多是词典中最长单词的长度，因此该搜索过程比精确搜索所需的计算量少得多。</span></li><li>The titles “50”, “1k” and “50k” are <font color=orangered>lexicon</font> sizes.<span style="font-size:80%;opacity:0.8"> 标题“50”，“1k”和“50k”是词典大小。</span></li><li>The “Full” <font color=orangered>lexicon</font> contains all per-image lexicon words.<span style="font-size:80%;opacity:0.8"> “完整”词典包含所有每个图像的词典单词。</span></li><li>The “Full” lexicon contains all per-image <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8"> “完整”词典包含所有每个图像的词典单词。</span></li><li>“None” means recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">  “无”表示没有词典的识别。</span></li><li>Without a <font color=orangered>lexicon</font>, the model takes less than 2ms recognizing an image.<span style="font-size:80%;opacity:0.8"> 没有词典，模型识别图像所需的时间不到2毫秒。</span></li><li>With a <font color=orangered>lexicon</font>, recognition speed depends on the lexicon size.<span style="font-size:80%;opacity:0.8"> 使用词典，识别速度取决于词典大小。</span></li><li>With a lexicon, recognition speed depends on the <font color=orangered>lexicon</font> size.<span style="font-size:80%;opacity:0.8"> 使用词典，识别速度取决于词典大小。</span></li><li>We adopt the precise search (Sec. 3.4) when <font color=orangered>lexicon</font> size ≤ 1k.<span style="font-size:80%;opacity:0.8"> 当词典大小≤1k时，我们采用精确搜索（第3.4节）。</span></li><li>On larger <font color=orangered>lexicons</font>, we adopt the approximate beam search (Sec. 3.4) with a beam width of 7.<span style="font-size:80%;opacity:0.8"> 在较大的词典中，我们采用近似beam搜索（第3.4节），光束宽度为7。</span></li><li>With a 50k-word <font color=orangered>lexicon</font>, the search takes ~200ms per image.<span style="font-size:80%;opacity:0.8"> 使用50k字的词典，每张图像搜索大约需要200毫秒。</span></li><li>For each image, there is a 50-word <font color=orangered>lexicon</font> and a 1000-word lexicon.<span style="font-size:80%;opacity:0.8"> 对于每个图像，有一个50字的词典和一个1000字的词典。</span></li><li>For each image, there is a 50-word lexicon and a 1000-word <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> 对于每个图像，有一个50字的词典和一个1000字的词典。</span></li><li>All <font color=orangered>lexicons</font> consist of a ground truth word and some randomly picked words.<span style="font-size:80%;opacity:0.8"> 所有词典都包含一个真实词和一些随机选择的词。</span></li><li>Each sample is associated with a 50-word <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> 每个样本与50个单词的词典相关联。</span></li><li>• ICDAR 2003 [24] (IC03) contains 860 cropped word images, each associated with a 50-word <font color=orangered>lexicon</font> defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8"> •ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>Besides, there is a “full <font color=orangered>lexicon</font>” which contains all lexicon words, and the Hunspell [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>Besides, there is a “full lexicon” which contains all <font color=orangered>lexicon</font> words, and the Hunspell [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>Besides, there is a “full lexicon” which contains all lexicon words, and the Hunspell [1] <font color=orangered>lexicon</font> which has 50k words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>On unconstrained recognition tasks (recognizing without a <font color=orangered>lexicon</font>), our model outperforms all the other methods in comparison.<span style="font-size:80%;opacity:0.8"> 在无约束的识别任务（没有词典识别）的情况下，我们的模型在比较中优于所有其他方法。</span></li><li>On constrained recognition tasks (recognizing with a <font color=orangered>lexicon</font>), RARE achieves state-of-the-art or highly competitive accuracies.<span style="font-size:80%;opacity:0.8"> 在受约束的识别任务（用词典识别）中，RARE实现了最先进或极具竞争力的准确性。</span></li><li>Each image is associated with a 50-word <font color=orangered>lexicon</font>, which is inherited from the SVT [35] dataset.<span style="font-size:80%;opacity:0.8"> 每个图像都与一个50字的词典相关联，该词典继承自SVT [35]数据集。</span></li><li>In addition, there is a “Full” <font color=orangered>lexicon</font> which contains all the per-image lexicon words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个“Full”词典，其中包含所有每个图像的词典单词。</span></li><li>In addition, there is a “Full” lexicon which contains all the per-image <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个“Full”词典，其中包含所有每个图像的词典单词。</span></li><li>“50” and “Full” represent recognition with 50-word <font color=orangered>lexicons</font> and the full lexicon respectively.<span style="font-size:80%;opacity:0.8">  “50”和“Full”分别表示50字词典和完整词典的识别。</span></li><li>“50” and “Full” represent recognition with 50-word lexicons and the full <font color=orangered>lexicon</font> respectively.<span style="font-size:80%;opacity:0.8">  “50”和“Full”分别表示50字词典和完整词典的识别。</span></li><li>“None” represents recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">  “无”表示没有词典的识别。</span></li><li>In the second and third columns, we compare the accuracies of recognition with the 50-word <font color=orangered>lexicon</font> and the full lexicon.<span style="font-size:80%;opacity:0.8"> 在第二和第三列中，我们将识别的准确性与50字词汇和完整词典进行比较。</span></li><li>In the second and third columns, we compare the accuracies of recognition with the 50-word lexicon and the full <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> 在第二和第三列中，我们将识别的准确性与50字词汇和完整词典进行比较。</span></li><li>Our method outperforms [29], which is a perspective text recognition method, by a large margin on both <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8"> 我们的方法优于[29]，这是一种透视文本识别方法，在两个词典上都有很大的余地。</span></li><li>In the comparisons with [32], which uses the same training set as RARE, we still observe significant improvements in both the Full <font color=orangered>lexicon</font> and the lexicon-free settings.<span style="font-size:80%;opacity:0.8"> 在与使用与RARE相同的训练集的[32]的比较中，我们仍然观察到Full lexicon和Lexicon-free设置的显着改进。</span></li><li>All models are evaluated without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> 所有模型都在没有词典的情况下进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> Alm´azan </td> <td>  </td> <td> 
<ul><li>For example, <font color=forestgreen>Alm´azan</font> et al. [2] propose to predict label embedding vectors from input images.<span style="font-size:80%;opacity:0.8"> 例如，Alm'azan等人[2]建议从输入图像预测标签嵌入向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> Jaderberg </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Jaderberg</font> et al. [17] address text recognition with a 90k-class convolutional neural network, where each class corresponds to an English word.<span style="font-size:80%;opacity:0.8"> Jaderberg等人[17]使用90k级卷积神经网络进行文本识别，其中每个类对应一个英语单词。</span></li><li>Our model is trained on the 8-million synthetic samples released by <font color=forestgreen>Jaderberg</font> et al. [15].<span style="font-size:80%;opacity:0.8"> 我们的模型在Jaderberg等人[15]发布的800万个合成样本上进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> k-class </td> <td> [!≈ keɪ klɑ:s] </td> <td> 
<ul><li>Jaderberg et al. [17] address text recognition with a 90<font color=orangered>k-class</font> convolutional neural network, where each class corresponds to an English word.<span style="font-size:80%;opacity:0.8"> Jaderberg等人[17]使用90k级卷积神经网络进行文本识别，其中每个类对应一个英语单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> unconstrained </td> <td> [ˌʌnkən'streɪnd] </td> <td> 
<ul><li>In [16], a CNN with a structured output layer is constructed for <font color=orangered>unconstrained</font> text recognition.<span style="font-size:80%;opacity:0.8"> 在[16]中，构造具有结构化输出层的CNN用于无约束文本识别。</span></li><li>On <font color=orangered>unconstrained</font> recognition tasks (recognizing without a lexicon), our model outperforms all the other methods in comparison.<span style="font-size:80%;opacity:0.8"> 在无约束的识别任务（没有词典识别）的情况下，我们的模型在比较中优于所有其他方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> HOG </td> <td> [hɒg] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of <font color=orangered>HOG</font> [10] descriptors, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8"> Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> descriptor </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of HOG [10] <font color=orangered>descriptors</font>, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8"> Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>Yao et al. [38] firstly propose the multi-oriented text detection problem, and deal with it by carefully designing rotation-invariant region <font color=orangered>descriptors</font>.<span style="font-size:80%;opacity:0.8"> 姚等人[38]首先提出了多方向文本检测问题，并通过仔细设计旋转不变区域描述符来处理它。</span></li><li>Phan et al. propose to explicitly rectify perspective distortions via SIFT [23] <font color=orangered>descriptor</font> matching.<span style="font-size:80%;opacity:0.8"> 潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. receptive field, and is a <font color=orangered>descriptor</font> for that region.<span style="font-size:80%;opacity:0.8"> 根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> recurrent </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of HOG [10] descriptors, and predict the corresponding character sequence with a <font color=orangered>recurrent</font> neural network (RNN).<span style="font-size:80%;opacity:0.8"> Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>Instead, following [32], we build a network that combines convolutional layers and <font color=orangered>recurrent</font> networks.<span style="font-size:80%;opacity:0.8"> 相反，在[32]之后，我们构建了一个结合了卷积层和循环网络的网络。</span></li><li>The BLSTM is a <font color=orangered>recurrent</font> network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8"> BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li><li>3.2.2 Decoder: <font color=orangered>Recurrent</font> Character Generator<span style="font-size:80%;opacity:0.8"> 3.2.2解码器：循环字符发生器</span></li><li>It is a <font color=orangered>recurrent</font> neural network with the attention structure proposed in [4, 8].<span style="font-size:80%;opacity:0.8"> 它是一种递归神经网络，具有[4,8]中提出的注意结构。</span></li><li>In the recurrency part, we adopt the Gated <font color=orangered>Recurrent</font> Unit (GRU) [7] as the cell.<span style="font-size:80%;opacity:0.8"> 在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li><li>The state $s_{t-1}$ is updated via the <font color=orangered>recurrent</font> process of GRU [7, 8]:<span style="font-size:80%;opacity:0.8"> 状态$s_{t-1}$通过GRU [7,8]的循环过程更新：</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> rotation-invariant </td> <td> [!≈ rəʊˈteɪʃn ɪnˈveəriənt] </td> <td> 
<ul><li>Yao et al. [38] firstly propose the multi-oriented text detection problem, and deal with it by carefully designing <font color=orangered>rotation-invariant</font> region descriptors.<span style="font-size:80%;opacity:0.8"> 姚等人[38]首先提出了多方向文本检测问题，并通过仔细设计旋转不变区域描述符来处理它。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> leverage </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>Zhang et al. [42] propose a character rectification method that <font color=orangered>leverages</font> the low-rank structures of text.<span style="font-size:80%;opacity:0.8"> 张等人 [42]提出了一种利用文本的低等级结构的字符整理方法。</span></li><li>Besides, the spatial dependencies between the patches are not exploited and <font color=orangered>leveraged</font>.<span style="font-size:80%;opacity:0.8"> 此外，图像块之间的空间依赖性未被利用和利用。</span></li><li>Restricted by the sizes of the receptive fields, the feature sequence <font color=orangered>leverages</font> limited image contexts.<span style="font-size:80%;opacity:0.8"> 受接收场的大小限制，特征序列利用有限的图像上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> Phan </td> <td> [fæn] </td> <td> 
<ul><li><font color=orangered>Phan</font> et al. propose to explicitly rectify perspective distortions via SIFT [23] descriptor matching.<span style="font-size:80%;opacity:0.8"> 潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> SIFT </td> <td> [sɪft] </td> <td> 
<ul><li>Phan et al. propose to explicitly rectify perspective distortions via <font color=orangered>SIFT</font> [23] descriptor matching.<span style="font-size:80%;opacity:0.8"> 潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> above-mentioned </td> <td> [ə'bʌv 'menʃnd] </td> <td> 
<ul><li>The <font color=orangered>above-mentioned</font> work brings insightful ideas into this issue.<span style="font-size:80%;opacity:0.8"> 上述工作为这个问题带来了深刻的见解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> insightful </td> <td> [ˈɪnsaɪtfʊl] </td> <td> 
<ul><li>The above-mentioned work brings <font color=orangered>insightful</font> ideas into this issue.<span style="font-size:80%;opacity:0.8"> 上述工作为这个问题带来了深刻的见解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> annotation </td> <td> [ˌænə'teɪʃn] </td> <td> 
<ul><li>Moreover, it does not require extra <font color=orangered>annotations</font> for the rectification process, since the STN is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8"> 此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> sampler </td> <td> [ˈsɑ:mplə(r)] </td> <td> 
<ul><li>The <font color=orangered>sampler</font> takes both the grid and the input image, it produces a rectified image $I^\prime$ by sampling on the grid points.<span style="font-size:80%;opacity:0.8"> 采样器同时采用网格和输入图像，通过对网格点进行采样，生成一个矫正的图像$I^\prime$。</span></li><li>A distinctive property of STN is that its <font color=orangered>sampler</font> is differentiable.<span style="font-size:80%;opacity:0.8"> STN的一个独特属性是其采样器是可微分的。</span></li><li>Structure of the STN. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The <font color=orangered>sampler</font> produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8"> 定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>3.1.3 <font color=orangered>Sampler</font><span style="font-size:80%;opacity:0.8"> 3.1.3 采样器</span></li><li>Lastly, in the <font color=orangered>sampler</font>, the pixel value of $p^\prime_i$ is bilinearly interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8"> 最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li><li>where $V$ represents the bilinear <font color=orangered>sampler</font> [18], which is also a differentiable module.<span style="font-size:80%;opacity:0.8"> 其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> differentiable </td> <td> [ˌdɪfə'renʃɪəbl] </td> <td> 
<ul><li>A distinctive property of STN is that its sampler is <font color=orangered>differentiable</font>.<span style="font-size:80%;opacity:0.8"> STN的一个独特属性是其采样器是可微分的。</span></li><li>Therefore, once we have a <font color=orangered>differentiable</font> localization network and a differentiable grid generator, the STN can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8"> 因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>Therefore, once we have a differentiable localization network and a <font color=orangered>differentiable</font> grid generator, the STN can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8"> 因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>The grid generator can back-propagate gradients, since its two matrix multiplications, Eq. 1 and Eq. 4, are both <font color=orangered>differentiable</font>.<span style="font-size:80%;opacity:0.8"> 网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>where $V$ represents the bilinear sampler [18], which is also a <font color=orangered>differentiable</font> module.<span style="font-size:80%;opacity:0.8"> 其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li><li>We address this problem in a more feasible and elegant way by adopting a <font color=orangered>differentiable</font> spatial transformer network module.<span style="font-size:80%;opacity:0.8"> 我们通过采用可区分的空间变换网络模块，以一种更可行和更优雅的方式解决了这个问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> back-propagate </td> <td> [!≈ bæk ˈprɒpəgeɪt] </td> <td> 
<ul><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the STN can <font color=orangered>back-propagate</font> error differentials and gets trained.<span style="font-size:80%;opacity:0.8"> 因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>The grid generator can <font color=orangered>back-propagate</font> gradients, since its two matrix multiplications, Eq. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8"> 网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> normalize </td> <td> [ˈnɔ:məlaɪz] </td> <td> 
<ul><li>We use a <font color=orangered>normalized</font> coordinate system whose origin is the image center, so that $x_k, y_k$ are within the range of $[-1, 1]$ .<span style="font-size:80%;opacity:0.8"> 我们使用归一化坐标系，其原点是图像中心，因此$x_k, y_k$在$[-1, 1]$的范围内。</span></li><li>Since K is a constant and the coordinate system is <font color=orangered>normalized</font>, $C^\prime$ is always a constant.<span style="font-size:80%;opacity:0.8"> 由于K是常数并且坐标系被归一化，因此$C^\prime$始终是常数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> propagate </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>Instead, the training of the localization network is completely supervised by the gradients <font color=orangered>propagated</font> by the other parts of the STN, following the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8"> 相反，定位网络的训练完全受到STN其他部分传播的梯度的监督，遵循反向传播算法[22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> marker </td> <td> [ˈmɑ:kə(r)] </td> <td> 
<ul><li>Green <font color=orangered>markers</font> on the left image are the fiducial points C.<span style="font-size:80%;opacity:0.8"> 左侧图像上的绿色标记是C点。</span></li><li>Cyan <font color=orangered>markers</font> on the right image are the base fiducial points $C^\prime$.<span style="font-size:80%;opacity:0.8"> 右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li><li>Green <font color=orangered>markers</font> are the predicted fiducial points on the input images.<span style="font-size:80%;opacity:0.8"> 绿色标记是输入图像上预测的特定点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> Cyan </td> <td> [ˈsaɪən] </td> <td> 
<ul><li><font color=orangered>Cyan</font> markers on the right image are the base fiducial points $C^\prime$.<span style="font-size:80%;opacity:0.8"> 右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> euclidean </td> <td> [ju:ˈklidiən] </td> <td> 
<ul><li>where the element on the i-th row and j-th column of R is $r_{i,j}=d_{i,j}^2$ , $d_{i,j}$ is the <font color=orangered>euclidean</font> distance between $c_i^\prime$ and $c_j^\prime$ .<span style="font-size:80%;opacity:0.8"> 其中R的第i行和第j列的元素是$r_{i,j}=d_{i,j}^2$，$d_{i,j}$是$c_i^\prime$和$c_j^\prime$之间的欧氏距离。</span></li><li>where $d_{i,k}$ is the <font color=orangered>euclidean</font> distance between $p^\prime_i$ and the $k$-th base fiducial point $c^\prime_k$ .<span style="font-size:80%;opacity:0.8"> 其中$d_{i,k}$是$p^\prime_i$和第k个基本点$c^\prime_k$之间的欧氏距离。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> iterate </td> <td> [ˈɪtəreɪt] </td> <td> 
<ul><li>By <font color=orangered>iterating</font> over all points in $P^\prime$ , we generate a grid $P=\{p_i\}_{i=1,\cdots,N}$ on the input image $I$.<span style="font-size:80%;opacity:0.8"> 通过迭代$P^\prime$中的所有点，我们在输入图像$I$上生成网格$P=\{p_i\}_{i=1,\cdots,N}$。</span></li><li>The process <font color=orangered>iterates</font> until a leaf node is reached.<span style="font-size:80%;opacity:0.8"> 该过程将迭代，直到到达叶节点。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires <font color=orangered>iterating</font> over all lexicon words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> multiplication </td> <td> [ˌmʌltɪplɪˈkeɪʃn] </td> <td> 
<ul><li>The grid generator can back-propagate gradients, since its two matrix <font color=orangered>multiplications</font>, Eq. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8"> 网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> Eq </td> <td>  </td> <td> 
<ul><li>The grid generator can back-propagate gradients, since its two matrix multiplications, <font color=forestgreen>Eq</font>. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8"> 网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>The grid generator can back-propagate gradients, since its two matrix multiplications, Eq. 1 and <font color=forestgreen>Eq</font>. 4, are both differentiable.<span style="font-size:80%;opacity:0.8"> 网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>where the probability $p(\cdot)$ is computed by <font color=forestgreen>Eq</font>. 8, $\theta$ is the parameters of both STN and SRN.<span style="font-size:80%;opacity:0.8"> 其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing <font color=forestgreen>Eq</font>. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> Lastly </td> <td> [ˈlɑ:stli] </td> <td> 
<ul><li><font color=orangered>Lastly</font>, in the sampler, the pixel value of $p^\prime_i$ is bilinearly interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8"> 最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> bilinearly </td> <td> [!≈ baɪ'lɪnɪəli] </td> <td> 
<ul><li>Lastly, in the sampler, the pixel value of $p^\prime_i$ is <font color=orangered>bilinearly</font> interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8"> 最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> interpolate </td> <td> [ɪnˈtɜ:pəleɪt] </td> <td> 
<ul><li>Lastly, in the sampler, the pixel value of $p^\prime_i$ is bilinearly <font color=orangered>interpolated</font> from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8"> 最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> bilinear </td> <td> [baɪ'lɪnɪə] </td> <td> 
<ul><li>where $V$ represents the <font color=orangered>bilinear</font> sampler [18], which is also a differentiable module.<span style="font-size:80%;opacity:0.8"> 其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> exibility </td> <td>  </td> <td> 
<ul><li>The ﬂ<font color=forestgreen>exibility</font> of the TPS transformation allows us to transform irregular text images into rectified images that contain regular text.<span style="font-size:80%;opacity:0.8"> TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> loosely-bounded </td> <td> [!≈ ˈlu:sli 'baʊndɪd] </td> <td> 
<ul><li>In fig. 4, we show some common types of irregular text, including a) <font color=orangered>loosely-bounded</font> text, which resulted by imperfect text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by side-view camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8"> 在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li><li>The STN can deal with several types of irregular text, including (a) <font color=orangered>loosely-bounded</font> text; (b) multi-oriented text; (c) perspective text; (d) curved text.<span style="font-size:80%;opacity:0.8"> STN可以处理几种类型的不规则文本，包括（a）松散有界的文本; （b）多方面文本; （c）透视文本; （d）弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> imperfect </td> <td> [ɪmˈpɜ:fɪkt] </td> <td> 
<ul><li>In fig. 4, we show some common types of irregular text, including a) loosely-bounded text, which resulted by <font color=orangered>imperfect</font> text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by side-view camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8"> 在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> inherently </td> <td> [ɪnˈhɪərəntlɪ] </td> <td> 
<ul><li>Since target words are <font color=orangered>inherently</font> sequences of characters, we model the recognition problem as a sequence recognition problem, and address it with a sequence recognition network.<span style="font-size:80%;opacity:0.8"> 由于目标词本质上是字符序列，我们将识别问题建模为序列识别问题，并用序列识别网络对其进行处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> horizontally </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>The input to the SRN is a rectified image $I^\prime$ , which ideally contains a word that is written <font color=orangered>horizontally</font> from left to right.<span style="font-size:80%;opacity:0.8"> SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> na¨ıve </td> <td>  </td> <td> 
<ul><li>A <font color=forestgreen>na¨ıve</font> approach for extracting a sequential representation for $I^\prime$ is to take local image patches from left to right, and describe each of them with a CNN.<span style="font-size:80%;opacity:0.8"> 用于提取$I^\prime$的顺序表示的一种简单方法是从左到右获取图像中的局部图像块，并用CNN描述每个图像块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> arbitrary </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>The network extracts a sequence of feature vectors, given an input image of <font color=orangered>arbitrary</font> size.<span style="font-size:80%;opacity:0.8"> 给定任意大小的输入图像，网络提取特征向量序列。</span></li><li>Both input and output sequences may have <font color=orangered>arbitrary</font> lengths.<span style="font-size:80%;opacity:0.8"> 输入和输出序列都可以具有任意长度。</span></li><li>[32] is able to recognize <font color=orangered>arbitrary</font> words, but it does not have a specific mechanism for handling curved text.<span style="font-size:80%;opacity:0.8"> [32]能够识别任意单词，但没有处理弯曲文本的特定机制。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> attens </td> <td>  </td> <td> 
<ul><li>Specifically, the “map-to-sequence” operation takes out the columns of the maps in the left-to-right order, and ﬂ<font color=forestgreen>attens</font> them into vectors.<span style="font-size:80%;opacity:0.8"> 具体而言，“map-to-sequence”操作以从左到右的顺序取出地图的列，并将fl视为向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> invariance </td> <td> [ɪn'veərɪəns] </td> <td> 
<ul><li>According to the translation <font color=orangered>invariance</font> property of CNN, each vector corresponds to a local image region, i.e. receptive field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8"> 根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> receptive </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. <font color=orangered>receptive</font> field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8"> 根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li><li>Restricted by the sizes of the <font color=orangered>receptive</font> fields, the feature sequence leverages limited image contexts.<span style="font-size:80%;opacity:0.8"> 受接收场的大小限制，特征序列利用有限的图像上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> ConvNet </td> <td>  </td> <td> 
<ul><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (<font color=forestgreen>ConvNet</font>) and a two-layer BLSTM network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8"> 编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> BLSTM </td> <td> [!≈ bi: el es ti: em] </td> <td> 
<ul><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer <font color=orangered>BLSTM</font> network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8"> 编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li><li>We further apply a two-layer Bidirectional Long-Short Term Memory (<font color=orangered>BLSTM</font>) [14, 13] network to the sequence, in order to model the long-term dependencies within the sequence.<span style="font-size:80%;opacity:0.8"> 我们进一步将两层双向长短期记忆（BLSTM）[14,13]网络应用于序列，以模拟序列内的长期依赖性。</span></li><li>The <font color=orangered>BLSTM</font> is a recurrent network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8"> BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li><li>On the top of the convolutional layers is a two-layer <font color=orangered>BLSTM</font> network, each LSTM has 256 hidden units.<span style="font-size:80%;opacity:0.8"> 在卷积层的顶部是两层BLSTM网络，每个LSTM具有256个隐藏单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> eo </td> <td>  </td> <td> 
<ul><li>The decoder generates a character sequence (including the <font color=forestgreen>EOS</font> token) conditioned on h.<span style="font-size:80%;opacity:0.8"> 解码器生成以h为条件的字符序列（包括EOS令牌）。</span></li><li>The label space includes all English alphanumeric characters, plus a special “end-ofsequence” (<font color=forestgreen>EOS</font>) token, which ends the generation process.<span style="font-size:80%;opacity:0.8"> 标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li><li>A prefix tree of three words: “ten”, “tea”, and “to”. $\epsilon$ and $\Omega$ are the tree root and the <font color=forestgreen>EOS</font> token respectively.<span style="font-size:80%;opacity:0.8">  $\epsilon$和$\Omega$分别是树根和EOS令牌。</span></li><li>Nodes on a path from the root to a leaf forms a word (including the <font color=forestgreen>EOS</font>).<span style="font-size:80%;opacity:0.8"> 从根到叶子的路径上的节点形成一个单词（包括EOS）。</span></li><li>For the decoder, we use a GRU cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 <font color=forestgreen>EOS</font> token).<span style="font-size:80%;opacity:0.8"> 对于解码器，我们使用具有256个存储器块和37个输出单元（26个字母，10个数字和1个EOS令牌）的GRU单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Long-Short </td> <td> [!≈ lɒŋ ʃɔ:t] </td> <td> 
<ul><li>We further apply a two-layer Bidirectional <font color=orangered>Long-Short</font> Term Memory (BLSTM) [14, 13] network to the sequence, in order to model the long-term dependencies within the sequence.<span style="font-size:80%;opacity:0.8"> 我们进一步将两层双向长短期记忆（BLSTM）[14,13]网络应用于序列，以模拟序列内的长期依赖性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> analyze </td> <td> ['ænəlaɪz] </td> <td> 
<ul><li>The BLSTM is a recurrent network that can <font color=orangered>analyze</font> the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8"> BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> recurrency </td> <td>  </td> <td> 
<ul><li>In the <font color=forestgreen>recurrency</font> part, we adopt the Gated Recurrent Unit (GRU) [7] as the cell.<span style="font-size:80%;opacity:0.8"> 在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> GRU </td> <td> [!≈ dʒi: ɑ:(r) ju:] </td> <td> 
<ul><li>In the recurrency part, we adopt the Gated Recurrent Unit (<font color=orangered>GRU</font>) [7] as the cell.<span style="font-size:80%;opacity:0.8"> 在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li><li>where $s_{t-1}$ is the state variable of the <font color=orangered>GRU</font> cell at the last step.<span style="font-size:80%;opacity:0.8"> 其中$s_{t-1}$是最后一步GRU单元的状态变量。</span></li><li>The state $s_{t-1}$ is updated via the recurrent process of <font color=orangered>GRU</font> [7, 8]:<span style="font-size:80%;opacity:0.8"> 状态$s_{t-1}$通过GRU [7,8]的循环过程更新：</span></li><li>For the decoder, we use a <font color=orangered>GRU</font> cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 EOS token).<span style="font-size:80%;opacity:0.8"> 对于解码器，我们使用具有256个存储器块和37个输出单元（26个字母，10个数字和1个EOS令牌）的GRU单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> linearly </td> <td> [ˈliniəli] </td> <td> 
<ul><li>Then, a glimpse $g_t$ is computed by <font color=orangered>linearly</font> combining the vectors in $h$: $g_t=\sum_{i=1}^L\alpha_{ti}h_i$.<span style="font-size:80%;opacity:0.8"> 然后，通过线性组合$h$：$g_t=\sum_{i=1}^L\alpha_{ti}h_i$中的向量来计算一瞥$g_t$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> alphanumeric </td> <td> [ˌælfənju:ˈmerɪk] </td> <td> 
<ul><li>The label space includes all English <font color=orangered>alphanumeric</font> characters, plus a special “end-ofsequence” (EOS) token, which ends the generation process.<span style="font-size:80%;opacity:0.8"> 标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> end-ofsequence </td> <td>  </td> <td> 
<ul><li>The label space includes all English alphanumeric characters, plus a special “<font color=forestgreen>end-ofsequence</font>” (EOS) token, which ends the generation process.<span style="font-size:80%;opacity:0.8"> 标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> log-likelihood </td> <td> [!≈ lɒg ˈlaɪklihʊd] </td> <td> 
<ul><li>To train the model, we minimize the negative <font color=orangered>log-likelihood</font> over X :<span style="font-size:80%;opacity:0.8"> 为了训练模型，我们最小化X上的负对数似然：</span></li><li>After each step, the list is updated to store the nodes with top-B accumulated <font color=orangered>log-likelihoods</font>, where B is the beam width.<span style="font-size:80%;opacity:0.8"> 在每个步骤之后，更新列表以存储具有前B累积对数似然的节点，其中B是波束宽度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> ADADELTA </td> <td> [!≈ eɪ di: eɪ di: i: el ti: eɪ] </td> <td> 
<ul><li>The optimization algorithm is the <font color=orangered>ADADELTA</font> [41], which we find fast in convergence speed.<span style="font-size:80%;opacity:0.8"> 优化算法是ADADELTA [41]，我们发现其收敛速度很快。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> convergence </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>The optimization algorithm is the ADADELTA [41], which we find fast in <font color=orangered>convergence</font> speed.<span style="font-size:80%;opacity:0.8"> 优化算法是ADADELTA [41]，我们发现其收敛速度很快。</span></li><li>Randomly initializing the localization network results in failure of <font color=orangered>convergence</font> during training.<span style="font-size:80%;opacity:0.8"> 随机初始化定位网络导致训练期间收敛失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> Empirically </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li><font color=orangered>Empirically</font>, we also find that the patterns displayed fig. 6. b and fig. 6. c yield relatively poorer performance.<span style="font-size:80%;opacity:0.8"> 根据经验，我们还发现图6.b和图6.c所示的模式产生相对较差的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> prefix </td> <td> [ˈpri:fɪks] </td> <td> 
<ul><li>A <font color=orangered>prefix</font> tree of three words: “ten”, “tea”, and “to”. $\epsilon$ and $\Omega$ are the tree root and the EOS token respectively.<span style="font-size:80%;opacity:0.8">  $\epsilon$和$\Omega$分别是树根和EOS令牌。</span></li><li>The motivation is that computation can be shared among words that share the same <font color=orangered>prefix</font>.<span style="font-size:80%;opacity:0.8"> 动机是计算可以在共享相同前缀的单词之间共享。</span></li><li>We first construct a <font color=orangered>prefix</font> tree over a given lexicon.<span style="font-size:80%;opacity:0.8"> 我们首先在给定的词典上构建一个pre fi x树。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> posterior </td> <td> [pɒˈstɪəriə(r)] </td> <td> 
<ul><li>At each step the <font color=orangered>posterior</font> probabilities of all child nodes are computed.<span style="font-size:80%;opacity:0.8"> 在每个步骤中，计算所有子节点的后验概率。</span></li><li>Numbers on the edges are the <font color=orangered>posterior</font> probabilities.<span style="font-size:80%;opacity:0.8"> 边缘上的数字是后验概率。</span></li><li>When a test image is associated with a lexicon, i.e. a set of words for selection, the recognition process is to pick the word with the highest <font color=orangered>posterior</font> conditional probability:<span style="font-size:80%;opacity:0.8"> 当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li><li>In testing, we start from the root node, every time the model outputs a distribution $\hat y_t$ , the child node with the highest <font color=orangered>posterior</font> probability is selected as the next node to move to.<span style="font-size:80%;opacity:0.8"> 在测试中，我们从根节点开始，每次模型输出分布$\hat y_t$时，选择具有最高后验概率的子节点作为要移动到的下一个节点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> conditional </td> <td> [kənˈdɪʃənl] </td> <td> 
<ul><li>When a test image is associated with a lexicon, i.e. a set of words for selection, the recognition process is to pick the word with the highest posterior <font color=orangered>conditional</font> probability:<span style="font-size:80%;opacity:0.8"> 当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> Hunspell </td> <td>  </td> <td> 
<ul><li>However, on very large lexicons, e.g. the <font color=forestgreen>Hunspell</font> [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8"> 但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>Besides, there is a “full lexicon” which contains all lexicon words, and the <font color=forestgreen>Hunspell</font> [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8"> 此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> incorporate </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>Recognition performance could be further improved by <font color=orangered>incorporating</font> beam search.<span style="font-size:80%;opacity:0.8"> 通过结合波束搜索可以进一步提高识别性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> top-B </td> <td> [!≈ tɒp bi:] </td> <td> 
<ul><li>After each step, the list is updated to store the nodes with <font color=orangered>top-B</font> accumulated log-likelihoods, where B is the beam width.<span style="font-size:80%;opacity:0.8"> 在每个步骤之后，更新列表以存储具有前B累积对数似然的节点，其中B是波束宽度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> synthetic </td> <td> [sɪnˈθetɪk] </td> <td> 
<ul><li>Our model is trained on the 8-million <font color=orangered>synthetic</font> samples released by Jaderberg et al. [15].<span style="font-size:80%;opacity:0.8"> 我们的模型在Jaderberg等人[15]发布的800万个合成样本上进行训练。</span></li><li>We use the same model trained on the <font color=orangered>synthetic</font> dataset without fine-tuning.<span style="font-size:80%;opacity:0.8"> 我们使用在合成数据集上训练的相同模型而不进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> resize </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>Following [17, 16], images are <font color=orangered>resized</font> to $100 \times 32$ in both training and testing.<span style="font-size:80%;opacity:0.8"> 在[17,16]之后，在训练和测试中将图像调整为$100 \times 32$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> epoch </td> <td> [ˈi:pɒk] </td> <td> 
<ul><li>Our model processes ~160 samples per second during training, and converges in 2 days after ~3 <font color=orangered>epochs</font> over the training dataset.<span style="font-size:80%;opacity:0.8"> 我们的模型在训练期间每秒处理~160个样本，并且在训练数据集的~3个时期之后的2天内收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> GPUaccelerated </td> <td>  </td> <td> 
<ul><li>Most parts of the model are <font color=forestgreen>GPUaccelerated</font>.<span style="font-size:80%;opacity:0.8"> 该模型的大多数部分都是GPU加速的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> Xeon </td> <td>  </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel <font color=forestgreen>Xeon</font>(R) E5-2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8"> 我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> NVIDIA </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an <font color=orangered>NVIDIA</font> GTX-Titan GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8"> 我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> GTX-Titan </td> <td>  </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an NVIDIA <font color=forestgreen>GTX-Titan</font> GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8"> 我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> RAM </td> <td> [ræm] </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and 64GB <font color=orangered>RAM</font>.<span style="font-size:80%;opacity:0.8"> 我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> k-word </td> <td> [!≈ keɪ wɜ:d] </td> <td> 
<ul><li>With a 50<font color=orangered>k-word</font> lexicon, the search takes ~200ms per image.<span style="font-size:80%;opacity:0.8"> 使用50k字的词典，每张图像搜索大约需要200毫秒。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> IIIT </td> <td> [!≈ aɪ aɪ aɪ ti:] </td> <td> 
<ul><li>• <font color=orangered>IIIT</font> 5K-Words [25] (IIIT5K) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8"> •IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> K-Words </td> <td> [!≈ keɪ wɜ:dz] </td> <td> 
<ul><li>• IIIT 5<font color=orangered>K-Words</font> [25] (IIIT5K) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8"> •IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> IIIT5K </td> <td>  </td> <td> 
<ul><li>• IIIT 5K-Words [25] (<font color=forestgreen>IIIT5K</font>) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8"> •IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li><li>On <font color=forestgreen>IIIT5K</font>, RARE outperforms prior art CRNN [32] by nearly 4 percentages, indicating a clear improvement in performance.<span style="font-size:80%;opacity:0.8"> 在IIIT5K上，RARE的性能比现有技术CRNN [32]高出近4个百分点，表明性能明显提高。</span></li><li>We observe that <font color=forestgreen>IIIT5K</font> contains a lot of irregular text, especially curved text, while RARE has an advantage in dealing with irregular text.<span style="font-size:80%;opacity:0.8"> 我们观察到IIIT5K包含大量不规则文本，尤其是弯曲文本，而RARE在处理不规则文本方面具有优势。</span></li><li>On <font color=forestgreen>IIIT5K</font>, SVT and IC03, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8"> 在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> SVT </td> <td> [!≈ es vi: ti:] </td> <td> 
<ul><li>• Street View Text [35] (<font color=orangered>SVT</font>) is collected from Google Street View.<span style="font-size:80%;opacity:0.8"> •街景文字[35]（SVT）是从Google街景中收集的。</span></li><li>Many images in <font color=orangered>SVT</font> are severely corrupted by noise and blur, or have very low resolutions.<span style="font-size:80%;opacity:0.8">  SVT中的许多图像受到噪声和模糊的严重破坏，或者具有非常低的分辨率。</span></li><li>On IIIT5K, <font color=orangered>SVT</font> and IC03, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8"> 在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li><li>Each image is associated with a 50-word lexicon, which is inherited from the <font color=orangered>SVT</font> [35] dataset.<span style="font-size:80%;opacity:0.8"> 每个图像都与一个50字的词典相关联，该词典继承自SVT [35]数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> ICDAR </td> <td> [!≈ aɪ si: di: eɪ ɑ:(r)] </td> <td> 
<ul><li>• <font color=orangered>ICDAR</font> 2003 [24] (IC03) contains 860 cropped word images, each associated with a 50-word lexicon defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8"> •ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>• <font color=orangered>ICDAR</font> 2013 [20] (IC13) inherits most of its samples from IC03.<span style="font-size:80%;opacity:0.8"> •ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> IC03 </td> <td>  </td> <td> 
<ul><li>• ICDAR 2003 [24] (<font color=forestgreen>IC03</font>) contains 860 cropped word images, each associated with a 50-word lexicon defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8"> •ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>• ICDAR 2013 [20] (IC13) inherits most of its samples from <font color=forestgreen>IC03</font>.<span style="font-size:80%;opacity:0.8"> •ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li><li>After filtering samples as done in <font color=forestgreen>IC03</font>, the dataset contains 857 samples.<span style="font-size:80%;opacity:0.8"> 在IC03中完成过滤样品后，数据集包含857个样品。</span></li><li>On IIIT5K, SVT and <font color=forestgreen>IC03</font>, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8"> 在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> non-alphanumeric </td> <td> [!≈ nɒn ˌælfənju:ˈmerɪk] </td> <td> 
<ul><li>Following [35], we discard images that contain <font color=orangered>non-alphanumeric</font> characters or have less than three characters.<span style="font-size:80%;opacity:0.8"> 按照[35]，我们丢弃包含非字母数字字符或少于三个字符的图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> IC13 </td> <td>  </td> <td> 
<ul><li>• ICDAR 2013 [20] (<font color=forestgreen>IC13</font>) inherits most of its samples from IC03.<span style="font-size:80%;opacity:0.8"> •ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> Tab </td> <td> [tæb] </td> <td> 
<ul><li>In <font color=orangered>Tab</font>. 1 we report our results, and compare them with other methods.<span style="font-size:80%;opacity:0.8"> 在表1中，我们报告实验结果，并与其他方法进行比较。</span></li><li>As reported in the last row of <font color=orangered>Tab</font>.<span style="font-size:80%;opacity:0.8"> 正如Tab的最后一行所报道的那样。</span></li><li><font color=orangered>Tab</font>. 2 summarizes the results.<span style="font-size:80%;opacity:0.8"> 表 2总结了结果。</span></li><li>Furthermore, recall the results in <font color=orangered>Tab</font>. 1, on SVTPerspective RARE outperforms [32] by a even larger margin.<span style="font-size:80%;opacity:0.8"> 此外，回顾表1中的结果，在SVTP上，RARE的表现优于[32]达到更大的余地。</span></li><li>From the results summarized in <font color=orangered>Tab</font>. 3, we see that RARE outperforms the other two methods by a large margin.<span style="font-size:80%;opacity:0.8"> 从表3中总结的结果，我们看到RARE的性能远远超过其他两种方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> k-dictionary </td> <td> [!≈ keɪ ˈdɪkʃənri] </td> <td> 
<ul><li>[17] only recognizes words that are in its 90<font color=orangered>k-dictionary</font>.<span style="font-size:80%;opacity:0.8"> [17]只识别其90k字典中的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> par </td> <td> [pɑ:(r)] </td> <td> 
<ul><li>On IIIT5K, SVT and IC03, constrained recognition accuracies are on <font color=orangered>par</font> with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8"> 在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> SRN-only </td> <td>  </td> <td> 
<ul><li>1, we see that the <font color=forestgreen>SRN-only</font> model is also a very competitive recognizer, achieving higher or competitive performance on most of the benchmarks.<span style="font-size:80%;opacity:0.8">  1，我们看到仅SRN模型也是一个非常有竞争力的识别器，在大多数基准测试中实现了更高或更具竞争力的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> validate </td> <td> [ˈvælɪdeɪt] </td> <td> 
<ul><li>To <font color=orangered>validate</font> the effectiveness of the rectification scheme, we evaluate RARE on the task of perspective text recognition.<span style="font-size:80%;opacity:0.8"> 为了验证整合方案的有效性，我们评估了RARE对透视文本识别的任务。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> SVT-perspective </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SVT-Perspective</font> [29] is specifically designed for evaluating performance of perspective text recognition algorithms.<span style="font-size:80%;opacity:0.8"> SVT-Perspective [29]专门用于评估透视文本识别算法的性能。</span></li><li>Text samples in <font color=forestgreen>SVT-Perspective</font> are picked from side view angles in Google Street View, thus most of them are heavily deformed by perspective distortion.<span style="font-size:80%;opacity:0.8"> SVT-Perspective中的文本样本是从Google街景中的侧视角中选取的，因此大多数文本样本都因透视变形而严重变形。</span></li><li>a. <font color=forestgreen>SVT-Perspective</font> consists of 639 cropped images for testing.<span style="font-size:80%;opacity:0.8">  SVT-Perspective由639个裁剪图像组成，用于测试。</span></li><li>Samples are taken from the <font color=forestgreen>SVT-Perspective</font> [29] dataset; b) Curved text. Samples are taken from the CUTE80 [30] dataset.<span style="font-size:80%;opacity:0.8"> 样本取自CUTE80 [30]数据集。</span></li><li>For comparison, we test the CRNN model [32] on <font color=forestgreen>SVT-Perspective</font>.<span style="font-size:80%;opacity:0.8"> 为了比较，我们在SVT-Perspective上测试CRNN模型[32]。</span></li><li>Table 2. Recognition accuracies on <font color=forestgreen>SVT-Perspective</font> [29].<span style="font-size:80%;opacity:0.8"> 表2. SVT-Perspective的识别准确度[29]。</span></li><li>The reason is that the <font color=forestgreen>SVT-perspective</font> dataset mainly consists of perspective text, which is inappropriate for direct recognition.<span style="font-size:80%;opacity:0.8"> 原因是SVT透视数据集主要由透视文本组成，不适合直接识别。</span></li><li>The first five rows are taken from <font color=forestgreen>SVT-Perspective</font> [29], the rest rows are taken from CUTE80 [30].<span style="font-size:80%;opacity:0.8"> 前五行取自SVT-透视图[29]，其余行取自CUTE80[30]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> deformed </td> <td> [dɪˈfɔ:md] </td> <td> 
<ul><li>Text samples in SVT-Perspective are picked from side view angles in Google Street View, thus most of them are heavily <font color=orangered>deformed</font> by perspective distortion.<span style="font-size:80%;opacity:0.8"> SVT-Perspective中的文本样本是从Google街景中的侧视角中选取的，因此大多数文本样本都因透视变形而严重变形。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> CUTE80 </td> <td>  </td> <td> 
<ul><li>Samples are taken from the SVT-Perspective [29] dataset; b) Curved text. Samples are taken from the <font color=forestgreen>CUTE80</font> [30] dataset.<span style="font-size:80%;opacity:0.8"> 样本取自CUTE80 [30]数据集。</span></li><li>The first five rows are taken from SVT-Perspective [29], the rest rows are taken from <font color=forestgreen>CUTE80</font> [30].<span style="font-size:80%;opacity:0.8"> 前五行取自SVT-透视图[29]，其余行取自CUTE80[30]。</span></li><li><font color=forestgreen>CUTE80</font> [30] focuses on the recognition of curved text.<span style="font-size:80%;opacity:0.8"> CUTE80 [30]专注于弯曲文本的识别。</span></li><li>Table 3. Recognition accuracies on <font color=forestgreen>CUTE80</font> [29].<span style="font-size:80%;opacity:0.8"> 表3.CUTE80上的识别准确度[29]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> SVTPerspective </td> <td>  </td> <td> 
<ul><li>Furthermore, recall the results in Tab. 1, on <font color=forestgreen>SVTPerspective</font> RARE outperforms [32] by a even larger margin.<span style="font-size:80%;opacity:0.8"> 此外，回顾表1中的结果，在SVTP上，RARE的表现优于[32]达到更大的余地。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> alleviate </td> <td> [əˈli:vieɪt] </td> <td> 
<ul><li>Our rectification scheme can significantly <font color=orangered>alleviate</font> this problem.<span style="font-size:80%;opacity:0.8"> 我们的整改计划可以显着缓解这一问题。</span></li><li>Generally, the rectification made by the STN is not perfect, but it <font color=orangered>alleviates</font> the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8"> 一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> gray-scale </td> <td> [ɡ'reɪsk'eɪl] </td> <td> 
<ul><li>The middle column is the rectified images (we use <font color=orangered>gray-scale</font> images for recognition).<span style="font-size:80%;opacity:0.8"> 中间一列是校正后的图像(我们使用灰度图像进行识别)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> mistakenly </td> <td> [mɪ'steɪkənlɪ] </td> <td> 
<ul><li>Green and red characters are correctly and <font color=orangered>mistakenly</font> recognized characters, respectively.<span style="font-size:80%;opacity:0.8"> 绿色和红色字符分别是正确和错误识别的字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> qualitative </td> <td> [ˈkwɒlɪtətɪv] </td> <td> 
<ul><li>In fig. 9 we present some <font color=orangered>qualitative</font> analysis.<span style="font-size:80%;opacity:0.8"> 在图9中，我们提出了一些定性分析。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> artistic-style </td> <td> [!≈ ɑ:ˈtɪstɪk staɪl] </td> <td> 
<ul><li>Curved text is a commonly seen <font color=orangered>artistic-style</font> text in natural scenes.<span style="font-size:80%;opacity:0.8"> 弯曲文本是自然场景中常见的艺术风格文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> advantageous </td> <td> [ˌædvənˈteɪdʒəs] </td> <td> 
<ul><li>Therefore, it is <font color=orangered>advantageous</font> on this task.<span style="font-size:80%;opacity:0.8"> 因此，在这项任务上是有利的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> acknowledgment </td> <td> [ək'nɒlɪdʒmənt] </td> <td> 
<ul><li><font color=orangered>Acknowledgments</font><span style="font-size:80%;opacity:0.8"> 致谢</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> NSFC </td> <td> [!≈ en es ef si:] </td> <td> 
<ul><li>This work was primarily supported by National Natural Science Foundation of China (<font color=orangered>NSFC</font>) (No. 61222308, No. 61573160 and No. 61503145), and Open Project Program of the State Key Laboratory of Digital Publishing Technology (No. F2016001).<span style="font-size:80%;opacity:0.8"> 本工作主要得到国家自然科学基金(61222308，61573160，61503145)和数字出版技术国家重点实验室开放项目(No. F2016001)的支持。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> F2016001 </td> <td>  </td> <td> 
<ul><li>This work was primarily supported by National Natural Science Foundation of China (NSFC) (No. 61222308, No. 61573160 and No. 61503145), and Open Project Program of the State Key Laboratory of Digital Publishing Technology (No. <font color=forestgreen>F2016001</font>).<span style="font-size:80%;opacity:0.8"> 本工作主要得到国家自然科学基金(61222308，61573160，61503145)和数字出版技术国家重点实验室开放项目(No. F2016001)的支持。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> lexicon<br>(39) </td> <td> [ˈleksɪkən] </td> <td> 
<ul><li>Following that, the detected characters are integrated into words by means of dynamic programming, <font color=orangered>lexicon</font> search [35], etc. Other work adopts top-down approaches, where text is directly recognized from entire input images, rather than detecting and recognizing individual characters.<span style="font-size:80%;opacity:0.8">之后，通过动态编程，词典搜索[35]等将检测到的字符集成到单词中。其他工作采用自上而下的方法，其中文本直接从整个输入图像中识别，而不是检测和识别单个字符。</span></li><li>3.4. Recognizing With a <font color=orangered>Lexicon</font><span style="font-size:80%;opacity:0.8">3.4 用词典识别</span></li><li>When a test image is associated with a <font color=orangered>lexicon</font>, i.e. a set of words for selection, the recognition process is to pick the word with the highest posterior conditional probability:<span style="font-size:80%;opacity:0.8">当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li><li>However, on very large <font color=orangered>lexicons</font>, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>We adopt an efficient approximate search scheme on large <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">我们在大词典上采用了有效的近似搜索方案。</span></li><li>We first construct a prefix tree over a given <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">我们首先在给定的词典上构建一个pre fi x树。</span></li><li>Since the tree depth is at most the length of the longest word in the <font color=orangered>lexicon</font>, this search process takes much less computation than the precise search.<span style="font-size:80%;opacity:0.8">由于树深度最多是词典中最长单词的长度，因此该搜索过程比精确搜索所需的计算量少得多。</span></li><li>The titles “50”, “1k” and “50k” are <font color=orangered>lexicon</font> sizes.<span style="font-size:80%;opacity:0.8">标题“50”，“1k”和“50k”是词典大小。</span></li><li>The “Full” <font color=orangered>lexicon</font> contains all per-image lexicon words.<span style="font-size:80%;opacity:0.8">“完整”词典包含所有每个图像的词典单词。</span></li><li>The “Full” lexicon contains all per-image <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8">“完整”词典包含所有每个图像的词典单词。</span></li><li>“None” means recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> “无”表示没有词典的识别。</span></li><li>Without a <font color=orangered>lexicon</font>, the model takes less than 2ms recognizing an image.<span style="font-size:80%;opacity:0.8">没有词典，模型识别图像所需的时间不到2毫秒。</span></li><li>With a <font color=orangered>lexicon</font>, recognition speed depends on the lexicon size.<span style="font-size:80%;opacity:0.8">使用词典，识别速度取决于词典大小。</span></li><li>With a lexicon, recognition speed depends on the <font color=orangered>lexicon</font> size.<span style="font-size:80%;opacity:0.8">使用词典，识别速度取决于词典大小。</span></li><li>We adopt the precise search (Sec. 3.4) when <font color=orangered>lexicon</font> size ≤ 1k.<span style="font-size:80%;opacity:0.8">当词典大小≤1k时，我们采用精确搜索（第3.4节）。</span></li><li>On larger <font color=orangered>lexicons</font>, we adopt the approximate beam search (Sec. 3.4) with a beam width of 7.<span style="font-size:80%;opacity:0.8">在较大的词典中，我们采用近似beam搜索（第3.4节），光束宽度为7。</span></li><li>With a 50k-word <font color=orangered>lexicon</font>, the search takes ~200ms per image.<span style="font-size:80%;opacity:0.8">使用50k字的词典，每张图像搜索大约需要200毫秒。</span></li><li>For each image, there is a 50-word <font color=orangered>lexicon</font> and a 1000-word lexicon.<span style="font-size:80%;opacity:0.8">对于每个图像，有一个50字的词典和一个1000字的词典。</span></li><li>For each image, there is a 50-word lexicon and a 1000-word <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">对于每个图像，有一个50字的词典和一个1000字的词典。</span></li><li>All <font color=orangered>lexicons</font> consist of a ground truth word and some randomly picked words.<span style="font-size:80%;opacity:0.8">所有词典都包含一个真实词和一些随机选择的词。</span></li><li>Each sample is associated with a 50-word <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">每个样本与50个单词的词典相关联。</span></li><li>• ICDAR 2003 [24] (IC03) contains 860 cropped word images, each associated with a 50-word <font color=orangered>lexicon</font> defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8">•ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>Besides, there is a “full <font color=orangered>lexicon</font>” which contains all lexicon words, and the Hunspell [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8">此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>Besides, there is a “full lexicon” which contains all <font color=orangered>lexicon</font> words, and the Hunspell [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8">此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>Besides, there is a “full lexicon” which contains all lexicon words, and the Hunspell [1] <font color=orangered>lexicon</font> which has 50k words.<span style="font-size:80%;opacity:0.8">此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li><li>On unconstrained recognition tasks (recognizing without a <font color=orangered>lexicon</font>), our model outperforms all the other methods in comparison.<span style="font-size:80%;opacity:0.8">在无约束的识别任务（没有词典识别）的情况下，我们的模型在比较中优于所有其他方法。</span></li><li>On constrained recognition tasks (recognizing with a <font color=orangered>lexicon</font>), RARE achieves state-of-the-art or highly competitive accuracies.<span style="font-size:80%;opacity:0.8">在受约束的识别任务（用词典识别）中，RARE实现了最先进或极具竞争力的准确性。</span></li><li>Each image is associated with a 50-word <font color=orangered>lexicon</font>, which is inherited from the SVT [35] dataset.<span style="font-size:80%;opacity:0.8">每个图像都与一个50字的词典相关联，该词典继承自SVT [35]数据集。</span></li><li>In addition, there is a “Full” <font color=orangered>lexicon</font> which contains all the per-image lexicon words.<span style="font-size:80%;opacity:0.8">此外，还有一个“Full”词典，其中包含所有每个图像的词典单词。</span></li><li>In addition, there is a “Full” lexicon which contains all the per-image <font color=orangered>lexicon</font> words.<span style="font-size:80%;opacity:0.8">此外，还有一个“Full”词典，其中包含所有每个图像的词典单词。</span></li><li>“50” and “Full” represent recognition with 50-word <font color=orangered>lexicons</font> and the full lexicon respectively.<span style="font-size:80%;opacity:0.8"> “50”和“Full”分别表示50字词典和完整词典的识别。</span></li><li>“50” and “Full” represent recognition with 50-word lexicons and the full <font color=orangered>lexicon</font> respectively.<span style="font-size:80%;opacity:0.8"> “50”和“Full”分别表示50字词典和完整词典的识别。</span></li><li>“None” represents recognition without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8"> “无”表示没有词典的识别。</span></li><li>In the second and third columns, we compare the accuracies of recognition with the 50-word <font color=orangered>lexicon</font> and the full lexicon.<span style="font-size:80%;opacity:0.8">在第二和第三列中，我们将识别的准确性与50字词汇和完整词典进行比较。</span></li><li>In the second and third columns, we compare the accuracies of recognition with the 50-word lexicon and the full <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">在第二和第三列中，我们将识别的准确性与50字词汇和完整词典进行比较。</span></li><li>Our method outperforms [29], which is a perspective text recognition method, by a large margin on both <font color=orangered>lexicons</font>.<span style="font-size:80%;opacity:0.8">我们的方法优于[29]，这是一种透视文本识别方法，在两个词典上都有很大的余地。</span></li><li>In the comparisons with [32], which uses the same training set as RARE, we still observe significant improvements in both the Full <font color=orangered>lexicon</font> and the lexicon-free settings.<span style="font-size:80%;opacity:0.8">在与使用与RARE相同的训练集的[32]的比较中，我们仍然观察到Full lexicon和Lexicon-free设置的显着改进。</span></li><li>All models are evaluated without a <font color=orangered>lexicon</font>.<span style="font-size:80%;opacity:0.8">所有模型都在没有词典的情况下进行评估。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> STN<br>(25) </td> <td> [!≈ es ti: en] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial Transformer Network (<font color=orangered>STN</font>) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8">RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial transformer network (<font color=orangered>STN</font>) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8">图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>The <font color=orangered>STN</font> transforms an input image to a rectified image, while the SRN recognizes text.<span style="font-size:80%;opacity:0.8">STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial Transformer Network [18] (<font color=orangered>STN</font>) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8">具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>In the <font color=orangered>STN</font>, an input image is spatially transformed into a rectified image.<span style="font-size:80%;opacity:0.8">在STN中，输入图像在空间上变换成校正后的图像。</span></li><li>Ideally, the <font color=orangered>STN</font> produces an image that contains regular text, which is a more appropriate input for the SRN than the original one.<span style="font-size:80%;opacity:0.8">在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>Consequently, for the <font color=orangered>STN</font>, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>In practice, the training eventually makes the <font color=orangered>STN</font> tend to produce images that contain regular text, which are desirable inputs for the SRN.<span style="font-size:80%;opacity:0.8">在实践中，训练最终会使STN倾向于产生包含常规文本的图像，这些图像正是SRN的理想输入。</span></li><li>Second, our model extends the <font color=orangered>STN</font> framework [18] with an attention-based model.<span style="font-size:80%;opacity:0.8">第二，我们的模型扩展了以注意为基础的STN框架的模型。</span></li><li>The original <font color=orangered>STN</font> is only tested on plain convolutional neural networks.<span style="font-size:80%;opacity:0.8">原本的STN仅在普通卷积神经网络上进行测试。</span></li><li>Moreover, it does not require extra annotations for the rectification process, since the <font color=orangered>STN</font> is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8">此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>The <font color=orangered>STN</font> transforms an input image I to a rectified image $I^\prime$ with a predicted TPS transformation.<span style="font-size:80%;opacity:0.8">STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>A distinctive property of <font color=orangered>STN</font> is that its sampler is differentiable.<span style="font-size:80%;opacity:0.8">STN的一个独特属性是其采样器是可微分的。</span></li><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the <font color=orangered>STN</font> can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8">因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>Structure of the <font color=orangered>STN</font>. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The sampler produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8">定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>Instead, the training of the localization network is completely supervised by the gradients propagated by the other parts of the <font color=orangered>STN</font>, following the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8">相反，定位网络的训练完全受到STN其他部分传播的梯度的监督，遵循反向传播算法[22]。</span></li><li>The <font color=orangered>STN</font> is able to rectify images that contain these types of irregular text, making them more readable for the following recognizer.<span style="font-size:80%;opacity:0.8">STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>Figure 4. The <font color=orangered>STN</font> rectifies images that contain several types of irregular text.<span style="font-size:80%;opacity:0.8">图4. STN重新构建包含多种不规则文本的图像。</span></li><li>The <font color=orangered>STN</font> can deal with several types of irregular text, including (a) loosely-bounded text; (b) multi-oriented text; (c) perspective text; (d) curved text.<span style="font-size:80%;opacity:0.8">STN可以处理几种类型的不规则文本，包括（a）松散有界的文本; （b）多方面文本; （c）透视文本; （d）弯曲文本。</span></li><li>where the probability $p(\cdot)$ is computed by Eq. 8, $\theta$ is the parameters of both <font color=orangered>STN</font> and SRN.<span style="font-size:80%;opacity:0.8">其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>Spatial Transformer Network The localization network of <font color=orangered>STN</font> has 4 convolution layers, each followed by a $2 \times 2$ max-pooling layer.<span style="font-size:80%;opacity:0.8">空间变换器网络STN的定位网络有4个卷积层，每个卷层都有一个$2 \times 2$最大池层。</span></li><li>The output size of the <font color=orangered>STN</font> is also $100 \times 32$.<span style="font-size:80%;opacity:0.8">STN的输出大小也是$100 \times 32$。</span></li><li>fiducial points predicted by the <font color=orangered>STN</font> are plotted on input images in green crosses.<span style="font-size:80%;opacity:0.8">由STN预测的基准点被绘制在绿色十字架的输入图像上。</span></li><li>We see that the <font color=orangered>STN</font> tends to place fiducial points along upper and lower edges of scene text, and<span style="font-size:80%;opacity:0.8">我们看到STN倾向于沿场景文本的上下边缘放置特定点，并且</span></li><li>Generally, the rectification made by the <font color=orangered>STN</font> is not perfect, but it alleviates the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8">一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> fiducial<br>(24) </td> <td> [fɪ'dju:ʃjəl] </td> <td> 
<ul><li>The TPS transformation is configured by a set of <font color=orangered>fiducial</font> points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8">TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS <font color=orangered>fiducial</font> points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>As illustrated in fig. 2, it first predicts a set of <font color=orangered>fiducial</font> points via its localization network.<span style="font-size:80%;opacity:0.8">如图2所示，它首先通过其定位网络预测一组特定点。</span></li><li>Then, inside the grid generator, it calculates the TPS transformation parameters from the <font color=orangered>fiducial</font> points, and generates a sampling grid on I.<span style="font-size:80%;opacity:0.8">然后，在网格生成器内部，它从各个点计算TPS变换参数，并在I上生成采样网格。</span></li><li>Structure of the STN. The localization network localizes a set of <font color=orangered>fiducial</font> points C, with which the grid generator generates a sampling grid P. The sampler produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8">定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>The localization network localizes K <font color=orangered>fiducial</font> points by directly regressing their $x,y$ -coordinates.<span style="font-size:80%;opacity:0.8">本地化网络通过直接回归其 $x,y$-coordinates来定位K个点。</span></li><li>The coordinates are denoted by $C=[c_1,\cdots, c_K] \in R^{2 \times K}$ , whose k-th column $c_k=[x_k,y_k]^T$ contains the coordinates of the k-th <font color=orangered>fiducial</font> point.<span style="font-size:80%;opacity:0.8">坐标由 $C=[c_1,\cdots, c_K] \in R^{2 \times K}$表示，其第k列包含第k个特定点的坐标。</span></li><li>The network localizes <font color=orangered>fiducial</font> points based on global image contexts.<span style="font-size:80%;opacity:0.8">网络基于全局图像上下文定位各个点。</span></li><li>It is expected to capture the overall text shape of an input image, and localizes <font color=orangered>fiducial</font> points accordingly.<span style="font-size:80%;opacity:0.8">期望捕获输入图像的整体文本形状，并相应地定位各个点。</span></li><li>It should be emphasized that we do not annotate coordinates of <font color=orangered>fiducial</font> points for any sample.<span style="font-size:80%;opacity:0.8">应该强调的是，我们不会为任何样本注释各个点的坐标。</span></li><li>We first define another set of <font color=orangered>fiducial</font> points, called the base fiducial points, denoted by $C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$.<span style="font-size:80%;opacity:0.8">我们首先定义了另一组特殊点，称为基本点，由$C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$表示。</span></li><li>We first define another set of fiducial points, called the base <font color=orangered>fiducial</font> points, denoted by $C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$.<span style="font-size:80%;opacity:0.8">我们首先定义了另一组特殊点，称为基本点，由$C^\prime=[c_1^\prime,\cdots, c_K^\prime] \in R^{2 \times K}$表示。</span></li><li>As illustrated in fig. 3, the base <font color=orangered>fiducial</font> points are evenly distributed along the top and bottom edge of a rectified image $I^\prime$.<span style="font-size:80%;opacity:0.8">如图3所示，基本金属点沿着矫正图像$I^\prime$的顶部和底部边缘均匀分布。</span></li><li>Figure 3. <font color=orangered>fiducial</font> points and the TPS transformation.<span style="font-size:80%;opacity:0.8">图3.基准点和TPS转换。</span></li><li>Green markers on the left image are the <font color=orangered>fiducial</font> points C.<span style="font-size:80%;opacity:0.8">左侧图像上的绿色标记是C点。</span></li><li>Cyan markers on the right image are the base <font color=orangered>fiducial</font> points $C^\prime$.<span style="font-size:80%;opacity:0.8">右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li><li>where $d_{i,k}$ is the euclidean distance between $p^\prime_i$ and the $k$-th base <font color=orangered>fiducial</font> point $c^\prime_k$ .<span style="font-size:80%;opacity:0.8">其中$d_{i,k}$是$p^\prime_i$和第k个基本点$c^\prime_k$之间的欧氏距离。</span></li><li>Green markers are the predicted <font color=orangered>fiducial</font> points on the input images.<span style="font-size:80%;opacity:0.8">绿色标记是输入图像上预测的特定点。</span></li><li>Figure 6. Some initialization patterns for the <font color=orangered>fiducial</font> points.<span style="font-size:80%;opacity:0.8">图6.各个点的一些初始化模式。</span></li><li>The initial biases are set to such values that yield the <font color=orangered>fiducial</font> points pattern displayed in fig. 6. a.<span style="font-size:80%;opacity:0.8">初始偏差设置为这样的值，产生图6.a中显示的金属点图案。</span></li><li>We set the number of <font color=orangered>fiducial</font> points to $K=20$ , meaning that the localization network outputs a 40-dimensional vector.<span style="font-size:80%;opacity:0.8">我们将多个点的数量设置为$K=20$，这意味着定位网络输出一个40维向量。</span></li><li>The left column is the input images, where green crosses are the predicted <font color=orangered>fiducial</font> points.<span style="font-size:80%;opacity:0.8">左列是输入图像，其中绿色十字是预测的基准点。</span></li><li><font color=orangered>fiducial</font> points predicted by the STN are plotted on input images in green crosses.<span style="font-size:80%;opacity:0.8">由STN预测的基准点被绘制在绿色十字架的输入图像上。</span></li><li>We see that the STN tends to place <font color=orangered>fiducial</font> points along upper and lower edges of scene text, and<span style="font-size:80%;opacity:0.8">我们看到STN倾向于沿场景文本的上下边缘放置特定点，并且</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> SRN<br>(20) </td> <td> [!≈ es ɑ:(r) en] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial Transformer Network (STN) and a Sequence Recognition Network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8">RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more “readable” image for the following <font color=orangered>SRN</font>, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8">在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial transformer network (STN) and a sequence recognition network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8">图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>The STN transforms an input image to a rectified image, while the <font color=orangered>SRN</font> recognizes text.<span style="font-size:80%;opacity:0.8">STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial Transformer Network [18] (STN) and a Sequence Recognition Network (<font color=orangered>SRN</font>).<span style="font-size:80%;opacity:0.8">具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>Ideally, the STN produces an image that contains regular text, which is a more appropriate input for the <font color=orangered>SRN</font> than the original one.<span style="font-size:80%;opacity:0.8">在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>Motivated by this, for the <font color=orangered>SRN</font> we construct an attention-based model [4] that recognizes text in a sequence recognition approach.<span style="font-size:80%;opacity:0.8">受此启发，我们构建了SRN，这是一种在序列识别中采用了注意力的模型。</span></li><li>The <font color=orangered>SRN</font> consists of an encoder and a decoder.<span style="font-size:80%;opacity:0.8">SRN由编码器和解码器构成。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>In practice, the training eventually makes the STN tend to produce images that contain regular text, which are desirable inputs for the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8">在实践中，训练最终会使STN倾向于产生包含常规文本的图像，这些图像正是SRN的理想输入。</span></li><li>Third, our model adopts a convolutional-recurrent structure in the encoder of the <font color=orangered>SRN</font>, thus is a novel variant of the attention-based model [4].<span style="font-size:80%;opacity:0.8">第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li><li>Moreover, it does not require extra annotations for the rectification process, since the STN is supervised by the <font color=orangered>SRN</font> during training.<span style="font-size:80%;opacity:0.8">此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>The input to the <font color=orangered>SRN</font> is a rectified image $I^\prime$ , which ideally contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8">SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li><li>In our model, the <font color=orangered>SRN</font> is an attention-based model [4, 8], which directly recognizes a sequence from an input image.<span style="font-size:80%;opacity:0.8">在我们的模型中，SRN是一种基于注意力的模型[4,8]，它直接识别来自输入图像的序列。</span></li><li>The <font color=orangered>SRN</font> consists of an encoder and a decoder.<span style="font-size:80%;opacity:0.8"> SRN由编码器和解码器组成。</span></li><li>Structure of the <font color=orangered>SRN</font>, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer BLSTM network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8">编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li><li>The <font color=orangered>SRN</font> directly maps a input sequence to another sequence.<span style="font-size:80%;opacity:0.8">SRN直接将输入序列映射到另一个序列。</span></li><li>where the probability $p(\cdot)$ is computed by Eq. 8, $\theta$ is the parameters of both STN and <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8">其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>Sequence Recognition Network In the <font color=orangered>SRN</font>, the encoder has 7 convolutional layers, whose {filter size, number of filters, stride, padding size} are respectively {3,64,1,1}, {3,128,1,1}, {3,256,1,1}, {3,256,1,1,}, {3,512,1,1}, {3,512,1,1}, and {2,512,1,0}.<span style="font-size:80%;opacity:0.8">序列识别网络在SRN中，编码器有7个卷积层，其{滤波器大小，滤波器数量，步幅，填充大小}分别为{3,64,1,1}，{3,128,1,1}，{3,256 ，1,1}，{3,256,1,1，}，{3,512,1,1}，{3,512,1,1}和{2,512,1,0}。</span></li><li>We also train and test a model that contains only the <font color=orangered>SRN</font>.<span style="font-size:80%;opacity:0.8">我们还训练和测试仅包含SRN的模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> rectify<br>(19) </td> <td> [ˈrektɪfaɪ] </td> <td> 
<ul><li>In testing, an image is firstly <font color=orangered>rectified</font> via a predicted Thin-Plate-Spline (TPS) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8">在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>The STN transforms an input image to a <font color=orangered>rectified</font> image, while the SRN recognizes text.<span style="font-size:80%;opacity:0.8">STN将输入图像变换为矫正图像，而SRN识别文本。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to <font color=orangered>rectify</font> input images into ones that are more “readable” by recognizers.<span style="font-size:80%;opacity:0.8">这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>In the STN, an input image is spatially transformed into a <font color=orangered>rectified</font> image.<span style="font-size:80%;opacity:0.8">在STN中，输入图像在空间上变换成校正后的图像。</span></li><li>The transformation is a thinplate-spline [6] (TPS) transformation, whose nonlinearity allows us to <font color=orangered>rectify</font> various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8">STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li><li>Phan et al. propose to explicitly <font color=orangered>rectify</font> perspective distortions via SIFT [23] descriptor matching.<span style="font-size:80%;opacity:0.8">潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li><li>Our method <font color=orangered>rectifies</font> several types of irregular text in a unified way.<span style="font-size:80%;opacity:0.8">我们的方法以统一的方式重新定义了几种不规则文本。</span></li><li>The STN transforms an input image I to a <font color=orangered>rectified</font> image $I^\prime$ with a predicted TPS transformation.<span style="font-size:80%;opacity:0.8">STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>The sampler takes both the grid and the input image, it produces a <font color=orangered>rectified</font> image $I^\prime$ by sampling on the grid points.<span style="font-size:80%;opacity:0.8">采样器同时采用网格和输入图像，通过对网格点进行采样，生成一个矫正的图像$I^\prime$。</span></li><li>Structure of the STN. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The sampler produces a <font color=orangered>rectified</font> image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8">定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>As illustrated in fig. 3, the base fiducial points are evenly distributed along the top and bottom edge of a <font color=orangered>rectified</font> image $I^\prime$.<span style="font-size:80%;opacity:0.8">如图3所示，基本金属点沿着矫正图像$I^\prime$的顶部和底部边缘均匀分布。</span></li><li>The grid of pixels on a <font color=orangered>rectified</font> image $I^\prime$ is denoted by $P^\prime = {p_i^\prime}_{i=1,\cdots,N}$ , where $p_i^\prime = {x_i^\prime, y_i^\prime}^T$ is the x,y coordinates of the i-th pixel, N is the number of pixels.<span style="font-size:80%;opacity:0.8">矫正图像上的像素网格由$P^\prime = {p_i^\prime}_{i=1,\cdots,N}$表示，其中$p_i^\prime = {x_i^\prime, y_i^\prime}^T$是第i个像素的x，y坐标，N是像素数。</span></li><li>By setting all pixel values, we get the <font color=orangered>rectified</font> image $T^\prime$ :<span style="font-size:80%;opacity:0.8">通过设置所有像素值，我们得到了矫正的图像$T^\prime$：</span></li><li>The ﬂexibility of the TPS transformation allows us to transform irregular text images into <font color=orangered>rectified</font> images that contain regular text.<span style="font-size:80%;opacity:0.8">TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li><li>The STN is able to <font color=orangered>rectify</font> images that contain these types of irregular text, making them more readable for the following recognizer.<span style="font-size:80%;opacity:0.8">STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>The input to the SRN is a <font color=orangered>rectified</font> image $I^\prime$ , which ideally contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8">SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li><li>Figure 4. The STN <font color=orangered>rectifies</font> images that contain several types of irregular text.<span style="font-size:80%;opacity:0.8">图4. STN重新构建包含多种不规则文本的图像。</span></li><li>The middle column is the <font color=orangered>rectified</font> images (we use gray-scale images for recognition).<span style="font-size:80%;opacity:0.8">中间一列是校正后的图像(我们使用灰度图像进行识别)。</span></li><li>Our model <font color=orangered>rectifies</font> images that contain curved text before recognizing them.<span style="font-size:80%;opacity:0.8">我们的模型在识别包含弯曲文本的图像之前对其进行校正。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> rectification<br>(11) </td> <td> [ˌrektɪfɪ'keɪʃn] </td> <td> 
<ul><li>Robust Scene Text Recognition with Automatic <font color=orangered>Rectification</font><span style="font-size:80%;opacity:0.8">具有自动校正的可靠场景文本识别器</span></li><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text recognizer with Automatic <font color=orangered>REctification</font>), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8">与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Zhang et al. [42] propose a character <font color=orangered>rectification</font> method that leverages the low-rank structures of text.<span style="font-size:80%;opacity:0.8">张等人 [42]提出了一种利用文本的低等级结构的字符整理方法。</span></li><li>Moreover, it does not require extra annotations for the <font color=orangered>rectification</font> process, since the STN is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8">此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li><li>To validate the effectiveness of the <font color=orangered>rectification</font> scheme, we evaluate RARE on the task of perspective text recognition.<span style="font-size:80%;opacity:0.8">为了验证整合方案的有效性，我们评估了RARE对透视文本识别的任务。</span></li><li>Our <font color=orangered>rectification</font> scheme can significantly alleviate this problem.<span style="font-size:80%;opacity:0.8">我们的整改计划可以显着缓解这一问题。</span></li><li>Figure 9. Examples showing the <font color=orangered>rectifications</font> our model makes and the recognition results.<span style="font-size:80%;opacity:0.8">图9. 示例显示了我们的模型所做的纠正和识别结果。</span></li><li>In Fig. 9, we demonstrate the effect of <font color=orangered>rectification</font> through some examples.<span style="font-size:80%;opacity:0.8">在图9中，我们通过一些例子演示了整改的效果。</span></li><li>Generally, the <font color=orangered>rectification</font> made by the STN is not perfect, but it alleviates the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8">一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li><li>Traditional solutions typically use a separate text <font color=orangered>rectification</font> component.<span style="font-size:80%;opacity:0.8">传统的解决方案通常使用单独的文本校正组件。</span></li><li>The extensive experimental results show that 1) without geometric supervision, the learned model can automatically generate more “readable” images for both human and the sequence recognition network; 2) the proposed text <font color=orangered>rectification</font> method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8">大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> TPS<br>(10) </td> <td> [!≈ ti: pi: es] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (<font color=orangered>TPS</font>) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8">在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>The transformation is a thinplate-spline [6] (<font color=orangered>TPS</font>) transformation, whose nonlinearity allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8">STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li><li>The <font color=orangered>TPS</font> transformation is configured by a set of fiducial points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8">TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the <font color=orangered>TPS</font> fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>The STN transforms an input image I to a rectified image $I^\prime$ with a predicted <font color=orangered>TPS</font> transformation.<span style="font-size:80%;opacity:0.8">STN将输入图像I转换为具有预测的TPS变换的矫正图像$I^\prime$。</span></li><li>Then, inside the grid generator, it calculates the <font color=orangered>TPS</font> transformation parameters from the fiducial points, and generates a sampling grid on I.<span style="font-size:80%;opacity:0.8">然后，在网格生成器内部，它从各个点计算TPS变换参数，并在I上生成采样网格。</span></li><li>The grid generator estimates the <font color=orangered>TPS</font> transformation parameters, and generates a sampling grid.<span style="font-size:80%;opacity:0.8">网格生成器估计TPS变换参数，并生成采样网格。</span></li><li>Figure 3. fiducial points and the <font color=orangered>TPS</font> transformation.<span style="font-size:80%;opacity:0.8">图3.基准点和TPS转换。</span></li><li>The parameters of the <font color=orangered>TPS</font> transformation is represented by a matrix $T \in \Re^{2 \times (K+3)}$ , which is computed by<span style="font-size:80%;opacity:0.8">TPS变换的参数由矩阵表示，其由下式计算</span></li><li>The ﬂexibility of the <font color=orangered>TPS</font> transformation allows us to transform irregular text images into rectified images that contain regular text.<span style="font-size:80%;opacity:0.8">TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> sequential<br>(8) </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>It bares some resemblance to a <font color=orangered>sequential</font> signal.<span style="font-size:80%;opacity:0.8">它有点类似于顺序信号。</span></li><li>Given an input image, the encoder generates a <font color=orangered>sequential</font> feature representation, which is a sequence of feature vectors.<span style="font-size:80%;opacity:0.8">编码器将输入的图像表示成序列的特征，即一系列的特征向量。</span></li><li>Su and Lu [34] extract <font color=orangered>sequential</font> image representation, which is a sequence of HOG [10] descriptors, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8">Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>We extract a <font color=orangered>sequential</font> representation from $I^\prime$ , and recognize a word from it.<span style="font-size:80%;opacity:0.8">我们从$I^\prime$中提取顺序表示，并从中识别出一个单词。</span></li><li>The encoder extracts a <font color=orangered>sequential</font> representation from the input image $I^\prime$.<span style="font-size:80%;opacity:0.8">编码器从输入图像$I^\prime$中提取顺序表示。</span></li><li>The decoder recurrently generates a sequence conditioned on the <font color=orangered>sequential</font> representation, by decoding the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8">解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li><li>A na¨ıve approach for extracting a <font color=orangered>sequential</font> representation for $I^\prime$ is to take local image patches from left to right, and describe each of them with a CNN.<span style="font-size:80%;opacity:0.8">用于提取$I^\prime$的顺序表示的一种简单方法是从左到右获取图像中的局部图像块，并用CNN描述每个图像块。</span></li><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer BLSTM network to extract a <font color=orangered>sequential</font> representation (h) for the input image.<span style="font-size:80%;opacity:0.8">编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> SVT-perspective<br>(8) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>SVT-Perspective</font> [29] is specifically designed for evaluating performance of perspective text recognition algorithms.<span style="font-size:80%;opacity:0.8">SVT-Perspective [29]专门用于评估透视文本识别算法的性能。</span></li><li>Text samples in <font color=forestgreen>SVT-Perspective</font> are picked from side view angles in Google Street View, thus most of them are heavily deformed by perspective distortion.<span style="font-size:80%;opacity:0.8">SVT-Perspective中的文本样本是从Google街景中的侧视角中选取的，因此大多数文本样本都因透视变形而严重变形。</span></li><li>a. <font color=forestgreen>SVT-Perspective</font> consists of 639 cropped images for testing.<span style="font-size:80%;opacity:0.8"> SVT-Perspective由639个裁剪图像组成，用于测试。</span></li><li>Samples are taken from the <font color=forestgreen>SVT-Perspective</font> [29] dataset; b) Curved text. Samples are taken from the CUTE80 [30] dataset.<span style="font-size:80%;opacity:0.8">样本取自CUTE80 [30]数据集。</span></li><li>For comparison, we test the CRNN model [32] on <font color=forestgreen>SVT-Perspective</font>.<span style="font-size:80%;opacity:0.8">为了比较，我们在SVT-Perspective上测试CRNN模型[32]。</span></li><li>Table 2. Recognition accuracies on <font color=forestgreen>SVT-Perspective</font> [29].<span style="font-size:80%;opacity:0.8">表2. SVT-Perspective的识别准确度[29]。</span></li><li>The reason is that the <font color=forestgreen>SVT-perspective</font> dataset mainly consists of perspective text, which is inappropriate for direct recognition.<span style="font-size:80%;opacity:0.8">原因是SVT透视数据集主要由透视文本组成，不适合直接识别。</span></li><li>The first five rows are taken from <font color=forestgreen>SVT-Perspective</font> [29], the rest rows are taken from CUTE80 [30].<span style="font-size:80%;opacity:0.8">前五行取自SVT-透视图[29]，其余行取自CUTE80[30]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> transformer<br>(7) </td> <td> [trænsˈfɔ:mə(r)] </td> <td> 
<ul><li>RARE is a specially designed deep neural network, which consists of a Spatial <font color=orangered>Transformer</font> Network (STN) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8">RARE是一种特殊设计的深度神经网络，由空间变换网络（STN）和序列识别网络（SRN）组成。</span></li><li>Figure 1. Schematic overview of RARE, which consists a spatial <font color=orangered>transformer</font> network (STN) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8">图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li><li>Specifically, we construct a deep neural network that combines a Spatial <font color=orangered>Transformer</font> Network [18] (STN) and a Sequence Recognition Network (SRN).<span style="font-size:80%;opacity:0.8">具体而言，我们构建了一个深度神经网络，它结合了空间变换器网络[18]（STN）和序列识别网络（SRN）。</span></li><li>3.1. Spatial <font color=orangered>Transformer</font> Network<span style="font-size:80%;opacity:0.8">3.1 空间变换网络</span></li><li>Spatial <font color=orangered>Transformer</font> Network The localization network of STN has 4 convolution layers, each followed by a $2 \times 2$ max-pooling layer.<span style="font-size:80%;opacity:0.8">空间变换器网络STN的定位网络有4个卷积层，每个卷层都有一个$2 \times 2$最大池层。</span></li><li>We address this problem in a more feasible and elegant way by adopting a differentiable spatial <font color=orangered>transformer</font> network module.<span style="font-size:80%;opacity:0.8">我们通过采用可区分的空间变换网络模块，以一种更可行和更优雅的方式解决了这个问题。</span></li><li>In addition, the spatial <font color=orangered>transformer</font> network is connected to an attention-based sequence recognizer, allowing us to train the whole model end-to-end.<span style="font-size:80%;opacity:0.8">此外，空间变换器网络连接到基于注意力的序列识别器，允许我们端到端地训练整个模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> recurrent<br>(7) </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of HOG [10] descriptors, and predict the corresponding character sequence with a <font color=orangered>recurrent</font> neural network (RNN).<span style="font-size:80%;opacity:0.8">Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>Instead, following [32], we build a network that combines convolutional layers and <font color=orangered>recurrent</font> networks.<span style="font-size:80%;opacity:0.8">相反，在[32]之后，我们构建了一个结合了卷积层和循环网络的网络。</span></li><li>The BLSTM is a <font color=orangered>recurrent</font> network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8">BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li><li>3.2.2 Decoder: <font color=orangered>Recurrent</font> Character Generator<span style="font-size:80%;opacity:0.8">3.2.2解码器：循环字符发生器</span></li><li>It is a <font color=orangered>recurrent</font> neural network with the attention structure proposed in [4, 8].<span style="font-size:80%;opacity:0.8">它是一种递归神经网络，具有[4,8]中提出的注意结构。</span></li><li>In the recurrency part, we adopt the Gated <font color=orangered>Recurrent</font> Unit (GRU) [7] as the cell.<span style="font-size:80%;opacity:0.8">在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li><li>The state $s_{t-1}$ is updated via the <font color=orangered>recurrent</font> process of GRU [7, 8]:<span style="font-size:80%;opacity:0.8">状态$s_{t-1}$通过GRU [7,8]的循环过程更新：</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> recognizer<br>(6) </td> <td> ['rekəgnaɪzə] </td> <td> 
<ul><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character placement, etc. We propose RARE (Robust text <font color=orangered>recognizer</font> with Automatic REctification), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8">与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Usually, a text <font color=orangered>recognizer</font> works best when its input images contain tightly-bounded regular text.<span style="font-size:80%;opacity:0.8">通常，文本识别器在其输入图像包含紧密有界的常规文本时效果最佳。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to rectify input images into ones that are more “readable” by <font color=orangered>recognizers</font>.<span style="font-size:80%;opacity:0.8">这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>The STN is able to rectify images that contain these types of irregular text, making them more readable for the following <font color=orangered>recognizer</font>.<span style="font-size:80%;opacity:0.8">STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>1, we see that the SRN-only model is also a very competitive <font color=orangered>recognizer</font>, achieving higher or competitive performance on most of the benchmarks.<span style="font-size:80%;opacity:0.8"> 1，我们看到仅SRN模型也是一个非常有竞争力的识别器，在大多数基准测试中实现了更高或更具竞争力的性能。</span></li><li>In addition, the spatial transformer network is connected to an attention-based sequence <font color=orangered>recognizer</font>, allowing us to train the whole model end-to-end.<span style="font-size:80%;opacity:0.8">此外，空间变换器网络连接到基于注意力的序列识别器，允许我们端到端地训练整个模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> sampler<br>(6) </td> <td> [ˈsɑ:mplə(r)] </td> <td> 
<ul><li>The <font color=orangered>sampler</font> takes both the grid and the input image, it produces a rectified image $I^\prime$ by sampling on the grid points.<span style="font-size:80%;opacity:0.8">采样器同时采用网格和输入图像，通过对网格点进行采样，生成一个矫正的图像$I^\prime$。</span></li><li>A distinctive property of STN is that its <font color=orangered>sampler</font> is differentiable.<span style="font-size:80%;opacity:0.8">STN的一个独特属性是其采样器是可微分的。</span></li><li>Structure of the STN. The localization network localizes a set of fiducial points C, with which the grid generator generates a sampling grid P. The <font color=orangered>sampler</font> produces a rectified image $I^\prime$ , given I and P.<span style="font-size:80%;opacity:0.8">定位网络定位一组特定点C，网格生成器利用该集合点生成采样网格P.给定I和P时，采样器产生一个矫正的图像$I^\prime$。</span></li><li>3.1.3 <font color=orangered>Sampler</font><span style="font-size:80%;opacity:0.8">3.1.3 采样器</span></li><li>Lastly, in the <font color=orangered>sampler</font>, the pixel value of $p^\prime_i$ is bilinearly interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8">最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li><li>where $V$ represents the bilinear <font color=orangered>sampler</font> [18], which is also a differentiable module.<span style="font-size:80%;opacity:0.8">其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> differentiable<br>(6) </td> <td> [ˌdɪfə'renʃɪəbl] </td> <td> 
<ul><li>A distinctive property of STN is that its sampler is <font color=orangered>differentiable</font>.<span style="font-size:80%;opacity:0.8">STN的一个独特属性是其采样器是可微分的。</span></li><li>Therefore, once we have a <font color=orangered>differentiable</font> localization network and a differentiable grid generator, the STN can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8">因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>Therefore, once we have a differentiable localization network and a <font color=orangered>differentiable</font> grid generator, the STN can back-propagate error differentials and gets trained.<span style="font-size:80%;opacity:0.8">因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>The grid generator can back-propagate gradients, since its two matrix multiplications, Eq. 1 and Eq. 4, are both <font color=orangered>differentiable</font>.<span style="font-size:80%;opacity:0.8">网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>where $V$ represents the bilinear sampler [18], which is also a <font color=orangered>differentiable</font> module.<span style="font-size:80%;opacity:0.8">其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li><li>We address this problem in a more feasible and elegant way by adopting a <font color=orangered>differentiable</font> spatial transformer network module.<span style="font-size:80%;opacity:0.8">我们通过采用可区分的空间变换网络模块，以一种更可行和更优雅的方式解决了这个问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> eo<br>(5) </td> <td>  </td> <td> 
<ul><li>The decoder generates a character sequence (including the <font color=forestgreen>EOS</font> token) conditioned on h.<span style="font-size:80%;opacity:0.8">解码器生成以h为条件的字符序列（包括EOS令牌）。</span></li><li>The label space includes all English alphanumeric characters, plus a special “end-ofsequence” (<font color=forestgreen>EOS</font>) token, which ends the generation process.<span style="font-size:80%;opacity:0.8">标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li><li>A prefix tree of three words: “ten”, “tea”, and “to”. $\epsilon$ and $\Omega$ are the tree root and the <font color=forestgreen>EOS</font> token respectively.<span style="font-size:80%;opacity:0.8"> $\epsilon$和$\Omega$分别是树根和EOS令牌。</span></li><li>Nodes on a path from the root to a leaf forms a word (including the <font color=forestgreen>EOS</font>).<span style="font-size:80%;opacity:0.8">从根到叶子的路径上的节点形成一个单词（包括EOS）。</span></li><li>For the decoder, we use a GRU cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 <font color=forestgreen>EOS</font> token).<span style="font-size:80%;opacity:0.8">对于解码器，我们使用具有256个存储器块和37个输出单元（26个字母，10个数字和1个EOS令牌）的GRU单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> Tab<br>(5) </td> <td> [tæb] </td> <td> 
<ul><li>In <font color=orangered>Tab</font>. 1 we report our results, and compare them with other methods.<span style="font-size:80%;opacity:0.8">在表1中，我们报告实验结果，并与其他方法进行比较。</span></li><li>As reported in the last row of <font color=orangered>Tab</font>.<span style="font-size:80%;opacity:0.8">正如Tab的最后一行所报道的那样。</span></li><li><font color=orangered>Tab</font>. 2 summarizes the results.<span style="font-size:80%;opacity:0.8">表 2总结了结果。</span></li><li>Furthermore, recall the results in <font color=orangered>Tab</font>. 1, on SVTPerspective RARE outperforms [32] by a even larger margin.<span style="font-size:80%;opacity:0.8">此外，回顾表1中的结果，在SVTP上，RARE的表现优于[32]达到更大的余地。</span></li><li>From the results summarized in <font color=orangered>Tab</font>. 3, we see that RARE outperforms the other two methods by a large margin.<span style="font-size:80%;opacity:0.8">从表3中总结的结果，我们看到RARE的性能远远超过其他两种方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> readable<br>(4) </td> <td> [ˈri:dəbl] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted Thin-Plate-Spline (TPS) transformation, into a more “<font color=orangered>readable</font>” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8">在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li><li>This motivates us to apply a spatial transformation prior to recognition, in order to rectify input images into ones that are more “<font color=orangered>readable</font>” by recognizers.<span style="font-size:80%;opacity:0.8">这促使我们在识别之前应用空间变换，以便将输入图像校正为识别器更“可读”的图像。</span></li><li>The STN is able to rectify images that contain these types of irregular text, making them more <font color=orangered>readable</font> for the following recognizer.<span style="font-size:80%;opacity:0.8">STN能够纠正包含这些类型的不规则文本的图像，使其对于以下识别器更具可读性。</span></li><li>The extensive experimental results show that 1) without geometric supervision, the learned model can automatically generate more “<font color=orangered>readable</font>” images for both human and the sequence recognition network; 2) the proposed text rectification method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8">大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> i.e.<br>(4) </td> <td> [ˌaɪ ˈi:] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any geometric ground truth, <font color=orangered>i.e.</font> the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, <font color=orangered>i.e.</font> receptive field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8">根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li><li>where $l_{t-1}$ is the $t-1$-th ground-truth label in training, while in testing, it is the label predicted in the previous step, <font color=orangered>i.e.</font> $\hat l_{t-1}$ .<span style="font-size:80%;opacity:0.8">其中$l_{t-1}$是训练中第$t-1$个真实标签，而在测试中，它是上一步中预测的标签，即$\hat l_{t-1}$。</span></li><li>When a test image is associated with a lexicon, <font color=orangered>i.e.</font> a set of words for selection, the recognition process is to pick the word with the highest posterior conditional probability:<span style="font-size:80%;opacity:0.8">当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> descriptor<br>(4) </td> <td> [dɪˈskrɪptə(r)] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of HOG [10] <font color=orangered>descriptors</font>, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8">Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li><li>Yao et al. [38] firstly propose the multi-oriented text detection problem, and deal with it by carefully designing rotation-invariant region <font color=orangered>descriptors</font>.<span style="font-size:80%;opacity:0.8">姚等人[38]首先提出了多方向文本检测问题，并通过仔细设计旋转不变区域描述符来处理它。</span></li><li>Phan et al. propose to explicitly rectify perspective distortions via SIFT [23] <font color=orangered>descriptor</font> matching.<span style="font-size:80%;opacity:0.8">潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. receptive field, and is a <font color=orangered>descriptor</font> for that region.<span style="font-size:80%;opacity:0.8">根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> Eq<br>(4) </td> <td>  </td> <td> 
<ul><li>The grid generator can back-propagate gradients, since its two matrix multiplications, <font color=forestgreen>Eq</font>. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8">网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>The grid generator can back-propagate gradients, since its two matrix multiplications, Eq. 1 and <font color=forestgreen>Eq</font>. 4, are both differentiable.<span style="font-size:80%;opacity:0.8">网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li><li>where the probability $p(\cdot)$ is computed by <font color=forestgreen>Eq</font>. 8, $\theta$ is the parameters of both STN and SRN.<span style="font-size:80%;opacity:0.8">其中概率$p(\cdot)$由方程式8计算，$\theta$是STN和SRN的参数。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing <font color=forestgreen>Eq</font>. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> BLSTM<br>(4) </td> <td> [!≈ bi: el es ti: em] </td> <td> 
<ul><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (ConvNet) and a two-layer <font color=orangered>BLSTM</font> network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8">编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li><li>We further apply a two-layer Bidirectional Long-Short Term Memory (<font color=orangered>BLSTM</font>) [14, 13] network to the sequence, in order to model the long-term dependencies within the sequence.<span style="font-size:80%;opacity:0.8">我们进一步将两层双向长短期记忆（BLSTM）[14,13]网络应用于序列，以模拟序列内的长期依赖性。</span></li><li>The <font color=orangered>BLSTM</font> is a recurrent network that can analyze the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8">BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li><li>On the top of the convolutional layers is a two-layer <font color=orangered>BLSTM</font> network, each LSTM has 256 hidden units.<span style="font-size:80%;opacity:0.8">在卷积层的顶部是两层BLSTM网络，每个LSTM具有256个隐藏单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> GRU<br>(4) </td> <td> [!≈ dʒi: ɑ:(r) ju:] </td> <td> 
<ul><li>In the recurrency part, we adopt the Gated Recurrent Unit (<font color=orangered>GRU</font>) [7] as the cell.<span style="font-size:80%;opacity:0.8">在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li><li>where $s_{t-1}$ is the state variable of the <font color=orangered>GRU</font> cell at the last step.<span style="font-size:80%;opacity:0.8">其中$s_{t-1}$是最后一步GRU单元的状态变量。</span></li><li>The state $s_{t-1}$ is updated via the recurrent process of <font color=orangered>GRU</font> [7, 8]:<span style="font-size:80%;opacity:0.8">状态$s_{t-1}$通过GRU [7,8]的循环过程更新：</span></li><li>For the decoder, we use a <font color=orangered>GRU</font> cell that has 256 memory blocks and 37 output units (26 letters, 10 digits, and 1 EOS token).<span style="font-size:80%;opacity:0.8">对于解码器，我们使用具有256个存储器块和37个输出单元（26个字母，10个数字和1个EOS令牌）的GRU单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> posterior<br>(4) </td> <td> [pɒˈstɪəriə(r)] </td> <td> 
<ul><li>At each step the <font color=orangered>posterior</font> probabilities of all child nodes are computed.<span style="font-size:80%;opacity:0.8">在每个步骤中，计算所有子节点的后验概率。</span></li><li>Numbers on the edges are the <font color=orangered>posterior</font> probabilities.<span style="font-size:80%;opacity:0.8">边缘上的数字是后验概率。</span></li><li>When a test image is associated with a lexicon, i.e. a set of words for selection, the recognition process is to pick the word with the highest <font color=orangered>posterior</font> conditional probability:<span style="font-size:80%;opacity:0.8">当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li><li>In testing, we start from the root node, every time the model outputs a distribution $\hat y_t$ , the child node with the highest <font color=orangered>posterior</font> probability is selected as the next node to move to.<span style="font-size:80%;opacity:0.8">在测试中，我们从根节点开始，每次模型输出分布$\hat y_t$时，选择具有最高后验概率的子节点作为要移动到的下一个节点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> IIIT5K<br>(4) </td> <td>  </td> <td> 
<ul><li>• IIIT 5K-Words [25] (<font color=forestgreen>IIIT5K</font>) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8">•IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li><li>On <font color=forestgreen>IIIT5K</font>, RARE outperforms prior art CRNN [32] by nearly 4 percentages, indicating a clear improvement in performance.<span style="font-size:80%;opacity:0.8">在IIIT5K上，RARE的性能比现有技术CRNN [32]高出近4个百分点，表明性能明显提高。</span></li><li>We observe that <font color=forestgreen>IIIT5K</font> contains a lot of irregular text, especially curved text, while RARE has an advantage in dealing with irregular text.<span style="font-size:80%;opacity:0.8">我们观察到IIIT5K包含大量不规则文本，尤其是弯曲文本，而RARE在处理不规则文本方面具有优势。</span></li><li>On <font color=forestgreen>IIIT5K</font>, SVT and IC03, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8">在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> SVT<br>(4) </td> <td> [!≈ es vi: ti:] </td> <td> 
<ul><li>• Street View Text [35] (<font color=orangered>SVT</font>) is collected from Google Street View.<span style="font-size:80%;opacity:0.8">•街景文字[35]（SVT）是从Google街景中收集的。</span></li><li>Many images in <font color=orangered>SVT</font> are severely corrupted by noise and blur, or have very low resolutions.<span style="font-size:80%;opacity:0.8"> SVT中的许多图像受到噪声和模糊的严重破坏，或者具有非常低的分辨率。</span></li><li>On IIIT5K, <font color=orangered>SVT</font> and IC03, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8">在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li><li>Each image is associated with a 50-word lexicon, which is inherited from the <font color=orangered>SVT</font> [35] dataset.<span style="font-size:80%;opacity:0.8">每个图像都与一个50字的词典相关联，该词典继承自SVT [35]数据集。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> IC03<br>(4) </td> <td>  </td> <td> 
<ul><li>• ICDAR 2003 [24] (<font color=forestgreen>IC03</font>) contains 860 cropped word images, each associated with a 50-word lexicon defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8">•ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>• ICDAR 2013 [20] (IC13) inherits most of its samples from <font color=forestgreen>IC03</font>.<span style="font-size:80%;opacity:0.8">•ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li><li>After filtering samples as done in <font color=forestgreen>IC03</font>, the dataset contains 857 samples.<span style="font-size:80%;opacity:0.8">在IC03中完成过滤样品后，数据集包含857个样品。</span></li><li>On IIIT5K, SVT and <font color=forestgreen>IC03</font>, constrained recognition accuracies are on par with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8">在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> CUTE80<br>(4) </td> <td>  </td> <td> 
<ul><li>Samples are taken from the SVT-Perspective [29] dataset; b) Curved text. Samples are taken from the <font color=forestgreen>CUTE80</font> [30] dataset.<span style="font-size:80%;opacity:0.8">样本取自CUTE80 [30]数据集。</span></li><li>The first five rows are taken from SVT-Perspective [29], the rest rows are taken from <font color=forestgreen>CUTE80</font> [30].<span style="font-size:80%;opacity:0.8">前五行取自SVT-透视图[29]，其余行取自CUTE80[30]。</span></li><li><font color=forestgreen>CUTE80</font> [30] focuses on the recognition of curved text.<span style="font-size:80%;opacity:0.8">CUTE80 [30]专注于弯曲文本的识别。</span></li><li>Table 3. Recognition accuracies on <font color=forestgreen>CUTE80</font> [29].<span style="font-size:80%;opacity:0.8">表3.CUTE80上的识别准确度[29]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> e.g.<br>(3) </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>In natural scenes, text appears on various kinds of objects, <font color=orangered>e.g.</font> road signs, billboards, and product packaging.<span style="font-size:80%;opacity:0.8">在自然场景中，文本出现在各种对象上，例如道路标志，广告牌和产品包装。</span></li><li>However, on very large lexicons, <font color=orangered>e.g.</font> the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>In the future, we plan to address the end-to-end scene text reading problem through the combination of RARE with a scene text detection method, <font color=orangered>e.g.</font> [43].<span style="font-size:80%;opacity:0.8">未来，我们计划通过将RARE与场景文本检测方法相结合来解决端到端场景文本阅读问题，例如[43]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> recurrently<br>(3) </td> <td> [rɪ'kʌrəntlɪ] </td> <td> 
<ul><li>The decoder <font color=orangered>recurrently</font> generates a character sequence conditioning on the input sequence, by decoding the relevant contents which are determined by its attention mechanism at each step.<span style="font-size:80%;opacity:0.8">解码器会根据注意力机制进行解码，循序地生成识别出的字符序列。</span></li><li>The decoder <font color=orangered>recurrently</font> generates a sequence conditioned on the sequential representation, by decoding the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8">解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li><li>The decoder <font color=orangered>recurrently</font> generates a sequence of characters, conditioned on the sequence produced by the encoder.<span style="font-size:80%;opacity:0.8">解码器以编码器产生的序列为条件，反复生成一系列字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> leverage<br>(3) </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>Zhang et al. [42] propose a character rectification method that <font color=orangered>leverages</font> the low-rank structures of text.<span style="font-size:80%;opacity:0.8">张等人 [42]提出了一种利用文本的低等级结构的字符整理方法。</span></li><li>Besides, the spatial dependencies between the patches are not exploited and <font color=orangered>leveraged</font>.<span style="font-size:80%;opacity:0.8">此外，图像块之间的空间依赖性未被利用和利用。</span></li><li>Restricted by the sizes of the receptive fields, the feature sequence <font color=orangered>leverages</font> limited image contexts.<span style="font-size:80%;opacity:0.8">受接收场的大小限制，特征序列利用有限的图像上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> marker<br>(3) </td> <td> [ˈmɑ:kə(r)] </td> <td> 
<ul><li>Green <font color=orangered>markers</font> on the left image are the fiducial points C.<span style="font-size:80%;opacity:0.8">左侧图像上的绿色标记是C点。</span></li><li>Cyan <font color=orangered>markers</font> on the right image are the base fiducial points $C^\prime$.<span style="font-size:80%;opacity:0.8">右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li><li>Green <font color=orangered>markers</font> are the predicted fiducial points on the input images.<span style="font-size:80%;opacity:0.8">绿色标记是输入图像上预测的特定点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> iterate<br>(3) </td> <td> [ˈɪtəreɪt] </td> <td> 
<ul><li>By <font color=orangered>iterating</font> over all points in $P^\prime$ , we generate a grid $P=\{p_i\}_{i=1,\cdots,N}$ on the input image $I$.<span style="font-size:80%;opacity:0.8">通过迭代$P^\prime$中的所有点，我们在输入图像$I$上生成网格$P=\{p_i\}_{i=1,\cdots,N}$。</span></li><li>The process <font color=orangered>iterates</font> until a leaf node is reached.<span style="font-size:80%;opacity:0.8">该过程将迭代，直到到达叶节点。</span></li><li>However, on very large lexicons, e.g. the Hunspell [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires <font color=orangered>iterating</font> over all lexicon words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> arbitrary<br>(3) </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>The network extracts a sequence of feature vectors, given an input image of <font color=orangered>arbitrary</font> size.<span style="font-size:80%;opacity:0.8">给定任意大小的输入图像，网络提取特征向量序列。</span></li><li>Both input and output sequences may have <font color=orangered>arbitrary</font> lengths.<span style="font-size:80%;opacity:0.8">输入和输出序列都可以具有任意长度。</span></li><li>[32] is able to recognize <font color=orangered>arbitrary</font> words, but it does not have a specific mechanism for handling curved text.<span style="font-size:80%;opacity:0.8">[32]能够识别任意单词，但没有处理弯曲文本的特定机制。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> prefix<br>(3) </td> <td> [ˈpri:fɪks] </td> <td> 
<ul><li>A <font color=orangered>prefix</font> tree of three words: “ten”, “tea”, and “to”. $\epsilon$ and $\Omega$ are the tree root and the EOS token respectively.<span style="font-size:80%;opacity:0.8"> $\epsilon$和$\Omega$分别是树根和EOS令牌。</span></li><li>The motivation is that computation can be shared among words that share the same <font color=orangered>prefix</font>.<span style="font-size:80%;opacity:0.8">动机是计算可以在共享相同前缀的单词之间共享。</span></li><li>We first construct a <font color=orangered>prefix</font> tree over a given lexicon.<span style="font-size:80%;opacity:0.8">我们首先在给定的词典上构建一个pre fi x树。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> placement<br>(2) </td> <td> [ˈpleɪsmənt] </td> <td> 
<ul><li>Different from those in documents, words in natural images often possess irregular shapes, which are caused by perspective distortion, curved character <font color=orangered>placement</font>, etc. We propose RARE (Robust text recognizer with Automatic REctification), a recognition model that is robust to irregular text.<span style="font-size:80%;opacity:0.8">与文档中的文字不同，自然图像中的文字通常具有不规则形状，这是由透视扭曲，弯曲字符放置等引起的。我们提出了RARE（具有自动重整功能的可靠文本识别器），这是一种对不规则文本具有可靠性的识别模型。</span></li><li>Due to its irregular character <font color=orangered>placement</font>, recognizing curved text is very challenging.<span style="font-size:80%;opacity:0.8">由于其不规则的字符放置，识别弯曲文本是非常具有挑战性的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> side-view<br>(2) </td> <td> ['saɪdvj'u:] </td> <td> 
<ul><li>For example, some scene text is perspective text [29], which is caused by <font color=orangered>side-view</font> camera angles; some has curved shapes, meaning that its characters are placed along curves rather than straight lines.<span style="font-size:80%;opacity:0.8">例如，一些场景文本是透视文本[29]，它是由侧视摄像机角度引起的;有些具有弯曲的形状，这意味着它的角色沿着曲线而不是直线放置。</span></li><li>In fig. 4, we show some common types of irregular text, including a) loosely-bounded text, which resulted by imperfect text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by <font color=orangered>side-view</font> camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8">在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> back-propagated<br>(2) </td> <td> [!≈ bæk ˈprɔpəɡeitid] </td> <td> 
<ul><li>The dashed lines represent the ﬂows of the <font color=orangered>back-propagated</font> gradients.<span style="font-size:80%;opacity:0.8">虚线表示反向传播的梯度的流动。</span></li><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials <font color=orangered>back-propagated</font> by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> ideally<br>(2) </td> <td> [aɪ'di:əlɪ] </td> <td> 
<ul><li><font color=orangered>Ideally</font>, the STN produces an image that contains regular text, which is a more appropriate input for the SRN than the original one.<span style="font-size:80%;opacity:0.8">在理想情况下，STN产生的图像是一类常规的文本图像，这比原来的不规则的文本图像更合适输入到SRN中。</span></li><li>The input to the SRN is a rectified image $I^\prime$ , which <font color=orangered>ideally</font> contains a word that is written horizontally from left to right.<span style="font-size:80%;opacity:0.8">SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> regress<br>(2) </td> <td> [rɪˈgres] </td> <td> 
<ul><li>The TPS transformation is configured by a set of fiducial points, whose coordinates are <font color=orangered>regressed</font> by a convolutional neural network.<span style="font-size:80%;opacity:0.8">TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li><li>The localization network localizes K fiducial points by directly <font color=orangered>regressing</font> their $x,y$ -coordinates.<span style="font-size:80%;opacity:0.8">本地化网络通过直接回归其 $x,y$-coordinates来定位K个点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> decode<br>(2) </td> <td> [ˌdi:ˈkəʊd] </td> <td> 
<ul><li>The decoder recurrently generates a character sequence conditioning on the input sequence, by <font color=orangered>decoding</font> the relevant contents which are determined by its attention mechanism at each step.<span style="font-size:80%;opacity:0.8">解码器会根据注意力机制进行解码，循序地生成识别出的字符序列。</span></li><li>The decoder recurrently generates a sequence conditioned on the sequential representation, by <font color=orangered>decoding</font> the relevant contents it attends to at each step.<span style="font-size:80%;opacity:0.8">解码器通过解码在每个步骤中所关注的相关内容，循环地生成以顺序表示为条件的序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> geometric<br>(2) </td> <td> [ˌdʒi:əˈmetrɪk] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any <font color=orangered>geometric</font> ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error differentials back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>The extensive experimental results show that 1) without <font color=orangered>geometric</font> supervision, the learned model can automatically generate more “readable” images for both human and the sequence recognition network; 2) the proposed text rectification method can significantly improve recognition accuracies on irregular scene text; 3) the proposed scene text recognition system is competitive compared with the state-of-the-arts.<span style="font-size:80%;opacity:0.8">大量的实验结果表明，1)在没有几何监督的情况下，学习模型可以自动为人类和序列识别网络生成更“可读”的图像；2)提出的文本校正方法可以显著提高不规则场景文本的识别准确率；3)与现有技术相比，提出的场景文本识别系统具有竞争力。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> differential<br>(2) </td> <td> [ˌdɪfəˈrenʃl] </td> <td> 
<ul><li>Consequently, for the STN, we do not need to label any geometric ground truth, i.e. the positions of the TPS fiducial points, but let its training be supervised by the error <font color=orangered>differentials</font> back-propagated by the SRN.<span style="font-size:80%;opacity:0.8">因此，对于STN，我们不需要标注任何几何位置，比如，TPS基准点的位置。 STN的训练可以由SRN反向传播的误差进行迭代。</span></li><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the STN can back-propagate error <font color=orangered>differentials</font> and gets trained.<span style="font-size:80%;opacity:0.8">因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> convolutional-recurrent<br>(2) </td> <td> [!≈ kɒnvə'lu:ʃənəl rɪˈkʌrənt] </td> <td> 
<ul><li>Third, our model adopts a <font color=orangered>convolutional-recurrent</font> structure in the encoder of the SRN, thus is a novel variant of the attention-based model [4].<span style="font-size:80%;opacity:0.8">第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li><li>3.2.1 Encoder: <font color=orangered>Convolutional-Recurrent</font> Network<span style="font-size:80%;opacity:0.8">3.2.1编码器：卷积 - 循环网络</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> Jaderberg<br>(2) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Jaderberg</font> et al. [17] address text recognition with a 90k-class convolutional neural network, where each class corresponds to an English word.<span style="font-size:80%;opacity:0.8">Jaderberg等人[17]使用90k级卷积神经网络进行文本识别，其中每个类对应一个英语单词。</span></li><li>Our model is trained on the 8-million synthetic samples released by <font color=forestgreen>Jaderberg</font> et al. [15].<span style="font-size:80%;opacity:0.8">我们的模型在Jaderberg等人[15]发布的800万个合成样本上进行训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> unconstrained<br>(2) </td> <td> [ˌʌnkən'streɪnd] </td> <td> 
<ul><li>In [16], a CNN with a structured output layer is constructed for <font color=orangered>unconstrained</font> text recognition.<span style="font-size:80%;opacity:0.8">在[16]中，构造具有结构化输出层的CNN用于无约束文本识别。</span></li><li>On <font color=orangered>unconstrained</font> recognition tasks (recognizing without a lexicon), our model outperforms all the other methods in comparison.<span style="font-size:80%;opacity:0.8">在无约束的识别任务（没有词典识别）的情况下，我们的模型在比较中优于所有其他方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> back-propagate<br>(2) </td> <td> [!≈ bæk ˈprɒpəgeɪt] </td> <td> 
<ul><li>Therefore, once we have a differentiable localization network and a differentiable grid generator, the STN can <font color=orangered>back-propagate</font> error differentials and gets trained.<span style="font-size:80%;opacity:0.8">因此，一旦我们拥有可区分的定位网络和可微分的网格生成器，STN就可以反向传播误差并进行训练。</span></li><li>The grid generator can <font color=orangered>back-propagate</font> gradients, since its two matrix multiplications, Eq. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8">网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> normalize<br>(2) </td> <td> [ˈnɔ:məlaɪz] </td> <td> 
<ul><li>We use a <font color=orangered>normalized</font> coordinate system whose origin is the image center, so that $x_k, y_k$ are within the range of $[-1, 1]$ .<span style="font-size:80%;opacity:0.8">我们使用归一化坐标系，其原点是图像中心，因此$x_k, y_k$在$[-1, 1]$的范围内。</span></li><li>Since K is a constant and the coordinate system is <font color=orangered>normalized</font>, $C^\prime$ is always a constant.<span style="font-size:80%;opacity:0.8">由于K是常数并且坐标系被归一化，因此$C^\prime$始终是常数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> euclidean<br>(2) </td> <td> [ju:ˈklidiən] </td> <td> 
<ul><li>where the element on the i-th row and j-th column of R is $r_{i,j}=d_{i,j}^2$ , $d_{i,j}$ is the <font color=orangered>euclidean</font> distance between $c_i^\prime$ and $c_j^\prime$ .<span style="font-size:80%;opacity:0.8">其中R的第i行和第j列的元素是$r_{i,j}=d_{i,j}^2$，$d_{i,j}$是$c_i^\prime$和$c_j^\prime$之间的欧氏距离。</span></li><li>where $d_{i,k}$ is the <font color=orangered>euclidean</font> distance between $p^\prime_i$ and the $k$-th base fiducial point $c^\prime_k$ .<span style="font-size:80%;opacity:0.8">其中$d_{i,k}$是$p^\prime_i$和第k个基本点$c^\prime_k$之间的欧氏距离。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> loosely-bounded<br>(2) </td> <td> [!≈ ˈlu:sli 'baʊndɪd] </td> <td> 
<ul><li>In fig. 4, we show some common types of irregular text, including a) <font color=orangered>loosely-bounded</font> text, which resulted by imperfect text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by side-view camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8">在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li><li>The STN can deal with several types of irregular text, including (a) <font color=orangered>loosely-bounded</font> text; (b) multi-oriented text; (c) perspective text; (d) curved text.<span style="font-size:80%;opacity:0.8">STN可以处理几种类型的不规则文本，包括（a）松散有界的文本; （b）多方面文本; （c）透视文本; （d）弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> receptive<br>(2) </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>According to the translation invariance property of CNN, each vector corresponds to a local image region, i.e. <font color=orangered>receptive</font> field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8">根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li><li>Restricted by the sizes of the <font color=orangered>receptive</font> fields, the feature sequence leverages limited image contexts.<span style="font-size:80%;opacity:0.8">受接收场的大小限制，特征序列利用有限的图像上下文。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> log-likelihood<br>(2) </td> <td> [!≈ lɒg ˈlaɪklihʊd] </td> <td> 
<ul><li>To train the model, we minimize the negative <font color=orangered>log-likelihood</font> over X :<span style="font-size:80%;opacity:0.8">为了训练模型，我们最小化X上的负对数似然：</span></li><li>After each step, the list is updated to store the nodes with top-B accumulated <font color=orangered>log-likelihoods</font>, where B is the beam width.<span style="font-size:80%;opacity:0.8">在每个步骤之后，更新列表以存储具有前B累积对数似然的节点，其中B是波束宽度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> convergence<br>(2) </td> <td> [kən'vɜ:dʒəns] </td> <td> 
<ul><li>The optimization algorithm is the ADADELTA [41], which we find fast in <font color=orangered>convergence</font> speed.<span style="font-size:80%;opacity:0.8">优化算法是ADADELTA [41]，我们发现其收敛速度很快。</span></li><li>Randomly initializing the localization network results in failure of <font color=orangered>convergence</font> during training.<span style="font-size:80%;opacity:0.8">随机初始化定位网络导致训练期间收敛失败。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> Hunspell<br>(2) </td> <td>  </td> <td> 
<ul><li>However, on very large lexicons, e.g. the <font color=forestgreen>Hunspell</font> [1] which contains more than 50k words, computing Eq. 10 is time consuming, as it requires iterating over all lexicon words.<span style="font-size:80%;opacity:0.8">但是，对于非常大的词典，例如Hunspell [1]包含超过50k字，计算Eq。 10是耗时的，因为它需要迭代所有词典单词。</span></li><li>Besides, there is a “full lexicon” which contains all lexicon words, and the <font color=forestgreen>Hunspell</font> [1] lexicon which has 50k words.<span style="font-size:80%;opacity:0.8">此外，还有一个包含所有词典单词的“完整词典”和包含50k单词的Hunspell [1]词典。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> synthetic<br>(2) </td> <td> [sɪnˈθetɪk] </td> <td> 
<ul><li>Our model is trained on the 8-million <font color=orangered>synthetic</font> samples released by Jaderberg et al. [15].<span style="font-size:80%;opacity:0.8">我们的模型在Jaderberg等人[15]发布的800万个合成样本上进行训练。</span></li><li>We use the same model trained on the <font color=orangered>synthetic</font> dataset without fine-tuning.<span style="font-size:80%;opacity:0.8">我们使用在合成数据集上训练的相同模型而不进行微调。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> ICDAR<br>(2) </td> <td> [!≈ aɪ si: di: eɪ ɑ:(r)] </td> <td> 
<ul><li>• <font color=orangered>ICDAR</font> 2003 [24] (IC03) contains 860 cropped word images, each associated with a 50-word lexicon defined by Wang et al. [35].<span style="font-size:80%;opacity:0.8">•ICDAR 2003 [24]（IC03）包含860个裁剪的单词图像，每个图像与Wang等人[35]定义的50个单词的词典相关联。</span></li><li>• <font color=orangered>ICDAR</font> 2013 [20] (IC13) inherits most of its samples from IC03.<span style="font-size:80%;opacity:0.8">•ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> alleviate<br>(2) </td> <td> [əˈli:vieɪt] </td> <td> 
<ul><li>Our rectification scheme can significantly <font color=orangered>alleviate</font> this problem.<span style="font-size:80%;opacity:0.8">我们的整改计划可以显着缓解这一问题。</span></li><li>Generally, the rectification made by the STN is not perfect, but it <font color=orangered>alleviates</font> the recognition difficulty to some extent.<span style="font-size:80%;opacity:0.8">一般来说，STN所做的纠正并不完美，但在一定程度上缓解了识别的困难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> unsolved<br>(1) </td> <td> [ˌʌnˈsɒlvd] </td> <td> 
<ul><li>Recognizing text in natural images is a challenging task with many <font color=orangered>unsolved</font> problems.<span style="font-size:80%;opacity:0.8">识别自然图像中的文本是一项具有挑战性的任务，存在许多未解决的问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> Thin-Plate-Spline<br>(1) </td> <td> [!≈ θɪn pleɪt splaɪn] </td> <td> 
<ul><li>In testing, an image is firstly rectified via a predicted <font color=orangered>Thin-Plate-Spline</font> (TPS) transformation, into a more “readable” image for the following SRN, which recognizes text through a sequence recognition approach.<span style="font-size:80%;opacity:0.8">在测试中，图像首先通过预测的薄板样条（TPS）插值变换矫正为更加“可读”的图像，用于后续SRN，通过序列识别方法识别文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> trainable<br>(1) </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>RARE is end-to-end <font color=orangered>trainable</font>, requiring only images and associated text labels, making it convenient to train and deploy the model in practical systems.<span style="font-size:80%;opacity:0.8">RARE是端到端的可训练的，只需要图像和相关的文本标签，便于在实际系统中训练和部署模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> billboard<br>(1) </td> <td> [ˈbɪlbɔ:d] </td> <td> 
<ul><li>In natural scenes, text appears on various kinds of objects, e.g. road signs, <font color=orangered>billboards</font>, and product packaging.<span style="font-size:80%;opacity:0.8">在自然场景中，文本出现在各种对象上，例如道路标志，广告牌和产品包装。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> semantic<br>(1) </td> <td> [sɪˈmæntɪk] </td> <td> 
<ul><li>It carries rich and high-level <font color=orangered>semantic</font> information that is important for image understanding.<span style="font-size:80%;opacity:0.8">它携带丰富的高级语义信息，这对于图像理解非常重要。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> real-world<br>(1) </td> <td> [!≈ ˈri:əl wɜ:ld] </td> <td> 
<ul><li>Recognizing text in images facilitates many <font color=orangered>real-world</font> applications, such as geolocation, driverless car, and image-based machine translation.<span style="font-size:80%;opacity:0.8">识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> geolocation<br>(1) </td> <td> [dʒɪɒləʊ'keɪʃn] </td> <td> 
<ul><li>Recognizing text in images facilitates many real-world applications, such as <font color=orangered>geolocation</font>, driverless car, and image-based machine translation.<span style="font-size:80%;opacity:0.8">识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> driverless<br>(1) </td> <td> [d'raɪvərles] </td> <td> 
<ul><li>Recognizing text in images facilitates many real-world applications, such as geolocation, <font color=orangered>driverless</font> car, and image-based machine translation.<span style="font-size:80%;opacity:0.8">识别图像中的文本有助于许多实际应用，例如地理定位，无人驾驶汽车和基于图像的机器翻译。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> frontal<br>(1) </td> <td> [ˈfrʌntl] </td> <td> 
<ul><li>We call such text irregular text, in contrast to regular text which is horizontal and <font color=orangered>frontal</font>.<span style="font-size:80%;opacity:0.8">我们将此类文本称为不规则文本，与常规文本（水平和正面）形成对比。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> Schematic<br>(1) </td> <td> [ski:ˈmætɪk] </td> <td> 
<ul><li>Figure 1. <font color=orangered>Schematic</font> overview of RARE, which consists a spatial transformer network (STN) and a sequence recognition network (SRN).<span style="font-size:80%;opacity:0.8">图1. RARE的示意图，包括空间变换器网络（STN）和序列识别网络（SRN）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> jointly<br>(1) </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>The two networks are <font color=orangered>jointly</font> trained by the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8">这两个网络由反向传播算法共同训练[22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> dash<br>(1) </td> <td> [dæʃ] </td> <td> 
<ul><li>The <font color=orangered>dashed</font> lines represent the ﬂows of the back-propagated gradients.<span style="font-size:80%;opacity:0.8">虚线表示反向传播的梯度的流动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> ow<br>(1) </td> <td> [aʊ] </td> <td> 
<ul><li>The dashed lines represent the ﬂ<font color=orangered>ows</font> of the back-propagated gradients.<span style="font-size:80%;opacity:0.8">虚线表示反向传播的梯度的流动。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> tightly-bounded<br>(1) </td> <td> [!≈ ˈtaɪtli 'baʊndɪd] </td> <td> 
<ul><li>Usually, a text recognizer works best when its input images contain <font color=orangered>tightly-bounded</font> regular text.<span style="font-size:80%;opacity:0.8">通常，文本识别器在其输入图像包含紧密有界的常规文本时效果最佳。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> spatially<br>(1) </td> <td> ['speɪʃəlɪ] </td> <td> 
<ul><li>In the STN, an input image is <font color=orangered>spatially</font> transformed into a rectified image.<span style="font-size:80%;opacity:0.8">在STN中，输入图像在空间上变换成校正后的图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> thinplate-spline<br>(1) </td> <td>  </td> <td> 
<ul><li>The transformation is a <font color=forestgreen>thinplate-spline</font> [6] (TPS) transformation, whose nonlinearity allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8">STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> nonlinearity<br>(1) </td> <td> [nɒnlɪnɪ'ærɪtɪ] </td> <td> 
<ul><li>The transformation is a thinplate-spline [6] (TPS) transformation, whose <font color=orangered>nonlinearity</font> allows us to rectify various types of irregular text, including perspective and curved text.<span style="font-size:80%;opacity:0.8">STN的空间变换是一个薄板样条（TPS）变换，这种变换的非线可以纠正各种类型的不规则文本，包括透视和弯曲文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> configure<br>(1) </td> <td> [kənˈfɪgə(r)] </td> <td> 
<ul><li>The TPS transformation is <font color=orangered>configured</font> by a set of fiducial points, whose coordinates are regressed by a convolutional neural network.<span style="font-size:80%;opacity:0.8">TPS变换是由一组基准点决定，这些基准点的坐标就是由STN这个卷积神经网络回归出来的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> variant<br>(1) </td> <td> [ˈveəriənt] </td> <td> 
<ul><li>Third, our model adopts a convolutional-recurrent structure in the encoder of the SRN, thus is a novel <font color=orangered>variant</font> of the attention-based model [4].<span style="font-size:80%;opacity:0.8">第三，在SRN的编码器中，我们采用卷积循环结构，这是注意力模型的一种新颖的变体。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> Hough<br>(1) </td> <td> [hɒk] </td> <td> 
<ul><li>Among the traditional methods, many adopt bottom-up approaches, where individual characters are firstly detected using sliding window [36, 35], connected components [28], or <font color=orangered>Hough</font> voting [39].<span style="font-size:80%;opacity:0.8">在传统方法中，许多方法采用自下而上的方法，其中首先使用滑动窗口[36,35]，连通组件[28]或霍夫投票[39]来检测单个字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> Alm´azan<br>(1) </td> <td>  </td> <td> 
<ul><li>For example, <font color=forestgreen>Alm´azan</font> et al. [2] propose to predict label embedding vectors from input images.<span style="font-size:80%;opacity:0.8">例如，Alm'azan等人[2]建议从输入图像预测标签嵌入向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> k-class<br>(1) </td> <td> [!≈ keɪ klɑ:s] </td> <td> 
<ul><li>Jaderberg et al. [17] address text recognition with a 90<font color=orangered>k-class</font> convolutional neural network, where each class corresponds to an English word.<span style="font-size:80%;opacity:0.8">Jaderberg等人[17]使用90k级卷积神经网络进行文本识别，其中每个类对应一个英语单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> HOG<br>(1) </td> <td> [hɒg] </td> <td> 
<ul><li>Su and Lu [34] extract sequential image representation, which is a sequence of <font color=orangered>HOG</font> [10] descriptors, and predict the corresponding character sequence with a recurrent neural network (RNN).<span style="font-size:80%;opacity:0.8">Su和Lu [34]提取序列图像表示，它是HOG [10]描述符的序列，并用递归神经网络（RNN）预测相应的字符序列。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> rotation-invariant<br>(1) </td> <td> [!≈ rəʊˈteɪʃn ɪnˈveəriənt] </td> <td> 
<ul><li>Yao et al. [38] firstly propose the multi-oriented text detection problem, and deal with it by carefully designing <font color=orangered>rotation-invariant</font> region descriptors.<span style="font-size:80%;opacity:0.8">姚等人[38]首先提出了多方向文本检测问题，并通过仔细设计旋转不变区域描述符来处理它。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> Phan<br>(1) </td> <td> [fæn] </td> <td> 
<ul><li><font color=orangered>Phan</font> et al. propose to explicitly rectify perspective distortions via SIFT [23] descriptor matching.<span style="font-size:80%;opacity:0.8">潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> SIFT<br>(1) </td> <td> [sɪft] </td> <td> 
<ul><li>Phan et al. propose to explicitly rectify perspective distortions via <font color=orangered>SIFT</font> [23] descriptor matching.<span style="font-size:80%;opacity:0.8">潘等人建议通过SIFT [23]描述符匹配明确纠正透视失真。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> above-mentioned<br>(1) </td> <td> [ə'bʌv 'menʃnd] </td> <td> 
<ul><li>The <font color=orangered>above-mentioned</font> work brings insightful ideas into this issue.<span style="font-size:80%;opacity:0.8">上述工作为这个问题带来了深刻的见解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> insightful<br>(1) </td> <td> [ˈɪnsaɪtfʊl] </td> <td> 
<ul><li>The above-mentioned work brings <font color=orangered>insightful</font> ideas into this issue.<span style="font-size:80%;opacity:0.8">上述工作为这个问题带来了深刻的见解。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> annotation<br>(1) </td> <td> [ˌænə'teɪʃn] </td> <td> 
<ul><li>Moreover, it does not require extra <font color=orangered>annotations</font> for the rectification process, since the STN is supervised by the SRN during training.<span style="font-size:80%;opacity:0.8">此外，它不需要额外的注释用于整理过程，因为STN在训练期间由SRN监督。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> propagate<br>(1) </td> <td> [ˈprɒpəgeɪt] </td> <td> 
<ul><li>Instead, the training of the localization network is completely supervised by the gradients <font color=orangered>propagated</font> by the other parts of the STN, following the back-propagation algorithm [22].<span style="font-size:80%;opacity:0.8">相反，定位网络的训练完全受到STN其他部分传播的梯度的监督，遵循反向传播算法[22]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> Cyan<br>(1) </td> <td> [ˈsaɪən] </td> <td> 
<ul><li><font color=orangered>Cyan</font> markers on the right image are the base fiducial points $C^\prime$.<span style="font-size:80%;opacity:0.8">右侧图像上的青色标记是$C^\prime$的基本特征点。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> multiplication<br>(1) </td> <td> [ˌmʌltɪplɪˈkeɪʃn] </td> <td> 
<ul><li>The grid generator can back-propagate gradients, since its two matrix <font color=orangered>multiplications</font>, Eq. 1 and Eq. 4, are both differentiable.<span style="font-size:80%;opacity:0.8">网格生成器可以反向传播梯度，因为它的两个矩阵乘法，Eq. 1和Eq. 4，都是可区分的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> Lastly<br>(1) </td> <td> [ˈlɑ:stli] </td> <td> 
<ul><li><font color=orangered>Lastly</font>, in the sampler, the pixel value of $p^\prime_i$ is bilinearly interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8">最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> bilinearly<br>(1) </td> <td> [!≈ baɪ'lɪnɪəli] </td> <td> 
<ul><li>Lastly, in the sampler, the pixel value of $p^\prime_i$ is <font color=orangered>bilinearly</font> interpolated from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8">最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> interpolate<br>(1) </td> <td> [ɪnˈtɜ:pəleɪt] </td> <td> 
<ul><li>Lastly, in the sampler, the pixel value of $p^\prime_i$ is bilinearly <font color=orangered>interpolated</font> from the pixels near $p_i$ on the input image.<span style="font-size:80%;opacity:0.8">最后，在采样器中，$p^\prime_i$的像素值是从输入图像上的$p_i$附近的像素进行双线性插值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> bilinear<br>(1) </td> <td> [baɪ'lɪnɪə] </td> <td> 
<ul><li>where $V$ represents the <font color=orangered>bilinear</font> sampler [18], which is also a differentiable module.<span style="font-size:80%;opacity:0.8">其中V代表双线性采样器[18]，它也是一个可微分的模块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> exibility<br>(1) </td> <td>  </td> <td> 
<ul><li>The ﬂ<font color=forestgreen>exibility</font> of the TPS transformation allows us to transform irregular text images into rectified images that contain regular text.<span style="font-size:80%;opacity:0.8">TPS转换的灵活性允许我们将不规则文本图像转换为包含常规文本的矫正图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> imperfect<br>(1) </td> <td> [ɪmˈpɜ:fɪkt] </td> <td> 
<ul><li>In fig. 4, we show some common types of irregular text, including a) loosely-bounded text, which resulted by <font color=orangered>imperfect</font> text detection; b) multi-oriented text, caused by non-horizontal camera views; c) perspective text, caused by side-view camera angles; d) curved text, a commonly seen artistic style.<span style="font-size:80%;opacity:0.8">在图4中，我们展示了一些常见类型的不规则文本，包括a）松散有界的文本，这是由不完美的文本检测引起的; b）由非水平摄像机视图引起的多向文本; c）由侧视摄像机角度引起的透视文本; d）弯曲的文字，一种常见的艺术风格。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> inherently<br>(1) </td> <td> [ɪnˈhɪərəntlɪ] </td> <td> 
<ul><li>Since target words are <font color=orangered>inherently</font> sequences of characters, we model the recognition problem as a sequence recognition problem, and address it with a sequence recognition network.<span style="font-size:80%;opacity:0.8">由于目标词本质上是字符序列，我们将识别问题建模为序列识别问题，并用序列识别网络对其进行处理。</span></li></ul>
 </td>
</tr>
<tr>
<td> 96 </td> <td> horizontally<br>(1) </td> <td> [ˌhɒrɪ'zɒntəlɪ] </td> <td> 
<ul><li>The input to the SRN is a rectified image $I^\prime$ , which ideally contains a word that is written <font color=orangered>horizontally</font> from left to right.<span style="font-size:80%;opacity:0.8">SRN的输入是一个矫正的图像$I^\prime$，理想情况下包含一个从左到右水平写入的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 97 </td> <td> na¨ıve<br>(1) </td> <td>  </td> <td> 
<ul><li>A <font color=forestgreen>na¨ıve</font> approach for extracting a sequential representation for $I^\prime$ is to take local image patches from left to right, and describe each of them with a CNN.<span style="font-size:80%;opacity:0.8">用于提取$I^\prime$的顺序表示的一种简单方法是从左到右获取图像中的局部图像块，并用CNN描述每个图像块。</span></li></ul>
 </td>
</tr>
<tr>
<td> 98 </td> <td> attens<br>(1) </td> <td>  </td> <td> 
<ul><li>Specifically, the “map-to-sequence” operation takes out the columns of the maps in the left-to-right order, and ﬂ<font color=forestgreen>attens</font> them into vectors.<span style="font-size:80%;opacity:0.8">具体而言，“map-to-sequence”操作以从左到右的顺序取出地图的列，并将fl视为向量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 99 </td> <td> invariance<br>(1) </td> <td> [ɪn'veərɪəns] </td> <td> 
<ul><li>According to the translation <font color=orangered>invariance</font> property of CNN, each vector corresponds to a local image region, i.e. receptive field, and is a descriptor for that region.<span style="font-size:80%;opacity:0.8">根据CNN的平移不变性，每个矢量对应于局部图像区域，即接收场，并且是该区域的描述符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 100 </td> <td> ConvNet<br>(1) </td> <td>  </td> <td> 
<ul><li>Structure of the SRN, which consists of an encoder and a decoder. The encoder uses several convolution layers (<font color=forestgreen>ConvNet</font>) and a two-layer BLSTM network to extract a sequential representation (h) for the input image.<span style="font-size:80%;opacity:0.8">编码器使用几个卷积层（ConvNet）和两层BLSTM网络来提取输入图像的顺序表示（h）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 101 </td> <td> Long-Short<br>(1) </td> <td> [!≈ lɒŋ ʃɔ:t] </td> <td> 
<ul><li>We further apply a two-layer Bidirectional <font color=orangered>Long-Short</font> Term Memory (BLSTM) [14, 13] network to the sequence, in order to model the long-term dependencies within the sequence.<span style="font-size:80%;opacity:0.8">我们进一步将两层双向长短期记忆（BLSTM）[14,13]网络应用于序列，以模拟序列内的长期依赖性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 102 </td> <td> analyze<br>(1) </td> <td> ['ænəlaɪz] </td> <td> 
<ul><li>The BLSTM is a recurrent network that can <font color=orangered>analyze</font> the dependencies within a sequence in both directions, it outputs another sequence which has the same length as the input one.<span style="font-size:80%;opacity:0.8">BLSTM是一个循环网络，可以在两个方向上分析序列中的依赖关系，它输出另一个序列，其长度与输入序列相同。</span></li></ul>
 </td>
</tr>
<tr>
<td> 103 </td> <td> recurrency<br>(1) </td> <td>  </td> <td> 
<ul><li>In the <font color=forestgreen>recurrency</font> part, we adopt the Gated Recurrent Unit (GRU) [7] as the cell.<span style="font-size:80%;opacity:0.8">在重发部分，我们采用门控循环单元（GRU）[7]作为单元。</span></li></ul>
 </td>
</tr>
<tr>
<td> 104 </td> <td> linearly<br>(1) </td> <td> [ˈliniəli] </td> <td> 
<ul><li>Then, a glimpse $g_t$ is computed by <font color=orangered>linearly</font> combining the vectors in $h$: $g_t=\sum_{i=1}^L\alpha_{ti}h_i$.<span style="font-size:80%;opacity:0.8">然后，通过线性组合$h$：$g_t=\sum_{i=1}^L\alpha_{ti}h_i$中的向量来计算一瞥$g_t$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 105 </td> <td> alphanumeric<br>(1) </td> <td> [ˌælfənju:ˈmerɪk] </td> <td> 
<ul><li>The label space includes all English <font color=orangered>alphanumeric</font> characters, plus a special “end-ofsequence” (EOS) token, which ends the generation process.<span style="font-size:80%;opacity:0.8">标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li></ul>
 </td>
</tr>
<tr>
<td> 106 </td> <td> end-ofsequence<br>(1) </td> <td>  </td> <td> 
<ul><li>The label space includes all English alphanumeric characters, plus a special “<font color=forestgreen>end-ofsequence</font>” (EOS) token, which ends the generation process.<span style="font-size:80%;opacity:0.8">标签空间包括所有英文字母数字字符，以及一个特殊的“结束序列”（EOS）令牌，它结束生成过程。</span></li></ul>
 </td>
</tr>
<tr>
<td> 107 </td> <td> ADADELTA<br>(1) </td> <td> [!≈ eɪ di: eɪ di: i: el ti: eɪ] </td> <td> 
<ul><li>The optimization algorithm is the <font color=orangered>ADADELTA</font> [41], which we find fast in convergence speed.<span style="font-size:80%;opacity:0.8">优化算法是ADADELTA [41]，我们发现其收敛速度很快。</span></li></ul>
 </td>
</tr>
<tr>
<td> 108 </td> <td> Empirically<br>(1) </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li><font color=orangered>Empirically</font>, we also find that the patterns displayed fig. 6. b and fig. 6. c yield relatively poorer performance.<span style="font-size:80%;opacity:0.8">根据经验，我们还发现图6.b和图6.c所示的模式产生相对较差的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 109 </td> <td> conditional<br>(1) </td> <td> [kənˈdɪʃənl] </td> <td> 
<ul><li>When a test image is associated with a lexicon, i.e. a set of words for selection, the recognition process is to pick the word with the highest posterior <font color=orangered>conditional</font> probability:<span style="font-size:80%;opacity:0.8">当测试图像与词典相关联时，即一组用于选择的单词时，识别过程是选择具有最高后验条件概率的单词：</span></li></ul>
 </td>
</tr>
<tr>
<td> 110 </td> <td> incorporate<br>(1) </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>Recognition performance could be further improved by <font color=orangered>incorporating</font> beam search.<span style="font-size:80%;opacity:0.8">通过结合波束搜索可以进一步提高识别性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 111 </td> <td> top-B<br>(1) </td> <td> [!≈ tɒp bi:] </td> <td> 
<ul><li>After each step, the list is updated to store the nodes with <font color=orangered>top-B</font> accumulated log-likelihoods, where B is the beam width.<span style="font-size:80%;opacity:0.8">在每个步骤之后，更新列表以存储具有前B累积对数似然的节点，其中B是波束宽度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 112 </td> <td> resize<br>(1) </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>Following [17, 16], images are <font color=orangered>resized</font> to $100 \times 32$ in both training and testing.<span style="font-size:80%;opacity:0.8">在[17,16]之后，在训练和测试中将图像调整为$100 \times 32$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 113 </td> <td> epoch<br>(1) </td> <td> [ˈi:pɒk] </td> <td> 
<ul><li>Our model processes ~160 samples per second during training, and converges in 2 days after ~3 <font color=orangered>epochs</font> over the training dataset.<span style="font-size:80%;opacity:0.8">我们的模型在训练期间每秒处理~160个样本，并且在训练数据集的~3个时期之后的2天内收敛。</span></li></ul>
 </td>
</tr>
<tr>
<td> 114 </td> <td> GPUaccelerated<br>(1) </td> <td>  </td> <td> 
<ul><li>Most parts of the model are <font color=forestgreen>GPUaccelerated</font>.<span style="font-size:80%;opacity:0.8">该模型的大多数部分都是GPU加速的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 115 </td> <td> Xeon<br>(1) </td> <td>  </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel <font color=forestgreen>Xeon</font>(R) E5-2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8">我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 116 </td> <td> NVIDIA<br>(1) </td> <td> [ɪn'vɪdɪə] </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an <font color=orangered>NVIDIA</font> GTX-Titan GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8">我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 117 </td> <td> GTX-Titan<br>(1) </td> <td>  </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an NVIDIA <font color=forestgreen>GTX-Titan</font> GPU, and 64GB RAM.<span style="font-size:80%;opacity:0.8">我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 118 </td> <td> RAM<br>(1) </td> <td> [ræm] </td> <td> 
<ul><li>All our experiments are carried out on a workstation which has one Intel Xeon(R) E5-2620 2.40GHz CPU, an NVIDIA GTX-Titan GPU, and 64GB <font color=orangered>RAM</font>.<span style="font-size:80%;opacity:0.8">我们所有的实验都是在一个工作站上进行的，该工作站有一个Intel Xeon（R）E5-2620 2.40GHz CPU，一个NVIDIA GTX-Titan GPU和64GB RAM。</span></li></ul>
 </td>
</tr>
<tr>
<td> 119 </td> <td> k-word<br>(1) </td> <td> [!≈ keɪ wɜ:d] </td> <td> 
<ul><li>With a 50<font color=orangered>k-word</font> lexicon, the search takes ~200ms per image.<span style="font-size:80%;opacity:0.8">使用50k字的词典，每张图像搜索大约需要200毫秒。</span></li></ul>
 </td>
</tr>
<tr>
<td> 120 </td> <td> IIIT<br>(1) </td> <td> [!≈ aɪ aɪ aɪ ti:] </td> <td> 
<ul><li>• <font color=orangered>IIIT</font> 5K-Words [25] (IIIT5K) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8">•IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 121 </td> <td> K-Words<br>(1) </td> <td> [!≈ keɪ wɜ:dz] </td> <td> 
<ul><li>• IIIT 5<font color=orangered>K-Words</font> [25] (IIIT5K) contains 3000 cropped word images for testing.<span style="font-size:80%;opacity:0.8">•IIIT 5K-Words [25]（IIIT5K）包含3000个用于测试的裁剪单词图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 122 </td> <td> non-alphanumeric<br>(1) </td> <td> [!≈ nɒn ˌælfənju:ˈmerɪk] </td> <td> 
<ul><li>Following [35], we discard images that contain <font color=orangered>non-alphanumeric</font> characters or have less than three characters.<span style="font-size:80%;opacity:0.8">按照[35]，我们丢弃包含非字母数字字符或少于三个字符的图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 123 </td> <td> IC13<br>(1) </td> <td>  </td> <td> 
<ul><li>• ICDAR 2013 [20] (<font color=forestgreen>IC13</font>) inherits most of its samples from IC03.<span style="font-size:80%;opacity:0.8">•ICDAR 2013 [20]（IC13）继承了IC03的大部分样本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 124 </td> <td> k-dictionary<br>(1) </td> <td> [!≈ keɪ ˈdɪkʃənri] </td> <td> 
<ul><li>[17] only recognizes words that are in its 90<font color=orangered>k-dictionary</font>.<span style="font-size:80%;opacity:0.8">[17]只识别其90k字典中的单词。</span></li></ul>
 </td>
</tr>
<tr>
<td> 125 </td> <td> par<br>(1) </td> <td> [pɑ:(r)] </td> <td> 
<ul><li>On IIIT5K, SVT and IC03, constrained recognition accuracies are on <font color=orangered>par</font> with [17], and slightly lower than [32].<span style="font-size:80%;opacity:0.8">在IIIT5K，SVT和IC03上，约束识别精度与[17]相当，略低于[32]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 126 </td> <td> SRN-only<br>(1) </td> <td>  </td> <td> 
<ul><li>1, we see that the <font color=forestgreen>SRN-only</font> model is also a very competitive recognizer, achieving higher or competitive performance on most of the benchmarks.<span style="font-size:80%;opacity:0.8"> 1，我们看到仅SRN模型也是一个非常有竞争力的识别器，在大多数基准测试中实现了更高或更具竞争力的性能。</span></li></ul>
 </td>
</tr>
<tr>
<td> 127 </td> <td> validate<br>(1) </td> <td> [ˈvælɪdeɪt] </td> <td> 
<ul><li>To <font color=orangered>validate</font> the effectiveness of the rectification scheme, we evaluate RARE on the task of perspective text recognition.<span style="font-size:80%;opacity:0.8">为了验证整合方案的有效性，我们评估了RARE对透视文本识别的任务。</span></li></ul>
 </td>
</tr>
<tr>
<td> 128 </td> <td> deformed<br>(1) </td> <td> [dɪˈfɔ:md] </td> <td> 
<ul><li>Text samples in SVT-Perspective are picked from side view angles in Google Street View, thus most of them are heavily <font color=orangered>deformed</font> by perspective distortion.<span style="font-size:80%;opacity:0.8">SVT-Perspective中的文本样本是从Google街景中的侧视角中选取的，因此大多数文本样本都因透视变形而严重变形。</span></li></ul>
 </td>
</tr>
<tr>
<td> 129 </td> <td> SVTPerspective<br>(1) </td> <td>  </td> <td> 
<ul><li>Furthermore, recall the results in Tab. 1, on <font color=forestgreen>SVTPerspective</font> RARE outperforms [32] by a even larger margin.<span style="font-size:80%;opacity:0.8">此外，回顾表1中的结果，在SVTP上，RARE的表现优于[32]达到更大的余地。</span></li></ul>
 </td>
</tr>
<tr>
<td> 130 </td> <td> gray-scale<br>(1) </td> <td> [ɡ'reɪsk'eɪl] </td> <td> 
<ul><li>The middle column is the rectified images (we use <font color=orangered>gray-scale</font> images for recognition).<span style="font-size:80%;opacity:0.8">中间一列是校正后的图像(我们使用灰度图像进行识别)。</span></li></ul>
 </td>
</tr>
<tr>
<td> 131 </td> <td> mistakenly<br>(1) </td> <td> [mɪ'steɪkənlɪ] </td> <td> 
<ul><li>Green and red characters are correctly and <font color=orangered>mistakenly</font> recognized characters, respectively.<span style="font-size:80%;opacity:0.8">绿色和红色字符分别是正确和错误识别的字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 132 </td> <td> qualitative<br>(1) </td> <td> [ˈkwɒlɪtətɪv] </td> <td> 
<ul><li>In fig. 9 we present some <font color=orangered>qualitative</font> analysis.<span style="font-size:80%;opacity:0.8">在图9中，我们提出了一些定性分析。</span></li></ul>
 </td>
</tr>
<tr>
<td> 133 </td> <td> artistic-style<br>(1) </td> <td> [!≈ ɑ:ˈtɪstɪk staɪl] </td> <td> 
<ul><li>Curved text is a commonly seen <font color=orangered>artistic-style</font> text in natural scenes.<span style="font-size:80%;opacity:0.8">弯曲文本是自然场景中常见的艺术风格文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 134 </td> <td> advantageous<br>(1) </td> <td> [ˌædvənˈteɪdʒəs] </td> <td> 
<ul><li>Therefore, it is <font color=orangered>advantageous</font> on this task.<span style="font-size:80%;opacity:0.8">因此，在这项任务上是有利的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 135 </td> <td> acknowledgment<br>(1) </td> <td> [ək'nɒlɪdʒmənt] </td> <td> 
<ul><li><font color=orangered>Acknowledgments</font><span style="font-size:80%;opacity:0.8">致谢</span></li></ul>
 </td>
</tr>
<tr>
<td> 136 </td> <td> NSFC<br>(1) </td> <td> [!≈ en es ef si:] </td> <td> 
<ul><li>This work was primarily supported by National Natural Science Foundation of China (<font color=orangered>NSFC</font>) (No. 61222308, No. 61573160 and No. 61503145), and Open Project Program of the State Key Laboratory of Digital Publishing Technology (No. F2016001).<span style="font-size:80%;opacity:0.8">本工作主要得到国家自然科学基金(61222308，61573160，61503145)和数字出版技术国家重点实验室开放项目(No. F2016001)的支持。</span></li></ul>
 </td>
</tr>
<tr>
<td> 137 </td> <td> F2016001<br>(1) </td> <td>  </td> <td> 
<ul><li>This work was primarily supported by National Natural Science Foundation of China (NSFC) (No. 61222308, No. 61573160 and No. 61503145), and Open Project Program of the State Key Laboratory of Digital Publishing Technology (No. <font color=forestgreen>F2016001</font>).<span style="font-size:80%;opacity:0.8">本工作主要得到国家自然科学基金(61222308，61573160，61503145)和数字出版技术国家重点实验室开放项目(No. F2016001)的支持。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>