<html>
<head>
<meta charset="utf-8">
<title> CTPN - Detecting Text in Natural Image with Connectionist Text Proposal Network </title>
<style type="text/css">
.inline-ul { font-size:0;}
.inline-ul ul li{ font-size: 12px; letter-spacing: normal; word-spacing: normal;
vertical-align:top; display: inline-block; *display:inline; *zoom:1;}
.inline-ul{ letter-spacing:-5px; }
.widget-title { font-size: 13px; font-weight: normal; color: #888888; padding: 20px 20px 0px; }
.widget-tab .widget-title{font-size: 0;}
.widget-tab .widget-title ul li{margin-left:3%;width:40%;text-align:center;margin-right:2%;padding:4px 1%;}
.widget-tab .widget-title ul li:hover{background:#F7F7F7}
.widget-tab .widget-title label{cursor:pointer;display:block; font-size: 0.8em;}
.widget-tab .widget-title ul li.active{background:#F0F0F0}
.widget-tab input{display:none}
.widget-tab .widget-box div{display:none}
#one:checked ~ .widget-title .one,#two:checked ~ .widget-title .two{background:#F7F7F7}
#one:checked ~ .widget-box .one-list,#two:checked ~ .widget-box .two-list{display:block}

body {font-family: arial,verdana,geneva,sans-serif; font-size: 1.25em; color: #000; word-wrap:break-word;}
table { border-collapse: collapse; margin: 0 auto; }
table td, table th { border: 1px solid #cad9ea; height: 30px; }
table thead th, table thead td { background-color: #CCE8EB; text-align: center; }
table tr:nth-child(odd) { background: #fff; }
table tr:nth-child(even) { background: #F5FAFA; }
table tr td:not(:last-child){ text-align: center; }
</style>
</head>
<body>
<div class="widget-tab">
<input type="radio" name="widget-tab" id="one" checked="checked"/>
<input type="radio" name="widget-tab" id="two"/>
<div class="widget-title inline-ul">
    <ul> <li class="one"> <label for="one">In order of appearance</label> </li>
        <li class="two"> <label for="two">In order of frequency</label> </li>
    </ul>
</div>
<div class="widget-box">
<div class="one-list">
<table>
<caption>
    <h2> Words List (appearance)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> connectionist </td> <td> [kə'nekʃənɪst] </td> <td> 
<ul><li>Detecting Text in Natural Image with <font color=orangered>Connectionist</font> Text Proposal Network<span style="font-size:80%;opacity:0.8"> 用连接式文本建议网络检测自然图像中的文本</span></li><li>We propose a novel <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) that accurately localizes text lines in natural image.<span style="font-size:80%;opacity:0.8"> 我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。</span></li><li>We propose a novel <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers.<span style="font-size:80%;opacity:0.8"> 我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。</span></li><li>Fig. 1: (a) Architecture of the <font color=orangered>Connectionist</font> Text Proposal Network (CTPN).<span style="font-size:80%;opacity:0.8"> 图1：（a）连接文本提议网络（CTPN）的架构。</span></li><li>3. <font color=orangered>Connectionist</font> Text Proposal Network<span style="font-size:80%;opacity:0.8"> 3. 连接文本提议网络</span></li><li>This section presents details of the <font color=orangered>Connectionist</font> Text Proposal Network (CTPN).<span style="font-size:80%;opacity:0.8"> 本节介绍连接文本提议网络（CTPN）的细节。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent <font color=orangered>connectionist</font> text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8"> 它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.2 Recurrent <font color=orangered>Connectionist</font> Text Proposals<span style="font-size:80%;opacity:0.8"> 3.2 循环连接文本提议</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the <font color=orangered>connectionist</font> receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8"> 我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>4.3 Recurrent <font color=orangered>Connectionist</font> Text Proposals<span style="font-size:80%;opacity:0.8"> 4.3 循环连接文本提议</span></li><li>We have presented a <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end trainable.<span style="font-size:80%;opacity:0.8"> 我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> CTPN </td> <td> [!≈ si: ti: pi: en] </td> <td> 
<ul><li>We propose a novel Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) that accurately localizes text lines in natural image.<span style="font-size:80%;opacity:0.8"> 我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。</span></li><li>The <font color=orangered>CTPN</font> detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps.<span style="font-size:80%;opacity:0.8"> CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。</span></li><li>This allows the <font color=orangered>CTPN</font> to explore rich context information of image, making it powerful to detect extremely ambiguous text.<span style="font-size:80%;opacity:0.8"> 这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。</span></li><li>The <font color=orangered>CTPN</font> works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering.<span style="font-size:80%;opacity:0.8"> CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。</span></li><li>The <font color=orangered>CTPN</font> is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8"> 通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>We propose a novel Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) that directly localizes text sequences in convolutional layers.<span style="font-size:80%;opacity:0.8"> 我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。</span></li><li>We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the <font color=orangered>CTPN</font> architecture which is described in Fig. 1.<span style="font-size:80%;opacity:0.8"> 我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。</span></li><li>Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (<font color=orangered>CTPN</font>).<span style="font-size:80%;opacity:0.8"> 图1：（a）连接文本提议网络（CTPN）的架构。</span></li><li>(b) The <font color=orangered>CTPN</font> outputs sequential fixed-width fine-scale text proposals.<span style="font-size:80%;opacity:0.8"> （b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>This section presents details of the Connectionist Text Proposal Network (<font color=orangered>CTPN</font>).<span style="font-size:80%;opacity:0.8"> 本节介绍连接文本提议网络（CTPN）的细节。</span></li><li>Similar to Region Proposal Network (RPN) [25], the <font color=orangered>CTPN</font> is essentially a fully convolutional network that allows an input image of arbitrary size.<span style="font-size:80%;opacity:0.8"> 类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>Architecture of the <font color=orangered>CTPN</font> is presented in Fig. 1 (a).<span style="font-size:80%;opacity:0.8"> CTPN的架构如图1（a）所示。</span></li><li>Fig. 3: Top: <font color=orangered>CTPN</font> without RNN.<span style="font-size:80%;opacity:0.8"> 图3：上：没有RNN的CTPN。</span></li><li>Bottom: <font color=orangered>CTPN</font> with RNN connection.<span style="font-size:80%;opacity:0.8"> 下：有RNN连接的CTPN。</span></li><li>The fine-scale text proposals are detected accurately and reliably by our <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8"> 我们的CTPN能够准确可靠地检测细粒度的文本提议。</span></li><li>Fig. 4: <font color=orangered>CTPN</font> detection with (red box) and without (yellow dashed box) the side-refinement.<span style="font-size:80%;opacity:0.8"> 图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li><li>The proposed <font color=orangered>CTPN</font> has three outputs which are jointly connected to the last FC layer, as shown in Fig. 1 (a).<span style="font-size:80%;opacity:0.8"> 提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。</span></li><li>The <font color=orangered>CTPN</font> can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (SGD).<span style="font-size:80%;opacity:0.8"> 通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li><li>This is crucial to detect small-scale text patterns, which is one of key advantages of the <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8"> 这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</span></li><li>We evaluate the <font color=orangered>CTPN</font> on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We discuss impact of recurrent connection on our <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8"> 我们讨论循环连接对CTPN的影响。</span></li><li>It is of great importance for recovering highly ambiguous text (e.g., extremely small-scale ones), which is one of main advantages of our <font color=orangered>CTPN</font>, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8"> 对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the <font color=orangered>CTPN</font> improves the FTPN substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Fig. 6: <font color=orangered>CTPN</font> detection results on extremely small-scale cases (in red boxes), where some ground truth boxes are missed.<span style="font-size:80%;opacity:0.8"> 图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。</span></li><li>The implementation time of our <font color=orangered>CTPN</font> (for whole detection processing) is about 0.14s per image with a fixed short side of 600, by using a single GPU.<span style="font-size:80%;opacity:0.8"> 通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。</span></li><li>The <font color=orangered>CTPN</font> without the RNN connection takes about 0.13s/image GPU time.<span style="font-size:80%;opacity:0.8"> 没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。</span></li><li>As can be found, the <font color=orangered>CTPN</font> works perfectly on these challenging cases, some of which are difficult for many previous methods.<span style="font-size:80%;opacity:0.8"> 可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。</span></li><li>Fig. 5: <font color=orangered>CTPN</font> detection results several challenging images, including multi-scale and multi-language text lines.<span style="font-size:80%;opacity:0.8"> 图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。</span></li><li>As shown in Table 1 and 2, our <font color=orangered>CTPN</font> achieves the best performance on all five datasets.<span style="font-size:80%;opacity:0.8"> 如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。</span></li><li>This may due to strong capability of <font color=orangered>CTPN</font> for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8"> 这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li><li>We have presented a Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) —— an efficient text detector that is end-to-end trainable.<span style="font-size:80%;opacity:0.8"> 我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li><li>The <font color=orangered>CTPN</font> detects a text line in a sequence of fine-scale text proposals directly in convolutional maps.<span style="font-size:80%;opacity:0.8"> CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。</span></li><li>The <font color=orangered>CTPN</font> is efficient by achieving new state-of-the-art performance on five benchmarks, with 0.14s/image running time.<span style="font-size:80%;opacity:0.8"> 通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> fine-scale </td> <td> [!≈ faɪn skeɪl] </td> <td> 
<ul><li>The CTPN detects a text line in a sequence of <font color=orangered>fine-scale</font> text proposals directly in convolutional feature maps.<span style="font-size:80%;opacity:0.8"> CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。</span></li><li>Then, an in-network recurrent architecture is proposed to connect these <font color=orangered>fine-scale</font> text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8"> 然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>(b) The CTPN outputs sequential fixed-width <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8"> （b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>First, we cast the problem of text detection into localizing a sequence of <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8"> 首先，我们将文本检测的问题转化为一系列细粒度的文本提议。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in <font color=orangered>fine-scale</font> proposals, recurrent connectionist text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8"> 它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.1 Detecting Text in <font color=orangered>Fine-scale</font> Proposals<span style="font-size:80%;opacity:0.8"> 3.1 在细粒度提议中检测文本</span></li><li>It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of <font color=orangered>fine-scale</font> (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).<span style="font-size:80%;opacity:0.8"> 它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</span></li><li>Right: <font color=orangered>Fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8"> 右：细粒度的文本提议。</span></li><li>It is natural to consider a text line as a sequence of <font color=orangered>fine-scale</font> text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width.<span style="font-size:80%;opacity:0.8"> 将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。</span></li><li>We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each <font color=orangered>fine-scale</font> proposal.<span style="font-size:80%;opacity:0.8"> 我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。</span></li><li>To this end, we design the <font color=orangered>fine-scale</font> text proposal as follow.<span style="font-size:80%;opacity:0.8"> 为此，我们设计如下的细粒度文本提议。</span></li><li>By the designed vertical anchor and <font color=orangered>fine-scale</font> detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image.<span style="font-size:80%;opacity:0.8"> 通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。</span></li><li>Compared to the RPN or Faster R-CNN system [25], our <font color=orangered>fine-scale</font> detection provides more detailed supervised information that naturally leads to a more accurate detection.<span style="font-size:80%;opacity:0.8"> 与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</span></li><li>To improve localization accuracy, we split a text line into a sequence of <font color=orangered>fine-scale</font> text proposals, and predict each of them separately.<span style="font-size:80%;opacity:0.8"> 为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。</span></li><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless in-network connection of the <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8"> 此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li><li>The <font color=orangered>fine-scale</font> text proposals are detected accurately and reliably by our CTPN.<span style="font-size:80%;opacity:0.8"> 我们的CTPN能够准确可靠地检测细粒度的文本提议。</span></li><li>The <font color=orangered>fine-scale</font> detection and RNN connection are able to predict accurate localizations in vertical direction.<span style="font-size:80%;opacity:0.8"> 细粒度的检测和RNN连接可以预测垂直方向的精确位置。</span></li><li>The side-proposals are defined as the start and end proposals when we connect a sequence of detected <font color=orangered>fine-scale</font> text proposals into a text line.<span style="font-size:80%;opacity:0.8"> 当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。</span></li><li>Color of <font color=orangered>fine-scale</font> proposal box indicate a text/non-text score.<span style="font-size:80%;opacity:0.8"> 细粒度提议边界框的颜色表示文本/非文本分数。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the <font color=orangered>fine-scale</font> text proposal detection or in-network recurrent connection.<span style="font-size:80%;opacity:0.8"> 在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>4.2 <font color=orangered>Fine-Scale</font> Text Proposal Network with Faster R-CNN<span style="font-size:80%;opacity:0.8"> 4.2 具有Faster R-CNN的细粒度文本提议网络</span></li><li>We first discuss our <font color=orangered>fine-scale</font> detection strategy against the RPN and Faster R-CNN system [25].<span style="font-size:80%;opacity:0.8"> 我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。</span></li><li>Obviously, the proposed <font color=orangered>fine-scale</font> text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8"> 显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of <font color=orangered>fine-scale</font> text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8"> 显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>The CTPN detects a text line in a sequence of <font color=orangered>fine-scale</font> text proposals directly in convolutional maps.<span style="font-size:80%;opacity:0.8"> CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> jointly </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>We develop a vertical anchor mechanism that <font color=orangered>jointly</font> predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy.<span style="font-size:80%;opacity:0.8"> 我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。</span></li><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which <font color=orangered>jointly</font> predicts text/non-text scores, y-axis coordinates and side-refinement offsets of k anchors.<span style="font-size:80%;opacity:0.8"> RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>We develop an anchor regression mechanism that <font color=orangered>jointly</font> predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy.<span style="font-size:80%;opacity:0.8"> 我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。</span></li><li>W is the width of the conv5. $H_t$ is a recurrent internal state that is computed <font color=orangered>jointly</font> from both current input ($X_t$) and previous states encoded in $H_{t-1}$.<span style="font-size:80%;opacity:0.8"> $H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。</span></li><li>The proposed CTPN has three outputs which are <font color=orangered>jointly</font> connected to the last FC layer, as shown in Fig. 1 (a).<span style="font-size:80%;opacity:0.8"> 提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。</span></li><li>We employ multi-task learning to <font color=orangered>jointly</font> optimize model parameters.<span style="font-size:80%;opacity:0.8"> 我们采用多任务学习来联合优化模型参数。</span></li><li>We develop vertical anchor mechanism that <font color=orangered>jointly</font> predicts precise location and text/non-text score for each proposal, which is the key to realize accurate localization of text.<span style="font-size:80%;opacity:0.8"> 我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> sequential </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>The <font color=orangered>sequential</font> proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8"> 序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>The <font color=orangered>sequential</font> windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8"> 每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>(b) The CTPN outputs <font color=orangered>sequential</font> fixed-width fine-scale text proposals.<span style="font-size:80%;opacity:0.8"> （b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>Second, we propose an in-network recurrence mechanism that elegantly connects <font color=orangered>sequential</font> text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>Text have strong <font color=orangered>sequential</font> characteristics where the sequential context information is crucial to make a reliable decision.<span style="font-size:80%;opacity:0.8"> 文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。</span></li><li>Text have strong sequential characteristics where the <font color=orangered>sequential</font> context information is crucial to make a reliable decision.<span style="font-size:80%;opacity:0.8"> 文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。</span></li><li>Their results have shown that the <font color=orangered>sequential</font> context information is greatly facilitate the recognition task on cropped word images.<span style="font-size:80%;opacity:0.8"> 他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</span></li><li>To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as <font color=orangered>sequential</font> inputs, and updates its internal state recurrently in the hidden layer, $H_t$,<span style="font-size:80%;opacity:0.8"> 为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，</span></li><li>The sliding-window moves densely from left to right, resulting in $t=1,2,…,W$ <font color=orangered>sequential</font> features for each row.<span style="font-size:80%;opacity:0.8"> W是conv5的宽度。</span></li><li>Hence the internal state in RNN hidden layer accesses the <font color=orangered>sequential</font> context information scanned by all previous windows through the recurrent connection.<span style="font-size:80%;opacity:0.8"> 因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。</span></li><li>We propose an in-network RNN layer that connects <font color=orangered>sequential</font> text proposals elegantly, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8"> 我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> recurrent </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a <font color=orangered>recurrent</font> neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8"> 序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Scene text detection, convolutional network, <font color=orangered>recurrent</font> neural network, anchor mechanism<span style="font-size:80%;opacity:0.8"> 场景文本检测；卷积网络；循环神经网络；锚点机制</span></li><li>Then, an in-network <font color=orangered>recurrent</font> architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8"> 然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>We strive for a further step by proposing an in-network <font color=orangered>recurrent</font> mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.<span style="font-size:80%;opacity:0.8"> 我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, <font color=orangered>recurrent</font> connectionist text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8"> 它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.2 <font color=orangered>Recurrent</font> Connectionist Text Proposals<span style="font-size:80%;opacity:0.8"> 3.2 循环连接文本提议</span></li><li>This has been verified by recent work [9] where a <font color=orangered>recurrent</font> neural network (RNN) is applied to encode this context information for text recognition.<span style="font-size:80%;opacity:0.8"> 最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。</span></li><li>W is the width of the conv5. $H_t$ is a <font color=orangered>recurrent</font> internal state that is computed jointly from both current input ($X_t$) and previous states encoded in $H_{t-1}$.<span style="font-size:80%;opacity:0.8"> $H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。</span></li><li>The recurrence is computed by using a non-linear function $\varphi$, which defines exact form of the <font color=orangered>recurrent</font> model.<span style="font-size:80%;opacity:0.8"> 递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。</span></li><li>Hence the internal state in RNN hidden layer accesses the sequential context information scanned by all previous windows through the <font color=orangered>recurrent</font> connection.<span style="font-size:80%;opacity:0.8"> 因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the <font color=orangered>recurrent</font> context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8"> 我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or in-network <font color=orangered>recurrent</font> connection.<span style="font-size:80%;opacity:0.8"> 在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>4.3 <font color=orangered>Recurrent</font> Connectionist Text Proposals<span style="font-size:80%;opacity:0.8"> 4.3 循环连接文本提议</span></li><li>We discuss impact of <font color=orangered>recurrent</font> connection on our CTPN.<span style="font-size:80%;opacity:0.8"> 我们讨论循环连接对CTPN的影响。</span></li><li>As shown in Table 1 (left), with our <font color=orangered>recurrent</font> connection, the CTPN improves the FTPN substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Therefore, the proposed in-network <font color=orangered>recurrent</font> mechanism increase model computation marginally, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8"> 因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> seamlessly </td> <td> ['si:mlisli] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is <font color=orangered>seamlessly</font> incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8"> 序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Third, both methods are integrated <font color=orangered>seamlessly</font> to meet the nature of text sequence, resulting in a unified end-to-end trainable model.<span style="font-size:80%;opacity:0.8"> 第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> incorporate </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly <font color=orangered>incorporated</font> into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8"> 序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> trainable </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end <font color=orangered>trainable</font> model.<span style="font-size:80%;opacity:0.8"> 序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end <font color=orangered>trainable</font> model.<span style="font-size:80%;opacity:0.8"> 第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。</span></li><li>Therefore, our integration with the RNN layer is elegant, resulting in an efficient model that is end-to-end <font color=orangered>trainable</font> without additional cost.<span style="font-size:80%;opacity:0.8"> 因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。</span></li><li>We have presented a Connectionist Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end <font color=orangered>trainable</font>.<span style="font-size:80%;opacity:0.8"> 我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> F-measure </td> <td> [!≈ ef ˈmeʒə(r)] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 <font color=orangered>F-measure</font> on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8"> 它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 <font color=orangered>F-measure</font> over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8"> 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 <font color=orangered>F-measure</font> over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8"> 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>By refining the RPN proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a <font color=orangered>F-measure</font> of 0.75.<span style="font-size:80%;opacity:0.8"> 通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN substantially from a <font color=orangered>F-measure</font> of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>On the SWT, our improvements are significant on both recall and <font color=orangered>F-measure</font>, with marginal gain on precision.<span style="font-size:80%;opacity:0.8"> 在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。</span></li><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the <font color=orangered>F-measure</font> from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li>It consistently obtains substantial improvements on <font color=orangered>F-measure</font> and recall.<span style="font-size:80%;opacity:0.8"> 它始终在F-measure和召回率方面取得重大进展。</span></li><li>Regardless of running time, our method outperforms the FASText substantially with $11\%$ improvement on <font color=orangered>F-measure</font>.<span style="font-size:80%;opacity:0.8"> 无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> ICDAR </td> <td> [!≈ aɪ si: di: eɪ ɑ:(r)] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 F-measure on the <font color=orangered>ICDAR</font> 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8"> 它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the <font color=orangered>ICDAR</font> 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8"> 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the <font color=orangered>ICDAR</font> 2015).<span style="font-size:80%;opacity:0.8"> 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the <font color=orangered>ICDAR</font> 2013) by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8"> 此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>Our model was trained on 3,000 natural images, including 229 images from the <font color=orangered>ICDAR</font> 2013 training set.<span style="font-size:80%;opacity:0.8"> 我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the <font color=orangered>ICDAR</font> 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], <font color=orangered>ICDAR</font> 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], <font color=orangered>ICDAR</font> 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>The <font color=orangered>ICDAR</font> 2013 is used for this component evaluation.<span style="font-size:80%;opacity:0.8"> ICDAR 2013用于该组件的评估。</span></li><li>The <font color=orangered>ICDAR</font> 2011 dataset [21] consists of 229 training images and 255 testing ones, where the images are labelled in word level.<span style="font-size:80%;opacity:0.8"> ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。</span></li><li>The <font color=orangered>ICDAR</font> 2013 [19] is similar as the ICDAR 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively.<span style="font-size:80%;opacity:0.8"> ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。</span></li><li>The ICDAR 2013 [19] is similar as the <font color=orangered>ICDAR</font> 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively.<span style="font-size:80%;opacity:0.8"> ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。</span></li><li>The <font color=orangered>ICDAR</font> 2015 (Incidental Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass.<span style="font-size:80%;opacity:0.8"> ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。</span></li><li>For the <font color=orangered>ICDAR</font> 2011 we use the standard protocol proposed by [30], the evaluation on the ICDAR 2013 follows the standard in [19].<span style="font-size:80%;opacity:0.8"> 对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。</span></li><li>For the ICDAR 2011 we use the standard protocol proposed by [30], the evaluation on the <font color=orangered>ICDAR</font> 2013 follows the standard in [19].<span style="font-size:80%;opacity:0.8"> 对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。</span></li><li>For the <font color=orangered>ICDAR</font> 2015, we used the online evaluation system provided by the organizers as in [18].<span style="font-size:80%;opacity:0.8"> 对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。</span></li><li>The RPN proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the <font color=orangered>ICDAR</font> 2013 standard.<span style="font-size:80%;opacity:0.8"> RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。</span></li><li>Table 1: Component evaluation on the <font color=orangered>ICDAR</font> 2013, and State-of-the-art results on the SWT and MULTILINGUAL.<span style="font-size:80%;opacity:0.8"> 表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>We set short side of images to 2000 for the SWT and <font color=orangered>ICDAR</font> 2015, and 600 for the other three.<span style="font-size:80%;opacity:0.8"> 我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。</span></li><li>On the <font color=orangered>ICDAR</font> 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li>Table 2: State-of-the-art results on the <font color=orangered>ICDAR</font> 2011, 2013 and 2015.<span style="font-size:80%;opacity:0.8"> 表2：ICDAR 2011，2013和2015上的最新结果。</span></li><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the <font color=orangered>ICDAR</font> 2013, which are compared competitively against Gupta et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> surpass </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, <font color=orangered>surpassing</font> recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8"> 它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> computationally </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>The CTPN is <font color=orangered>computationally</font> efficient with 0.14s/image, by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8"> 通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>Furthermore, it is <font color=orangered>computationally</font> efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8"> 此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>Another limitation is that the sliding-window methods are <font color=orangered>computationally</font> expensive, by running a classifier on a huge number of the sliding windows.<span style="font-size:80%;opacity:0.8"> 另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> VGG16 </td> <td>  </td> <td> 
<ul><li>The CTPN is computationally efficient with 0.14s/image, by using the very deep <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8"> 通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8"> 我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。</span></li><li>Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8"> 此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>We take the very deep 16-layer vggNet (<font color=forestgreen>VGG16</font>) [27] as an example to describe our approach, which is readily applicable to other deep models.<span style="font-size:80%;opacity:0.8"> 我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。</span></li><li>We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the <font color=forestgreen>VGG16</font>).<span style="font-size:80%;opacity:0.8"> 我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。</span></li><li>Given an input image, we have $W \times H \times C$ conv5 features maps (by using the <font color=forestgreen>VGG16</font> model), where C is the number of feature maps or channels, and $W \times H$ is the spatial arrangement.<span style="font-size:80%;opacity:0.8"> 给定输入图像，我们有$W \times H \times C$ conv5特征映射（通过使用VGG16模型），其中C是特征映射或通道的数目，并且$W \times H$是空间布置。</span></li><li>We follow the standard practice, and explore the very deep <font color=forestgreen>VGG16</font> model [27] pre-trained on the ImageNet data [26].<span style="font-size:80%;opacity:0.8"> 我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> retrieval </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>This is due to its numerous practical applications such as image OCR, multi-language translation, image <font color=orangered>retrieval</font>, etc. It includes two sub tasks: text detection and recognition.<span style="font-size:80%;opacity:0.8"> 这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> variance </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>Large <font color=orangered>variance</font> of text patterns and highly cluttered background pose main challenge of accurate text localization.<span style="font-size:80%;opacity:0.8"> 文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> clutter </td> <td> [ˈklʌtə(r)] </td> <td> 
<ul><li>Large variance of text patterns and highly <font color=orangered>cluttered</font> background pose main challenge of accurate text localization.<span style="font-size:80%;opacity:0.8"> 文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> verification </td> <td> [ˌverɪfɪ'keɪʃn] </td> <td> 
<ul><li>They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line <font color=orangered>verification</font>.<span style="font-size:80%;opacity:0.8"> 它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> robustness </td> <td> [rəʊ'bʌstnəs] </td> <td> 
<ul><li>These multi-step bottom-up approaches are generally complicated with less <font color=orangered>robustness</font> and reliability.<span style="font-size:80%;opacity:0.8"> 这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> connected-component </td> <td> [!≈ kə'nektɪd kəmˈpəʊnənt] </td> <td> 
<ul><li>Their performance heavily rely on the results of character detection, and <font color=orangered>connected-components</font> methods or sliding-window methods have been proposed.<span style="font-size:80%;opacity:0.8"> 它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。</span></li><li>They can be roughly grouped into two categories, <font color=orangered>connected-components</font> (CCs) based approaches and sliding-window based methods.<span style="font-size:80%;opacity:0.8"> 它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> e.g. </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>These methods commonly explore low-level features (<font color=orangered>e.g.</font>, based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8"> 这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li><li>For object detection, a typical correct detection is defined loosely, <font color=orangered>e.g.</font>, by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8"> 对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (<font color=orangered>e.g.</font>, the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8"> 对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, <font color=orangered>e.g.</font>, the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21].<span style="font-size:80%;opacity:0.8"> 因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (<font color=orangered>e.g.</font>, 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8"> 第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, <font color=orangered>e.g.</font>, intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8"> 基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li><li>However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, <font color=orangered>e.g.</font>, the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8"> 然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li><li>It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (<font color=orangered>e.g.</font>, fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).<span style="font-size:80%;opacity:0.8"> 它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</span></li><li>We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (<font color=orangered>e.g.</font>, the conv5 of the VGG16).<span style="font-size:80%;opacity:0.8"> 我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。</span></li><li>Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, <font color=orangered>e.g.</font>, detecting part of a word.<span style="font-size:80%;opacity:0.8"> 文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。</span></li><li>It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, <font color=orangered>e.g.</font>, a text piece with 16-pixel width.<span style="font-size:80%;opacity:0.8"> 将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, <font color=orangered>e.g.</font>, $228 \times width$.<span style="font-size:80%;opacity:0.8"> 我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>This may lead to an inaccurate localization when the text proposals in both horizontal sides are not exactly covered by a ground truth text line area, or some side proposals are discarded (<font color=orangered>e.g.</font>, having a low text score), as shown in Fig. 4.<span style="font-size:80%;opacity:0.8"> 如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。</span></li><li>where $x_{side}$ is the predicted x-coordinate of the nearest horizontal side (<font color=orangered>e.g.</font>, left or right side) to current anchor.<span style="font-size:80%;opacity:0.8"> 其中，$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的x坐标。</span></li><li>k is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (<font color=orangered>e.g.</font>, 32-pixel) to the left or right side of a ground truth text line bounding box.<span style="font-size:80%;opacity:0.8"> k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li><li>We initialize the new layers (<font color=orangered>e.g.</font>, the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard deviation.<span style="font-size:80%;opacity:0.8"> 我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, <font color=orangered>e.g.</font>, the fine-scale text proposal detection or in-network recurrent connection.<span style="font-size:80%;opacity:0.8"> 在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>It is of great importance for recovering highly ambiguous text (<font color=orangered>e.g.</font>, extremely small-scale ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8"> 对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>It is able to handle multi-scale and multi-language efficiently (<font color=orangered>e.g.</font>, Chinese and Korean).<span style="font-size:80%;opacity:0.8"> 它能够有效地处理多尺度和多语言（例如中文和韩文）。</span></li><li>This may due to strong capability of CTPN for detecting extremely challenging text, <font color=orangered>e.g.</font>, very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8"> 这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> SWT </td> <td> ['esd'əbəlju:t'i:] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on <font color=orangered>SWT</font> [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8"> 这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li><li>The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the <font color=orangered>SWT</font> and Multi-Lingual datasets.<span style="font-size:80%;opacity:0.8"> 边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], <font color=orangered>SWT</font> [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>Epshtein et al. [3] introduced the <font color=orangered>SWT</font> dataset containing 307 images which include many extremely small-scale text.<span style="font-size:80%;opacity:0.8"> Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li><li>The evaluations on the <font color=orangered>SWT</font> and Multilingual datasets follow the protocols defined in [3] and [24] respectively.<span style="font-size:80%;opacity:0.8"> SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</span></li><li>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the <font color=orangered>SWT</font> and MULTILINGUAL.<span style="font-size:80%;opacity:0.8"> 表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>We set short side of images to 2000 for the <font color=orangered>SWT</font> and ICDAR 2015, and 600 for the other three.<span style="font-size:80%;opacity:0.8"> 我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。</span></li><li>On the <font color=orangered>SWT</font>, our improvements are significant on both recall and F-measure, with marginal gain on precision.<span style="font-size:80%;opacity:0.8"> 在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> MSER </td> <td> [!≈ em es i: ɑ:(r)] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on SWT [3,13], <font color=orangered>MSER</font> [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8"> 这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> HoG </td> <td> [hɒg] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or <font color=orangered>HoG</font> [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8"> 这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> sequentially </td> <td> [sɪ'kwenʃəlɪ] </td> <td> 
<ul><li>Furthermore, these false detections are easily accumulated <font color=orangered>sequentially</font> in bottom-up pipeline, as pointed out in [28].<span style="font-size:80%;opacity:0.8"> 此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。</span></li><li>Then a text line is constructed by <font color=orangered>sequentially</font> connecting the pairs having a same proposal.<span style="font-size:80%;opacity:0.8"> 然后通过顺序连接具有相同提议的对来构建文本行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> in-network </td> <td> [!≈ ɪn ˈnetwɜ:k] </td> <td> 
<ul><li>Then, an <font color=orangered>in-network</font> recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8"> 然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>We strive for a further step by proposing an <font color=orangered>in-network</font> recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.<span style="font-size:80%;opacity:0.8"> 我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</span></li><li>Second, we propose an <font color=orangered>in-network</font> recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless <font color=orangered>in-network</font> connection of the fine-scale text proposals.<span style="font-size:80%;opacity:0.8"> 此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or <font color=orangered>in-network</font> recurrent connection.<span style="font-size:80%;opacity:0.8"> 在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>Therefore, the proposed <font color=orangered>in-network</font> recurrent mechanism increase model computation marginally, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8"> 因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li><li>We propose an <font color=orangered>in-network</font> RNN layer that connects sequential text proposals elegantly, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8"> 我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> substantially </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Deep Convolutional Neural Networks (CNN) have recently advanced general object detection <font color=orangered>substantially</font> [25,5,6].<span style="font-size:80%;opacity:0.8"> 深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。</span></li><li>Convolutional Neural Networks (CNN) have recently advanced general object detection <font color=orangered>substantially</font> [25,5,6].<span style="font-size:80%;opacity:0.8"> 卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。</span></li><li>However, text differs from generic objects <font color=orangered>substantially</font>, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2].<span style="font-size:80%;opacity:0.8"> 然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN <font color=orangered>substantially</font> from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Regardless of running time, our method outperforms the FASText <font color=orangered>substantially</font> with $11\%$ improvement on F-measure.<span style="font-size:80%;opacity:0.8"> 无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> Region-CNN </td> <td>  </td> <td> 
<ul><li>The state-of-the-art method is Faster <font color=forestgreen>Region-CNN</font> (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> RPN </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (<font color=orangered>RPN</font>) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li><li>Then the <font color=orangered>RPN</font> proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection.<span style="font-size:80%;opacity:0.8"> 然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>In this work, we fill this gap by extending the <font color=orangered>RPN</font> architecture [25] to accurate text line localization.<span style="font-size:80%;opacity:0.8"> 在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。</span></li><li>This departs from the <font color=orangered>RPN</font> prediction of a whole object, which is difficult to provide a satisfied localization accuracy.<span style="font-size:80%;opacity:0.8"> 这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</span></li><li>They proposed a Region Proposal Network (<font color=orangered>RPN</font>) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。</span></li><li>The <font color=orangered>RPN</font> is fast by sharing convolutional computation.<span style="font-size:80%;opacity:0.8"> 通过共享卷积计算RPN是快速的。</span></li><li>However, the <font color=orangered>RPN</font> proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8"> 然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li><li>Similar to Region Proposal Network (<font color=orangered>RPN</font>) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size.<span style="font-size:80%;opacity:0.8"> 类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the <font color=orangered>RPN</font> to detect multi-scale objects with a single-scale window.<span style="font-size:80%;opacity:0.8"> 在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。</span></li><li>An example is shown in Fig. 2, where the <font color=orangered>RPN</font> is directly trained for localizing text lines in an image.<span style="font-size:80%;opacity:0.8"> 一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</span></li><li>Fig. 2: Left: <font color=orangered>RPN</font> proposals.<span style="font-size:80%;opacity:0.8"> 图2：左：RPN提议。</span></li><li>We observed that word detection by the <font color=orangered>RPN</font> is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word.<span style="font-size:80%;opacity:0.8"> 我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。</span></li><li>This reduces the search space, compared to the <font color=orangered>RPN</font> which predicts 4 coordinates of an object.<span style="font-size:80%;opacity:0.8"> 与预测目标4个坐标的RPN相比，这减少了搜索空间。</span></li><li>Compared to the <font color=orangered>RPN</font> or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection.<span style="font-size:80%;opacity:0.8"> 与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</span></li><li>Similar to <font color=orangered>RPN</font> [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding GT box.<span style="font-size:80%;opacity:0.8"> 与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</span></li><li>We first discuss our fine-scale detection strategy against the <font color=orangered>RPN</font> and Faster R-CNN system [25].<span style="font-size:80%;opacity:0.8"> 我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。</span></li><li>As can be found in Table 1 (left), the individual <font color=orangered>RPN</font> is difficult to perform accurate text localization, by generating a large amount of false detections (low precision).<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。</span></li><li>By refining the <font color=orangered>RPN</font> proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a F-measure of 0.75.<span style="font-size:80%;opacity:0.8"> 通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。</span></li><li>One observation is that the Faster R-CNN also increases the recall of original <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8"> 一个观察结果是Faster R-CNN也增加了原始RPN的召回率。</span></li><li>The <font color=orangered>RPN</font> proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the ICDAR 2013 standard.<span style="font-size:80%;opacity:0.8"> RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> class-agnostic </td> <td> [!≈ klɑ:s ægˈnɒstɪk] </td> <td> 
<ul><li>The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality <font color=orangered>class-agnostic</font> object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li><li>Selective Search (SS) [4] which generates <font color=orangered>class-agnostic</font> object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8"> 生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li><li>They proposed a Region Proposal Network (RPN) that generates high-quality <font color=orangered>class-agnostic</font> object proposals directly from the convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> refinement </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and <font color=orangered>refinement</font>, leading to the state-of-the-art performance on generic object detection.<span style="font-size:80%;opacity:0.8"> 然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or <font color=orangered>refinement</font>.<span style="font-size:80%;opacity:0.8"> 我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</span></li><li>However, the RPN proposals are not discriminative, and require a further <font color=orangered>refinement</font> and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8"> 然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> generic </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on <font color=orangered>generic</font> object detection.<span style="font-size:80%;opacity:0.8"> 然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>In <font color=orangered>generic</font> object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes.<span style="font-size:80%;opacity:0.8"> 在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。</span></li><li>We present several technical developments that tailor <font color=orangered>generic</font> object detection model elegantly towards our problem.<span style="font-size:80%;opacity:0.8"> 我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。</span></li><li>However, text differs from <font color=orangered>generic</font> objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2].<span style="font-size:80%;opacity:0.8"> 然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。</span></li><li>Obviously, a text line is a sequence which is the main difference between text and <font color=orangered>generic</font> objects.<span style="font-size:80%;opacity:0.8"> 显然，文本行是一个序列，它是文本和通用目标之间的主要区别。</span></li><li>This inaccuracy may be not crucial in <font color=orangered>generic</font> object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words.<span style="font-size:80%;opacity:0.8"> 这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li><li>This is different from <font color=orangered>generic</font> object detection where the impact of condition (ii) may be not significant.<span style="font-size:80%;opacity:0.8"> 这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> bounding </td> <td> [baundɪŋ] </td> <td> 
<ul><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected <font color=orangered>bounding</font> box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8"> 对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>The explicit vertical coordinates are measured by the height and y-axis center of a proposal <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8"> 明确的垂直坐标是通过提议边界框的高度和y轴中心来度量的。</span></li><li>We compute relative predicted vertical coordinates ($\textbf{v}$) with respect to the <font color=orangered>bounding</font> box location of an anchor as,<span style="font-size:80%;opacity:0.8"> 我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：</span></li><li>Therefore, each predicted text proposal has a <font color=orangered>bounding</font> box with size of $h\times 16$ (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right).<span style="font-size:80%;opacity:0.8"> 因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。</span></li><li>$x^*_{side}$ is the ground truth (GT) side coordinate in x-axis, which is pre-computed from the GT <font color=orangered>bounding</font> box and anchor location.<span style="font-size:80%;opacity:0.8"> $x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>We only use the offsets of the side-proposals to refine the final text line <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8"> 我们只使用边缘提议的偏移量来优化最终的文本行边界框。</span></li><li>k is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (e.g., 32-pixel) to the left or right side of a ground truth text line <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8"> k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li><li>It is defined by computing the IoU overlap with the GT <font color=orangered>bounding</font> box (divided by anchor location).<span style="font-size:80%;opacity:0.8"> 它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。</span></li><li>We collected the other images ourselves and manually labelled them with text line <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8"> 我们自己收集了其他图像，并用文本行边界框进行了手工标注。</span></li><li>This may benefit from joint <font color=orangered>bounding</font> box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted bounding box.<span style="font-size:80%;opacity:0.8"> 这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。</span></li><li>This may benefit from joint bounding box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8"> 这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> PASCAL </td> <td> ['pæskәl] </td> <td> 
<ul><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the <font color=orangered>PASCAL</font> standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8"> 对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> comprehensively </td> <td> [ˌkɒmprɪˈhensɪvli] </td> <td> 
<ul><li>By contrast, reading text <font color=orangered>comprehensively</font> is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word.<span style="font-size:80%;opacity:0.8"> 相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> fine-grained </td> <td> [faɪn'greɪnd] </td> <td> 
<ul><li>By contrast, reading text comprehensively is a <font color=orangered>fine-grained</font> recognition task which requires a correct detection that covers a full region of a text line or word.<span style="font-size:80%;opacity:0.8"> 相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> elegantly </td> <td> ['elɪɡəntlɪ] </td> <td> 
<ul><li>We present several technical developments that tailor generic object detection model <font color=orangered>elegantly</font> towards our problem.<span style="font-size:80%;opacity:0.8"> 我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。</span></li><li>Second, we propose an in-network recurrence mechanism that <font color=orangered>elegantly</font> connects sequential text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8"> 其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>We propose an in-network RNN layer that connects sequential text proposals <font color=orangered>elegantly</font>, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8"> 我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> leverage </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>We <font color=orangered>leverage</font> the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1.<span style="font-size:80%;opacity:0.8"> 我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> recurrently </td> <td> [rɪ'kʌrəntlɪ] </td> <td> 
<ul><li>The sequential windows in each row are <font color=orangered>recurrently</font> connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8"> 每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>RNN provides a natural choice for encoding this information <font color=orangered>recurrently</font> using its hidden layers.<span style="font-size:80%;opacity:0.8"> RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。</span></li><li>To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as sequential inputs, and updates its internal state <font color=orangered>recurrently</font> in the hidden layer, $H_t$,<span style="font-size:80%;opacity:0.8"> 为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> bi-directional </td> <td> ['bɪdɪr'ekʃənl] </td> <td> 
<ul><li>The sequential windows in each row are recurrently connected by a <font color=orangered>Bi-directional</font> LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8"> 每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>We further extend the RNN layer by using a <font color=orangered>bi-directional</font> LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8"> 我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> BLSTM </td> <td> [!≈ bi: el es ti: em] </td> <td> 
<ul><li>The sequential windows in each row are recurrently connected by a Bi-directional LSTM (<font color=orangered>BLSTM</font>) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8"> 每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D <font color=orangered>BLSTM</font> (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8"> 每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> y-axis </td> <td> [ˈwaiˌæksis] </td> <td> 
<ul><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, <font color=orangered>y-axis</font> coordinates and side-refinement offsets of k anchors.<span style="font-size:80%;opacity:0.8"> RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and <font color=orangered>y-axis</font> location of each fine-scale proposal.<span style="font-size:80%;opacity:0.8"> 我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。</span></li><li>The explicit vertical coordinates are measured by the height and <font color=orangered>y-axis</font> center of a proposal bounding box.<span style="font-size:80%;opacity:0.8"> 明确的垂直坐标是通过提议边界框的高度和y轴中心来度量的。</span></li><li>$c_y^a$ and $h^a$ are the center (<font color=orangered>y-axis</font>) and height of the anchor box, which can be pre-computed from an input image.<span style="font-size:80%;opacity:0.8"> $c_y^a$和$h^a$是锚盒的中心（y轴）和高度，可以从输入图像预先计算。</span></li><li>$c_y$ and $h$ are the predicted <font color=orangered>y-axis</font> coordinates in the input image, while $c^*_y$ and $h^*$ are the ground truth coordinates.<span style="font-size:80%;opacity:0.8"> $c_y$和$h$是输入图像中预测的y轴坐标，而$c^*_y$和$h^*$是实际坐标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> side-refinement </td> <td> [!≈ saɪd rɪˈfaɪnmənt] </td> <td> 
<ul><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and <font color=orangered>side-refinement</font> offsets of k anchors.<span style="font-size:80%;opacity:0.8"> RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and <font color=orangered>side-refinement</font>.<span style="font-size:80%;opacity:0.8"> 它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.3 <font color=orangered>Side-refinement</font><span style="font-size:80%;opacity:0.8"> 3.3 边缘细化</span></li><li>To address this problem, we propose a <font color=orangered>side-refinement</font> approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or side-proposal).<span style="font-size:80%;opacity:0.8"> 为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>Several detection examples improved by <font color=orangered>side-refinement</font> are presented in Fig. 4.<span style="font-size:80%;opacity:0.8"> 通过边缘细化改进的几个检测示例如图4所示。</span></li><li>The <font color=orangered>side-refinement</font> further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and Multi-Lingual datasets.<span style="font-size:80%;opacity:0.8"> 边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li><li>Notice that the offset for <font color=orangered>side-refinement</font> is predicted simultaneously by our model, as shown in Fig. 1.<span style="font-size:80%;opacity:0.8"> 请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。</span></li><li>Fig. 4: CTPN detection with (red box) and without (yellow dashed box) the <font color=orangered>side-refinement</font>.<span style="font-size:80%;opacity:0.8"> 图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li><li>q. (2) and <font color=orangered>side-refinement</font> offset ($\textbf{o}$). We explore k anchors to predict them on each spatial location in the conv5, resulting in 2k, 2k and k parameters in the output layer, respectively.<span style="font-size:80%;opacity:0.8"> 我们将探索k个锚点来预测它们在conv5中的每个空间位置，从而在输出层分别得到2k，2k和k个参数。</span></li><li>We introduce three loss functions, $L^{cl}_s, L^{re}_v and l^{re}_o$, which compute errors of text/non-text score, coordinate and <font color=orangered>side-refinement</font>, respectively.<span style="font-size:80%;opacity:0.8"> 我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> multi-lingual </td> <td> [!≈ 'mʌlti ˈlɪŋgwəl] </td> <td> 
<ul><li>Our method is able to handle multi-scale and <font color=orangered>multi-lingual</font> text in a single process, avoiding further post filtering or refinement.<span style="font-size:80%;opacity:0.8"> 我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</span></li><li>The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and <font color=orangered>Multi-Lingual</font> datasets.<span style="font-size:80%;opacity:0.8"> 边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> cc </td> <td> [ˌsi: ˈsi:] </td> <td> 
<ul><li>They can be roughly grouped into two categories, connected-components (<font color=orangered>CCs</font>) based approaches and sliding-window based methods.<span style="font-size:80%;opacity:0.8"> 它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。</span></li><li>The <font color=orangered>CCs</font> based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8"> 基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> greedily </td> <td> ['gri:dɪlɪ] </td> <td> 
<ul><li>The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are <font color=orangered>greedily</font> grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8"> 基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> robustly </td> <td> [rəʊ'bʌstlɪ] </td> <td> 
<ul><li>Furthermore, <font color=orangered>robustly</font> filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14].<span style="font-size:80%;opacity:0.8"> 此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> inexpensive </td> <td> [ˌɪnɪkˈspensɪv] </td> <td> 
<ul><li>A common strategy is to generate a number of object proposals by employing <font color=orangered>inexpensive</font> low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals.<span style="font-size:80%;opacity:0.8"> 一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> Selective </td> <td> [sɪˈlektɪv] </td> <td> 
<ul><li><font color=orangered>Selective</font> Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8"> 生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> SS </td> <td> [!≈ es es] </td> <td> 
<ul><li>Selective Search (<font color=orangered>SS</font>) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8"> 生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> discriminative </td> <td> [dɪs'krɪmɪnətɪv] </td> <td> 
<ul><li>However, the RPN proposals are not <font color=orangered>discriminative</font>, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8"> 然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> domain-specific </td> <td> [!≈ dəˈmeɪn spəˈsɪfɪk] </td> <td> 
<ul><li>More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly <font color=orangered>domain-specific</font> task.<span style="font-size:80%;opacity:0.8"> 更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> arbitrary </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>Similar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of <font color=orangered>arbitrary</font> size.<span style="font-size:80%;opacity:0.8"> 类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>This dataset is more challenging than previous ones by including <font color=orangered>arbitrary</font> orientation, very small-scale and low resolution text.<span style="font-size:80%;opacity:0.8"> 这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> applicable </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily <font color=orangered>applicable</font> to other deep models.<span style="font-size:80%;opacity:0.8"> 我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> receptive </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>The size of conv5 feature maps is determined by the size of input image, while the total stride and <font color=orangered>receptive</font> field are fixed as 16 and 228 pixels, respectively.<span style="font-size:80%;opacity:0.8"> conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。</span></li><li>Both the total stride and <font color=orangered>receptive</font> field are fixed by the network architecture.<span style="font-size:80%;opacity:0.8"> 网络架构决定总步长和感受野。</span></li><li>Generally, an text proposal is largely smaller than its effective <font color=orangered>receptive</font> field which is $228\times228$.<span style="font-size:80%;opacity:0.8"> 一般来说，文本提议在很大程度上要比它的有效感受野$228\times228$要小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> y-coordinate </td> <td> [ˌwaikəuˈɔ:dinət,-neit] </td> <td> 
<ul><li>Then we design k vertical anchors to predict <font color=orangered>y-coordinates</font> for each proposal.<span style="font-size:80%;opacity:0.8"> 然后，我们设计k个垂直锚点来预测每个提议的y坐标。</span></li><li>Our detector outputs the text/non-text scores and the predicted <font color=orangered>y-coordinates</font> ($\textbf{v}$) for k anchors at each window location.<span style="font-size:80%;opacity:0.8"> 我们的检测器在每个窗口位置输出k个锚点的文本/非文本分数和预测的y轴坐标（$\textbf{v}$）。</span></li><li>Similar to the <font color=orangered>y-coordinate</font> prediction, we compute relative offset as,<span style="font-size:80%;opacity:0.8"> 与y坐标预测类似，我们计算相对偏移为：</span></li><li>$\textbf{s}_i^*=\lbrace 0,1\rbrace$ is the ground truth. j is the index of an anchor in the set of valid anchors for <font color=orangered>y-coordinates</font> regression, which are defined as follow.<span style="font-size:80%;opacity:0.8"> $\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。</span></li><li>$\textbf{v}_j$ and $\textbf{v}_j^*$ are the prediction and ground truth <font color=orangered>y-coordinates</font> associated with the $j-{th}$ anchor.<span style="font-size:80%;opacity:0.8"> $\textbf{v}_j$和$\textbf{v}_j^*$是与第j个锚点关联的预测的和真实的y坐标。</span></li><li>The training labels for the <font color=orangered>y-coordinate</font> regression ($\textbf{v}^*$) and offset regression ($\textbf{o}^*$) are computed as E. q. (2) and (4) respectively.<span style="font-size:80%;opacity:0.8"> y坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> x-coordinate </td> <td> ['ekskəʊ'ɔ:dnɪt] </td> <td> 
<ul><li>For each prediction, the horizontal location (<font color=orangered>x-coordinates</font>) and k-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image.<span style="font-size:80%;opacity:0.8"> 对于每个预测，水平位置（x轴坐标）和k个锚点位置是固定的，可以通过将conv5中的空间窗口位置映射到输入图像上来预先计算。</span></li><li>where $x_{side}$ is the predicted <font color=orangered>x-coordinate</font> of the nearest horizontal side (e.g., left or right side) to current anchor.<span style="font-size:80%;opacity:0.8"> 其中，$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的x坐标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> k-anchor </td> <td> [!≈ keɪ ˈæŋkə(r)] </td> <td> 
<ul><li>For each prediction, the horizontal location (x-coordinates) and <font color=orangered>k-anchor</font> locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image.<span style="font-size:80%;opacity:0.8"> 对于每个预测，水平位置（x轴坐标）和k个锚点位置是固定的，可以通过将conv5中的空间窗口位置映射到输入图像上来预先计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> suppression </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>The detected text proposals are generated from the anchors having a text/non-text score of &gt;0.7 (with non-maximum <font color=orangered>suppression</font>).<span style="font-size:80%;opacity:0.8"> 检测到的文本提议是从具有> 0.7（具有非极大值抑制）的文本/非文本分数的锚点生成的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> localizations </td> <td> [!≈ ˌləʊkəlaɪ'zeɪʃnz] </td> <td> 
<ul><li>This further reduces its computation, and at the same time, predicting accurate <font color=orangered>localizations</font> of the text lines.<span style="font-size:80%;opacity:0.8"> 这进一步减少了计算量，同时预测了文本行的准确位置。</span></li><li>The fine-scale detection and RNN connection are able to predict accurate <font color=orangered>localizations</font> in vertical direction.<span style="font-size:80%;opacity:0.8"> 细粒度的检测和RNN连接可以预测垂直方向的精确位置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> outliers </td> <td> [aʊt'laɪəz] </td> <td> 
<ul><li>This may lead to a number of false detections on non-text objects which have a similar structure as text patterns, such as windows, bricks, leaves, etc. (referred as text-like <font color=orangered>outliers</font> in [13]).<span style="font-size:80%;opacity:0.8"> 这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。</span></li><li>As shown in Fig. 3, the context information is greatly helpful to reduce false detections, such as text-like <font color=orangered>outliers</font>.<span style="font-size:80%;opacity:0.8"> 如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> seamless </td> <td> [ˈsi:mləs] </td> <td> 
<ul><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and <font color=orangered>seamless</font> in-network connection of the fine-scale text proposals.<span style="font-size:80%;opacity:0.8"> 此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> multiplicative </td> <td> ['mʌltɪplɪkeɪtɪv] </td> <td> 
<ul><li>The LSTM was proposed specially to address vanishing gradient problem, by introducing three additional <font color=orangered>multiplicative</font> gates: the input gate, forget gate and output gate.<span style="font-size:80%;opacity:0.8"> 通过引入三个附加乘法门：输入门，忘记门和输出门，专门提出了LSTM以解决梯度消失问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> inaccuracy </td> <td> [ɪn'ækjərəsɪ] </td> <td> 
<ul><li>This <font color=orangered>inaccuracy</font> may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words.<span style="font-size:80%;opacity:0.8"> 这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> small-scale </td> <td> [ˈsmɔ:lˈskeɪl] </td> <td> 
<ul><li>This inaccuracy may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those <font color=orangered>small-scale</font> text lines or words.<span style="font-size:80%;opacity:0.8"> 这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li><li>This is crucial to detect <font color=orangered>small-scale</font> text patterns, which is one of key advantages of the CTPN.<span style="font-size:80%;opacity:0.8"> 这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</span></li><li>This dataset is more challenging than previous ones by including arbitrary orientation, very <font color=orangered>small-scale</font> and low resolution text.<span style="font-size:80%;opacity:0.8"> 这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li><li>Epshtein et al. [3] introduced the SWT dataset containing 307 images which include many extremely <font color=orangered>small-scale</font> text.<span style="font-size:80%;opacity:0.8"> Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li><li>It is of great importance for recovering highly ambiguous text (e.g., extremely <font color=orangered>small-scale</font> ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8"> 对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>Fig. 6: CTPN detection results on extremely <font color=orangered>small-scale</font> cases (in red boxes), where some ground truth boxes are missed.<span style="font-size:80%;opacity:0.8"> 图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。</span></li><li>This may due to strong capability of CTPN for detecting extremely challenging text, e.g., very <font color=orangered>small-scale</font> ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8"> 这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> side-anchor </td> <td> [!≈ saɪd ˈæŋkə(r)] </td> <td> 
<ul><li>To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as <font color=orangered>side-anchor</font> or side-proposal).<span style="font-size:80%;opacity:0.8"> 为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>k is the index of a <font color=orangered>side-anchor</font>, which is defined as a set of anchors within a horizontal distance (e.g., 32-pixel) to the left or right side of a ground truth text line bounding box.<span style="font-size:80%;opacity:0.8"> k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> side-proposal </td> <td> [!≈ saɪd prəˈpəʊzl] </td> <td> 
<ul><li>To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or <font color=orangered>side-proposal</font>).<span style="font-size:80%;opacity:0.8"> 为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>The <font color=orangered>side-proposals</font> are defined as the start and end proposals when we connect a sequence of detected fine-scale text proposals into a text line.<span style="font-size:80%;opacity:0.8"> 当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。</span></li><li>We only use the offsets of the <font color=orangered>side-proposals</font> to refine the final text line bounding box.<span style="font-size:80%;opacity:0.8"> 我们只使用边缘提议的偏移量来优化最终的文本行边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> GT </td> <td> [dʒi:'ti:] </td> <td> 
<ul><li>$x^*_{side}$ is the ground truth (<font color=orangered>GT</font>) side coordinate in x-axis, which is pre-computed from the GT bounding box and anchor location.<span style="font-size:80%;opacity:0.8"> $x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>$x^*_{side}$ is the ground truth (GT) side coordinate in x-axis, which is pre-computed from the <font color=orangered>GT</font> bounding box and anchor location.<span style="font-size:80%;opacity:0.8"> $x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>Similar to RPN [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding <font color=orangered>GT</font> box.<span style="font-size:80%;opacity:0.8"> 与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</span></li><li>It is defined by computing the IoU overlap with the <font color=orangered>GT</font> bounding box (divided by anchor location).<span style="font-size:80%;opacity:0.8"> 它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。</span></li><li>A positive anchor is defined as : (i) an anchor that has an &gt; 0.7 IoU overlap with any <font color=orangered>GT</font> box; or (ii) the anchor with the highest IoU overlap with a GT box.<span style="font-size:80%;opacity:0.8"> 正锚点被定义为：（i）与任何实际边界框具有>0.7的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。</span></li><li>A positive anchor is defined as : (i) an anchor that has an &gt; 0.7 IoU overlap with any GT box; or (ii) the anchor with the highest IoU overlap with a <font color=orangered>GT</font> box.<span style="font-size:80%;opacity:0.8"> 正锚点被定义为：（i）与任何实际边界框具有>0.7的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。</span></li><li>The negative anchors are defined as &lt;0.5 IoU overlap with all <font color=orangered>GT</font> boxes.<span style="font-size:80%;opacity:0.8"> 负锚点定义为与所有实际边界框具有<0.5的IoU重叠。</span></li><li>As shown in Fig. 6, those challenging ones are detected correctly by our detector, but some of them are even missed by the <font color=orangered>GT</font> labelling, which may reduce our precision in evaluation.<span style="font-size:80%;opacity:0.8"> 如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> x-axis </td> <td> [ˈeksˌæksis] </td> <td> 
<ul><li>$x^*_{side}$ is the ground truth (GT) side coordinate in <font color=orangered>x-axis</font>, which is pre-computed from the GT bounding box and anchor location.<span style="font-size:80%;opacity:0.8"> $x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>$c_x^a$ is the center of anchor in <font color=orangered>x-axis</font>.<span style="font-size:80%;opacity:0.8"> $c_x^a$是x轴的锚点的中心。</span></li><li>$\textbf{o}_k$ and $\textbf{o}_k^*$ are the predicted and ground truth offsets in <font color=orangered>x-axis</font> associated to the $k-{th}$ anchor.<span style="font-size:80%;opacity:0.8"> $L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> dash </td> <td> [dæʃ] </td> <td> 
<ul><li>Fig. 4: CTPN detection with (red box) and without (yellow <font color=orangered>dashed</font> box) the side-refinement.<span style="font-size:80%;opacity:0.8"> 图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> Intersection-over-Union </td> <td> [!≈ ˌɪntəˈsekʃn ˈəʊvə(r) ˈju:niən] </td> <td> 
<ul><li>A valid anchor is a defined positive anchor ($\textbf{s}_j^*=1$, described below), or has an <font color=orangered>Intersection-over-Union</font> (IoU) &gt;0.5 overlap with a ground truth text proposal.<span style="font-size:80%;opacity:0.8"> 有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）>0.5。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> empirically </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li>$\lambda_1$ and $\lambda_2$ are loss weights to balance different tasks, which are <font color=orangered>empirically</font> set to 1.0 and 2.0. $N_{s}$, $N_{v}$ and $N_{o}$ are normalization parameters, denoting the total number of anchors used by $L^{cl}_s$, $L^{re}_v$ and $L^{re}_o$, respectively.<span style="font-size:80%;opacity:0.8"> $N_{s}$, $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> stochastic </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and <font color=orangered>stochastic</font> gradient descent (SGD).<span style="font-size:80%;opacity:0.8"> 通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> descent </td> <td> [dɪˈsent] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient <font color=orangered>descent</font> (SGD).<span style="font-size:80%;opacity:0.8"> 通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> SGD </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (<font color=orangered>SGD</font>).<span style="font-size:80%;opacity:0.8"> 通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> resize </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The input image is <font color=orangered>resized</font> by setting its short side to 600 for training, while keeping its original aspect ratio.<span style="font-size:80%;opacity:0.8"> 为了训练，通过将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> Gaussian </td> <td> ['gaʊsɪən] </td> <td> 
<ul><li>We initialize the new layers (e.g., the RNN and output layers) by using random weights with <font color=orangered>Gaussian</font> distribution of 0 mean and 0.01 standard deviation.<span style="font-size:80%;opacity:0.8"> 我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> deviation </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>We initialize the new layers (e.g., the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard <font color=orangered>deviation</font>.<span style="font-size:80%;opacity:0.8"> 我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> momentum </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We used 0.9 <font color=orangered>momentum</font> and 0.0005 weight decay.<span style="font-size:80%;opacity:0.8"> 我们使用0.9的动量和0.0005的重量衰减。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> Caffe </td> <td>  </td> <td> 
<ul><li>Our model was implemented in <font color=forestgreen>Caffe</font> framework [17].<span style="font-size:80%;opacity:0.8"> 我们的模型在Caffe框架[17]中实现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> Multilingual </td> <td> [ˌmʌltiˈlɪŋgwəl] </td> <td> 
<ul><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and <font color=orangered>Multilingual</font> dataset [24].<span style="font-size:80%;opacity:0.8"> 我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>The <font color=orangered>Multilingual</font> scene text dataset is collected by [24].<span style="font-size:80%;opacity:0.8"> Multilingual场景文本数据集由[24]收集。</span></li><li>The evaluations on the SWT and <font color=orangered>Multilingual</font> datasets follow the protocols defined in [3] and [24] respectively.<span style="font-size:80%;opacity:0.8"> SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</span></li><li>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the SWT and <font color=orangered>MULTILINGUAL</font>.<span style="font-size:80%;opacity:0.8"> 表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>Our detector performs favourably against the TextFlow on the <font color=orangered>Multilingual</font>, suggesting that our method generalize well to various languages.<span style="font-size:80%;opacity:0.8"> 我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> Incidental </td> <td> [ˌɪnsɪˈdentl] </td> <td> 
<ul><li>The ICDAR 2015 (<font color=orangered>Incidental</font> Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass.<span style="font-size:80%;opacity:0.8"> ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> orientation </td> <td> [ˌɔ:riənˈteɪʃn] </td> <td> 
<ul><li>This dataset is more challenging than previous ones by including arbitrary <font color=orangered>orientation</font>, very small-scale and low resolution text.<span style="font-size:80%;opacity:0.8"> 这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> Epshtein </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Epshtein</font> et al. [3] introduced the SWT dataset containing 307 images which include many extremely small-scale text.<span style="font-size:80%;opacity:0.8"> Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> organizer </td> <td> ['ɔ:ɡənaɪzə(r)] </td> <td> 
<ul><li>We follow previous work by using standard evaluation protocols which are provided by the dataset creators or competition <font color=orangered>organizers</font>.<span style="font-size:80%;opacity:0.8"> 我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。</span></li><li>For the ICDAR 2015, we used the online evaluation system provided by the <font color=orangered>organizers</font> as in [18].<span style="font-size:80%;opacity:0.8"> 对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> FTPN </td> <td> [!≈ ef ti: pi: en] </td> <td> 
<ul><li>Obviously, the proposed fine-scale text proposal network (<font color=orangered>FTPN</font>) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8"> 显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the <font color=orangered>FTPN</font> is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8"> 显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the <font color=orangered>FTPN</font> substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> remarkably </td> <td> [rɪ'mɑ:kəblɪ] </td> <td> 
<ul><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN <font color=orangered>remarkably</font> in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8"> 显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] <font color=orangered>remarkably</font> by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> marginally </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>Therefore, the proposed in-network recurrent mechanism increase model computation <font color=orangered>marginally</font>, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8"> 因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> TextFlow </td> <td>  </td> <td> 
<ul><li>Our detector performs favourably against the <font color=forestgreen>TextFlow</font> on the Multilingual, suggesting that our method generalize well to various languages.<span style="font-size:80%;opacity:0.8"> 我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。</span></li><li>On the ICDAR 2013, it outperforms recent <font color=forestgreen>TextFlow</font> [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> FASText </td> <td> [fɑːs'tekst] </td> <td> 
<ul><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and <font color=orangered>FASText</font> [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li><font color=orangered>FASText</font> [1] achieves 0.15s/image CPU time.<span style="font-size:80%;opacity:0.8"> FASText[1]达到0.15s每张图像的CPU时间。</span></li><li>Regardless of running time, our method outperforms the <font color=orangered>FASText</font> substantially with $11\%$ improvement on F-measure.<span style="font-size:80%;opacity:0.8"> 无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> submission </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>In addition, we further compare our method against [8,11,35], which were published after our initial <font color=orangered>submission</font>.<span style="font-size:80%;opacity:0.8"> 此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> consistently </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>It <font color=orangered>consistently</font> obtains substantial improvements on F-measure and recall.<span style="font-size:80%;opacity:0.8"> 它始终在F-measure和召回率方面取得重大进展。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> capability </td> <td> [ˌkeɪpəˈbɪləti] </td> <td> 
<ul><li>This may due to strong <font color=orangered>capability</font> of CTPN for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8"> 这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> competitively </td> <td> [!≈ kəmˈpetətɪvli] </td> <td> 
<ul><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared <font color=orangered>competitively</font> against Gupta et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> Gupta </td> <td>  </td> <td> 
<ul><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared competitively against <font color=forestgreen>Gupta</font> et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8"> 在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
</table>
</div>
<div class="two-list">
<table>
<caption>
    <h2> Words List (frequency)</h2>
</caption>
<thead>
<tr>
<td> # </td> <td> word (frequency) </td> <td> phonetic </td> <td> sentence </td>
</tr>
</thead>
<tr>
<td> 1 </td> <td> CTPN<br>(33) </td> <td> [!≈ si: ti: pi: en] </td> <td> 
<ul><li>We propose a novel Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) that accurately localizes text lines in natural image.<span style="font-size:80%;opacity:0.8">我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。</span></li><li>The <font color=orangered>CTPN</font> detects a text line in a sequence of fine-scale text proposals directly in convolutional feature maps.<span style="font-size:80%;opacity:0.8">CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。</span></li><li>This allows the <font color=orangered>CTPN</font> to explore rich context information of image, making it powerful to detect extremely ambiguous text.<span style="font-size:80%;opacity:0.8">这使得CTPN可以探索丰富的图像上下文信息，使其能够检测极其模糊的文本。</span></li><li>The <font color=orangered>CTPN</font> works reliably on multi-scale and multi-language text without further post-processing, departing from previous bottom-up methods requiring multi-step post filtering.<span style="font-size:80%;opacity:0.8">CTPN在多尺度和多语言文本上可靠地工作，而不需要进一步的后处理，脱离了以前的自底向上需要多步后过滤的方法。</span></li><li>The <font color=orangered>CTPN</font> is computationally efficient with 0.14s/image, by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8">通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>We propose a novel Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) that directly localizes text sequences in convolutional layers.<span style="font-size:80%;opacity:0.8">我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。</span></li><li>We leverage the advantages of strong deep convolutional features and sharing computation mechanism, and propose the <font color=orangered>CTPN</font> architecture which is described in Fig. 1.<span style="font-size:80%;opacity:0.8">我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。</span></li><li>Fig. 1: (a) Architecture of the Connectionist Text Proposal Network (<font color=orangered>CTPN</font>).<span style="font-size:80%;opacity:0.8">图1：（a）连接文本提议网络（CTPN）的架构。</span></li><li>(b) The <font color=orangered>CTPN</font> outputs sequential fixed-width fine-scale text proposals.<span style="font-size:80%;opacity:0.8">（b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>This section presents details of the Connectionist Text Proposal Network (<font color=orangered>CTPN</font>).<span style="font-size:80%;opacity:0.8">本节介绍连接文本提议网络（CTPN）的细节。</span></li><li>Similar to Region Proposal Network (RPN) [25], the <font color=orangered>CTPN</font> is essentially a fully convolutional network that allows an input image of arbitrary size.<span style="font-size:80%;opacity:0.8">类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>Architecture of the <font color=orangered>CTPN</font> is presented in Fig. 1 (a).<span style="font-size:80%;opacity:0.8">CTPN的架构如图1（a）所示。</span></li><li>Fig. 3: Top: <font color=orangered>CTPN</font> without RNN.<span style="font-size:80%;opacity:0.8">图3：上：没有RNN的CTPN。</span></li><li>Bottom: <font color=orangered>CTPN</font> with RNN connection.<span style="font-size:80%;opacity:0.8">下：有RNN连接的CTPN。</span></li><li>The fine-scale text proposals are detected accurately and reliably by our <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8">我们的CTPN能够准确可靠地检测细粒度的文本提议。</span></li><li>Fig. 4: <font color=orangered>CTPN</font> detection with (red box) and without (yellow dashed box) the side-refinement.<span style="font-size:80%;opacity:0.8">图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li><li>The proposed <font color=orangered>CTPN</font> has three outputs which are jointly connected to the last FC layer, as shown in Fig. 1 (a).<span style="font-size:80%;opacity:0.8">提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。</span></li><li>The <font color=orangered>CTPN</font> can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (SGD).<span style="font-size:80%;opacity:0.8">通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li><li>This is crucial to detect small-scale text patterns, which is one of key advantages of the <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8">这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</span></li><li>We evaluate the <font color=orangered>CTPN</font> on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We discuss impact of recurrent connection on our <font color=orangered>CTPN</font>.<span style="font-size:80%;opacity:0.8">我们讨论循环连接对CTPN的影响。</span></li><li>It is of great importance for recovering highly ambiguous text (e.g., extremely small-scale ones), which is one of main advantages of our <font color=orangered>CTPN</font>, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8">对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the <font color=orangered>CTPN</font> improves the FTPN substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Fig. 6: <font color=orangered>CTPN</font> detection results on extremely small-scale cases (in red boxes), where some ground truth boxes are missed.<span style="font-size:80%;opacity:0.8">图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。</span></li><li>The implementation time of our <font color=orangered>CTPN</font> (for whole detection processing) is about 0.14s per image with a fixed short side of 600, by using a single GPU.<span style="font-size:80%;opacity:0.8">通过使用单个GPU，我们的CTPN（用于整个检测处理）的执行时间为每张图像大约0.14s，固定短边为600。</span></li><li>The <font color=orangered>CTPN</font> without the RNN connection takes about 0.13s/image GPU time.<span style="font-size:80%;opacity:0.8">没有RNN连接的CTPN每张图像GPU时间大约需要0.13s。</span></li><li>As can be found, the <font color=orangered>CTPN</font> works perfectly on these challenging cases, some of which are difficult for many previous methods.<span style="font-size:80%;opacity:0.8">可以发现，CTPN在这些具有挑战性的情况上可以完美的工作，其中一些对于许多以前的方法来说是困难的。</span></li><li>Fig. 5: <font color=orangered>CTPN</font> detection results several challenging images, including multi-scale and multi-language text lines.<span style="font-size:80%;opacity:0.8">图5：CTPN在几个具有挑战性的图像上的检测结果，包括多尺度和多语言文本行。</span></li><li>As shown in Table 1 and 2, our <font color=orangered>CTPN</font> achieves the best performance on all five datasets.<span style="font-size:80%;opacity:0.8">如表1和表2所示，我们的CTPN在所有的五个数据集上都实现了最佳性能。</span></li><li>This may due to strong capability of <font color=orangered>CTPN</font> for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8">这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li><li>We have presented a Connectionist Text Proposal Network (<font color=orangered>CTPN</font>) —— an efficient text detector that is end-to-end trainable.<span style="font-size:80%;opacity:0.8">我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li><li>The <font color=orangered>CTPN</font> detects a text line in a sequence of fine-scale text proposals directly in convolutional maps.<span style="font-size:80%;opacity:0.8">CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。</span></li><li>The <font color=orangered>CTPN</font> is efficient by achieving new state-of-the-art performance on five benchmarks, with 0.14s/image running time.<span style="font-size:80%;opacity:0.8">通过在五个基准数据集测试中实现了最佳性能，每张图像运行时间为0.14s，CTPN是有效的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 2 </td> <td> fine-scale<br>(25) </td> <td> [!≈ faɪn skeɪl] </td> <td> 
<ul><li>The CTPN detects a text line in a sequence of <font color=orangered>fine-scale</font> text proposals directly in convolutional feature maps.<span style="font-size:80%;opacity:0.8">CTPN直接在卷积特征映射中的一系列细粒度文本提议中检测文本行。</span></li><li>Then, an in-network recurrent architecture is proposed to connect these <font color=orangered>fine-scale</font> text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8">然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>(b) The CTPN outputs sequential fixed-width <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8">（b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>First, we cast the problem of text detection into localizing a sequence of <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8">首先，我们将文本检测的问题转化为一系列细粒度的文本提议。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in <font color=orangered>fine-scale</font> proposals, recurrent connectionist text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8">它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.1 Detecting Text in <font color=orangered>Fine-scale</font> Proposals<span style="font-size:80%;opacity:0.8">3.1 在细粒度提议中检测文本</span></li><li>It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of <font color=orangered>fine-scale</font> (e.g., fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).<span style="font-size:80%;opacity:0.8">它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</span></li><li>Right: <font color=orangered>Fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8">右：细粒度的文本提议。</span></li><li>It is natural to consider a text line as a sequence of <font color=orangered>fine-scale</font> text proposals, where each proposal generally represents a small part of a text line, e.g., a text piece with 16-pixel width.<span style="font-size:80%;opacity:0.8">将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。</span></li><li>We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and y-axis location of each <font color=orangered>fine-scale</font> proposal.<span style="font-size:80%;opacity:0.8">我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。</span></li><li>To this end, we design the <font color=orangered>fine-scale</font> text proposal as follow.<span style="font-size:80%;opacity:0.8">为此，我们设计如下的细粒度文本提议。</span></li><li>By the designed vertical anchor and <font color=orangered>fine-scale</font> detection strategy, our detector is able to handle text lines in a wide range of scales and aspect ratios by using a single-scale image.<span style="font-size:80%;opacity:0.8">通过设计的垂直锚点和细粒度的检测策略，我们的检测器能够通过使用单尺度图像处理各种尺度和长宽比的文本行。</span></li><li>Compared to the RPN or Faster R-CNN system [25], our <font color=orangered>fine-scale</font> detection provides more detailed supervised information that naturally leads to a more accurate detection.<span style="font-size:80%;opacity:0.8">与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</span></li><li>To improve localization accuracy, we split a text line into a sequence of <font color=orangered>fine-scale</font> text proposals, and predict each of them separately.<span style="font-size:80%;opacity:0.8">为了提高定位精度，我们将文本行分成一系列细粒度的文本提议，并分别预测每个文本提议。</span></li><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless in-network connection of the <font color=orangered>fine-scale</font> text proposals.<span style="font-size:80%;opacity:0.8">此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li><li>The <font color=orangered>fine-scale</font> text proposals are detected accurately and reliably by our CTPN.<span style="font-size:80%;opacity:0.8">我们的CTPN能够准确可靠地检测细粒度的文本提议。</span></li><li>The <font color=orangered>fine-scale</font> detection and RNN connection are able to predict accurate localizations in vertical direction.<span style="font-size:80%;opacity:0.8">细粒度的检测和RNN连接可以预测垂直方向的精确位置。</span></li><li>The side-proposals are defined as the start and end proposals when we connect a sequence of detected <font color=orangered>fine-scale</font> text proposals into a text line.<span style="font-size:80%;opacity:0.8">当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。</span></li><li>Color of <font color=orangered>fine-scale</font> proposal box indicate a text/non-text score.<span style="font-size:80%;opacity:0.8">细粒度提议边界框的颜色表示文本/非文本分数。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the <font color=orangered>fine-scale</font> text proposal detection or in-network recurrent connection.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>4.2 <font color=orangered>Fine-Scale</font> Text Proposal Network with Faster R-CNN<span style="font-size:80%;opacity:0.8">4.2 具有Faster R-CNN的细粒度文本提议网络</span></li><li>We first discuss our <font color=orangered>fine-scale</font> detection strategy against the RPN and Faster R-CNN system [25].<span style="font-size:80%;opacity:0.8">我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。</span></li><li>Obviously, the proposed <font color=orangered>fine-scale</font> text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8">显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of <font color=orangered>fine-scale</font> text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8">显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>The CTPN detects a text line in a sequence of <font color=orangered>fine-scale</font> text proposals directly in convolutional maps.<span style="font-size:80%;opacity:0.8">CTPN直接在卷积映射的一系列细粒度文本提议中检测文本行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 3 </td> <td> ICDAR<br>(22) </td> <td> [!≈ aɪ si: di: eɪ ɑ:(r)] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 F-measure on the <font color=orangered>ICDAR</font> 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8">它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the <font color=orangered>ICDAR</font> 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8">第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the <font color=orangered>ICDAR</font> 2015).<span style="font-size:80%;opacity:0.8">第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the <font color=orangered>ICDAR</font> 2013) by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8">此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>Our model was trained on 3,000 natural images, including 229 images from the <font color=orangered>ICDAR</font> 2013 training set.<span style="font-size:80%;opacity:0.8">我们的模型在3000张自然图像上训练，其中包括来自ICDAR 2013训练集的229张图像。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the <font color=orangered>ICDAR</font> 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], <font color=orangered>ICDAR</font> 2013 [19], ICDAR 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], <font color=orangered>ICDAR</font> 2015 [18], SWT [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>The <font color=orangered>ICDAR</font> 2013 is used for this component evaluation.<span style="font-size:80%;opacity:0.8">ICDAR 2013用于该组件的评估。</span></li><li>The <font color=orangered>ICDAR</font> 2011 dataset [21] consists of 229 training images and 255 testing ones, where the images are labelled in word level.<span style="font-size:80%;opacity:0.8">ICDAR 2011数据集[21]由229张训练图像和255张测试图像组成，图像以字级别标记。</span></li><li>The <font color=orangered>ICDAR</font> 2013 [19] is similar as the ICDAR 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively.<span style="font-size:80%;opacity:0.8">ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。</span></li><li>The ICDAR 2013 [19] is similar as the <font color=orangered>ICDAR</font> 2011, and has in total 462 images, including 229 images and 233 images for training and testing, respectively.<span style="font-size:80%;opacity:0.8">ICDAR 2013[19]与ICDAR 2011类似，共有462张图像，其中包括229张训练图像和233张测试图像。</span></li><li>The <font color=orangered>ICDAR</font> 2015 (Incidental Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass.<span style="font-size:80%;opacity:0.8">ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。</span></li><li>For the <font color=orangered>ICDAR</font> 2011 we use the standard protocol proposed by [30], the evaluation on the ICDAR 2013 follows the standard in [19].<span style="font-size:80%;opacity:0.8">对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。</span></li><li>For the ICDAR 2011 we use the standard protocol proposed by [30], the evaluation on the <font color=orangered>ICDAR</font> 2013 follows the standard in [19].<span style="font-size:80%;opacity:0.8">对于ICDAR 2011，我们使用[30]提出的标准协议，对ICDAR 2013的评估遵循[19]中的标准。</span></li><li>For the <font color=orangered>ICDAR</font> 2015, we used the online evaluation system provided by the organizers as in [18].<span style="font-size:80%;opacity:0.8">对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。</span></li><li>The RPN proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the <font color=orangered>ICDAR</font> 2013 standard.<span style="font-size:80%;opacity:0.8">RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。</span></li><li>Table 1: Component evaluation on the <font color=orangered>ICDAR</font> 2013, and State-of-the-art results on the SWT and MULTILINGUAL.<span style="font-size:80%;opacity:0.8">表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>We set short side of images to 2000 for the SWT and <font color=orangered>ICDAR</font> 2015, and 600 for the other three.<span style="font-size:80%;opacity:0.8">我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。</span></li><li>On the <font color=orangered>ICDAR</font> 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li>Table 2: State-of-the-art results on the <font color=orangered>ICDAR</font> 2011, 2013 and 2015.<span style="font-size:80%;opacity:0.8">表2：ICDAR 2011，2013和2015上的最新结果。</span></li><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the <font color=orangered>ICDAR</font> 2013, which are compared competitively against Gupta et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8">在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 4 </td> <td> e.g.<br>(20) </td> <td> [ˌi: ˈdʒi:] </td> <td> 
<ul><li>These methods commonly explore low-level features (<font color=orangered>e.g.</font>, based on SWT [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8">这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li><li>For object detection, a typical correct detection is defined loosely, <font color=orangered>e.g.</font>, by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8">对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (<font color=orangered>e.g.</font>, the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8">对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>Therefore, text detection generally requires a more accurate localization, leading to a different evaluation standard, <font color=orangered>e.g.</font>, the Wolf’s standard [30] which is commonly employed by text benchmarks [19,21].<span style="font-size:80%;opacity:0.8">因此，文本检测通常需要更准确的定义，导致不同的评估标准，例如文本基准中常用的Wolf标准[19，21]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (<font color=orangered>e.g.</font>, 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8">第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, <font color=orangered>e.g.</font>, intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8">基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li><li>However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, <font color=orangered>e.g.</font>, the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8">然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li><li>It detects a text line by densely sliding a small window in the convolutional feature maps, and outputs a sequence of fine-scale (<font color=orangered>e.g.</font>, fixed 16-pixel width) text proposals, as shown in Fig. 1 (b).<span style="font-size:80%;opacity:0.8">它通过在卷积特征映射中密集地滑动小窗口来检测文本行，并且输出一系列细粒度的（例如，宽度为固定的16个像素）文本提议，如图1（b）所示。</span></li><li>We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (<font color=orangered>e.g.</font>, the conv5 of the VGG16).<span style="font-size:80%;opacity:0.8">我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。</span></li><li>Text detection is defined in word or text line level, so that it may be easy to make an incorrect detection by defining it as a single object, <font color=orangered>e.g.</font>, detecting part of a word.<span style="font-size:80%;opacity:0.8">文本检测是在单词或文本行级别中定义的，因此通过将其定义为单个目标（例如检测单词的一部分）可能很容易进行错误的检测。</span></li><li>It is natural to consider a text line as a sequence of fine-scale text proposals, where each proposal generally represents a small part of a text line, <font color=orangered>e.g.</font>, a text piece with 16-pixel width.<span style="font-size:80%;opacity:0.8">将文本行视为一系列细粒度的文本提议是很自然的，其中每个提议通常代表文本行的一小部分，例如宽度为16个像素的文本块。</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, <font color=orangered>e.g.</font>, $228 \times width$.<span style="font-size:80%;opacity:0.8">我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>This may lead to an inaccurate localization when the text proposals in both horizontal sides are not exactly covered by a ground truth text line area, or some side proposals are discarded (<font color=orangered>e.g.</font>, having a low text score), as shown in Fig. 4.<span style="font-size:80%;opacity:0.8">如图4所示，当两个水平边的文本提议没有完全被实际文本行区域覆盖，或者某些边的提议被丢弃（例如文本得分较低）时，这可能会导致不准确的定位。</span></li><li>where $x_{side}$ is the predicted x-coordinate of the nearest horizontal side (<font color=orangered>e.g.</font>, left or right side) to current anchor.<span style="font-size:80%;opacity:0.8">其中，$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的x坐标。</span></li><li>k is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (<font color=orangered>e.g.</font>, 32-pixel) to the left or right side of a ground truth text line bounding box.<span style="font-size:80%;opacity:0.8">k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li><li>We initialize the new layers (<font color=orangered>e.g.</font>, the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard deviation.<span style="font-size:80%;opacity:0.8">我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, <font color=orangered>e.g.</font>, the fine-scale text proposal detection or in-network recurrent connection.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>It is of great importance for recovering highly ambiguous text (<font color=orangered>e.g.</font>, extremely small-scale ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8">对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>It is able to handle multi-scale and multi-language efficiently (<font color=orangered>e.g.</font>, Chinese and Korean).<span style="font-size:80%;opacity:0.8">它能够有效地处理多尺度和多语言（例如中文和韩文）。</span></li><li>This may due to strong capability of CTPN for detecting extremely challenging text, <font color=orangered>e.g.</font>, very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8">这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 5 </td> <td> RPN<br>(20) </td> <td> [!≈ ɑ:(r) pi: en] </td> <td> 
<ul><li>The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (<font color=orangered>RPN</font>) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8">最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li><li>Then the <font color=orangered>RPN</font> proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on generic object detection.<span style="font-size:80%;opacity:0.8">然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>In this work, we fill this gap by extending the <font color=orangered>RPN</font> architecture [25] to accurate text line localization.<span style="font-size:80%;opacity:0.8">在这项工作中，我们通过将RPN架构[25]扩展到准确的文本行定义来填补这个空白。</span></li><li>This departs from the <font color=orangered>RPN</font> prediction of a whole object, which is difficult to provide a satisfied localization accuracy.<span style="font-size:80%;opacity:0.8">这背离了整个目标的RPN预测，RPN预测难以提供令人满意的定位精度。</span></li><li>They proposed a Region Proposal Network (<font color=orangered>RPN</font>) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps.<span style="font-size:80%;opacity:0.8">他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。</span></li><li>The <font color=orangered>RPN</font> is fast by sharing convolutional computation.<span style="font-size:80%;opacity:0.8">通过共享卷积计算RPN是快速的。</span></li><li>However, the <font color=orangered>RPN</font> proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8">然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li><li>Similar to Region Proposal Network (<font color=orangered>RPN</font>) [25], the CTPN is essentially a fully convolutional network that allows an input image of arbitrary size.<span style="font-size:80%;opacity:0.8">类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>In [25], Ren et al. proposed an efficient anchor regression mechanism that allows the <font color=orangered>RPN</font> to detect multi-scale objects with a single-scale window.<span style="font-size:80%;opacity:0.8">在[25]中，Ren等人提出了一种有效的锚点回归机制，允许RPN使用单尺度窗口检测多尺度目标。</span></li><li>An example is shown in Fig. 2, where the <font color=orangered>RPN</font> is directly trained for localizing text lines in an image.<span style="font-size:80%;opacity:0.8">一个例子如图2所示，其中RPN直接被训练用于定位图像中的文本行。</span></li><li>Fig. 2: Left: <font color=orangered>RPN</font> proposals.<span style="font-size:80%;opacity:0.8">图2：左：RPN提议。</span></li><li>We observed that word detection by the <font color=orangered>RPN</font> is difficult to accurately predict the horizontal sides of words, since each character within a word is isolated or separated, making it confused to find the start and end locations of a word.<span style="font-size:80%;opacity:0.8">我们观察到由RPN进行的单词检测很难准确预测单词的水平边，因为单词中的每个字符都是孤立的或分离的，这使得查找单词的开始和结束位置很混乱。</span></li><li>This reduces the search space, compared to the <font color=orangered>RPN</font> which predicts 4 coordinates of an object.<span style="font-size:80%;opacity:0.8">与预测目标4个坐标的RPN相比，这减少了搜索空间。</span></li><li>Compared to the <font color=orangered>RPN</font> or Faster R-CNN system [25], our fine-scale detection provides more detailed supervised information that naturally leads to a more accurate detection.<span style="font-size:80%;opacity:0.8">与RPN或Faster R-CNN系统[25]相比，我们的细粒度检测提供更详细的监督信息，自然会导致更精确的检测。</span></li><li>Similar to <font color=orangered>RPN</font> [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding GT box.<span style="font-size:80%;opacity:0.8">与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</span></li><li>We first discuss our fine-scale detection strategy against the <font color=orangered>RPN</font> and Faster R-CNN system [25].<span style="font-size:80%;opacity:0.8">我们首先讨论我们关于RPN和Faster R-CNN系统[25]的细粒度检测策略。</span></li><li>As can be found in Table 1 (left), the individual <font color=orangered>RPN</font> is difficult to perform accurate text localization, by generating a large amount of false detections (low precision).<span style="font-size:80%;opacity:0.8">如表1（左）所示，通过产生大量的错误检测（低精度），单独的RPN难以执行准确的文本定位。</span></li><li>By refining the <font color=orangered>RPN</font> proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a F-measure of 0.75.<span style="font-size:80%;opacity:0.8">通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。</span></li><li>One observation is that the Faster R-CNN also increases the recall of original <font color=orangered>RPN</font>.<span style="font-size:80%;opacity:0.8">一个观察结果是Faster R-CNN也增加了原始RPN的召回率。</span></li><li>The <font color=orangered>RPN</font> proposals may roughly localize a major part of a text line or word, but they are not accurate enough by the ICDAR 2013 standard.<span style="font-size:80%;opacity:0.8">RPN提议可以粗略定位文本行或文字的主要部分，但根据ICDAR 2013的标准这不够准确。</span></li></ul>
 </td>
</tr>
<tr>
<td> 6 </td> <td> recurrent<br>(16) </td> <td> [rɪˈkʌrənt] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a <font color=orangered>recurrent</font> neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8">序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Scene text detection, convolutional network, <font color=orangered>recurrent</font> neural network, anchor mechanism<span style="font-size:80%;opacity:0.8">场景文本检测；卷积网络；循环神经网络；锚点机制</span></li><li>Then, an in-network <font color=orangered>recurrent</font> architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8">然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>We strive for a further step by proposing an in-network <font color=orangered>recurrent</font> mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.<span style="font-size:80%;opacity:0.8">我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, <font color=orangered>recurrent</font> connectionist text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8">它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.2 <font color=orangered>Recurrent</font> Connectionist Text Proposals<span style="font-size:80%;opacity:0.8">3.2 循环连接文本提议</span></li><li>This has been verified by recent work [9] where a <font color=orangered>recurrent</font> neural network (RNN) is applied to encode this context information for text recognition.<span style="font-size:80%;opacity:0.8">最近的工作已经证实了这一点[9]，其中应用递归神经网络（RNN）来编码用于文本识别的上下文信息。</span></li><li>W is the width of the conv5. $H_t$ is a <font color=orangered>recurrent</font> internal state that is computed jointly from both current input ($X_t$) and previous states encoded in $H_{t-1}$.<span style="font-size:80%;opacity:0.8">$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。</span></li><li>The recurrence is computed by using a non-linear function $\varphi$, which defines exact form of the <font color=orangered>recurrent</font> model.<span style="font-size:80%;opacity:0.8">递归是通过使用非线性函数$\varphi$来计算的，它定义了循环模型的确切形式。</span></li><li>Hence the internal state in RNN hidden layer accesses the sequential context information scanned by all previous windows through the <font color=orangered>recurrent</font> connection.<span style="font-size:80%;opacity:0.8">因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the <font color=orangered>recurrent</font> context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8">我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or in-network <font color=orangered>recurrent</font> connection.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>4.3 <font color=orangered>Recurrent</font> Connectionist Text Proposals<span style="font-size:80%;opacity:0.8">4.3 循环连接文本提议</span></li><li>We discuss impact of <font color=orangered>recurrent</font> connection on our CTPN.<span style="font-size:80%;opacity:0.8">我们讨论循环连接对CTPN的影响。</span></li><li>As shown in Table 1 (left), with our <font color=orangered>recurrent</font> connection, the CTPN improves the FTPN substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Therefore, the proposed in-network <font color=orangered>recurrent</font> mechanism increase model computation marginally, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8">因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 7 </td> <td> connectionist<br>(11) </td> <td> [kə'nekʃənɪst] </td> <td> 
<ul><li>Detecting Text in Natural Image with <font color=orangered>Connectionist</font> Text Proposal Network<span style="font-size:80%;opacity:0.8">用连接式文本建议网络检测自然图像中的文本</span></li><li>We propose a novel <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) that accurately localizes text lines in natural image.<span style="font-size:80%;opacity:0.8">我们提出了一种新颖的连接文本提议网络（CTPN），它能够准确定位自然图像中的文本行。</span></li><li>We propose a novel <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) that directly localizes text sequences in convolutional layers.<span style="font-size:80%;opacity:0.8">我们提出了一种新颖的连接文本提议网络（CTPN），它可以直接定位卷积层中的文本序列。</span></li><li>Fig. 1: (a) Architecture of the <font color=orangered>Connectionist</font> Text Proposal Network (CTPN).<span style="font-size:80%;opacity:0.8">图1：（a）连接文本提议网络（CTPN）的架构。</span></li><li>3. <font color=orangered>Connectionist</font> Text Proposal Network<span style="font-size:80%;opacity:0.8">3. 连接文本提议网络</span></li><li>This section presents details of the <font color=orangered>Connectionist</font> Text Proposal Network (CTPN).<span style="font-size:80%;opacity:0.8">本节介绍连接文本提议网络（CTPN）的细节。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent <font color=orangered>connectionist</font> text proposals, and side-refinement.<span style="font-size:80%;opacity:0.8">它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.2 Recurrent <font color=orangered>Connectionist</font> Text Proposals<span style="font-size:80%;opacity:0.8">3.2 循环连接文本提议</span></li><li>We further extend the RNN layer by using a bi-directional LSTM, which allows it to encode the recurrent context in both directions, so that the <font color=orangered>connectionist</font> receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8">我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li><li>4.3 Recurrent <font color=orangered>Connectionist</font> Text Proposals<span style="font-size:80%;opacity:0.8">4.3 循环连接文本提议</span></li><li>We have presented a <font color=orangered>Connectionist</font> Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end trainable.<span style="font-size:80%;opacity:0.8">我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 8 </td> <td> sequential<br>(11) </td> <td> [sɪˈkwenʃl] </td> <td> 
<ul><li>The <font color=orangered>sequential</font> proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8">序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>The <font color=orangered>sequential</font> windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8">每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>(b) The CTPN outputs <font color=orangered>sequential</font> fixed-width fine-scale text proposals.<span style="font-size:80%;opacity:0.8">（b）CTPN输出连续的固定宽度细粒度文本提议。</span></li><li>Second, we propose an in-network recurrence mechanism that elegantly connects <font color=orangered>sequential</font> text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8">其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>Text have strong <font color=orangered>sequential</font> characteristics where the sequential context information is crucial to make a reliable decision.<span style="font-size:80%;opacity:0.8">文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。</span></li><li>Text have strong sequential characteristics where the <font color=orangered>sequential</font> context information is crucial to make a reliable decision.<span style="font-size:80%;opacity:0.8">文本具有强大的序列特征，序列上下文信息对做出可靠决策至关重要。</span></li><li>Their results have shown that the <font color=orangered>sequential</font> context information is greatly facilitate the recognition task on cropped word images.<span style="font-size:80%;opacity:0.8">他们的结果表明，序列上下文信息极大地促进了对裁剪的单词图像的识别任务。</span></li><li>To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as <font color=orangered>sequential</font> inputs, and updates its internal state recurrently in the hidden layer, $H_t$,<span style="font-size:80%;opacity:0.8">为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，</span></li><li>The sliding-window moves densely from left to right, resulting in $t=1,2,…,W$ <font color=orangered>sequential</font> features for each row.<span style="font-size:80%;opacity:0.8">W是conv5的宽度。</span></li><li>Hence the internal state in RNN hidden layer accesses the <font color=orangered>sequential</font> context information scanned by all previous windows through the recurrent connection.<span style="font-size:80%;opacity:0.8">因此，RNN隐藏层中的内部状态可以访问所有先前窗口通过循环连接扫描的序列上下文信息。</span></li><li>We propose an in-network RNN layer that connects <font color=orangered>sequential</font> text proposals elegantly, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8">我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 9 </td> <td> bounding<br>(11) </td> <td> [baundɪŋ] </td> <td> 
<ul><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected <font color=orangered>bounding</font> box and its ground truth (e.g., the PASCAL standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8">对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li><li>The explicit vertical coordinates are measured by the height and y-axis center of a proposal <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8">明确的垂直坐标是通过提议边界框的高度和y轴中心来度量的。</span></li><li>We compute relative predicted vertical coordinates ($\textbf{v}$) with respect to the <font color=orangered>bounding</font> box location of an anchor as,<span style="font-size:80%;opacity:0.8">我们计算相对于锚点的边界框位置的相对预测的垂直坐标（$\textbf{v}$），如下所示：</span></li><li>Therefore, each predicted text proposal has a <font color=orangered>bounding</font> box with size of $h\times 16$ (in the input image), as shown in Fig. 1 (b) and Fig. 2 (right).<span style="font-size:80%;opacity:0.8">因此，如图1（b）和图2（右）所示，每个预测文本提议都有一个大小为$h\times 16$的边界框（在输入图像中）。</span></li><li>$x^*_{side}$ is the ground truth (GT) side coordinate in x-axis, which is pre-computed from the GT <font color=orangered>bounding</font> box and anchor location.<span style="font-size:80%;opacity:0.8">$x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>We only use the offsets of the side-proposals to refine the final text line <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8">我们只使用边缘提议的偏移量来优化最终的文本行边界框。</span></li><li>k is the index of a side-anchor, which is defined as a set of anchors within a horizontal distance (e.g., 32-pixel) to the left or right side of a ground truth text line <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8">k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li><li>It is defined by computing the IoU overlap with the GT <font color=orangered>bounding</font> box (divided by anchor location).<span style="font-size:80%;opacity:0.8">它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。</span></li><li>We collected the other images ourselves and manually labelled them with text line <font color=orangered>bounding</font> boxes.<span style="font-size:80%;opacity:0.8">我们自己收集了其他图像，并用文本行边界框进行了手工标注。</span></li><li>This may benefit from joint <font color=orangered>bounding</font> box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted bounding box.<span style="font-size:80%;opacity:0.8">这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。</span></li><li>This may benefit from joint bounding box regression mechanism of the Fast R-CNN, which improves the accuracy of a predicted <font color=orangered>bounding</font> box.<span style="font-size:80%;opacity:0.8">这可能受益于Fast R-CNN的联合边界框回归机制，其提高了预测边界框的准确性。</span></li></ul>
 </td>
</tr>
<tr>
<td> 10 </td> <td> side-refinement<br>(10) </td> <td> [!≈ saɪd rɪˈfaɪnmənt] </td> <td> 
<ul><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, y-axis coordinates and <font color=orangered>side-refinement</font> offsets of k anchors.<span style="font-size:80%;opacity:0.8">RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>It includes three key contributions that make it reliable and accurate for text localization: detecting text in fine-scale proposals, recurrent connectionist text proposals, and <font color=orangered>side-refinement</font>.<span style="font-size:80%;opacity:0.8">它包括三个关键的贡献，使文本定位可靠和准确：检测细粒度提议中的文本，循环连接文本提议和边缘细化。</span></li><li>3.3 <font color=orangered>Side-refinement</font><span style="font-size:80%;opacity:0.8">3.3 边缘细化</span></li><li>To address this problem, we propose a <font color=orangered>side-refinement</font> approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or side-proposal).<span style="font-size:80%;opacity:0.8">为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>Several detection examples improved by <font color=orangered>side-refinement</font> are presented in Fig. 4.<span style="font-size:80%;opacity:0.8">通过边缘细化改进的几个检测示例如图4所示。</span></li><li>The <font color=orangered>side-refinement</font> further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and Multi-Lingual datasets.<span style="font-size:80%;opacity:0.8">边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li><li>Notice that the offset for <font color=orangered>side-refinement</font> is predicted simultaneously by our model, as shown in Fig. 1.<span style="font-size:80%;opacity:0.8">请注意，我们的模型同时预测了边缘细化的偏移量，如图1所示。</span></li><li>Fig. 4: CTPN detection with (red box) and without (yellow dashed box) the <font color=orangered>side-refinement</font>.<span style="font-size:80%;opacity:0.8">图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li><li>q. (2) and <font color=orangered>side-refinement</font> offset ($\textbf{o}$). We explore k anchors to predict them on each spatial location in the conv5, resulting in 2k, 2k and k parameters in the output layer, respectively.<span style="font-size:80%;opacity:0.8">我们将探索k个锚点来预测它们在conv5中的每个空间位置，从而在输出层分别得到2k，2k和k个参数。</span></li><li>We introduce three loss functions, $L^{cl}_s, L^{re}_v and l^{re}_o$, which compute errors of text/non-text score, coordinate and <font color=orangered>side-refinement</font>, respectively.<span style="font-size:80%;opacity:0.8">我们引入了三种损失函数：$L^{cl}_s$，$L^{re}_v$和$l^{re}_o$，其分别计算文本/非文本分数，坐标和边缘细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 11 </td> <td> F-measure<br>(9) </td> <td> [!≈ ef ˈmeʒə(r)] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 <font color=orangered>F-measure</font> on the ICDAR 2013 and 2015 benchmarks, surpassing recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8">它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 <font color=orangered>F-measure</font> over 0.83 in [8] on the ICDAR 2013, and 0.61 F-measure over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8">第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>Fourth, our method achieves new state-of-the-art results on a number of benchmarks, significantly improving recent results (e.g., 0.88 F-measure over 0.83 in [8] on the ICDAR 2013, and 0.61 <font color=orangered>F-measure</font> over 0.54 in [35] on the ICDAR 2015).<span style="font-size:80%;opacity:0.8">第四，我们的方法在许多基准数据集上达到了新的最先进成果，显著改善了最近的结果（例如，0.88的F-measure超过了2013年ICDAR的[8]中的0.83，而0.64的F-measure超过了ICDAR2015上[35]中的0.54 ）。</span></li><li>By refining the RPN proposals with a Fast R-CNN detection model [5], the Faster R-CNN system improves localization accuracy considerably, with a <font color=orangered>F-measure</font> of 0.75.<span style="font-size:80%;opacity:0.8">通过使用Fast R-CNN检测模型[5]完善RPN提议，Faster R-CNN系统显著提高了定位精度，其F-measure为0.75。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN substantially from a <font color=orangered>F-measure</font> of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>On the SWT, our improvements are significant on both recall and <font color=orangered>F-measure</font>, with marginal gain on precision.<span style="font-size:80%;opacity:0.8">在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。</span></li><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] remarkably by improving the <font color=orangered>F-measure</font> from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li>It consistently obtains substantial improvements on <font color=orangered>F-measure</font> and recall.<span style="font-size:80%;opacity:0.8">它始终在F-measure和召回率方面取得重大进展。</span></li><li>Regardless of running time, our method outperforms the FASText substantially with $11\%$ improvement on <font color=orangered>F-measure</font>.<span style="font-size:80%;opacity:0.8">无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 12 </td> <td> SWT<br>(8) </td> <td> ['esd'əbəlju:t'i:] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on <font color=orangered>SWT</font> [3,13], MSER [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8">这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li><li>The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the <font color=orangered>SWT</font> and Multi-Lingual datasets.<span style="font-size:80%;opacity:0.8">边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], <font color=orangered>SWT</font> [3], and Multilingual dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>Epshtein et al. [3] introduced the <font color=orangered>SWT</font> dataset containing 307 images which include many extremely small-scale text.<span style="font-size:80%;opacity:0.8">Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li><li>The evaluations on the <font color=orangered>SWT</font> and Multilingual datasets follow the protocols defined in [3] and [24] respectively.<span style="font-size:80%;opacity:0.8">SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</span></li><li>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the <font color=orangered>SWT</font> and MULTILINGUAL.<span style="font-size:80%;opacity:0.8">表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>We set short side of images to 2000 for the <font color=orangered>SWT</font> and ICDAR 2015, and 600 for the other three.<span style="font-size:80%;opacity:0.8">我们为SWT和ICDAR 2015设置图像短边为2000，其他三个的短边为600。</span></li><li>On the <font color=orangered>SWT</font>, our improvements are significant on both recall and F-measure, with marginal gain on precision.<span style="font-size:80%;opacity:0.8">在SWT上，我们的改进对于召回和F-measure都非常重要，并在精确度上取得了很小的收益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 13 </td> <td> GT<br>(8) </td> <td> [dʒi:'ti:] </td> <td> 
<ul><li>$x^*_{side}$ is the ground truth (<font color=orangered>GT</font>) side coordinate in x-axis, which is pre-computed from the GT bounding box and anchor location.<span style="font-size:80%;opacity:0.8">$x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>$x^*_{side}$ is the ground truth (GT) side coordinate in x-axis, which is pre-computed from the <font color=orangered>GT</font> bounding box and anchor location.<span style="font-size:80%;opacity:0.8">$x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>Similar to RPN [25], training samples are the anchors, whose locations can be pre computed in input image, so that the training labels of each anchor can be computed from corresponding <font color=orangered>GT</font> box.<span style="font-size:80%;opacity:0.8">与RPN[25]类似，训练样本是锚点，其位置可以在输入图像中预先计算，以便可以从相应的实际边界框中计算每个锚点的训练标签。</span></li><li>It is defined by computing the IoU overlap with the <font color=orangered>GT</font> bounding box (divided by anchor location).<span style="font-size:80%;opacity:0.8">它通过计算与实际边界框的IoU重叠（除以锚点位置）来定义。</span></li><li>A positive anchor is defined as : (i) an anchor that has an &gt; 0.7 IoU overlap with any <font color=orangered>GT</font> box; or (ii) the anchor with the highest IoU overlap with a GT box.<span style="font-size:80%;opacity:0.8">正锚点被定义为：（i）与任何实际边界框具有>0.7的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。</span></li><li>A positive anchor is defined as : (i) an anchor that has an &gt; 0.7 IoU overlap with any GT box; or (ii) the anchor with the highest IoU overlap with a <font color=orangered>GT</font> box.<span style="font-size:80%;opacity:0.8">正锚点被定义为：（i）与任何实际边界框具有>0.7的IoU重叠；或者（ii）与实际边界框具有最高IoU重叠。</span></li><li>The negative anchors are defined as &lt;0.5 IoU overlap with all <font color=orangered>GT</font> boxes.<span style="font-size:80%;opacity:0.8">负锚点定义为与所有实际边界框具有<0.5的IoU重叠。</span></li><li>As shown in Fig. 6, those challenging ones are detected correctly by our detector, but some of them are even missed by the <font color=orangered>GT</font> labelling, which may reduce our precision in evaluation.<span style="font-size:80%;opacity:0.8">如图6所示，我们的检测器可以正确地检测到那些具有挑战性的图像，但有些甚至会被真实标签遗漏，这可能会降低我们的评估精度。</span></li></ul>
 </td>
</tr>
<tr>
<td> 14 </td> <td> jointly<br>(7) </td> <td> [dʒɔɪntlɪ] </td> <td> 
<ul><li>We develop a vertical anchor mechanism that <font color=orangered>jointly</font> predicts location and text/non-text score of each fixed-width proposal, considerably improving localization accuracy.<span style="font-size:80%;opacity:0.8">我们开发了一个垂直锚点机制，联合预测每个固定宽度提议的位置和文本/非文本分数，大大提高了定位精度。</span></li><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which <font color=orangered>jointly</font> predicts text/non-text scores, y-axis coordinates and side-refinement offsets of k anchors.<span style="font-size:80%;opacity:0.8">RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>We develop an anchor regression mechanism that <font color=orangered>jointly</font> predicts vertical location and text/non-text score of each text proposal, resulting in an excellent localization accuracy.<span style="font-size:80%;opacity:0.8">我们开发了一个锚点回归机制，可以联合预测每个文本提议的垂直位置和文本/非文本分数，从而获得出色的定位精度。</span></li><li>W is the width of the conv5. $H_t$ is a recurrent internal state that is computed <font color=orangered>jointly</font> from both current input ($X_t$) and previous states encoded in $H_{t-1}$.<span style="font-size:80%;opacity:0.8">$H_t$是从当前输入（$X_t$）和以$H_{t-1}$编码的先前状态联合计算的循环内部状态。</span></li><li>The proposed CTPN has three outputs which are <font color=orangered>jointly</font> connected to the last FC layer, as shown in Fig. 1 (a).<span style="font-size:80%;opacity:0.8">提出的CTPN有三个输出共同连接到最后的FC层，如图1（a）所示。</span></li><li>We employ multi-task learning to <font color=orangered>jointly</font> optimize model parameters.<span style="font-size:80%;opacity:0.8">我们采用多任务学习来联合优化模型参数。</span></li><li>We develop vertical anchor mechanism that <font color=orangered>jointly</font> predicts precise location and text/non-text score for each proposal, which is the key to realize accurate localization of text.<span style="font-size:80%;opacity:0.8">我们开发了垂直锚点机制，联合预测每个提议的精确位置和文本/非文本分数，这是实现文本准确定位的关键。</span></li></ul>
 </td>
</tr>
<tr>
<td> 15 </td> <td> VGG16<br>(7) </td> <td>  </td> <td> 
<ul><li>The CTPN is computationally efficient with 0.14s/image, by using the very deep <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8">通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>We densely slide a 3×3 spatial window through the last convolutional maps (conv5 ) of the <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8">我们通过VGG16模型[27]的最后一个卷积映射（conv5）密集地滑动3×3空间窗口。</span></li><li>Furthermore, it is computationally efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep <font color=forestgreen>VGG16</font> model [27].<span style="font-size:80%;opacity:0.8">此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>We take the very deep 16-layer vggNet (<font color=forestgreen>VGG16</font>) [27] as an example to describe our approach, which is readily applicable to other deep models.<span style="font-size:80%;opacity:0.8">我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。</span></li><li>We use a small spatial window, 3×3, to slide the feature maps of last convolutional layer (e.g., the conv5 of the <font color=forestgreen>VGG16</font>).<span style="font-size:80%;opacity:0.8">我们使用一个小的空间窗口3×3来滑动最后的卷积层特征映射（例如，VGG16的conv5）。</span></li><li>Given an input image, we have $W \times H \times C$ conv5 features maps (by using the <font color=forestgreen>VGG16</font> model), where C is the number of feature maps or channels, and $W \times H$ is the spatial arrangement.<span style="font-size:80%;opacity:0.8">给定输入图像，我们有$W \times H \times C$ conv5特征映射（通过使用VGG16模型），其中C是特征映射或通道的数目，并且$W \times H$是空间布置。</span></li><li>We follow the standard practice, and explore the very deep <font color=forestgreen>VGG16</font> model [27] pre-trained on the ImageNet data [26].<span style="font-size:80%;opacity:0.8">我们遵循标准实践，并在ImageNet数据[26]上探索预先训练的非常深的VGG16模型[27]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 16 </td> <td> in-network<br>(7) </td> <td> [!≈ ɪn ˈnetwɜ:k] </td> <td> 
<ul><li>Then, an <font color=orangered>in-network</font> recurrent architecture is proposed to connect these fine-scale text proposals in sequences, allowing them to encode rich context information.<span style="font-size:80%;opacity:0.8">然后，我们提出了一种网内循环架构，用于按顺序连接这些细粒度的文本提议，从而允许它们编码丰富的上下文信息。</span></li><li>We strive for a further step by proposing an <font color=orangered>in-network</font> recurrent mechanism that allows our model to detect text sequence directly in the convolutional maps, avoiding further post-processing by an additional costly CNN detection model.<span style="font-size:80%;opacity:0.8">我们通过提出一种网络内循环机制争取更进一步，使我们的模型能够直接在卷积映射中检测文本序列，避免通过额外昂贵的CNN检测模型进行进一步的后处理。</span></li><li>Second, we propose an <font color=orangered>in-network</font> recurrence mechanism that elegantly connects sequential text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8">其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and seamless <font color=orangered>in-network</font> connection of the fine-scale text proposals.<span style="font-size:80%;opacity:0.8">此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li><li>In our experiments, we first verify the efficiency of each proposed component individually, e.g., the fine-scale text proposal detection or <font color=orangered>in-network</font> recurrent connection.<span style="font-size:80%;opacity:0.8">在我们的实验中，我们首先单独验证每个提议组件的效率，例如细粒度文本提议检测或网内循环连接。</span></li><li>Therefore, the proposed <font color=orangered>in-network</font> recurrent mechanism increase model computation marginally, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8">因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li><li>We propose an <font color=orangered>in-network</font> RNN layer that connects sequential text proposals elegantly, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8">我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 17 </td> <td> generic<br>(7) </td> <td> [dʒəˈnerɪk] </td> <td> 
<ul><li>Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and refinement, leading to the state-of-the-art performance on <font color=orangered>generic</font> object detection.<span style="font-size:80%;opacity:0.8">然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>In <font color=orangered>generic</font> object detection, each object has a well-defined closed boundary [2], while such a well-defined boundary may not exist in text, since a text line or word is composed of a number of separate characters or strokes.<span style="font-size:80%;opacity:0.8">在通用目标检测中，每个目标都有一个明确的封闭边界[2]，而在文本中可能不存在这样一个明确定义的边界，因为文本行或单词是由许多单独的字符或笔划组成的。</span></li><li>We present several technical developments that tailor <font color=orangered>generic</font> object detection model elegantly towards our problem.<span style="font-size:80%;opacity:0.8">我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。</span></li><li>However, text differs from <font color=orangered>generic</font> objects substantially, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2].<span style="font-size:80%;opacity:0.8">然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。</span></li><li>Obviously, a text line is a sequence which is the main difference between text and <font color=orangered>generic</font> objects.<span style="font-size:80%;opacity:0.8">显然，文本行是一个序列，它是文本和通用目标之间的主要区别。</span></li><li>This inaccuracy may be not crucial in <font color=orangered>generic</font> object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words.<span style="font-size:80%;opacity:0.8">这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li><li>This is different from <font color=orangered>generic</font> object detection where the impact of condition (ii) may be not significant.<span style="font-size:80%;opacity:0.8">这不同于通用目标检测，通用目标检测中条件（ii）的影响可能不显著。</span></li></ul>
 </td>
</tr>
<tr>
<td> 18 </td> <td> small-scale<br>(7) </td> <td> [ˈsmɔ:lˈskeɪl] </td> <td> 
<ul><li>This inaccuracy may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those <font color=orangered>small-scale</font> text lines or words.<span style="font-size:80%;opacity:0.8">这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li><li>This is crucial to detect <font color=orangered>small-scale</font> text patterns, which is one of key advantages of the CTPN.<span style="font-size:80%;opacity:0.8">这对于检测小规模文本模式至关重要，这是CTPN的主要优势之一。</span></li><li>This dataset is more challenging than previous ones by including arbitrary orientation, very <font color=orangered>small-scale</font> and low resolution text.<span style="font-size:80%;opacity:0.8">这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li><li>Epshtein et al. [3] introduced the SWT dataset containing 307 images which include many extremely <font color=orangered>small-scale</font> text.<span style="font-size:80%;opacity:0.8">Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li><li>It is of great importance for recovering highly ambiguous text (e.g., extremely <font color=orangered>small-scale</font> ones), which is one of main advantages of our CTPN, as demonstrated in Fig. 6.<span style="font-size:80%;opacity:0.8">对于恢复高度模糊的文本（例如极小的文本）来说，这非常重要，这是我们CTPN的主要优势之一，如图6所示。</span></li><li>Fig. 6: CTPN detection results on extremely <font color=orangered>small-scale</font> cases (in red boxes), where some ground truth boxes are missed.<span style="font-size:80%;opacity:0.8">图6：在极小尺度的情况下（红色框内）CTPN检测结果，其中一些真实边界框被遗漏。</span></li><li>This may due to strong capability of CTPN for detecting extremely challenging text, e.g., very <font color=orangered>small-scale</font> ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8">这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 19 </td> <td> y-coordinate<br>(6) </td> <td> [ˌwaikəuˈɔ:dinət,-neit] </td> <td> 
<ul><li>Then we design k vertical anchors to predict <font color=orangered>y-coordinates</font> for each proposal.<span style="font-size:80%;opacity:0.8">然后，我们设计k个垂直锚点来预测每个提议的y坐标。</span></li><li>Our detector outputs the text/non-text scores and the predicted <font color=orangered>y-coordinates</font> ($\textbf{v}$) for k anchors at each window location.<span style="font-size:80%;opacity:0.8">我们的检测器在每个窗口位置输出k个锚点的文本/非文本分数和预测的y轴坐标（$\textbf{v}$）。</span></li><li>Similar to the <font color=orangered>y-coordinate</font> prediction, we compute relative offset as,<span style="font-size:80%;opacity:0.8">与y坐标预测类似，我们计算相对偏移为：</span></li><li>$\textbf{s}_i^*=\lbrace 0,1\rbrace$ is the ground truth. j is the index of an anchor in the set of valid anchors for <font color=orangered>y-coordinates</font> regression, which are defined as follow.<span style="font-size:80%;opacity:0.8">$\textbf{s}_i^*=\lbrace 0,1\rbrace$是真实值。$j$是$y$坐标回归中有效锚点集合中锚点的索引，定义如下。</span></li><li>$\textbf{v}_j$ and $\textbf{v}_j^*$ are the prediction and ground truth <font color=orangered>y-coordinates</font> associated with the $j-{th}$ anchor.<span style="font-size:80%;opacity:0.8">$\textbf{v}_j$和$\textbf{v}_j^*$是与第j个锚点关联的预测的和真实的y坐标。</span></li><li>The training labels for the <font color=orangered>y-coordinate</font> regression ($\textbf{v}^*$) and offset regression ($\textbf{o}^*$) are computed as E. q. (2) and (4) respectively.<span style="font-size:80%;opacity:0.8">y坐标回归（$\textbf{v}^*$）和偏移回归（$\textbf{o}^*$）的训练标签分别按公式（2）和（4）计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 20 </td> <td> substantially<br>(5) </td> <td> [səbˈstænʃəli] </td> <td> 
<ul><li>Deep Convolutional Neural Networks (CNN) have recently advanced general object detection <font color=orangered>substantially</font> [25,5,6].<span style="font-size:80%;opacity:0.8">深度卷积神经网络（CNN）最近已经基本实现了一般物体检测[25，5，6]。</span></li><li>Convolutional Neural Networks (CNN) have recently advanced general object detection <font color=orangered>substantially</font> [25,5,6].<span style="font-size:80%;opacity:0.8">卷积神经网络（CNN）近来在通用目标检测[25，5，6]上已经取得了实质的进步。</span></li><li>However, text differs from generic objects <font color=orangered>substantially</font>, which generally have a well-defined enclosed boundary and center, allowing inferring whole object from even a part of it [2].<span style="font-size:80%;opacity:0.8">然而，实质上文本与普通目标不同，它们通常具有明确的封闭边界和中心，可以从它的一部分推断整个目标[2]。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the FTPN <font color=orangered>substantially</font> from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li><li>Regardless of running time, our method outperforms the FASText <font color=orangered>substantially</font> with $11\%$ improvement on F-measure.<span style="font-size:80%;opacity:0.8">无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 21 </td> <td> y-axis<br>(5) </td> <td> [ˈwaiˌæksis] </td> <td> 
<ul><li>The RNN layer is connected to a 512D fully-connected layer, followed by the output layer, which jointly predicts text/non-text scores, <font color=orangered>y-axis</font> coordinates and side-refinement offsets of k anchors.<span style="font-size:80%;opacity:0.8">RNN层连接到512维的全连接层，接着是输出层，联合预测k个锚点的文本/非文本分数，y轴坐标和边缘调整偏移。</span></li><li>We develop a vertical anchor mechanism that simultaneously predicts a text/non-text score and <font color=orangered>y-axis</font> location of each fine-scale proposal.<span style="font-size:80%;opacity:0.8">我们开发了垂直锚点机制，可以同时预测每个细粒度提议的文本/非文本分数和y轴的位置。</span></li><li>The explicit vertical coordinates are measured by the height and <font color=orangered>y-axis</font> center of a proposal bounding box.<span style="font-size:80%;opacity:0.8">明确的垂直坐标是通过提议边界框的高度和y轴中心来度量的。</span></li><li>$c_y^a$ and $h^a$ are the center (<font color=orangered>y-axis</font>) and height of the anchor box, which can be pre-computed from an input image.<span style="font-size:80%;opacity:0.8">$c_y^a$和$h^a$是锚盒的中心（y轴）和高度，可以从输入图像预先计算。</span></li><li>$c_y$ and $h$ are the predicted <font color=orangered>y-axis</font> coordinates in the input image, while $c^*_y$ and $h^*$ are the ground truth coordinates.<span style="font-size:80%;opacity:0.8">$c_y$和$h$是输入图像中预测的y轴坐标，而$c^*_y$和$h^*$是实际坐标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 22 </td> <td> Multilingual<br>(5) </td> <td> [ˌmʌltiˈlɪŋgwəl] </td> <td> 
<ul><li>We evaluate the CTPN on five text detection benchmarks, namely the ICDAR 2011 [21], ICDAR 2013 [19], ICDAR 2015 [18], SWT [3], and <font color=orangered>Multilingual</font> dataset [24].<span style="font-size:80%;opacity:0.8">我们在五个文本检测基准数据集上评估CTPN，即ICDAR 2011[21]，ICDAR 2013[19]，ICDAR 2015[18]，SWT[3]和Multilingual[24]数据集。</span></li><li>The <font color=orangered>Multilingual</font> scene text dataset is collected by [24].<span style="font-size:80%;opacity:0.8">Multilingual场景文本数据集由[24]收集。</span></li><li>The evaluations on the SWT and <font color=orangered>Multilingual</font> datasets follow the protocols defined in [3] and [24] respectively.<span style="font-size:80%;opacity:0.8">SWT和Multilingual数据集的评估分别遵循[3]和[24]中定义的协议。</span></li><li>Table 1: Component evaluation on the ICDAR 2013, and State-of-the-art results on the SWT and <font color=orangered>MULTILINGUAL</font>.<span style="font-size:80%;opacity:0.8">表1：ICDAR 2013的组件评估以及在SWT和MULTILENGUAL数据集上的最新成果。</span></li><li>Our detector performs favourably against the TextFlow on the <font color=orangered>Multilingual</font>, suggesting that our method generalize well to various languages.<span style="font-size:80%;opacity:0.8">我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。</span></li></ul>
 </td>
</tr>
<tr>
<td> 23 </td> <td> trainable<br>(4) </td> <td> [t'reɪnəbl] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly incorporated into the convolutional network, resulting in an end-to-end <font color=orangered>trainable</font> model.<span style="font-size:80%;opacity:0.8">序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Third, both methods are integrated seamlessly to meet the nature of text sequence, resulting in a unified end-to-end <font color=orangered>trainable</font> model.<span style="font-size:80%;opacity:0.8">第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。</span></li><li>Therefore, our integration with the RNN layer is elegant, resulting in an efficient model that is end-to-end <font color=orangered>trainable</font> without additional cost.<span style="font-size:80%;opacity:0.8">因此，我们与RNN层的集成非常优雅，从而形成了一种高效的模型，可以在无需额外成本的情况下进行端到端的训练。</span></li><li>We have presented a Connectionist Text Proposal Network (CTPN) —— an efficient text detector that is end-to-end <font color=orangered>trainable</font>.<span style="font-size:80%;opacity:0.8">我们提出了连接文本提议网络（CTPN）—— 一种可端到端训练的高效文本检测器。</span></li></ul>
 </td>
</tr>
<tr>
<td> 24 </td> <td> computationally<br>(3) </td> <td> [!≈ ˌkɒmpjuˈteɪʃənli] </td> <td> 
<ul><li>The CTPN is <font color=orangered>computationally</font> efficient with 0.14s/image, by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8">通过使用非常深的VGG16模型[27]，CTPN的计算效率为0.14s每张图像。</span></li><li>Furthermore, it is <font color=orangered>computationally</font> efficient, resulting in a 0.14s/image running time (on the ICDAR 2013) by using the very deep VGG16 model [27].<span style="font-size:80%;opacity:0.8">此外，通过使用非常深的VGG16模型[27]，这在计算上是高效的，导致了每张图像0.14s的运行时间（在ICDAR 2013上）。</span></li><li>Another limitation is that the sliding-window methods are <font color=orangered>computationally</font> expensive, by running a classifier on a huge number of the sliding windows.<span style="font-size:80%;opacity:0.8">另一个限制是通过在大量的滑动窗口上运行分类器，滑动窗口方法在计算上是昂贵的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 25 </td> <td> class-agnostic<br>(3) </td> <td> [!≈ klɑ:s ægˈnɒstɪk] </td> <td> 
<ul><li>The state-of-the-art method is Faster Region-CNN (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality <font color=orangered>class-agnostic</font> object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8">最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li><li>Selective Search (SS) [4] which generates <font color=orangered>class-agnostic</font> object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8">生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li><li>They proposed a Region Proposal Network (RPN) that generates high-quality <font color=orangered>class-agnostic</font> object proposals directly from the convolutional feature maps.<span style="font-size:80%;opacity:0.8">他们提出了一个区域提议网络（RPN），可以直接从卷积特征映射中生成高质量的类别不可知的目标提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 26 </td> <td> refinement<br>(3) </td> <td> [rɪˈfaɪnmənt] </td> <td> 
<ul><li>Then the RPN proposals are fed into a Fast R-CNN [5] model for further classification and <font color=orangered>refinement</font>, leading to the state-of-the-art performance on generic object detection.<span style="font-size:80%;opacity:0.8">然后将RPN提议输入Faster R-CNN[5]模型进行进一步的分类和微调，从而实现通用目标检测的最新性能。</span></li><li>Our method is able to handle multi-scale and multi-lingual text in a single process, avoiding further post filtering or <font color=orangered>refinement</font>.<span style="font-size:80%;opacity:0.8">我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</span></li><li>However, the RPN proposals are not discriminative, and require a further <font color=orangered>refinement</font> and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8">然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 27 </td> <td> elegantly<br>(3) </td> <td> ['elɪɡəntlɪ] </td> <td> 
<ul><li>We present several technical developments that tailor generic object detection model <font color=orangered>elegantly</font> towards our problem.<span style="font-size:80%;opacity:0.8">我们提出了几种技术发展，针对我们的问题可以优雅地调整通用目标检测模型。</span></li><li>Second, we propose an in-network recurrence mechanism that <font color=orangered>elegantly</font> connects sequential text proposals in the convolutional feature maps.<span style="font-size:80%;opacity:0.8">其次，我们提出了一种在卷积特征映射中优雅连接序列文本提议的网络内循环机制。</span></li><li>We propose an in-network RNN layer that connects sequential text proposals <font color=orangered>elegantly</font>, allowing it to explore meaningful context information.<span style="font-size:80%;opacity:0.8">我们提出了一个网内RNN层，可以优雅地连接顺序文本提议，使其能够探索有意义的上下文信息。</span></li></ul>
 </td>
</tr>
<tr>
<td> 28 </td> <td> recurrently<br>(3) </td> <td> [rɪ'kʌrəntlɪ] </td> <td> 
<ul><li>The sequential windows in each row are <font color=orangered>recurrently</font> connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8">每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>RNN provides a natural choice for encoding this information <font color=orangered>recurrently</font> using its hidden layers.<span style="font-size:80%;opacity:0.8">RNN提供了一种自然选择，使用其隐藏层对这些信息进行循环编码。</span></li><li>To this end, we propose to design a RNN layer upon the conv5, which takes the convolutional feature of each window as sequential inputs, and updates its internal state <font color=orangered>recurrently</font> in the hidden layer, $H_t$,<span style="font-size:80%;opacity:0.8">为此，我们提出在conv5上设计一个RNN层，它将每个窗口的卷积特征作为序列输入，并在隐藏层中循环更新其内部状态：$H_t$，</span></li></ul>
 </td>
</tr>
<tr>
<td> 29 </td> <td> receptive<br>(3) </td> <td> [rɪˈseptɪv] </td> <td> 
<ul><li>The size of conv5 feature maps is determined by the size of input image, while the total stride and <font color=orangered>receptive</font> field are fixed as 16 and 228 pixels, respectively.<span style="font-size:80%;opacity:0.8">conv5特征映射的大小由输入图像的大小决定，而总步长和感受野分别固定为16个和228个像素。</span></li><li>Both the total stride and <font color=orangered>receptive</font> field are fixed by the network architecture.<span style="font-size:80%;opacity:0.8">网络架构决定总步长和感受野。</span></li><li>Generally, an text proposal is largely smaller than its effective <font color=orangered>receptive</font> field which is $228\times228$.<span style="font-size:80%;opacity:0.8">一般来说，文本提议在很大程度上要比它的有效感受野$228\times228$要小。</span></li></ul>
 </td>
</tr>
<tr>
<td> 30 </td> <td> side-proposal<br>(3) </td> <td> [!≈ saɪd prəˈpəʊzl] </td> <td> 
<ul><li>To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as side-anchor or <font color=orangered>side-proposal</font>).<span style="font-size:80%;opacity:0.8">为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>The <font color=orangered>side-proposals</font> are defined as the start and end proposals when we connect a sequence of detected fine-scale text proposals into a text line.<span style="font-size:80%;opacity:0.8">当我们将一系列检测到的细粒度文本提议连接到文本行中时，这些提议被定义为开始和结束提议。</span></li><li>We only use the offsets of the <font color=orangered>side-proposals</font> to refine the final text line bounding box.<span style="font-size:80%;opacity:0.8">我们只使用边缘提议的偏移量来优化最终的文本行边界框。</span></li></ul>
 </td>
</tr>
<tr>
<td> 31 </td> <td> x-axis<br>(3) </td> <td> [ˈeksˌæksis] </td> <td> 
<ul><li>$x^*_{side}$ is the ground truth (GT) side coordinate in <font color=orangered>x-axis</font>, which is pre-computed from the GT bounding box and anchor location.<span style="font-size:80%;opacity:0.8">$x^*_{side}$是x轴的实际（GT）边缘坐标，它是从实际边界框和锚点位置预先计算的。</span></li><li>$c_x^a$ is the center of anchor in <font color=orangered>x-axis</font>.<span style="font-size:80%;opacity:0.8">$c_x^a$是x轴的锚点的中心。</span></li><li>$\textbf{o}_k$ and $\textbf{o}_k^*$ are the predicted and ground truth offsets in <font color=orangered>x-axis</font> associated to the $k-{th}$ anchor.<span style="font-size:80%;opacity:0.8">$L^{cl}_s$是我们使用Softmax损失区分文本和非文本的分类损失。</span></li></ul>
 </td>
</tr>
<tr>
<td> 32 </td> <td> FTPN<br>(3) </td> <td> [!≈ ef ti: pi: en] </td> <td> 
<ul><li>Obviously, the proposed fine-scale text proposal network (<font color=orangered>FTPN</font>) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8">显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN remarkably in both precision and recall, suggesting that the <font color=orangered>FTPN</font> is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8">显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>As shown in Table 1 (left), with our recurrent connection, the CTPN improves the <font color=orangered>FTPN</font> substantially from a F-measure of 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">如表1（左）所示，使用我们的循环连接，CTPN大幅度改善了FTPN，将F-measure从0.80的提高到0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 33 </td> <td> FASText<br>(3) </td> <td> [fɑːs'tekst] </td> <td> 
<ul><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and <font color=orangered>FASText</font> [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li><li><font color=orangered>FASText</font> [1] achieves 0.15s/image CPU time.<span style="font-size:80%;opacity:0.8">FASText[1]达到0.15s每张图像的CPU时间。</span></li><li>Regardless of running time, our method outperforms the <font color=orangered>FASText</font> substantially with $11\%$ improvement on F-measure.<span style="font-size:80%;opacity:0.8">无论运行时间如何，我们的方法都大大优于FASText，F-measure的性能提高了11%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 34 </td> <td> seamlessly<br>(2) </td> <td> ['si:mlisli] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is <font color=orangered>seamlessly</font> incorporated into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8">序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li><li>Third, both methods are integrated <font color=orangered>seamlessly</font> to meet the nature of text sequence, resulting in a unified end-to-end trainable model.<span style="font-size:80%;opacity:0.8">第三，两种方法无缝集成，以符合文本序列的性质，从而形成统一的端到端可训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 35 </td> <td> connected-component<br>(2) </td> <td> [!≈ kə'nektɪd kəmˈpəʊnənt] </td> <td> 
<ul><li>Their performance heavily rely on the results of character detection, and <font color=orangered>connected-components</font> methods or sliding-window methods have been proposed.<span style="font-size:80%;opacity:0.8">它们的性能很大程度上依赖于字符检测的结果，并且已经提出了连接组件方法或滑动窗口方法。</span></li><li>They can be roughly grouped into two categories, <font color=orangered>connected-components</font> (CCs) based approaches and sliding-window based methods.<span style="font-size:80%;opacity:0.8">它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。</span></li></ul>
 </td>
</tr>
<tr>
<td> 36 </td> <td> sequentially<br>(2) </td> <td> [sɪ'kwenʃəlɪ] </td> <td> 
<ul><li>Furthermore, these false detections are easily accumulated <font color=orangered>sequentially</font> in bottom-up pipeline, as pointed out in [28].<span style="font-size:80%;opacity:0.8">此外，正如[28]所指出的，这些误检很容易在自下而上的过程中连续累积。</span></li><li>Then a text line is constructed by <font color=orangered>sequentially</font> connecting the pairs having a same proposal.<span style="font-size:80%;opacity:0.8">然后通过顺序连接具有相同提议的对来构建文本行。</span></li></ul>
 </td>
</tr>
<tr>
<td> 37 </td> <td> bi-directional<br>(2) </td> <td> ['bɪdɪr'ekʃənl] </td> <td> 
<ul><li>The sequential windows in each row are recurrently connected by a <font color=orangered>Bi-directional</font> LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8">每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>We further extend the RNN layer by using a <font color=orangered>bi-directional</font> LSTM, which allows it to encode the recurrent context in both directions, so that the connectionist receipt field is able to cover the whole image width, e.g., $228 \times width$.<span style="font-size:80%;opacity:0.8">我们通过使用双向LSTM来进一步扩展RNN层，这使得它能够在两个方向上对递归上下文进行编码，以便连接感受野能够覆盖整个图像宽度，例如$228\times width$。</span></li></ul>
 </td>
</tr>
<tr>
<td> 38 </td> <td> BLSTM<br>(2) </td> <td> [!≈ bi: el es ti: em] </td> <td> 
<ul><li>The sequential windows in each row are recurrently connected by a Bi-directional LSTM (<font color=orangered>BLSTM</font>) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D BLSTM (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8">每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li><li>The sequential windows in each row are recurrently connected by a Bi-directional LSTM (BLSTM) [7], where the convolutional feature (3×3×C) of each window is used as input of the 256D <font color=orangered>BLSTM</font> (including two 128D LSTMs).<span style="font-size:80%;opacity:0.8">每行的序列窗口通过双向LSTM（BLSTM）[7]循环连接，其中每个窗口的卷积特征（3×3×C）被用作256维的BLSTM（包括两个128维的LSTM）的输入。</span></li></ul>
 </td>
</tr>
<tr>
<td> 39 </td> <td> multi-lingual<br>(2) </td> <td> [!≈ 'mʌlti ˈlɪŋgwəl] </td> <td> 
<ul><li>Our method is able to handle multi-scale and <font color=orangered>multi-lingual</font> text in a single process, avoiding further post filtering or refinement.<span style="font-size:80%;opacity:0.8">我们的方法能够在单个过程中处理多尺度和多语言的文本，避免进一步的后过滤或细化。</span></li><li>The side-refinement further improves the localization accuracy, leading to about $2\%$ performance improvements on the SWT and <font color=orangered>Multi-Lingual</font> datasets.<span style="font-size:80%;opacity:0.8">边缘细化进一步提高了定位精度，从而使SWT和Multi-Lingual数据集上的性能提高了约2%。</span></li></ul>
 </td>
</tr>
<tr>
<td> 40 </td> <td> cc<br>(2) </td> <td> [ˌsi: ˈsi:] </td> <td> 
<ul><li>They can be roughly grouped into two categories, connected-components (<font color=orangered>CCs</font>) based approaches and sliding-window based methods.<span style="font-size:80%;opacity:0.8">它们可以粗略地分为两类，基于连接组件（CC）的方法和基于滑动窗口的方法。</span></li><li>The <font color=orangered>CCs</font> based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are greedily grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8">基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 41 </td> <td> arbitrary<br>(2) </td> <td> [ˈɑ:bɪtrəri] </td> <td> 
<ul><li>Similar to Region Proposal Network (RPN) [25], the CTPN is essentially a fully convolutional network that allows an input image of <font color=orangered>arbitrary</font> size.<span style="font-size:80%;opacity:0.8">类似于区域提议网络（RPN）[25]，CTPN本质上是一个全卷积网络，允许任意大小的输入图像。</span></li><li>This dataset is more challenging than previous ones by including <font color=orangered>arbitrary</font> orientation, very small-scale and low resolution text.<span style="font-size:80%;opacity:0.8">这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 42 </td> <td> x-coordinate<br>(2) </td> <td> ['ekskəʊ'ɔ:dnɪt] </td> <td> 
<ul><li>For each prediction, the horizontal location (<font color=orangered>x-coordinates</font>) and k-anchor locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image.<span style="font-size:80%;opacity:0.8">对于每个预测，水平位置（x轴坐标）和k个锚点位置是固定的，可以通过将conv5中的空间窗口位置映射到输入图像上来预先计算。</span></li><li>where $x_{side}$ is the predicted <font color=orangered>x-coordinate</font> of the nearest horizontal side (e.g., left or right side) to current anchor.<span style="font-size:80%;opacity:0.8">其中，$x_{side}$是最接近水平边（例如，左边或右边）到当前锚点的预测的x坐标。</span></li></ul>
 </td>
</tr>
<tr>
<td> 43 </td> <td> localizations<br>(2) </td> <td> [!≈ ˌləʊkəlaɪ'zeɪʃnz] </td> <td> 
<ul><li>This further reduces its computation, and at the same time, predicting accurate <font color=orangered>localizations</font> of the text lines.<span style="font-size:80%;opacity:0.8">这进一步减少了计算量，同时预测了文本行的准确位置。</span></li><li>The fine-scale detection and RNN connection are able to predict accurate <font color=orangered>localizations</font> in vertical direction.<span style="font-size:80%;opacity:0.8">细粒度的检测和RNN连接可以预测垂直方向的精确位置。</span></li></ul>
 </td>
</tr>
<tr>
<td> 44 </td> <td> outliers<br>(2) </td> <td> [aʊt'laɪəz] </td> <td> 
<ul><li>This may lead to a number of false detections on non-text objects which have a similar structure as text patterns, such as windows, bricks, leaves, etc. (referred as text-like <font color=orangered>outliers</font> in [13]).<span style="font-size:80%;opacity:0.8">这可能会导致对与文本模式类似的非文本目标的误检，如窗口，砖块，树叶等（在文献[13]中称为类文本异常值）。</span></li><li>As shown in Fig. 3, the context information is greatly helpful to reduce false detections, such as text-like <font color=orangered>outliers</font>.<span style="font-size:80%;opacity:0.8">如图3所示，上下文信息对于减少误检非常有用，例如类似文本的异常值。</span></li></ul>
 </td>
</tr>
<tr>
<td> 45 </td> <td> side-anchor<br>(2) </td> <td> [!≈ saɪd ˈæŋkə(r)] </td> <td> 
<ul><li>To address this problem, we propose a side-refinement approach that accurately estimates the offset for each anchor/proposal in both left and right horizontal sides (referred as <font color=orangered>side-anchor</font> or side-proposal).<span style="font-size:80%;opacity:0.8">为了解决这个问题，我们提出了一种边缘细化的方法，可以精确地估计左右两侧水平方向上的每个锚点/提议的偏移量（称为边缘锚点或边缘提议）。</span></li><li>k is the index of a <font color=orangered>side-anchor</font>, which is defined as a set of anchors within a horizontal distance (e.g., 32-pixel) to the left or right side of a ground truth text line bounding box.<span style="font-size:80%;opacity:0.8">k是边缘锚点的索引，其被定义为在实际文本行边界框的左侧或右侧水平距离（例如32个像素）内的一组锚点。$\textbf{o}_k$和$\textbf{o}_k^*$是与第k个锚点关联的x轴的预测和实际偏移量。</span></li></ul>
 </td>
</tr>
<tr>
<td> 46 </td> <td> organizer<br>(2) </td> <td> ['ɔ:ɡənaɪzə(r)] </td> <td> 
<ul><li>We follow previous work by using standard evaluation protocols which are provided by the dataset creators or competition <font color=orangered>organizers</font>.<span style="font-size:80%;opacity:0.8">我们遵循以前的工作，使用由数据集创建者或竞赛组织者提供的标准评估协议。</span></li><li>For the ICDAR 2015, we used the online evaluation system provided by the <font color=orangered>organizers</font> as in [18].<span style="font-size:80%;opacity:0.8">对于ICDAR 2015，我们使用了由组织者提供的在线评估系统[18]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 47 </td> <td> remarkably<br>(2) </td> <td> [rɪ'mɑ:kəblɪ] </td> <td> 
<ul><li>Obviously, the proposed fine-scale text proposal network (FTPN) improves the Faster R-CNN <font color=orangered>remarkably</font> in both precision and recall, suggesting that the FTPN is more accurate and reliable, by predicting a sequence of fine-scale text proposals rather than a whole text line.<span style="font-size:80%;opacity:0.8">显然，所提出的细粒度文本提议网络（FTPN）在精确度和召回率方面都显著改进了Faster R-CNN，表明通过预测一系列细粒度文本提议而不是整体文本行，FTPN更精确可靠。</span></li><li>On the ICDAR 2013, it outperforms recent TextFlow [28] and FASText [1] <font color=orangered>remarkably</font> by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 48 </td> <td> TextFlow<br>(2) </td> <td>  </td> <td> 
<ul><li>Our detector performs favourably against the <font color=forestgreen>TextFlow</font> on the Multilingual, suggesting that our method generalize well to various languages.<span style="font-size:80%;opacity:0.8">我们的检测器在Multilingual上比TextFlow表现更好，表明我们的方法能很好地泛化到各种语言。</span></li><li>On the ICDAR 2013, it outperforms recent <font color=forestgreen>TextFlow</font> [28] and FASText [1] remarkably by improving the F-measure from 0.80 to 0.88.<span style="font-size:80%;opacity:0.8">在ICDAR 2013上，它的性能优于最近的TextFlow[28]和FASText[1]，将F-measure从0.80提高到了0.88。</span></li></ul>
 </td>
</tr>
<tr>
<td> 49 </td> <td> incorporate<br>(1) </td> <td> [ɪnˈkɔ:pəreɪt] </td> <td> 
<ul><li>The sequential proposals are naturally connected by a recurrent neural network, which is seamlessly <font color=orangered>incorporated</font> into the convolutional network, resulting in an end-to-end trainable model.<span style="font-size:80%;opacity:0.8">序列提议通过循环神经网络自然地连接起来，该网络无缝地结合到卷积网络中，从而形成端到端的可训练模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 50 </td> <td> surpass<br>(1) </td> <td> [səˈpɑ:s] </td> <td> 
<ul><li>It achieves 0.88 and 0.61 F-measure on the ICDAR 2013 and 2015 benchmarks, <font color=orangered>surpassing</font> recent results [8,35] by a large margin.<span style="font-size:80%;opacity:0.8">它在ICDAR 2013和2015的基准数据集上达到了0.88和0.61的F-measure，大大超过了最近的结果[8，35]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 51 </td> <td> retrieval<br>(1) </td> <td> [rɪˈtri:vl] </td> <td> 
<ul><li>This is due to its numerous practical applications such as image OCR, multi-language translation, image <font color=orangered>retrieval</font>, etc. It includes two sub tasks: text detection and recognition.<span style="font-size:80%;opacity:0.8">这是由于它的许多实际应用，如图像OCR，多语言翻译，图像检索等。它包括两个子任务：文本检测和识别。</span></li></ul>
 </td>
</tr>
<tr>
<td> 52 </td> <td> variance<br>(1) </td> <td> [ˈveəriəns] </td> <td> 
<ul><li>Large <font color=orangered>variance</font> of text patterns and highly cluttered background pose main challenge of accurate text localization.<span style="font-size:80%;opacity:0.8">文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 53 </td> <td> clutter<br>(1) </td> <td> [ˈklʌtə(r)] </td> <td> 
<ul><li>Large variance of text patterns and highly <font color=orangered>cluttered</font> background pose main challenge of accurate text localization.<span style="font-size:80%;opacity:0.8">文本模式的大变化和高度杂乱的背景构成了精确文本定位的主要挑战。</span></li></ul>
 </td>
</tr>
<tr>
<td> 54 </td> <td> verification<br>(1) </td> <td> [ˌverɪfɪ'keɪʃn] </td> <td> 
<ul><li>They commonly start from low-level character or stroke detection, which is typically followed by a number of subsequent steps: non-text component filtering, text line construction and text line <font color=orangered>verification</font>.<span style="font-size:80%;opacity:0.8">它们通常从低级别字符或笔画检测开始，后面通常会跟随一些后续步骤：非文本组件过滤，文本行构建和文本行验证。</span></li></ul>
 </td>
</tr>
<tr>
<td> 55 </td> <td> robustness<br>(1) </td> <td> [rəʊ'bʌstnəs] </td> <td> 
<ul><li>These multi-step bottom-up approaches are generally complicated with less <font color=orangered>robustness</font> and reliability.<span style="font-size:80%;opacity:0.8">这些自底向上的多步骤方法通常复杂，鲁棒性和可靠性较差。</span></li></ul>
 </td>
</tr>
<tr>
<td> 56 </td> <td> MSER<br>(1) </td> <td> [!≈ em es i: ɑ:(r)] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on SWT [3,13], <font color=orangered>MSER</font> [14,33,23], or HoG [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8">这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 57 </td> <td> HoG<br>(1) </td> <td> [hɒg] </td> <td> 
<ul><li>These methods commonly explore low-level features (e.g., based on SWT [3,13], MSER [14,33,23], or <font color=orangered>HoG</font> [28]) to distinguish text candidates from background.<span style="font-size:80%;opacity:0.8">这些方法通常探索低级特征（例如，基于SWT[3，13]，MSER[14，33，23]或HoG[28]）来区分候选文本和背景。</span></li></ul>
 </td>
</tr>
<tr>
<td> 58 </td> <td> Region-CNN<br>(1) </td> <td>  </td> <td> 
<ul><li>The state-of-the-art method is Faster <font color=forestgreen>Region-CNN</font> (R-CNN) system [25] where a Region Proposal Network (RPN) is proposed to generate high-quality class-agnostic object proposals directly from convolutional feature maps.<span style="font-size:80%;opacity:0.8">最先进的方法是Faster Region-CNN（R-CNN）系统[25]，其中提出了区域提议网络（RPN）直接从卷积特征映射中生成高质量类别不可知的目标提议。</span></li></ul>
 </td>
</tr>
<tr>
<td> 59 </td> <td> PASCAL<br>(1) </td> <td> ['pæskәl] </td> <td> 
<ul><li>For object detection, a typical correct detection is defined loosely, e.g., by an overlap of &gt; 0.5 between the detected bounding box and its ground truth (e.g., the <font color=orangered>PASCAL</font> standard [4]), since people can recognize an object easily from major part of it.<span style="font-size:80%;opacity:0.8">对于目标检测，典型的正确检测是松散定义的，例如，检测到的边界框与其实际边界框（例如，PASCAL标准[4]）之间的重叠>0.5，因为人们可以容易地从目标的主要部分识别它。</span></li></ul>
 </td>
</tr>
<tr>
<td> 60 </td> <td> comprehensively<br>(1) </td> <td> [ˌkɒmprɪˈhensɪvli] </td> <td> 
<ul><li>By contrast, reading text <font color=orangered>comprehensively</font> is a fine-grained recognition task which requires a correct detection that covers a full region of a text line or word.<span style="font-size:80%;opacity:0.8">相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。</span></li></ul>
 </td>
</tr>
<tr>
<td> 61 </td> <td> fine-grained<br>(1) </td> <td> [faɪn'greɪnd] </td> <td> 
<ul><li>By contrast, reading text comprehensively is a <font color=orangered>fine-grained</font> recognition task which requires a correct detection that covers a full region of a text line or word.<span style="font-size:80%;opacity:0.8">相比之下，综合阅读文本是一个细粒度的识别任务，需要正确的检测，覆盖文本行或字的整个区域。</span></li></ul>
 </td>
</tr>
<tr>
<td> 62 </td> <td> leverage<br>(1) </td> <td> [ˈli:vərɪdʒ] </td> <td> 
<ul><li>We <font color=orangered>leverage</font> the advantages of strong deep convolutional features and sharing computation mechanism, and propose the CTPN architecture which is described in Fig. 1.<span style="font-size:80%;opacity:0.8">我们利用强深度卷积特性和共享计算机制的优点，提出了如图1所示的CTPN架构。</span></li></ul>
 </td>
</tr>
<tr>
<td> 63 </td> <td> greedily<br>(1) </td> <td> ['gri:dɪlɪ] </td> <td> 
<ul><li>The CCs based approaches discriminate text and non-text pixels by using a fast filter, and then text pixels are <font color=orangered>greedily</font> grouped into stroke or character candidates, by using low-level properties, e.g., intensity, color, gradient, etc. [33,14,32,13,3].<span style="font-size:80%;opacity:0.8">基于CC的方法通过使用快速滤波器来区分文本和非文本像素，然后通过使用低级属性（例如强度，颜色，梯度等[33，14，32，13，3]）将文本像素贪婪地分为笔划或候选字符。</span></li></ul>
 </td>
</tr>
<tr>
<td> 64 </td> <td> robustly<br>(1) </td> <td> [rəʊ'bʌstlɪ] </td> <td> 
<ul><li>Furthermore, <font color=orangered>robustly</font> filtering out non-character components or confidently verifying detected text lines are even difficult themselves [1,33,14].<span style="font-size:80%;opacity:0.8">此外，强大地过滤非字符组件或者自信地验证检测到的文本行本身就更加困难[1，33，14]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 65 </td> <td> inexpensive<br>(1) </td> <td> [ˌɪnɪkˈspensɪv] </td> <td> 
<ul><li>A common strategy is to generate a number of object proposals by employing <font color=orangered>inexpensive</font> low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals.<span style="font-size:80%;opacity:0.8">一个常见的策略是通过使用廉价的低级特征来生成许多目标提议，然后使用强CNN分类器来进一步对生成的提议进行分类和细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 66 </td> <td> Selective<br>(1) </td> <td> [sɪˈlektɪv] </td> <td> 
<ul><li><font color=orangered>Selective</font> Search (SS) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8">生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 67 </td> <td> SS<br>(1) </td> <td> [!≈ es es] </td> <td> 
<ul><li>Selective Search (<font color=orangered>SS</font>) [4] which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) [6] and its extensions [5].<span style="font-size:80%;opacity:0.8">生成类别不可知目标提议的选择性搜索（SS）[4]是目前领先的目标检测系统中应用最广泛的方法之一，如CNN（R-CNN）[6]及其扩展[5]。</span></li></ul>
 </td>
</tr>
<tr>
<td> 68 </td> <td> discriminative<br>(1) </td> <td> [dɪs'krɪmɪnətɪv] </td> <td> 
<ul><li>However, the RPN proposals are not <font color=orangered>discriminative</font>, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model [5].<span style="font-size:80%;opacity:0.8">然而，RPN提议不具有判别性，需要通过额外的成本高昂的CNN模型（如Fast R-CNN模型[5]）进一步细化和分类。</span></li></ul>
 </td>
</tr>
<tr>
<td> 69 </td> <td> domain-specific<br>(1) </td> <td> [!≈ dəˈmeɪn spəˈsɪfɪk] </td> <td> 
<ul><li>More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly <font color=orangered>domain-specific</font> task.<span style="font-size:80%;opacity:0.8">更重要的是，文本与一般目标有很大的不同，因此很难直接将通用目标检测系统应用到这个高度领域化的任务中。</span></li></ul>
 </td>
</tr>
<tr>
<td> 70 </td> <td> applicable<br>(1) </td> <td> [əˈplɪkəbl] </td> <td> 
<ul><li>We take the very deep 16-layer vggNet (VGG16) [27] as an example to describe our approach, which is readily <font color=orangered>applicable</font> to other deep models.<span style="font-size:80%;opacity:0.8">我们以非常深的16层vggNet（VGG16）[27]为例来描述我们的方法，该方法很容易应用于其他深度模型。</span></li></ul>
 </td>
</tr>
<tr>
<td> 71 </td> <td> k-anchor<br>(1) </td> <td> [!≈ keɪ ˈæŋkə(r)] </td> <td> 
<ul><li>For each prediction, the horizontal location (x-coordinates) and <font color=orangered>k-anchor</font> locations are fixed, which can be pre-computed by mapping the spatial window location in the conv5 onto the input image.<span style="font-size:80%;opacity:0.8">对于每个预测，水平位置（x轴坐标）和k个锚点位置是固定的，可以通过将conv5中的空间窗口位置映射到输入图像上来预先计算。</span></li></ul>
 </td>
</tr>
<tr>
<td> 72 </td> <td> suppression<br>(1) </td> <td> [səˈpreʃn] </td> <td> 
<ul><li>The detected text proposals are generated from the anchors having a text/non-text score of &gt;0.7 (with non-maximum <font color=orangered>suppression</font>).<span style="font-size:80%;opacity:0.8">检测到的文本提议是从具有> 0.7（具有非极大值抑制）的文本/非文本分数的锚点生成的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 73 </td> <td> seamless<br>(1) </td> <td> [ˈsi:mləs] </td> <td> 
<ul><li>Furthermore, we aim to encode this information directly in the convolutional layer, resulting in an elegant and <font color=orangered>seamless</font> in-network connection of the fine-scale text proposals.<span style="font-size:80%;opacity:0.8">此外，我们的目标是直接在卷积层中编码这些信息，从而实现细粒度文本提议优雅无缝的网内连接。</span></li></ul>
 </td>
</tr>
<tr>
<td> 74 </td> <td> multiplicative<br>(1) </td> <td> ['mʌltɪplɪkeɪtɪv] </td> <td> 
<ul><li>The LSTM was proposed specially to address vanishing gradient problem, by introducing three additional <font color=orangered>multiplicative</font> gates: the input gate, forget gate and output gate.<span style="font-size:80%;opacity:0.8">通过引入三个附加乘法门：输入门，忘记门和输出门，专门提出了LSTM以解决梯度消失问题。</span></li></ul>
 </td>
</tr>
<tr>
<td> 75 </td> <td> inaccuracy<br>(1) </td> <td> [ɪn'ækjərəsɪ] </td> <td> 
<ul><li>This <font color=orangered>inaccuracy</font> may be not crucial in generic object detection, but should not be ignored in text detection, particularly for those small-scale text lines or words.<span style="font-size:80%;opacity:0.8">这种不准确性在通用目标检测中可能并不重要，但在文本检测中不应忽视，特别是对于那些小型文本行或文字。</span></li></ul>
 </td>
</tr>
<tr>
<td> 76 </td> <td> dash<br>(1) </td> <td> [dæʃ] </td> <td> 
<ul><li>Fig. 4: CTPN detection with (red box) and without (yellow <font color=orangered>dashed</font> box) the side-refinement.<span style="font-size:80%;opacity:0.8">图4：CTPN检测有（红色框）和没有（黄色虚线框）边缘细化。</span></li></ul>
 </td>
</tr>
<tr>
<td> 77 </td> <td> Intersection-over-Union<br>(1) </td> <td> [!≈ ˌɪntəˈsekʃn ˈəʊvə(r) ˈju:niən] </td> <td> 
<ul><li>A valid anchor is a defined positive anchor ($\textbf{s}_j^*=1$, described below), or has an <font color=orangered>Intersection-over-Union</font> (IoU) &gt;0.5 overlap with a ground truth text proposal.<span style="font-size:80%;opacity:0.8">有效的锚点是定义的正锚点（$\textbf{s}_j^*=1$，如下所述），或者与实际文本提议重叠的交并比（IoU）>0.5。</span></li></ul>
 </td>
</tr>
<tr>
<td> 78 </td> <td> empirically<br>(1) </td> <td> [ɪm'pɪrɪklɪ] </td> <td> 
<ul><li>$\lambda_1$ and $\lambda_2$ are loss weights to balance different tasks, which are <font color=orangered>empirically</font> set to 1.0 and 2.0. $N_{s}$, $N_{v}$ and $N_{o}$ are normalization parameters, denoting the total number of anchors used by $L^{cl}_s$, $L^{re}_v$ and $L^{re}_o$, respectively.<span style="font-size:80%;opacity:0.8">$N_{s}$, $N_{v}$和$N_{o}$是标准化参数，表示$L^{cl}_s$，$L^{re}_v$，$L^{re}_o$分别使用的锚点总数。</span></li></ul>
 </td>
</tr>
<tr>
<td> 79 </td> <td> stochastic<br>(1) </td> <td> [stə'kæstɪk] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and <font color=orangered>stochastic</font> gradient descent (SGD).<span style="font-size:80%;opacity:0.8">通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 80 </td> <td> descent<br>(1) </td> <td> [dɪˈsent] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient <font color=orangered>descent</font> (SGD).<span style="font-size:80%;opacity:0.8">通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 81 </td> <td> SGD<br>(1) </td> <td> ['esdʒ'i:d'i:] </td> <td> 
<ul><li>The CTPN can be trained end-to-end by using the standard back-propagation and stochastic gradient descent (<font color=orangered>SGD</font>).<span style="font-size:80%;opacity:0.8">通过使用标准的反向传播和随机梯度下降（SGD），可以对CTPN进行端对端训练。</span></li></ul>
 </td>
</tr>
<tr>
<td> 82 </td> <td> resize<br>(1) </td> <td> [ˌri:ˈsaɪz] </td> <td> 
<ul><li>The input image is <font color=orangered>resized</font> by setting its short side to 600 for training, while keeping its original aspect ratio.<span style="font-size:80%;opacity:0.8">为了训练，通过将输入图像的短边设置为600来调整输入图像的大小，同时保持其原始长宽比。</span></li></ul>
 </td>
</tr>
<tr>
<td> 83 </td> <td> Gaussian<br>(1) </td> <td> ['gaʊsɪən] </td> <td> 
<ul><li>We initialize the new layers (e.g., the RNN and output layers) by using random weights with <font color=orangered>Gaussian</font> distribution of 0 mean and 0.01 standard deviation.<span style="font-size:80%;opacity:0.8">我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 84 </td> <td> deviation<br>(1) </td> <td> [ˌdi:viˈeɪʃn] </td> <td> 
<ul><li>We initialize the new layers (e.g., the RNN and output layers) by using random weights with Gaussian distribution of 0 mean and 0.01 standard <font color=orangered>deviation</font>.<span style="font-size:80%;opacity:0.8">我们通过使用具有0均值和0.01标准差的高斯分布的随机权重来初始化新层（例如，RNN和输出层）。</span></li></ul>
 </td>
</tr>
<tr>
<td> 85 </td> <td> momentum<br>(1) </td> <td> [məˈmentəm] </td> <td> 
<ul><li>We used 0.9 <font color=orangered>momentum</font> and 0.0005 weight decay.<span style="font-size:80%;opacity:0.8">我们使用0.9的动量和0.0005的重量衰减。</span></li></ul>
 </td>
</tr>
<tr>
<td> 86 </td> <td> Caffe<br>(1) </td> <td>  </td> <td> 
<ul><li>Our model was implemented in <font color=forestgreen>Caffe</font> framework [17].<span style="font-size:80%;opacity:0.8">我们的模型在Caffe框架[17]中实现。</span></li></ul>
 </td>
</tr>
<tr>
<td> 87 </td> <td> Incidental<br>(1) </td> <td> [ˌɪnsɪˈdentl] </td> <td> 
<ul><li>The ICDAR 2015 (<font color=orangered>Incidental</font> Scene Text - Challenge 4) [18] includes 1,500 images which were collected by using the Google Glass.<span style="font-size:80%;opacity:0.8">ICDAR 2015年（Incidental Scene Text —— Challenge 4）[18]包括使用Google Glass收集的1500张图像。</span></li></ul>
 </td>
</tr>
<tr>
<td> 88 </td> <td> orientation<br>(1) </td> <td> [ˌɔ:riənˈteɪʃn] </td> <td> 
<ul><li>This dataset is more challenging than previous ones by including arbitrary <font color=orangered>orientation</font>, very small-scale and low resolution text.<span style="font-size:80%;opacity:0.8">这个数据集比以前的数据集更具挑战性，包括任意方向，非常小的尺度和低分辨率的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 89 </td> <td> Epshtein<br>(1) </td> <td>  </td> <td> 
<ul><li><font color=forestgreen>Epshtein</font> et al. [3] introduced the SWT dataset containing 307 images which include many extremely small-scale text.<span style="font-size:80%;opacity:0.8">Epshtein等[3]引入了包含307张图像的SWT数据集，其中包含许多极小尺度的文本。</span></li></ul>
 </td>
</tr>
<tr>
<td> 90 </td> <td> marginally<br>(1) </td> <td> [ˈmɑ:dʒɪnəli] </td> <td> 
<ul><li>Therefore, the proposed in-network recurrent mechanism increase model computation <font color=orangered>marginally</font>, with considerable performance gain obtained.<span style="font-size:80%;opacity:0.8">因此，所提出的网内循环机制稍微增加了模型计算，并获得了相当大的性能增益。</span></li></ul>
 </td>
</tr>
<tr>
<td> 91 </td> <td> submission<br>(1) </td> <td> [səbˈmɪʃn] </td> <td> 
<ul><li>In addition, we further compare our method against [8,11,35], which were published after our initial <font color=orangered>submission</font>.<span style="font-size:80%;opacity:0.8">此外，我们进一步与[8,11,35]比较了我们的方法，它们是在我们的首次提交后发布的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 92 </td> <td> consistently<br>(1) </td> <td> [kən'sɪstəntlɪ] </td> <td> 
<ul><li>It <font color=orangered>consistently</font> obtains substantial improvements on F-measure and recall.<span style="font-size:80%;opacity:0.8">它始终在F-measure和召回率方面取得重大进展。</span></li></ul>
 </td>
</tr>
<tr>
<td> 93 </td> <td> capability<br>(1) </td> <td> [ˌkeɪpəˈbɪləti] </td> <td> 
<ul><li>This may due to strong <font color=orangered>capability</font> of CTPN for detecting extremely challenging text, e.g., very small-scale ones, some of which are even difficult for human.<span style="font-size:80%;opacity:0.8">这可能是由于CTPN在非常具有挑战性的文本上具有很强的检测能力，例如非常小的文本，其中一些甚至对人来说都很难。</span></li></ul>
 </td>
</tr>
<tr>
<td> 94 </td> <td> competitively<br>(1) </td> <td> [!≈ kəmˈpetətɪvli] </td> <td> 
<ul><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared <font color=orangered>competitively</font> against Gupta et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8">在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
<tr>
<td> 95 </td> <td> Gupta<br>(1) </td> <td>  </td> <td> 
<ul><li>By using the scale of 450, it is reduced to 0.09s/image, while obtaining P/R/F of 0.92/0.77/0.84 on the ICDAR 2013, which are compared competitively against <font color=forestgreen>Gupta</font> et al.’ s approach [8] using 0.07s/image with GPU.<span style="font-size:80%;opacity:0.8">在ICDAR 2013中，使用450的缩放比例时间降低到0.09s每张图像，同时获得0.92/0.77/0.84的P/R/F，与Gupta等人的方法[8]相比，GPU时间为0.07s每张图像，我们的方法是具有竞争力的。</span></li></ul>
 </td>
</tr>
</table>
</div>
</div>
</div>
</body>
</html>